{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/myrtle/skewl/cs 429 info retrieval/project/indexer'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/myrtle/skewl/cs 429 info retrieval/project\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=1.0000000000000000715e-18, min=-1.189731495357231765e+4932, max=1.189731495357231765e+4932, dtype=float128)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(np.longdouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_tokenizer(text: str) -> list[str]:\n",
    "    toks = re.split(r\"\\s\", text)\n",
    "    toks = [s for s in toks if len(s) > 0]\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_preprocessor(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^a-zA-Z0-9_\\-\\s]', '', text)\n",
    "    text = \"\".join(ch for ch in text if ord(ch) <= 127)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'holal'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"holaˈl̪ˠ\"\n",
    "\"\".join(ch for ch in t if ord(ch) <= 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from 20newsgroups data\"\"\"\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "news = fetch_20newsgroups(subset=\"all\", shuffle=False, remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "\n",
    "\n",
    "# documents = [\"my\", \"list\", \"of\", \"docs\"]\n",
    "documents = news.data\n",
    "\n",
    "\n",
    "#####   STOLEN FROM https://stackoverflow.com/questions/68003003/python-sklearn-tfidfvectorizer-vectorize-documents-ahead-of-query-for-semantic\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=temp_preprocessor, tokenizer=temp_tokenizer)\n",
    "\n",
    "\n",
    "# fit_transform does two things: fits the vectorizer and transforms documents\n",
    "doc_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# inverted_doc_vectors = doc_vectors.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Okay let's just get the data from the csv file\"\"\"\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/List_of_common_misconceptions\n"
     ]
    }
   ],
   "source": [
    "import linecache\n",
    "\n",
    "line = linecache.getline(\"corpus/items.jsonl\", 1)\n",
    "doc = json.loads(line)\n",
    "print(doc[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Each entry on this list o...', 'The Archbasilica of Saint...', 'In lexical semantics, opp...', 'A DOI aims to resolve to ...', 'The influence of the art ...', 'The International Standar...', 'Water vapor, water vapour...', 'Hunter Stockton Thompson ...', 'In 1994, The Chronicle of...', 'From the invention of mov...']\n",
      "<100x34006 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 89847 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = []\n",
    "\n",
    "with open(\"corpus/items.jsonl\", \"r\") as f:\n",
    "    for doc_str in f.readlines():\n",
    "        doc = json.loads(doc_str)\n",
    "        documents.append(doc[\"text\"])\n",
    "\n",
    "print([d[:25] + \"...\" for d in documents[:10]])\n",
    "\n",
    "#####   STOLEN FROM https://stackoverflow.com/questions/68003003/python-sklearn-tfidfvectorizer-vectorize-documents-ahead-of-query-for-semantic\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=temp_preprocessor, \n",
    "    tokenizer=temp_tokenizer\n",
    ")\n",
    "\n",
    "doc_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "pprint(doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 49  1 26 77]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_terms = \"Jesus\"\n",
    "k = 5\n",
    "\n",
    "search_term_vector = vectorizer.transform([search_terms])\n",
    "\n",
    "# REPLACE THIS ! ! ! ! !\n",
    "\n",
    "cosine_similarities = linear_kernel(doc_vectors, search_term_vector).flatten()\n",
    "\n",
    "doc_inds_by_score_reversed = np.argsort(cosine_similarities)\n",
    "top_k_inds_by_score_reversed = doc_inds_by_score_reversed[len(doc_inds_by_score_reversed)-k:]\n",
    "top_k_inds_by_score = top_k_inds_by_score_reversed[::-1]\n",
    "\n",
    "print(top_k_inds_by_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_doc(index, score, text):\n",
    "    \n",
    "    doc_words = re.split(r\"\\s\", text)\n",
    "    \n",
    "    # line_len = 74\n",
    "    line_len = 10\n",
    "    def line_start(i):\n",
    "        return line_len*i\n",
    "    def line_end(doc, i):\n",
    "        return min(line_start(i) + line_len, len(doc))\n",
    "\n",
    "    # print(\n",
    "    #     *(best_doc[line_start(i) : line_end(text, i)] \n",
    "    #       + (\"\" if (text[line_end(text, i)] == \" \" or text[line_end(text, i) - 1] == \" \") else \"-\")\n",
    "    #     for i in range(len(text) // line_len)),\n",
    "    #     sep=\"\\n\"\n",
    "    # )\n",
    "\n",
    "    print(\n",
    "        f\"Best match (idx={index}, {score}):\\n\"\n",
    "    )\n",
    "    print(\n",
    "        *(\" \".join(doc_words[line_start(i) : line_end(doc_words, i)])\n",
    "        for i in range(min(5, len(doc_words) // line_len))),\n",
    "        sep=\"\\n\",\n",
    "        end=\"\"\n",
    "    )\n",
    "    if (len(doc_words) // line_len >= 5):\n",
    "        print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best match (idx=34, 0.02188715971168302):\n",
      "\n",
      "The predecessor of the present church was probably built in\n",
      "the early fourth century and that church was itself the\n",
      "successor to one of the tituli, early Christian basilicas ascribed\n",
      "to a patron and perhaps literally inscribed with his name.\n",
      "Although nothing remains to establish with certainty where any of...\n",
      "\n",
      "Best match (idx=49, 0.01751561775796079):\n",
      "\n",
      "The earliest use of the term was in reference to\n",
      "the canonical hour, also called the vigil, which was originally\n",
      "celebrated by monks from about two hours after midnight to,\n",
      "at latest, the dawn, the time for the canonical hour\n",
      "of lauds (a practice still followed in certain orders). It...\n",
      "\n",
      "Best match (idx=1, 0.007652297580682315):\n",
      "\n",
      "The Archbasilica of Saint John Lateran (formally named the \"Major\n",
      "Papal, Patriarchal and Roman Archbasilica Cathedral of the Most Holy\n",
      "Savior and Saints John the Baptist and the Evangelist in\n",
      "Lateran, Mother and Head of All Churches in Rome and\n",
      "in the World\", and commonly known as the Lateran Basilica...\n",
      "\n",
      "Best match (idx=26, 0.006976911417607495):\n",
      "\n",
      "Before the 8th century AD there were several Christian rites\n",
      "in Western Europe. Such diversity of practice was often considered\n",
      "unimportant so long as Rome's primacy was accepted. Gradually the\n",
      "diversity tended to lessen so that by the time of\n",
      "the final fusion in the Carolingian period the Roman Rite,...\n",
      "\n",
      "Best match (idx=77, 0.006940065173755626):\n",
      "\n",
      "As a pharaoh, Akhenaten is noted for abandoning Egypt's traditionalpolytheism\n",
      "and introducing Atenism, or worship centered around Aten. The views\n",
      "of Egyptologists differ as to whether the religious policy was\n",
      "absolutely monotheistic, or whether it was monolatristic, syncretistic, or henotheistic.[14][15]\n",
      "This culture shift away from traditional religion was reversed after...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in top_k_inds_by_score:\n",
    "    print_doc(i, cosine_similarities[i], documents[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
