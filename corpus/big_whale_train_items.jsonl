{"url": "https://en.m.wikipedia.org/wiki/Whale", "text": "Whales are fully aquatic, open-ocean animals: they can feed, mate, give birth, suckle and raise their young at sea. Whales range in size from the 2.6 metres (8.5 ft) and 135 kilograms (298 lb) dwarf sperm whale to the 29.9 metres (98 ft) and 190 tonnes (210 short tons) blue whale, which is the largest known animal that has ever lived. The sperm whale is the largest toothed predator on Earth. Several whale species exhibit sexual dimorphism, in that the females are larger than males. Baleen whales have no teeth; instead, they have plates of baleen, fringe-like structures that enable them to expel the huge mouthfuls of water they take in while retaining the krill and plankton they feed on. Because their heads are enormous—making up as much as 40% of their total body mass—and they have throat pleats that enable them to expand their mouths, they are able to take huge quantities of water into their mouth at a time. Baleen whales also have a well-developed sense of smell. Toothed whales, in contrast, have conical teeth adapted to catching fish or squid. They also have such keen hearing—whether above or below the surface of the water—that some can survive even if they are blind. Some species, such as sperm whales, are particularly well adapted for diving to great depths to catch squid and other favoured prey. Whales evolved from land-living mammals, and must regularly surface to breathe air, although they can remain underwater for long periods of time. Some species, such as the sperm whale, can stay underwater for up to 90 minutes.[2] They have blowholes (modified nostrils) located on top of their heads, through which air is taken in and expelled. They are warm-blooded, and have a layer of fat, or blubber, under the skin. With streamlined fusiform bodies and two limbs that are modified into flippers, whales can travel at speeds of up to 20 knots, though they are not as flexible or agile as seals. Whales produce a great variety of vocalizations, notably the extended songs of the humpback whale. Although whales are widespread, most species prefer the colder waters of the Northern and Southern Hemispheres and migrate to the equator to give birth. Species such as humpbacks and blue whales are capable of travelling thousands of miles without feeding. Males typically mate with multiple females every year, but females only mate every two to three years. Calves are typically born in the spring and summer; females bear all the responsibility for raising them. Mothers in some species fast and nurse their young for one to two years. Once relentlessly hunted for their products, whales are now protected by international law. The North Atlantic right whales nearly became extinct in the twentieth century, with a population low of 450, and the North Pacific grey whale population is ranked Critically Endangered by the IUCN. Besides the threat from whalers, they also face threats from bycatch and marine pollution. The meat, blubber and baleen of whales have traditionally been used by indigenous peoples of the Arctic. Whales have been depicted in various cultures worldwide, notably by the Inuit and the coastal peoples of Vietnam and Ghana, who sometimes hold whale funerals. Whales occasionally feature in literature and film. A famous example is the great white whale in Herman Melville's novel Moby-Dick. Small whales, such as belugas, are sometimes kept in captivity and trained to perform tricks, but breeding success has been poor and the animals often die within a few months of capture. Whale watching has become a form of tourism around the world. The term \"whale\" is sometimes used interchangeably with dolphins and porpoises, acting as a synonym for Cetacea. Six species of dolphins have the word \"whale\" in their name, collectively known as blackfish: the orca, or killer whale, the melon-headed whale, the pygmy killer whale, the false killer whale, and the two species of pilot whales, all of which are classified under the family Delphinidae (oceanic dolphins).[6] Each species has a different reason for it, for example, the killer whale was named \"Ballena asesina\" 'killer whale' by Spanish sailors.[7] The term \"Great Whales\" covers those currently regulated by the International Whaling Commission:[8] the Odontoceti family Physeteridae (sperm whales); and the Mysticeti families Balaenidae (right and bowhead whales), Eschrichtiidae (grey whales), and some of the Balaenopteridae (Minke, Bryde's, Sei, Blue and Fin; not Eden's and Omura's whales).[9] Cetaceans are divided into two parvorders. The larger parvorder, Mysticeti (baleen whales), is characterized by the presence of baleen, a sieve-like structure in the upper jaw made of keratin, which it uses to filter plankton, among others, from the water. Odontocetes (toothed whales) are characterized by bearing sharp teeth for hunting, as opposed to their counterparts' baleen.[10] Cetaceans and artiodactyls now are classified under the order Cetartiodactyla, often still referred to as Artiodactyla, which includes both whales and hippopotamuses. The hippopotamus and pygmy hippopotamus are the whales' closest terrestrial living relatives.[11] Mysticetes Mysticetes are also known as baleen whales. They have a pair of blowholes side by side and lack teeth; instead they have baleen plates which form a sieve-like structure in the upper jaw made of keratin, which they use to filter plankton from the water. Some whales, such as the humpback, reside in the polar regions where they feed on a reliable source of schooling fish and krill.[12] These animals rely on their well-developed flippers and tail fin to propel themselves through the water; they swim by moving their fore-flippers and tail fin up and down. Whale ribs loosely articulate with their thoracic vertebrae at the proximal end, but do not form a rigid rib cage. This adaptation allows the chest to compress during deep dives as the pressure increases.[13] Mysticetes consist of four families: rorquals (balaenopterids), cetotheriids, right whales (balaenids), and grey whales (eschrichtiids). The main difference between each family of mysticete is in their feeding adaptations and subsequent behaviour. Balaenopterids are the rorquals. These animals, along with the cetotheriids, rely on their throat pleats to gulp large amounts of water while feeding. The throat pleats extend from the mouth to the navel and allow the mouth to expand to a large volume for more efficient capture of the small animals they feed on. Balaenopterids consist of two genera and eight species.[14] Balaenids are the right whales. These animals have very large heads, which can make up as much as 40% of their body mass, and much of the head is the mouth. This allows them to take in large amounts of water into their mouths, letting them feed more effectively.[15] Eschrichtiids have one living member: the grey whale. They are bottom feeders, mainly eating crustaceans and benthic invertebrates. They feed by turning on their sides and taking in water mixed with sediment, which is then expelled through the baleen, leaving their prey trapped inside. This is an efficient method of hunting, in which the whale has no major competitors.[16] Odontocetes Odontocetes are known as toothed whales; they have teeth and only one blowhole. They rely on their well-developed sonar to find their way in the water. Toothed whales send out ultrasonic clicks using the melon. Sound waves travel through the water. Upon striking an object in the water, the sound waves bounce back at the whale. These vibrations are received through fatty tissues in the jaw, which is then rerouted into the ear-bone and into the brain where the vibrations are interpreted.[17] All toothed whales are opportunistic, meaning they will eat anything they can fit in their throat because they are unable to chew. These animals rely on their well-developed flippers and tail fin to propel themselves through the water; they swim by moving their fore-flippers and tail fin up and down. Whale ribs loosely articulate with their thoracic vertebrae at the proximal end, but they do not form a rigid rib cage. This adaptation allows the chest to compress during deep dives as opposed to resisting the force of water pressure.[13] Excluding dolphins and porpoises, odontocetes consist of four families: belugas and narwhals (monodontids), sperm whales (physeterids), dwarf and pygmy sperm whales (kogiids), and beaked whales (ziphiids).[6] The differences between families of odontocetes include size, feeding adaptations and distribution. Monodontids consist of two species: the beluga and the narwhal. They both reside in the frigid arctic and both have large amounts of blubber. Belugas, being white, hunt in large pods near the surface and around pack ice, their coloration acting as camouflage. Narwhals, being black, hunt in large pods in the aphotic zone, but their underbelly still remains white to remain camouflaged when something is looking directly up or down at them. They have no dorsal fin to prevent collision with pack ice.[18] Physeterids and Kogiids consist of sperm whales. Sperm whales consist the largest and smallest odontocetes, and spend a large portion of their life hunting squid. P. macrocephalus spends most of its life in search of squid in the depths; these animals do not require any degree of light at all, in fact, blind sperm whales have been caught in perfect health. The behaviour of Kogiids remains largely unknown, but, due to their small lungs, they are thought to hunt in the photic zone.[19] Ziphiids consist of 22 species of beaked whale. These vary from size, to coloration, to distribution, but they all share a similar hunting style. They use a suction technique, aided by a pair of grooves on the underside of their head, not unlike the throat pleats on the rorquals, to feed.[20] Evolution Whales are descendants of land-dwelling mammals of the artiodactylorder (even-toed ungulates). They are related to the Indohyus, an extinct chevrotain-like ungulate, from which they split approximately 48 million years ago.[21][22] Primitive cetaceans, or archaeocetes, first took to the sea approximately 49 million years ago and became fully aquatic 5–10 million years later. What defines an archaeocete is the presence of anatomical features exclusive to cetaceans, alongside other primitive features not found in modern cetaceans, such as visible legs or asymmetrical teeth.[23][24][25][11] Their features became adapted for living in the marine environment. Major anatomical changes included their hearing set-up that channeled vibrations from the jaw to the earbone (Ambulocetus 49 mya), a streamlined body and the growth of flukes on the tail (Protocetus 43 mya), the migration of the nostrils toward the top of the cranium (blowholes), and the modification of the forelimbs into flippers (Basilosaurus 35 mya), and the shrinking and eventual disappearance of the hind limbs (the first odontocetes and mysticetes 34 mya).[26][27][28] Whale morphology shows several examples of convergent evolution, the most obvious being the streamlined fish-like body shape.[29] Other examples include the use of echolocation for hunting in low light conditions — which is the same hearing adaptation used by bats — and, in the rorqual whales, jaw adaptations, similar to those found in pelicans, that enable engulfment feeding.[30] Today, the closest living relatives of cetaceans are the hippopotamuses; these share a semi-aquatic ancestor that branched off from other artiodactyls some 60 mya.[11] Around 40 mya, a common ancestor between the two branched off into cetacea and anthracotheres; nearly all anthracotheres became extinct at the end of the Pleistocene 2.5 mya, eventually leaving only one surviving lineage – the hippopotamus.[31] Whales split into two separate parvorders around 34 mya – the baleen whales (Mysticetes) and the toothed whales (Odontocetes).[32][33][34] Biology Anatomy Whales have torpedo-shaped bodies with non-flexible necks, limbs modified into flippers, non-existent external ear flaps, a large tail fin, and flat heads (with the exception of monodontids and ziphiids). Whale skulls have small eye orbits, long snouts (with the exception of monodontids and ziphiids) and eyes placed on the sides of its head. Whales range in size from the 2.6-metre (8.5 ft) and 135-kilogram (298 lb) dwarf sperm whale to the 34-metre (112 ft) and 190-metric-ton (210-short-ton) blue whale. Overall, they tend to dwarf other cetartiodactyls; the blue whale is the largest creature on Earth. Several species have female-biased sexual dimorphism, with the females being larger than the males. One exception is with the sperm whale, which has males larger than the females.[35][36] Odontocetes, such as the sperm whale, possess teeth with cementum cells overlying dentine cells. Unlike human teeth, which are composed mostly of enamel on the portion of the tooth outside of the gum, whale teeth have cementum outside the gum. Only in larger whales, where the cementum is worn away on the tip of the tooth, does enamel show. Mysticetes have large whalebone, as opposed to teeth, made of keratin. Mysticetes have two blowholes, whereas Odontocetes contain only one.[37] Breathing involves expelling stale air from the blowhole, forming an upward, steamy spout, followed by inhaling fresh air into the lungs; a humpback whale's lungs can hold about 5,000 litres (1,300 US gal) of air. Spout shapes differ among species, which facilitates identification.[38][39] All whales have a thick layer of blubber. In species that live near the poles, the blubber can be as thick as 11 inches (28 cm). This blubber can help with buoyancy (which is helpful for a 100-ton whale), protection to some extent as predators would have a hard time getting through a thick layer of fat, and energy for fasting when migrating to the equator; the primary usage for blubber is insulation from the harsh climate. It can constitute as much as 50% of a whale's body weight. Calves are born with only a thin layer of blubber, but some species compensate for this with thick lanugos.[40][41] Whales have a two- to three-chambered stomach that is similar in structure to those of terrestrial carnivores. Mysticetes contain a proventriculus as an extension of the oesophagus; this contains stones that grind up food. They also have fundic and pyloric chambers.[42] Locomotion Whales have two flippers on the front, and a tail fin. These flippers contain four digits. Although whales do not possess fully developed hind limbs, some, such as the sperm whale and bowhead whale, possess discrete rudimentary appendages, which may contain feet and digits. Whales are fast swimmers in comparison to seals, which typically cruise at 5–15 kn, or 9–28 kilometres per hour (5.6–17.4 mph); the fin whale, in comparison, can travel at speeds up to 47 kilometres per hour (29 mph) and the sperm whale can reach speeds of 35 kilometres per hour (22 mph). The fusing of the neck vertebrae, while increasing stability when swimming at high speeds, decreases flexibility; whales are unable to turn their heads. When swimming, whales rely on their tail fin to propel them through the water. Flipper movement is continuous. Whales swim by moving their tail fin and lower body up and down, propelling themselves through vertical movement, while their flippers are mainly used for steering. Some species log out of the water, which may allow them to travel faster. Their skeletal anatomy allows them to be fast swimmers. Most species have a dorsal fin.[28][43] Whales are adapted for diving to great depths. In addition to their streamlined bodies, they can slow their heart rate to conserve oxygen; blood is rerouted from tissue tolerant of water pressure to the heart and brain among other organs; haemoglobin and myoglobin store oxygen in body tissue; and they have twice the concentration of myoglobin than haemoglobin. Before going on long dives, many whales exhibit a behaviour known as sounding; they stay close to the surface for a series of short, shallow dives while building their oxygen reserves, and then make a sounding dive.[13][44] Senses The whale ear has specific adaptations to the marine environment. In humans, the middle ear works as an impedance equalizer between the outside air's low impedance and the cochlear fluid's high impedance. In whales, and other marine mammals, there is no great difference between the outer and inner environments. Instead of sound passing through the outer ear to the middle ear, whales receive sound through the throat, from which it passes through a low-impedance fat-filled cavity to the inner ear.[45] The whale ear is acoustically isolated from the skull by air-filled sinus pockets, which allow for greater directional hearing underwater.[46] Odontocetes send out high-frequency clicks from an organ known as a melon. This melon consists of fat, and the skull of any such creature containing a melon will have a large depression. The melon size varies between species, the bigger the more dependent they are on it. A beaked whale for example has a small bulge sitting on top of its skull, whereas a sperm whale's head is filled up mainly with the melon.[47][48][49][50] The whale eye is relatively small for its size, yet they do retain a good degree of eyesight. As well as this, the eyes of a whale are placed on the sides of its head, so their vision consists of two fields, rather than a binocular view like humans have. When belugas surface, their lens and cornea correct the nearsightedness that results from the refraction of light; they contain both rod and cone cells, meaning they can see in both dim and bright light, but they have far more rod cells than they do cone cells. Whales do, however, lack short wavelength sensitive visual pigments in their cone cells indicating a more limited capacity for colour vision than most mammals.[51] Most whales have slightly flattened eyeballs, enlarged pupils (which shrink as they surface to prevent damage), slightly flattened corneas and a tapetum lucidum; these adaptations allow for large amounts of light to pass through the eye and, therefore, a very clear image of the surrounding area. They also have glands on the eyelids and outer corneal layer that act as protection for the cornea.[52][48] Whales are not thought to have a good sense of taste, as their taste buds are atrophied or missing altogether. However, some toothed whales have preferences between different kinds of fish, indicating some sort of attachment to taste. The presence of the Jacobson's organ indicates that whales can smell food once inside their mouth, which might be similar to the sensation of taste.[54] Communication Whale vocalization is likely to serve several purposes. Some species, such as the humpback whale, communicate using melodic sounds, known as whale song. These sounds may be extremely loud, depending on the species. Humpback whales only have been heard making clicks, while toothed whales use sonar that may generate up to 20,000 watts of sound (+73 dBm or +43 dBw)[55] and be heard for many miles. Captive whales have occasionally been known to mimic human speech. Scientists have suggested this indicates a strong desire on behalf of the whales to communicate with humans, as whales have a very different vocal mechanism, so imitating human speech likely takes considerable effort.[56] Whales emit two distinct kinds of acoustic signals, which are called whistles and clicks:[57] Clicks are quick broadband burst pulses, used for sonar, although some lower-frequency broadband vocalizations may serve a non-echolocative purpose such as communication; for example, the pulsed calls of belugas. Pulses in a click train are emitted at intervals of ≈35–50 milliseconds, and in general these inter-click intervals are slightly greater than the round-trip time of sound to the target. Whistles are narrow-band frequency modulated (FM) signals, used for communicative purposes, such as contact calls. Intelligence Whales are known to teach, learn, cooperate, scheme, and grieve.[58] The neocortex of many species of whale is home to elongated spindle neurons that, prior to 2007, were known only in hominids.[59] In humans, these cells are involved in social conduct, emotions, judgement, and theory of mind. Whale spindle neurons are found in areas of the brain that are homologous to where they are found in humans, suggesting that they perform a similar function.[60] Bubble net feeding Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain-to-body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately the ⅔ or ¾ exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalisation quotient that can be used as another indication of animal intelligence. Sperm whales have the largest brain mass of any animal on Earth, averaging 8,000 cubic centimetres (490 in3) and 7.8 kilograms (17 lb) in mature males, in comparison to the average human brain which averages 1,450 cubic centimetres (88 in3) in mature males.[61] The brain-to-body mass ratio in some odontocetes, such as belugas and narwhals, is second only to humans.[62] Small whales are known to engage in complex play behaviour, which includes such things as producing stable underwater toroidal air-core vortex rings or \"bubble rings\". There are two main methods of bubble ring production: rapid puffing of a burst of air into the water and allowing it to rise to the surface, forming a ring, or swimming repeatedly in a circle and then stopping to inject air into the helical vortex currents thus formed. They also appear to enjoy biting the vortex-rings, so that they burst into many separate bubbles and then rise quickly to the surface.[63] Some believe this is a means of communication.[64] Whales are also known to produce bubble-nets for the purpose of foraging.[65] Larger whales are also thought, to some degree, to engage in play. The southern right whale, for example, elevates their tail fluke above the water, remaining in the same position for a considerable amount of time. This is known as \"sailing\". It appears to be a form of play and is most commonly seen off the coast of Argentina and South Africa. Humpback whales, among others, are also known to display this behaviour.[66] Life cycle Whales are fully aquatic creatures, which means that birth and courtship behaviours are very different from terrestrial and semi-aquatic creatures. Since they are unable to go onto land to calve, they deliver the baby with the fetus positioned for tail-first delivery. This prevents the baby from drowning either upon or during delivery. To feed the newborn, whales, being aquatic, must squirt the milk into the mouth of the calf. Nursing can occur while the mother whale is in the vertical or horizontal position. While nursing in the vertical position, a mother whale may sometimes rest with her tail flukes remaining stationary above the water. This position with the flukes above the water is known as \"whale-tail-sailing.\" Not all whale-tail-sailing includes nursing of the young, as whales have also been observed tail-sailing while no calves were present.[67] Being mammals, they have mammary glands used for nursing calves; they are weaned off at about 11 months of age. This milk contains high amounts of fat which is meant to hasten the development of blubber; it contains so much fat that it has the consistency of toothpaste.[68] As with humans, females whales typically deliver a single calf. The whale pregnancy/ gestation period lasts longer than the typical 9 month gestation period for humans. The whale gestation period is about 12 months. Once born, the absolute dependency of the calf upon the mother lasts from one to two years. Sexual maturity is achieved at around seven to ten years of age. The length of the developmental phases of a whale's early life varies between different whale species.[12] As with humans, the whale mode of reproduction typically produces but one offspring approximately once each year. While whales have fewer offspring over time than most species, the survival probability for each calf is also greater than for most other species. Female whales are referred to as \"cows.\" Female cows assume full responsibility for the care and training of their young. Male whales, referred to as \"bulls,\" typically play no role in the process of calf rearing. Most baleen whales reside at the poles. So, to prevent the unborn baleen whale calves from dying of frostbite, the baleen mother must migrate to warmer calving/mating grounds. They will then stay there for a matter of months until the calf has developed enough blubber to survive the bitter temperatures of the poles. Until then, the baleen calves will feed on the mother's fatty milk.[69] With the exception of the humpback whale, it is largely unknown when whales migrate. Most will travel from the Arctic or Antarctic into the tropics to mate, calve, and raise during the winter and spring; they will migrate back to the poles in the warmer summer months so the calf can continue growing while the mother can continue eating, as they fast in the breeding grounds. One exception to this is the southern right whale, which migrates to Patagonia and western New Zealand to calve; both are well out of the tropic zone.[70] Sleep Unlike most animals, whales are conscious breathers. All mammals sleep, but whales cannot afford to become unconscious for long because they may drown. While knowledge of sleep in wild cetaceans is limited, toothed cetaceans in captivity have been recorded to sleep with one side of their brain at a time, so that they may swim, breathe consciously, and avoid both predators and social contact during their period of rest.[71] A 2008 study found that sperm whales sleep in vertical postures just under the surface in passive shallow 'drift-dives', generally during the day, during which whales do not respond to passing vessels unless they are in contact, leading to the suggestion that whales possibly sleep during such dives.[71] Ecology Foraging and predation All whales are carnivorous and predatory. Odontocetes, as a whole, mostly feed on fish and cephalopods, and then followed by crustaceans and bivalves. All species are generalist and opportunistic feeders. Mysticetes, as a whole, mostly feed on krill and plankton, followed by crustaceans and other invertebrates. A few are specialists. Examples include the blue whale, which eats almost exclusively krill, the minke whale, which eats mainly schooling fish, the sperm whale, which specialize on squid, and the grey whale which feed on bottom-dwelling invertebrates.[14][72][73] The elaborate baleen \"teeth\" of filter-feeding species, mysticetes, allow them to remove water before they swallow their planktonic food by using the teeth as a sieve.[68] Usually, whales hunt solitarily, but they do sometimes hunt cooperatively in small groups. The former behaviour is typical when hunting non-schooling fish, slow-moving or immobile invertebrates or endothermic prey. When large amounts of prey are available, whales such as certain mysticetes hunt cooperatively in small groups.[74] Some cetaceans may forage with other kinds of animals, such as other species of whales or certain species of pinnipeds.[75][76] Large whales, such as mysticetes, are not usually subject to predation, but smaller whales, such as monodontids or ziphiids, are. These species are preyed on by the orca. To subdue and kill whales, orcas continuously ram them with their heads; this can sometimes kill bowhead whales, or severely injure them. Other times they corral the narwhals or belugas before striking. They are typically hunted by groups of 10 or fewer orcas, but they are seldom attacked by an individual. Calves are more commonly taken by orcas, but adults can be targeted as well.[77] These small whales are also targeted by terrestrial and pagophilic predators. The polar bear is well adapted for hunting Arctic whales and calves. Bears are known to use sit-and-wait tactics as well as active stalking and pursuit of prey on ice or water. Whales lessen the chance of predation by gathering in groups. This however means less room around the breathing hole as the ice slowly closes the gap. When out at sea, whales dive out of the reach of surface-hunting orcas. Polar bear attacks on belugas and narwhals are usually successful in winter, but rarely inflict any damage in summer.[78] Whale pump \"Whale pump\" – the role played by whales in recycling ocean nutrients.[79] A 2010 study considered whales to be a positive influence to the productivity of ocean fisheries, in what has been termed a \"whale pump.\" Whales carry nutrients such as nitrogen from the depths back to the surface. This functions as an upward biological pump, reversing an earlier presumption that whales accelerate the loss of nutrients to the bottom. This nitrogen input in the Gulf of Maine is \"more than the input of all rivers combined\" emptying into the gulf, some 23,000 metric tons (25,000 short tons) each year.[80][79]Whales defecate at the ocean's surface; their excrement is important for fisheries because it is rich in iron and nitrogen. The whale faeces are liquid and instead of sinking, they stay at the surface where phytoplankton feed off it.[79][81][82] Whale fall Upon death, whale carcasses fall to the deep ocean and provide a substantial habitat for marine life. Evidence of whale falls in present-day and fossil records shows that deep sea whale falls support a rich assemblage of creatures, with a global diversity of 407 species, comparable to other neritic biodiversity hotspots, such as cold seeps and hydrothermal vents.[83] Deterioration of whale carcasses happens though a series of three stages. Initially, moving organisms such as sharks and hagfish, scavenge the soft tissues at a rapid rate over a period of months, and as long as two years. This is followed by the colonization of bones and surrounding sediments (which contain organic matter) by enrichment opportunists, such as crustaceans and polychaetes, throughout a period of years. Finally, sulfophilic bacteria reduce the bones releasing hydrogen sulfide enabling the growth of chemoautotrophic organisms, which in turn, support other organisms such as mussels, clams, limpets, and sea snails. This stage may last for decades and supports a rich assemblage of species, averaging 185 species per site.[83][84] Relationship with humans Whaling Whaling by humans has existed since the Stone Age. Ancient whalers used harpoons to spear the bigger animals from boats out at sea.[85] People from Norway and Japan started hunting whales around 2000 B.C.[86] Whales are typically hunted for their meat and blubber by aboriginal groups; they used baleen for baskets or roofing, and made tools and masks out of bones.[86] The Inuit hunted whales in the Arctic Ocean.[86] The Basques started whaling as early as the 11th century, sailing as far as Newfoundland in the 16th century in search of right whales.[87][88] 18th- and 19th-century whalers hunted whales mainly for their oil, which was used as lamp fuel and a lubricant, baleen or whalebone, which was used for items such as corsets and skirt hoops,[86] and ambergris, which was used as a fixative for perfumes. The most successful whaling nations at this time were the Netherlands, Japan, and the United States.[89] Commercial whaling was historically important as an industry well throughout the 17th, 18th and 19th centuries. Whaling was at that time a sizeable European industry with ships from Britain, France, Spain, Denmark, the Netherlands and Germany, sometimes collaborating to hunt whales in the Arctic, sometimes in competition leading even to war.[90] By the early 1790s, whalers, namely the Americans and Australians, focused efforts in the South Pacific where they mainly hunted sperm whales and right whales, with catches of up to 39,000 right whales by Americans alone.[87][91] By 1853, US profits reached US$11,000,000 (£6.5m), equivalent to US$348,000,000 (£230m) today, the most profitable year for the American whaling industry.[92] Commonly exploited species included North Atlantic right whales, sperm whales, which were mainly hunted by Americans, bowhead whales, which were mainly hunted by the Dutch, common minke whales, blue whales, and grey whales. The scale of whale harvesting decreased substantially after 1982 when the International Whaling Commission (IWC) placed a moratorium which set a catch limit for each country, excluding aboriginal groups until 2004.[93] Current whaling nations are Norway, Iceland, and Japan, despite their joining to the IWC, as well as the aboriginal communities of Siberia, Alaska, and northern Canada.[94] Subsistence hunters typically use whale products for themselves and depend on them for survival. National and international authorities have given special treatment to aboriginal hunters since their methods of hunting are seen as less destructive and wasteful. This distinction is being questioned as these aboriginal groups are using more modern weaponry and mechanized transport to hunt with, and are selling whale products in the marketplace. Some anthropologists argue that the term \"subsistence\" should also apply to these cash-based exchanges as long as they take place within local production and consumption.[95][96][97] In 1946, the IWC placed a moratorium, limiting the annual whale catch. Since then, yearly profits for these \"subsistence\" hunters have been close to US$31 million (£20m) per year.[93] Other threats Whales can also be threatened by humans more indirectly. They are unintentionally caught in fishing nets by commercial fisheries as bycatch and accidentally swallow fishing hooks. Gillnetting and Seine netting is a significant cause of mortality in whales and other marine mammals.[98] Species commonly entangled include beaked whales. Whales are also affected by marine pollution. High levels of organic chemicals accumulate in these animals since they are high in the food chain. They have large reserves of blubber, more so for toothed whales as they are higher up the food chain than baleen whales. Lactating mothers can pass the toxins on to their young. These pollutants can cause gastrointestinal cancers and greater vulnerability to infectious diseases.[99] They can also be poisoned by swallowing litter, such as plastic bags.[100] Advanced military sonar harms whales. Sonar interferes with the basic biological functions of whales—such as feeding and mating—by impacting their ability to echolocate. Whales swim in response to sonar and sometimes experience decompression sickness due to rapid changes in depth. Mass strandings have been triggered by sonar activity, resulting in injury or death.[101][102][103][104] Whales are sometimes killed or injured during collisions with ships or boats. This is considered to be a significant threat to vulnerable whale populations such as the North Atlantic right whale, whose total population numbers less than 500.[105] Conservation Whaling decreased substantially after 1946 when, in response to the steep decline in whale populations, the International Whaling Commission placed a moratorium which set a catch limit for each country; this excluded aboriginal groups up until 2004.[89][96][106][107] As of 2015, aboriginal communities are allowed to take 280 bowhead whales off Alaska and two from the western coast of Greenland, 620 grey whales off Washington state, three common minke whales off the eastern coast of Greenland and 178 on their western coast, 10 fin whales from the west coast of Greenland, nine humpback whales from the west coast of Greenland and 20 off St. Vincent and the Grenadines each year.[107] Several species that were commercially exploited have rebounded in numbers; for example, grey whales may be as numerous as they were prior to harvesting, but the North Atlantic population is functionally extinct. Conversely, the North Atlantic right whale was extirpated from much of its former range, which stretched across the North Atlantic, and only remains in small fragments along the coast of Canada, Greenland, and is considered functionally extinct along the European coastline.[108] The IWC has designated two whale sanctuaries: the Southern Ocean Whale Sanctuary, and the Indian Ocean Whale Sanctuary. The Southern Ocean whale sanctuary spans 30,560,860 square kilometres (11,799,610 sq mi) and envelopes Antarctica.[109] The Indian Ocean whale sanctuary takes up all of the Indian Ocean south of 55°S.[110] The IWC is a voluntary organization, with no treaty. Any nation may leave as they wish; the IWC cannot enforce any law it makes. Whale watching An estimated 13 million people went whale watching globally in 2008, in all oceans except the Arctic.[120] Rules and codes of conduct have been created to minimize harassment of the whales.[121] Iceland, Japan and Norway have both whaling and whale watching industries. Whale watching lobbyists are concerned that the most inquisitive whales, which approach boats closely and provide much of the entertainment on whale-watching trips, will be the first to be taken if whaling is resumed in the same areas.[122] Whale watching generated US$2.1 billion (£1.4 billion) per annum in tourism revenue worldwide, employing around 13,000 workers.[120] In contrast, the whaling industry, with the moratorium in place, generates US$31 million (£20 million) per year.[93] The size and rapid growth of the industry has led to complex and continuing debates with the whaling industry about the best use of whales as a natural resource. In myth, literature and art As marine creatures that reside in either the depths or the poles, humans knew very little about whales over the course of human history; many feared or revered them. The Vikings and various arctic tribes revered the whale as they were important pieces of their lives. In Inuitcreation myths, when 'Big Raven', a deity in human form, found a stranded whale, he was told by the Great Spirit where to find special mushrooms that would give him the strength to drag the whale back to the sea and thus, return order to the world. In an Icelandic legend, a man threw a stone at a fin whale and hit the blowhole, causing the whale to burst. The man was told not to go to sea for twenty years, but during the nineteenth year he went fishing and a whale came and killed him. Whales played a major part in shaping the art forms of many coastal civilizations, such as the Norse, with some dating to the Stone Age. Petroglyphs off a cliff face in Bangudae, South Korea show 300 depictions of various animals, a third of which are whales. Some show particular detail in which there are throat pleats, typical of rorquals. These petroglyphs show these people, of around 7,000 to 3,500 B.C.E. in South Korea, had a very high dependency on whales.[123] In coastal regions of China, Korea and Vietnam, the worship of whale gods, who were associated with Dragon Kings after the arrival of Buddhism, was present along with related legends.[125]The god of the seas, according to Chinese folklore, was a large whale with human limbs. Illustration by Gustave Doré of Baron Munchausen's tale of being swallowed by a whale. While the Biblical Book of Jonah refers to the Prophet Jonah being swallowed by \"a big fish\", in later derivations that \"fish\" was identified as a whale. Whales have also played a role in sacred texts. The story of Jonah being swallowed by a great fish is told both in the Qur'an[130] and in the biblical Book of Jonah (and is mentioned by Jesus in the New Testament: Matthew 12:40.[131]). This episode was frequently depicted in medieval art (for example, on a 12th-century column capital at the abbey church of Mozac, France). The Bible also mentions whales in Genesis 1:21, Job 7:12, and Ezekiel 32:2. The \"leviathan\" described at length in Job 41:1-34 is generally understood to refer to a whale. The \"sea monsters\" in Lamentations 4:3 have been taken by some to refer to marine mammals, in particular whales, although most modern versions use the word \"jackals\" instead.[132] In 1585, Alessandro Farnese, 1585, and Francois, Duke of Anjou, 1582, were greeted on his ceremonial entry into the port city of Antwerp by floats including \"Neptune and the Whale\", indicating at least the city's dependence on the sea for its wealth.[133] Niki Caro's film the Whale Rider has a Māori girl ride a whale in her journey to be a suitable heir to the chieftain-ship.[136] Walt Disney's film Pinocchio features a showdown with a giant whale named Monstro at the end of the film. In captivity Belugas were the first whales to be kept in captivity. Other species were too rare, too shy, or too big. The first beluga was shown at Barnum's Museum in New York City in 1861.[138] For most of the 20th century, Canada was the predominant source of wild belugas.[139] They were taken from the St. Lawrence River estuary until the late 1960s, after which they were predominantly taken from the Churchill River estuary until capture was banned in 1992.[139] Russia has become the largest provider since it had been banned in Canada.[139] Belugas are caught in the Amur River delta and their eastern coast, and then are either transported domestically to aquariums or dolphinariums in Moscow, St. Petersburg, and Sochi, or exported to other countries, such as Canada.[139] Most captive belugas are caught in the wild, since captive-breeding programs are not very successful.[140] As of 2006, 30 belugas were in Canada and 28 in the United States, and 42 deaths in captivity had been reported up to that time.[139] A single specimen can reportedly fetch up to US$100,000 (£64,160) on the market. The beluga's popularity is due to its unique colour and its facial expressions. The latter is possible because while most cetacean \"smiles\" are fixed, the extra movement afforded by the beluga's unfused cervical vertebrae allows a greater range of apparent expression.[141] Between 1960 and 1992, the Navy carried out a program that included the study of marine mammals' abilities with sonar, with the objective of improving the detection of underwater objects. A large number of belugas were used from 1975 on, the first being dolphins.[141][142] The program also included training them to carry equipment and material to divers working underwater by holding cameras in their mouths to locate lost objects, survey ships and submarines, and underwater monitoring.[142] A similar program was used by the Russian Navy during the Cold War, in which belugas were also trained for antimining operations in the Arctic.[143] Aquariums have tried housing other species of whales in captivity. The success of belugas turned attention to maintaining their relative, the narwhal, in captivity. However, in repeated attempts in the 1960s and 1970s, all narwhals kept in captivity died within months. A pair of pygmy right whales were retained in an enclosed area (with nets); they were eventually released in South Africa. There was one attempt to keep a stranded Sowerby's beaked whale calf in captivity; the calf rammed into the tank wall, breaking its rostrum, which resulted in death. It was thought that Sowerby's beaked whale evolved to swim fast in a straight line, and a 30-metre (98 ft) tank was not big enough.[144] There have been attempts to keep baleen whales in captivity. There were three attempts to keep grey whales in captivity. Gigi was a grey whale calf that died in transport. Gigi II was another grey whale calf that was captured in the Ojo de Liebre Lagoon, and was transported to SeaWorld.[145] The 680-kilogram (1,500 lb) calf was a popular attraction, and behaved normally, despite being separated from his mother. A year later, the 8,000-kilogram (18,000 lb) whale grew too big to keep in captivity and was released; it was the first of two grey whales, the other being another grey whale calf named JJ, to successfully be kept in captivity.[145] There were three attempts to keep minke whales in captivity in Japan. They were kept in a tidal pool with a sea-gate at the Izu Mito Sea Paradise. Another, unsuccessful, attempt was made by the U.S. [146] One stranded humpback whale calf was kept in captivity for rehabilitation, but died days later.[147]"}
{"url": "https://en.m.wikipedia.org/wiki/Aquatic_animal", "text": "Contents The term aquatic can be applied to animals that live in either fresh water or salt water. However, the adjective marine is most commonly used for animals that live in saltwater, i.e. in oceans, seas, etc. Aquatic animals (especially freshwater animals) are often of special concern to conservationists because of the fragility of their environments. Aquatic animals are subject to pressure from overfishing, destructive fishing, marine pollution, hunting, and climate change. Many habitats are at risk which puts aquatic animals at risk as well.[2] Aquatic animals play an important role in the world. The biodiversity of aquatic animals provide food, energy, and even jobs.[3] Fresh water creates a hypotonic environment for aquatic organisms. This is problematic for some organisms with pervious skins or with gill membranes, whose cell membranes may burst if excess water is not excreted. Some protists accomplish this using contractile vacuoles, while freshwater fish excrete excess water via the kidney.[4] Although most aquatic organisms have a limited ability to regulate their osmotic balance and therefore can only live within a narrow range of salinity, diadromousfish have the ability to migrate between fresh water and saline water bodies. During these migrations they undergo changes to adapt to the surroundings of the changed salinities; these processes are hormonally controlled. The European eel (Anguilla anguilla) uses the hormoneprolactin,[5] while in salmon (Salmo salar) the hormone cortisol plays a key role during this process.[6] In addition to water breathing animals, e.g., fish, most mollusks, etc., the term \"aquatic animal\" can be applied to air-breathing aquatic or sea mammals such as those in the orders Cetacea (whales) and Sirenia (sea cows), which cannot survive on land, as well as the pinnipeds (true seals, eared seals, and the walrus). The term \"aquatic mammal\" is also applied to four-footed mammals like the river otter (Lontra canadensis) and beavers (family Castoridae), although these are technically amphibious or semiaquatic. There are up to one million types of aquatic animals and aquatic species.[9] Amphibians, like frogs (the order Anura), while requiring water, are separated into their own environmental classification. The majority of amphibians (class Amphibia) have an aquatic larval stage, like a tadpole, but then live as terrestrial adults, and may return to the water to mate. Most mollusks have gills, while some fresh water ones have a lung instead (e.g. Planorbidae) and some amphibious ones have both (e.g. This depends on the animals. Ampullariidae). Many species of aquatic animals lack a backbone or are invertebrates.[9] Aquatic animals play an important role for the environment as well as human's daily usage. The importance of aquatic animals comes from the fact that they are organisms that provide humans with sources such as food, medicine, energy shelter, and raw materials that are used for daily life. Each aquatic species plays a different role to help us make every day easier, healthier, and also more productive. They also help with the atmospheric pressure and global climate change.[10]"}
{"url": "https://en.m.wikipedia.org/wiki/Plankton", "text": "Plankton Plankton are the diverse collection of organisms found in water (or air) that are unable to propel themselves against a current (or wind).[1][2] The individual organisms constituting plankton are called plankters.[3] In the ocean, they provide a crucial source of food to many small and large aquatic organisms, such as bivalves, fish, and baleen whales. Although plankton are usually thought of as inhabiting water, there are also airborne versions that live part of their lives drifting in the atmosphere. These aeroplankton include plant spores, pollen and wind-scattered seeds. They may also include microorganisms swept into the air from terrestrial dust storms and oceanic plankton swept into the air by sea spray. Though many planktonic species are microscopic in size, plankton includes organisms over a wide range of sizes, including large organisms such as jellyfish.[5] This is because plankton are defined by their ecological niche and level of motility rather than by any phylogenetic or taxonomic classification. The \"plankton\" category differentiates these organisms from those that float on the water's surface, called neuston, those that can swim against a current, called nekton, and those that live on the deep sea floor, called benthos. Plankton (organisms that drift with water currents) can be contrasted with nekton (organisms that swim against water currents), neuston (organisms that live at the ocean surface) and benthos (organisms that live at the ocean floor). The name plankton was coined by German marine biologist Victor Hensen in 1887 from shortening the word halyplankton from Greekᾰ̔́λςháls \"sea\" and πλανάωplanáō to \"drift\" or \"wander\".[6]: 1 While some forms are capable of independent movement and can swim hundreds of meters vertically in a single day (a behavior called diel vertical migration), their horizontal position is primarily determined by the surrounding water movement, and plankton typically flow with ocean currents. This is in contrast to nekton organisms, such as fish, squid and marine mammals, which can swim against the ambient flow and control their position in the environment. The study of plankton is termed planktology and a planktonic individual is referred to as a plankter.[9] The adjective planktonic is widely used in both the scientific and popular literature, and is a generally accepted term. However, from the standpoint of prescriptive grammar, the less-commonly used planktic is more strictly the correct adjective. When deriving English words from their Greek or Latin roots, the gender-specific ending (in this case, \"-on\" which indicates the word is neuter) is normally dropped, using only the root of the word in the derivation.[10] Mixotrophs. Plankton have traditionally been categorized as producer, consumer, and recycler groups, but some plankton are able to benefit from more than just one trophic level. In this mixed trophic strategy—known as mixotrophy—organisms act as both producers and consumers, either at the same time or switching between modes of nutrition in response to ambient conditions. This makes it possible to use photosynthesis for growth when nutrients and light are abundant, but switching to eat phytoplankton, zooplankton, or each other when growing conditions are poor. Mixotrophs are divided into two groups; constitutive mixotrophs, CMs, which are able to perform photosynthesis on their own, and non-constitutive mixotrophs, NCMs, which use phagocytosis to engulf phototrophic prey that are either kept alive inside the host cell, which benefit from its photosynthesis, or they digest their prey except for the plastids, which continues to perform photosynthesis (kleptoplasty).[14] Recognition of the importance of mixotrophy as an ecological strategy is increasing,[15] as well as the wider role this may play in marine biogeochemistry.[16] Studies have shown that mixotrophs are much more important for marine ecology than previously assumed and comprise more than half of all microscopic plankton.[17][18] Their presence acts as a buffer that prevents the collapse of ecosystems during times with little to no light.[19] However, some of these terms may be used with very different boundaries, especially on the larger end. The existence and importance of nano- and even smaller plankton was only discovered during the 1980s, but they are thought to make up the largest proportion of all plankton in number and diversity. The microplankton and smaller groups are microorganisms and operate at low Reynolds numbers, where the viscosity of water is more important than its mass or inertia. [22] Aeroplankton are tiny lifeforms that float and drift in the air, carried by the current of the wind; they are the atmosphericanalogue to oceanic plankton. Most of the living things that make up aeroplankton are very small to microscopic in size, and many can be difficult to identify because of their tiny size. Scientists can collect them for study in traps and sweep nets from aircraft, kites or balloons.[24] Aeroplankton is made up of numerous microbes, including viruses, about 1000 different species of bacteria, around 40,000 varieties of fungi, and hundreds of species of protists, algae, mosses and liverworts that live some part of their life cycle as aeroplankton, often as spores, pollen, and wind-scattered seeds. Additionally, peripatetic microorganisms are swept into the air from terrestrial dust storms, and an even larger amount of airborne marine microorganisms are propelled high into the atmosphere in sea spray. Aeroplankton deposits hundreds of millions of airborne viruses and tens of millions of bacteria every day on every square meter around the planet. The sea surface microlayer, compared to the sub-surface waters, contains elevated concentration of bacteria and viruses.[25][26] These materials can be transferred from the sea-surface to the atmosphere in the form of wind-generated aqueous aerosols due to their high vapour tension and a process known as volatilisation.[27] When airborne, these microbes can be transported long distances to coastal regions. If they hit land they can have an effect on animal, vegetation and human health.[28] Marine aerosols that contain viruses can travel hundreds of kilometers from their source and remain in liquid form as long as the humidity is high enough (over 70%).[29][30][31] These aerosols are able to remain suspended in the atmosphere for about 31 days.[32] Evidence suggests that bacteria can remain viable after being transported inland through aerosols. Some reached as far as 200 meters at 30 meters above sea level.[33] The process which transfers this material to the atmosphere causes further enrichment in both bacteria and viruses in comparison to either the SML or sub-surface waters (up to three orders of magnitude in some locations).[33] Many animals live in terrestrial environments by thriving in transient often microscopic bodies of water and moisture, these include rotifers and gastrotrichs which lay resilient eggs capable of surviving years in dry environments, and some of which can go dormant themselves. Nematodes are usually microscopic with this lifestyle. Water bears, despite only having lifespans of a few months, famously can enter suspended animation during dry or hostile conditions and survive for decades. This allows them to be ubiquitous in terrestrial environments despite needing water to grow and reproduce. Many microscopic crustacean groups like copepods and amphipods (of which sandhoppers are members) and seed shrimp are known to go dormant when dry and live in transient bodies of water too[34] Gelatinous zooplankton are fragile animals that live in the water column in the ocean. Their delicate bodies have no hard parts and are easily damaged or destroyed.[36] Gelatinous zooplankton are often transparent.[37] All jellyfish are gelatinous zooplankton, but not all gelatinous zooplankton are jellyfish. The most commonly encountered organisms include ctenophores, medusae, salps, and Chaetognatha in coastal waters. However, almost all marine phyla, including Annelida, Mollusca and Arthropoda, contain gelatinous species, but many of those odd species live in the open ocean and the deep sea and are less available to the casual ocean observer.[38] Salmon egg hatching into a sac fry. In a few days, the sac fry will absorb the yolk sac and start feeding on smaller plankton. Ichthyoplankton are the eggs and larvae of fish. They are mostly found in the sunlit zone of the water column, less than 200 metres deep, which is sometimes called the epipelagic or photic zone. Ichthyoplankton are planktonic, meaning they cannot swim effectively under their own power, but must drift with the ocean currents. Fish eggs cannot swim at all, and are unambiguously planktonic. Early stage larvae swim poorly, but later stage larvae swim better and cease to be planktonic as they grow into juveniles. Fish larvae are part of the zooplankton that eat smaller plankton, while fish eggs carry their food supply. Both eggs and larvae are themselves eaten by larger animals.[39][40] Fish can produce high numbers of eggs which are often released into the open water column. Fish eggs typically have a diameter of about 1 millimetre (0.039 in). The newly hatched young of oviparous fish are called larvae. They are usually poorly formed, carry a large yolk sac (for nourishment), and are very different in appearance from juvenile and adult specimens. The larval period in oviparous fish is relatively short (usually only several weeks), and larvae rapidly grow and change appearance and structure (a process termed metamorphosis) to become juveniles. During this transition larvae must switch from their yolk sac to feeding on zooplankton prey, a process which depends on typically inadequate zooplankton density, starving many larvae. In time fish larvae become able to swim against currents, at which point they cease to be plankton and become juvenile fish. Meroplankton are a wide variety of aquatic organisms that have both planktonic and benthic stages in their life cycles. Much of the meroplankton consists of larval stages of larger organisms.[34] Meroplankton can be contrasted with holoplankton, which are planktonic organisms that stay in the pelagic zone as plankton throughout their entire life cycle.[44] After some time in the plankton, many meroplankton graduate to the nekton or adopt a benthic (often sessile) lifestyle on the seafloor. The larval stages of benthic invertebrates make up a significant proportion of planktonic communities.[45] The planktonic larval stage is particularly crucial to many benthic invertebrates in order to disperse their young. Depending on the particular species and the environmental conditions, larval or juvenile-stage meroplankton may remain in the pelagic zone for durations ranging from hours to months.[34] Tychoplankton are organisms, such as free-living or attached benthic organisms and other non-planktonic organisms, that are carried into the plankton through a disturbance of their benthic habitat, or by winds and currents.[47] This can occur by direct turbulence or by disruption of the substrate and subsequent entrainment in the water column.[47][48] Tychoplankton are, therefore, a primary subdivision for sorting planktonic organisms by duration of lifecycle spent in the plankton, as neither their entire lives nor particular reproductive portions are confined to planktonic existence.[49] Tychoplankton are sometimes called accidental plankton. World concentrations of surface ocean chlorophyll as viewed by satellite during the northern spring, averaged from 1998 to 2004. Chlorophyll is a marker for the distribution and abundance of phytoplankton. Apart from aeroplankton, plankton inhabits oceans, seas, lakes and ponds. Local abundance varies horizontally, vertically and seasonally. The primary cause of this variability is the availability of light. All plankton ecosystems are driven by the input of solar energy (but see chemosynthesis), confining primary production to surface waters, and to geographical regions and seasons having abundant light. A secondary variable is nutrient availability. Although large areas of the tropical and sub-tropical oceans have abundant light, they experience relatively low primary production because they offer limited nutrients such as nitrate, phosphate and silicate. This results from large-scale ocean circulation and water column stratification. In such regions, primary production usually occurs at greater depth, although at a reduced level (because of reduced light). While plankton are most abundant in surface waters, they live throughout the water column. At depths where no primary production occurs, zooplankton and bacterioplankton instead consume organic material sinking from more productive surface waters above. This flux of sinking material, so-called marine snow, can be especially high following the termination of spring blooms. Primarily by grazing on phytoplankton, zooplankton provide carbon to the planktic foodweb, either respiring it to provide metabolic energy, or upon death as biomass or detritus. Organic material tends to be denser than seawater, so it sinks into open ocean ecosystems away from the coastlines, transporting carbon along with it. This process, called the biological pump, is one reason that oceans constitute the largest carbon sink on Earth. However, it has been shown to be influenced by increments of temperature.[54][55][56][57] In 2019, a study indicated that at ongoing rates of seawater acidification, Antarctic phytoplanktons could become smaller and less effective at storing carbon before the end of the century.[58] Phytoplankton absorb energy from the Sun and nutrients from the water to produce their own nourishment or energy. In the process of photosynthesis, phytoplankton release molecular oxygen (O 2) into the water as a waste byproduct. It is estimated that about 50% of the world's oxygen is produced via phytoplankton photosynthesis.[61] The rest is produced via photosynthesis on land by plants.[61] Furthermore, phytoplankton photosynthesis has controlled the atmospheric CO 2/O 2 balance since the early Precambrian Eon.[62] The absorption efficiency (AE) of plankton is the proportion of food absorbed by the plankton that determines how available the consumed organic materials are in meeting the required physiological demands.[63] Depending on the feeding rate and prey composition, variations in absorption efficiency may lead to variations in fecal pellet production, and thus regulates how much organic material is recycled back to the marine environment. Low feeding rates typically lead to high absorption efficiency and small, dense pellets, while high feeding rates typically lead to low absorption efficiency and larger pellets with more organic content. Another contributing factor to dissolved organic matter (DOM) release is respiration rate. Physical factors such as oxygen availability, pH, and light conditions may affect overall oxygen consumption and how much carbon is loss from zooplankton in the form of respired CO2. The relative sizes of zooplankton and prey also mediate how much carbon is released via sloppy feeding. Smaller prey are ingested whole, whereas larger prey may be fed on more \"sloppily\", that is more biomatter is released through inefficient consumption.[64][65] There is also evidence that diet composition can impact nutrient release, with carnivorous diets releasing more dissolved organic carbon (DOC) and ammonium than omnivorous diets.[66] The growth of phytoplankton populations is dependent on light levels and nutrient availability. The chief factor limiting growth varies from region to region in the world's oceans. On a broad scale, growth of phytoplankton in the oligotrophic tropical and subtropical gyres is generally limited by nutrient supply, while light often limits phytoplankton growth in subarctic gyres. Environmental variability at multiple scales influences the nutrient and light available for phytoplankton, and as these organisms form the base of the marine food web, this variability in phytoplankton growth influences higher trophic levels. For example, at interannual scales phytoplankton levels temporarily plummet during El Niño periods, influencing populations of zooplankton, fishes, sea birds, and marine mammals. The effects of anthropogenic warming on the global population of phytoplankton is an area of active research. Changes in the vertical stratification of the water column, the rate of temperature-dependent biological reactions, and the atmospheric supply of nutrients are expected to have important impacts on future phytoplankton productivity.[67] Additionally, changes in the mortality of phytoplankton due to rates of zooplankton grazing may be significant. Zooplankton are the initial prey item for almost all fish larvae as they switch from their yolk sacs to external feeding. Fish rely on the density and distribution of zooplankton to match that of new larvae, which can otherwise starve. Natural factors (e.g., current variations, temperature changes) and man-made factors (e.g. river dams, ocean acidification, rising temperatures) can strongly affect zooplankton, which can in turn strongly affect larval survival, and therefore breeding success. It's been shown that plankton can be patchy in marine environments where there aren't significant fish populations and additionally, where fish are abundant, zooplankton dynamics are influenced by the fish predation rate in their environment. Depending on the predation rate, they could express regular or chaotic behavior.[69] A negative effect that fish larvae can have on planktonic algal blooms is that the larvae will prolong the blooming event by diminishing available zooplankton numbers; this in turn permits excessive phytoplankton growth allowing the bloom to flourish .[53] The importance of both phytoplankton and zooplankton is also well-recognized in extensive and semi-intensive pond fish farming. Plankton population-based pond management strategies for fish rearing have been practiced by traditional fish farmers for decades, illustrating the importance of plankton even in man-made environments. Whales & plankton Of all animal fecal matter, it is whale feces that is the 'trophy' in terms of increasing nutrient availability. Phytoplankton are the powerhouse of open ocean primary production and they can acquire many nutrients from whale feces.[70] In the marine food web, phytoplankton are at the base of the food web and are consumed by zooplankton & krill, which are preyed upon by larger and larger marine organisms, including whales, so it can be said that whale poop fuels the entire food web. Around 70% of the oxygen in the atmosphere is produced in the oceans from phytoplankton performing photosynthesis, meaning that the majority of the oxygen available for us and other organisms that respire aerobically is produced by plankton.[71] Plankton also make up the base of the marine food web, providing food for all the trophic levels above. Recent studies have analyzed the marine food web to see if the system runs on a top-down or bottom-up approach. Essentially, this research is focused on understanding whether changes in the food web are driven by nutrients at the bottom of the food web or predators at the top. The general conclusion is that the bottom-up approach seemed to be more predictive of food web behavior.[72] This indicates that plankton have more sway in determining the success of the primary consumer species that prey on them than do the secondary consumers that prey on the primary consumers. In some cases, plankton act as an intermediate host for deadly parasites in humans. One such case is that of cholera, an infection caused by several strains of Vibrio cholerae. These species have been shown to have a symbiotic relationship with chitinous zooplankton species like copepods. These bacteria benefit not only from the food provided by the chiton from the zooplankton, but also from the protection from acidic environments. Once the copepods have been ingested by a human host, the chitinous exterior protects the bacteria from the stomach acids in the stomach and proceed to the intestines. Once there, the bacteria bind with the surface of the small intestine and the host will start developing symptoms, including extreme diarrhea, within five days.[73]"}
{"url": "https://en.m.wikipedia.org/wiki/S2CID_(identifier)", "text": "Contents Semantic Scholar provides a one-sentence summary of scientific literature. One of its aims was to address the challenge of reading numerous titles and lengthy abstracts on mobile devices.[7] It also seeks to ensure that the three million scientific papers published yearly reach readers, since it is estimated that only half of this literature is ever read.[8] Another key AI-powered feature is Research Feeds, an adaptive research recommender that uses AI to quickly learn what papers users care about reading and recommends the latest research to help scholars stay up to date. It uses a state-of-the-art paper embedding model trained using contrastive learning to find papers similar to those in each Library folder.[11] Semantic Scholar also offers Semantic Reader, an augmented reader with the potential to revolutionize scientific reading by making it more accessible and richly contextual.[12] Semantic Reader provides in-line citation cards that allow users to see citations with TLDR summaries as they read and skimming highlights that capture key points of a paper so users can digest faster. In contrast with Google Scholar and PubMed, Semantic Scholar is designed to highlight the most important and influential elements of a paper.[13] The AI technology is designed to identify hidden connections and links between research topics.[14] Like the previously cited search engines, Semantic Scholar also exploits graph structures, which include the Microsoft Academic Knowledge Graph, Springer Nature's SciGraph, and the Semantic Scholar Corpus.[15] One study compared the index scope of Semantic Scholar to Google Scholar, and found that for the papers cited by secondary studies in computer science, the two indices had comparable coverage, each only missing a handful of the papers.[16] As of January 2018, following a 2017 project that added biomedical papers and topic summaries, the Semantic Scholar corpus included more than 40 million papers from computer science and biomedicine.[17] In March 2018, Doug Raymond, who developed machine learning initiatives for the Amazon Alexa platform, was hired to lead the Semantic Scholar project.[18] As of August 2019[update], the number of included papers metadata (not the actual PDFs) had grown to more than 173 million[19] after the addition of the Microsoft Academic Graph records.[20] In 2020, a partnership between Semantic Scholar and the University of Chicago Press Journals made all articles published under the University of Chicago Press available in the Semantic Scholar corpus.[21] At the end of 2020, Semantic Scholar had indexed 190 million papers.[22] In 2020, Semantic Scholar reached seven million users per month.[7] ^Matthews, David (1 September 2021). \"Drowning in the literature? These smart software tools can help\". Nature. Retrieved 5 September 2022. ...the publicly available corpus compiled by Semantic Scholar – a tool set up in 2015 by the Allen Institute for Artificial Intelligence in Seattle, Washington – amounting to around 200 million articles, including preprints."}
{"url": "https://en.m.wikipedia.org/wiki/PMC_(identifier)", "text": "PubMed Central PubMed Central (PMC) is a free digital repository that archives open access full-text scholarly articles that have been published in biomedical and life sciences journals. As one of the major research databases developed by the National Center for Biotechnology Information (NCBI), PubMed Central is more than a document repository. Submissions to PMC are indexed and formatted for enhanced metadata, medical ontology, and unique identifiers which enrich the XML structured data for each article.[1] Content within PMC can be linked to other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to discover, read and build upon its biomedical knowledge.[2] PubMed Central is distinct from PubMed.[3] PubMed Central is a free digital archive of full articles, accessible to anyone from anywhere via a web browser (with varying provisions for reuse). Conversely, although PubMed is a searchable database of biomedical citations and abstracts, the full-text article resides elsewhere (in print or online, free or behind a subscriber paywall). As of December 2018[update], the PMC archive contained over 5.2 million articles,[4] with contributions coming from publishers or authors depositing their manuscripts into the repository per the NIH Public Access Policy. Earlier data shows that from January 2013 to January 2014 author-initiated deposits exceeded 103,000 papers during a 12-month period.[5] PMC identifies about 4,000 journals which participate in some capacity to deposit their published content into the PMC repository.[6] Some publishers delay the release of their articles on PubMed Central for a set time after publication, referred to as an \"embargo period\", ranging from a few months to a few years depending on the journal. (Embargoes of six to twelve months are the most common.) PubMed Central is a key example of \"systematic external distribution by a third party\",[7] which is still prohibited by the contributor agreements of many publishers. But my views broadened abruptly one morning in December of 1998 when I met Pat Brown for coffee, at the café that was formerly the famed Tassajara Bakery, on the corner of Cole and Parnassus, during a visit to San Francisco. [...] A few weeks before our coffee, Pat had learned about the methods being used by the physicist Paul Ginsparg and his colleagues at Los Alamos to allow physicists and mathematicians to share their work with one another over the Internet. They were posting \"preprints\" (articles not yet submitted or accepted for publication) at a publicly accessible website (called LanX or arXiv) for anyone to read and critique. [...] The more I thought about this, the more I was convinced that a radical restructuring of methods for publishing, transmitting, storing, and using biomedical research reports might be possible and beneficial. In a spirit of enthusiasm and political innocence, I wrote a lengthy manifesto, proposing the creation of an NIH-supported online system, called E-biomed. The goal of E-biomed was to provide free access to all biomedical research. Papers submitted to E-biomed could take one of two routes: either immediately published as a preprint, or through a traditional peer review process. The peer review process was to resemble contemporary overlay journals, with an external editorial board retaining control over the process of reviewing, curating, and listing papers which would otherwise be freely accessible on the central E-biomed server. Varmus intended to realize the new possibilities presented by communicating scientific results digitally, imagining continuous conversation about published work, versioned documents, and enriched \"layered\" formats allowing for multiple levels of detail.[8] The proposal to create a central index of biomedical research was a radical departure from prevailing publishing norms. Prior to the internet, publication indexes operated largely like ISBNs: allocated by registration agencies to secondary publishers. The idea that anyone could own their own address space via a domain name and create their own indexing system was a wholly new idea.[11][12] Major commercial publishers had begun experimenting with an indexing system for scientific papers shared across publishers as early as 1993, and were spurred to action following the E-biomed proposal. At the October 1999 STM Annual Frankfurt Conference, several publishers led by Springer-Verlag reached a hurried conference room consensus to launch their competitor prototype:[13] At the Board meeting of the STM association, held the afternoon of Monday, October 11, before the fair's Wednesday opening, discussion focused on an emerging U.S. National Library of Medicine (NLM) initiative called E-Biomed (later PubMed Central) that had been proposed by Harold Varmus of the National Institutes of Health in the spring of 1999. Varmus envisioned a digital archive of journals, accessible free of charge and with the added value of reference linking. \"Our consensus was that publishers should be the ones doing the linking,\" said Bob Campbell, who chaired the meeting. \"Since we were 'higher up the stream,' so to speak, we should be able to link our articles ahead of the NLM as part of the process of producing them. Stefan von Holtzbrinck then set the ball rolling by offering to link Nature publications with anyone else's. We decided to issue an announcement of a broad STM reference linking initiative. It was, of course, a strategic move only, since we had neither plan nor prototype.\" A small group led by Arnoud de Kemp of Springer-Verlag met in an adjacent room immediately following the Board meeting to draft the announcement, which was distributed to all attendees of the STM annual meeting the following day and published in an STM membership publication. [...] The potential benefit of the service that would become CrossRef was immediately apparent. Organizations such as AIP and IOP (Institute of Physics) had begun to link to each other's publications, and the impossibility of replicating such one-off arrangements across the industry was obvious. As Tim Ingoldsby later put it, \"All those linking agreements were going to kill us.\" Under pressure from vigorous lobbying from commercial publishers and scientific societies who feared for lost profits,[14] NIH officials announced a revised PubMed Central proposal in August 1999.[9] PMC would receive submissions from publishers, rather than from authors as in E-biomed. Publications were allowed time-embargoed paywalls up to one year. PMC would only allow peer-reviewed work — no preprints.[15] The then-unnamed publisher-led linking system shortly thereafter became CrossRef and the larger DOI system. Varmus, Brown, and others including Michael Eisen went on to found the Public Library of Science (PLoS) in 2001, reaching the conclusion \"that if we really want to change the publication of scientific research, we must do the publishing ourselves.\"[16] Launched in February 2000, the repository has grown rapidly as the NIH Public Access Policy is designed to make all research funded by the National Institutes of Health (NIH) freely accessible to anyone, and, in addition, many publishers are working cooperatively with the NIH to provide free access to their works.[citation needed] In late 2007, the Consolidated Appropriations Act of 2008 (H.R. 2764) was signed into law and included a provision requiring the NIH to modify its policies and require inclusion into PubMed Central complete electronic copies of their peer-reviewed research and findings from NIH-funded research. These articles are required to be included within 12 months of publication. This is the first time the US government has required an agency to provide open access to research and is an evolution from the 2005 policy, in which the NIH asked researchers to voluntarily add their research to PubMed Central.[17] With the release of public access plans for many agencies beyond NIH, PMC is in the process of becoming the repository for a wider variety of articles.[23] This includes NASA content, with the interface branded as \"PubSpace\".[24][25] Articles are sent to PubMed Central by publishers in XML or SGML, using a variety of article DTDs. Older and larger publishers may have their own established in-house DTDs, but many publishers use the NLM Journal Publishing DTD (see above). Received articles are converted via XSLT to the very similar NLM Archiving and Interchange DTD. This process may reveal errors that are reported back to the publisher for correction. Graphics are also converted to standard formats and sizes. The original and converted forms are archived. The converted form is moved into a relational database, along with associated files for graphics, multimedia, or other associated data. Many publishers also provide PDF of their articles, and these are made available without change.[26] Bibliographic citations are parsed and automatically linked to the relevant abstracts in PubMed, articles in PubMed Central, and resources on publishers' Web sites. PubMed links also lead to PubMed Central. Unresolvable references, such as to journals or particular articles not yet available at one of these sources, are tracked in the database and automatically come \"live\" when the resources become available. An in-house indexing system provides search capability, and is aware of biological and medical terminology, such as generic vs. proprietary drug names, and alternate names for organisms, diseases and anatomical parts. When a user accesses a journal issue, a table of contents is automatically generated by retrieving all articles, letters, editorials, etc. for that issue. When an actual item such as an article is reached, PubMed Central converts the NLM markup to HTML for delivery, and provides links to related data objects. This is feasible because the variety of incoming data has first been converted to standard DTDs and graphic formats. In a separate submission stream, NIH-funded authors may deposit articles into PubMed Central using the NIH Manuscript Submission (NIHMS). Articles thus submitted typically go through XML markup in order to be converted to NLM DTD. Reactions to PubMed Central among the scholarly publishing community range between a genuine enthusiasm by some,[27] to cautious concern by others.[28] While PMC is a welcome partner to open access publishers in its ability to augment the discovery and dissemination of biomedical knowledge, that same truth causes others to worry about traffic being diverted from the published version of record, the economic consequences of less readership, as well as the effect on maintaining a community of scholars within learned societies.[29][30] A 2013 analysis found strong evidence that public repositories of published articles were responsible for \"drawing significant numbers of readers away from journal websites\" and that \"the effect of PMC is growing over time\".[31] Libraries, universities, open access supporters, consumer health advocacy groups, and patient rights organizations have applauded PubMed Central, and hope to see similar public access repositories developed by other federal funding agencies so to freely share any research publications that were the result of taxpayer support.[32] The Antelman study of open access publishing found that in philosophy, political science, electrical and electronic engineering and mathematics, open access papers had a greater research impact.[33] A randomised trial found an increase in content downloads of open access papers, with no citation advantage over subscription access one year after publication.[34] The NIH policy and open access repository work has inspired a 2013 presidential directive which has sparked action in other federal agencies as well. The PMCID (PubMed Central identifier), also known as the PMC reference number, is a bibliographic identifier for the PubMed Central open access database, much like the PMID is the bibliographic identifier for the PubMed database. The two identifiers are distinct however. It consists of \"PMC\" followed by a string of numbers. The format is:[36] PMCID: PMC1852221 Authors applying for NIH awards must include the PMCID in their application. ^Beck J (2010). \"Report from the Field: PubMed Central, an XML-based Archive of Life Sciences Journal Articles\". Proceedings of the International Symposium on XML for the Long Haul: Issues in the Long-term Preservation of XML. 6. doi:10.4242/BalisageVol6.Beck01. ISBN978-1-935958-02-4. ^\"The goal of the semantic web is to express real life. Many things in real life, real questions which we will face are not efficiently computable. There are two solutions to this: The classical (pre-web) solution is to constrain the language of expression so that all queries terminate in finite time. The weblike solution is to allow the expression of facts and rules in an overall language which is sufficiently flexible and powerful to express real life. Create subsets fo the web in which specific constraints give you specific computational properties. An anlogy is with the human-information systems which existed before the web. Most forced one to keep ones data in a hierarchy (sometimes of fixed depth or a matrix (often with a specific number of dimensions). This gave consistency properties within the information system. I bet DARPA has many of these systems and still does. They only way they could be integrated was to express them in terms of a much more powerful language - global hypertext. Hypertext did not have any of these reassuring properties. People were frightened about getting lost in it. You could follow links forever. As it turns out, it is true of course that there is a problem that you can follow links forever in the Web. \" Berners-Lee, Tim. \"What the Semantic Web can represent\". Web design issues. Archived from the original on October 19, 2023. Retrieved October 20, 2023."}
{"url": "https://en.m.wikipedia.org/wiki/Thoracic_vertebrae", "text": "Thoracic vertebrae In vertebrates, thoracic vertebrae compose the middle segment of the vertebral column, between the cervical vertebrae and the lumbar vertebrae.[1] In humans, there are twelve thoracic vertebrae and they are intermediate in size between the cervical and lumbar vertebrae; they increase in size going towards the lumbar vertebrae, with the lower ones being much larger than the upper.[citation needed] They are distinguished by the presence of facets on the sides of the bodies for articulation with the heads of the ribs, as well as facets on the transverse processes of all, except the eleventh and twelfth, for articulation with the tubercles of the ribs. By convention, the human thoracic vertebrae are numbered T1–T12, with the first one (T1) located closest to the skull and the others going down the spine toward the lumbar region. These are the general characteristics of the second through eighth thoracic vertebrae. The first and ninth through twelfth vertebrae contain certain peculiarities, and are detailed below. The bodies in the middle of the thoracic region are heart-shaped and as broad in the anteroposterior as in the transverse direction. At the ends of the thoracic region they resemble respectively those of the cervical and lumbar vertebrae. They are slightly thicker behind than in front, flat above and below, convex from side to side in front, deeply concave behind, and slightly constricted laterally and in front. They present, on either side, two costal demi-facets, one above, near the root of the pedicle, the other below, in front of the inferior vertebral notch; these are covered with cartilage in the fresh state, and, when the vertebrae are articulated with one another, form, with the intervening intervertebral fibrocartilages, oval surfaces for the reception of the heads of the ribs. Thoracic vertebra The pedicles are directed backward and slightly upward, and the inferior vertebral notches are of large size, and deeper than in any other region of the vertebral column. The laminae are broad, thick, and imbricated – that is to say, they overlap those of subjacent vertebrae like tiles on a roof and connect with the pedicles to surround and protect the spinal cord. The intervertebral foramen is small, and circular, with two at each intervertebral level, one for the right and one for the left exiting nerve roots. The vertebral foramen is the large opening posterior to the vertebral body also known as the spinal canal. It contains and protects the spinal cord at the thoracic level. The spinous process is long, triangular on coronal section, directed obliquely downward, arising from the lamina and ending in a tuberculated extremity. These processes overlap from the fifth to the eighth, but are less oblique in direction above and below. The superior articular processes are thin plates of bone projecting upward from the junctions of the pedicles and laminae; their articular facets are practically flat, and are directed backward and a little lateralward and upward. The inferior articular processes are fused to a considerable extent with the laminae, and project but slightly beyond their lower borders; their facets are directed forward and a little medialward and downward. The transverse processes arise from the arch behind the superior articular processes and pedicles; they are thick, strong, and of considerable length, directed obliquely backward and lateralward, and each ends in a clubbed extremity, on the front of which is a small, concave surface, for articulation with the tubercle of a rib. The fifth thoracic vertebra, together with the fourth, is at the same level as the sternal angle. The human trachea divides into two main bronchi at the level of the 5th thoracic vertebra, but may also end higher or lower, depending on breathing. The ninth thoracic vertebra may have no demi-facets below. In some subjects however, it has two demi-facets on either side; when this occurs the tenth doesn't have facets but demi-facets at the upper part. The tenth thoracic vertebra has an entire articular facet (not demi-facet) on either side, which is placed partly on the lateral surface of the pedicle. It doesn't have any kind of facet below, because the following ribs only have one facet on their heads. In the eleventh thoracic vertebra the body approaches in its form and size to that of the lumbar vertebrae. The articular facets for the heads of the ribs are of medium size, and placed chiefly on the pedicles, which are thicker and stronger in this and the next vertebra than in any other part of the thoracic region. The spinous process is short, and nearly horizontal in direction. The transverse processes are very short, tuberculated at their extremities, and do not have articular facets. The twelfth thoracic vertebra has the same general characteristics as the eleventh, but may be distinguished from it by its inferior articular surfaces being convex and directed lateralward, like those of the lumbar vertebrae; by the general form of the body, laminae, and spinous process, in which it resembles the lumbar vertebrae; and by each transverse process being subdivided into three elevations, the superior, inferior, and lateral tubercles: the superior and inferior correspond to the mammillary and accessory processes of the lumbar vertebrae. Traces of similar elevations are found on the transverse processes of the tenth and eleventh thoracic vertebrae. ^The thoracic vertebrae were historically called dorsal vertebrae; cf. [1]. Especially due to the free copying of old public domain versions of Gray's Anatomy, the old term may still be encountered, however the old term is long obsolete and misleading, as the dorsum refers to the whole back and not just the thoracic part of the back."}
{"url": "https://en.m.wikipedia.org/wiki/IUCN_Red_List", "text": "The goals of the Red List are to provide scientifically based information on the status of species and subspecies at a global level, to draw attention to the magnitude and importance of threatened biodiversity, to influence national and international policy and decision-making, and to provide information to guide actions to conserve biological diversity.[2] The IUCN aims to have the category of every species re-evaluated at least every ten years, and every five years if possible. This is done in a peer reviewed manner through IUCN Species Survival Commission Specialist Groups (SSC), which are Red List Authorities (RLA) responsible for a species, group of species or specific geographic area, or in the case of BirdLife International, an entire class (Aves). The red list unit works with staff from the IUCN Global Species Programme as well as current program partners to recommend new partners or networks to join as new Red List Authorities.[3] The number of species which have been assessed for the Red List has been increasing over time.[4] As of 2023,[update] of 150,388 species surveyed, 42,108 are considered at risk of extinction because of human activity, in particular overfishing, hunting, and land development.[5][6] The percentage of species in several groups which are documented as extinct, critically endangered, endangered, or vulnerable on the 2023 IUCN Red List. The data for some groups are insufficient, so the plotted percentages can be much lower than the actual rates of vulnerability. The first attempt to create a Red Data Book for a nonspecialist public came in 1969 with The Red Book: Wildlife in Danger.[13] This book covered varies groups but was predominantly about mammals and birds, with smaller sections on reptiles, amphibians, fishes, and plants. The 2008 Red List was released on 6 October 2008 at the IUCN World Conservation Congress in Barcelona and \"confirmed an extinction crisis, with almost one in four [mammals] at risk of disappearing forever\". The study shows at least 1,141 of the 5,487 mammals on Earth are known to be threatened with extinction, and 836 are listed as Data Deficient.[16] The Red List of 2012 was released 19 July 2012 at Rio+20 Earth Summit;[17] nearly 2,000 species were added,[18] with 4 species to the extinct list, 2 to the rediscovered list.[19] The IUCN assessed a total of 63,837 species which revealed 19,817 are threatened with extinction.[20] 3,947 were described as \"critically endangered\" and 5,766 as \"endangered\", while more than 10,000 species are listed as \"vulnerable\".[21] At threat are 41% of amphibian species, 33% of reef-building corals, 30% of conifers, 25% of mammals, and 13% of birds.[20] The IUCN Red List has listed 132 species of plants and animals from India as \"Critically Endangered\".[22] Species are classified by the IUCN Red List into nine groups,[23] specified through criteria such as rate of decline, population size, area of geographic distribution, and degree of population and distribution fragmentation.[24] There is an emphasis on the acceptability of applying any criteria in the absence of high quality data including suspicion and potential future threats, \"so long as these can reasonably be supported\".: 6 [25] Extinct (EX) – beyond reasonable doubt that the species is no longer extant. Extinct in the wild (EW) – survives only in captivity, cultivation and/or outside native range, as presumed after exhaustive surveys. Conservation Dependent (CD) - a category whose documentation previously was absent - inferred to simply be an oversight - which has introduced into this article to preserve its 'Term - Definition' structure. Least concern (LC) – unlikely to become endangered or extinct in the near future. The tag of \"possibly extinct\" (PE)[26] is used by Birdlife International, the Red List Authority for birds for the IUCN Red List.[27] BirdLife International has recommended PE become an official tag for Critically Endangered species, and this has now been adopted, along with a \"Possibly Extinct in the Wild\" tag for species with populations surviving in captivity but likely to be extinct in the wild.[citation needed] 1994 IUCN Red List categories (version 2.3), used for species which have not been reassessed since 2001. In 1997, the IUCN Red List received criticism on the grounds of secrecy (or at least poor documentation) surrounding the sources of its data.[30] These allegations have led to efforts by the IUCN to improve its documentation and data quality, and to include peer reviews of taxa on the Red List.[24] The list is also open to petitions against its classifications, on the basis of documentation or criteria.[31] In the November 2016 issue of Science Advances, a research article claims there are serious inconsistencies in the way species are classified by the IUCN. The researchers contend that the IUCN's process of categorization is \"out-dated, and leaves room for improvement\", and further emphasize the importance of readily available and easy-to-include geospatial data, such as satellite and aerial imaging. Their conclusion questioned not only the IUCN's method but also the validity of where certain species fall on the List. They believe that combining geographical data can significantly increase the number of species that need to be reclassified to a higher risk category.[33] ^CITES (Convention on International Trade in Endangered Species of Wild Fauna and Flora), Joint Meeting of the Animals and Plants Committees, Shepherdstown (United States of America), 7–9 December 2000, Retrieved 14 November 2012. ^Fitter, Richard; Fitter, Maisie, eds. (1987). The Road to Extinction: Problems of Categorizing the Status of Taxa Threatened with Extinction. Gland: International Union for Conservation of Nature and Natural Resources. ISBN2880329299. Sharrock, S. and Jones, M. 2009. Conserving Europe's threatened plants – Report on the lack of a European Red List and the creation of a consolidated list of the threatened plants of Europe. Retrieved 2011-03-23."}
{"url": "https://en.m.wikipedia.org/wiki/Critically_Endangered", "text": "The IUCN Red List provides the public with information regarding the conservation status of animal, fungi, and plant species.[3] It divides various species into seven different categories of conservation that are based on habitat range, population size, habitat, threats, etc. Each category representing a different level of global extinction risk. Species that are considered to be Critically Endangered are placed within the \"threatened\" category.[4] As the IUCN Red List does not consider a species extinct until extensive, targeted surveys have been conducted, species that are possibly extinct are still listed as Critically Endangered. IUCN maintains a list[5] of \"possibly extinct\" and \"possibly extinct in the wild\" species, modelled on categories used by BirdLife International to categorize these taxa. To be defined as Critically Endangered in the Red List, a species must meet any of the following criteria (A–E) (\"3G/10Y\" signifies three generations or ten years—whichever is longer—over a maximum of 100 years; \"MI\" signifies Mature Individuals):[6] A: Population Size Reduction The rate of reduction is measured either over a 10 year span or across three different generations within that species. The cause for this decline must also be known. If the reasons for population reduction no longer occur and can be reversed, the population needs to have been reduced by at least 90% If not, then the population needs to have been reduced by at least 80% B: Reduction Across a Geographic Range This reduction must occur over less than 100 km2 OR the area of occupancy is less than 10 km2. The Beluga sturgeon (Huso huso) is an example of a critically endangered species. Their wild populations have been reduced due to overharvesting for its caviar. The current extinction crisis is witnessing extinction rates that are occurring at a faster rate than that of the natural extinction rate. It has largely been credited towards human impacts on climate change and the loss of biodiversity. This is along with natural forces that may create stress on the species or cause an animal population to become extinct.[7] Currently the biggest reason for species extinction is human interaction that results in habitat loss.[8] Species rely on their habitat for the resources needed for their survival. If the habitat becomes destroyed, the population will see a decline in their numbers. Activities that cause loss of habitat include pollution, urbanization, and agriculture. Another reason for plants and animals to become endangered is due to the introduction of invasive species. Invasive species invade and exploit a new habitat for its natural resources as a method to outcompete the native organisms, eventually taking over the habitat. This can lead to either the native species' extinction or causing them to become endangered, which also eventually causes extinction. Plants and animals may also go extinct due to disease. The introduction of a disease into a new habitat can cause it to spread amongst the native species. Due to their lack of familiarity with the disease or little resistance, the native species can die off."}
{"url": "https://en.m.wikipedia.org/wiki/Paleogene", "text": "This period consists of the Paleocene, Eocene, and Oligocene epochs. The end of the Paleocene (56 Mya) was marked by the Paleocene–Eocene Thermal Maximum, one of the most significant periods of global change during the Cenozoic, which changed oceanic and atmospheric circulation and resulted in the extinction of numerous deep-sea benthic foraminifera and on land, a major extinction of mammals. The term \"Paleogene System\" applies to the rocks deposited during the Paleogene Period. Contents The global climate of the Palaeogene began with the brief but intense \"impact winter\" caused by the Chicxulub impact. This cold period was terminated by an abrupt warming. After temperatures stabilised, the steady cooling and drying of the Late Cretaceous-Early Palaeogene Cool Interval (LKEPCI) that had spanned the last two stages of the Late Cretaceous continued.[9] About 62.2 Ma, the Latest Danian Event, a hyperthermal event, took place.[10][11][12] About 59 Ma, the LKEPCI was brought to an end by the Thanetian Thermal Event, a change from the relative cool of the Early and Middle Palaeocene and the beginning of an intense supergreenhouse effect.[9] From about 56 to 48 Mya, annual air temperatures over land and at mid-latitude averaged about 23–29 °C (± 4.7 °C), which is 5–10 °C warmer than most previous estimates.[13][14][15] For comparison, this was 10 to 15 °C greater than the current annual mean temperatures in these areas.[15] At the Palaeocene-Eocene boundary occurred the Paleocene–Eocene Thermal Maximum (PETM),[16] one of the warmest times of the Phanerozoic eon, during which global mean surface temperatures increased to 31.6.[17] It was followed by the less severe Eocene Thermal Maximum 2 (ETM2) about 53.69 Ma.[18] Eocene Thermal Maximum 3 (ETM3) occurred about 53 Ma. The Early Eocene Climatic Optimum was brought to an end by the Azolla event, a change of climate about 48.5 Mya, believed to have been caused by a proliferation of aquatic ferns from the genus Azolla, resulting in the sequestering of large amounts of carbon dioxide by those plants. From this time until about 34 Mya, there was a slow cooling trend known as the Middle-Late Eocene Cooling (MLEC).[9] Approximately 41.5 Ma, this cooling was interrupted temporarily by the Middle Eocene Climatic Optimum (MECO).[19] Then, about 39.4 Mya, a temperature decrease termed the Late Eocene Cool Event (LECE) is detected in the oxygen isotope record.[9] A rapid decrease of global temperatures and formation of continental glaciers on Antarctica marked the end of the Eocene.[20] This sudden cooling was caused partly by the formation of the Antarctic Circumpolar Current,[21] which significantly lowered oceanic water temperatures.[22] During the earliest Oligocene occurred the Early Oligocene Glacial Maximum (Oi1), which lasted for about 200 thousand years.[23] After Oi1, global mean surface temperature continued to decrease gradually during the Rupelian Age. [9] Another major cooling event occurred at the end of the Rupelian; its most likely cause was extreme biological productivity in the Southern Ocean fostered by tectonic reorganisation of ocean currents and an influx of nutrients from Antarctica.[24] In the Late Oligocene, global temperatures began to warm slightly, though they continued to be significantly lower than during the previous epochs of the Palaeogene and polar ice remained.[9] During the Paleogene, the continents continued to drift closer to their current positions. India was in the process of colliding with Asia, forming the Himalayas. The Atlantic Ocean continued to widen by a few centimeters each year. Africa was moving north to collide with Europe and form the Mediterranean Sea, while South America was moving closer to North America (they would later connect at the Isthmus of Panama). Inland seas retreated from North America early in the period. Australia had also separated from Antarctica and was drifting toward Southeast Asia. The 1.2 Myear cycle of obliquity amplitude modulation governed eustatic sea level changes on shorter timescales, with periods of low amplitude coinciding with intervals of low sea levels and vice versa.[25] Tropical taxa diversified faster than those at higher latitudes after the Cretaceous–Paleogene extinction event, resulting in the development of a significant latitudinal diversity gradient.[26]Mammals began a rapid diversification during this period. After the Cretaceous–Paleogene extinction event, which saw the demise of the non-avian dinosaurs, mammals began to evolve from a few small and generalized forms into most of the modern varieties we see presently. Some of these mammals evolved into large forms that dominated the land, while others became capable of living in marine, specialized terrestrial, and airborne environments. Those that adapted to the oceans became modern cetaceans, while those that adapted to trees became primates, the group to which humans belong. Birds, extant dinosaurs which were already well established by the end of the Cretaceous, also experienced adaptive radiation as they took over the skies left empty by the now extinct pterosaurs. Some flightless birds such as penguins, ratites, and terror birds also filled niches left by the hesperornithes and other extinct dinosaurs. Pronounced cooling in the Oligocene resulted in a massive floral shift, and many extant modern plants arose during this time. Grasses and herbs, such as Artemisia, began to proliferate, at the expense of tropical plants, which began to decrease. Conifer forests developed in mountainous areas. This cooling trend continued, with major fluctuation, until the end of the Pleistocene period.[27] This evidence for this floral shift is found in the palynological record.[28]"}
{"url": "https://en.m.wikipedia.org/wiki/Laurasiatheria", "text": "Contents The name of this superorder derives from the theory that this group of mammals originated on the supercontinent of Laurasia.[1] In contrast, extinct primitive mammals called Gondwanatheria existed in the supercontinent of Gondwana. Phylogenetic position of laurasiatherians (in green) among placentals in a genus-level molecular phylogeny of 116 extant mammals inferred from the gene tree information of 14,509 coding DNA sequences.[8] The other major clades are colored: marsupials (magenta), xenarthrans (orange), afrotherians (red), and euarchontoglires (blue). Uncertainty still exists regarding the phylogenetic tree for extant laurasiatherians, primarily due to disagreement about the placement of orders Chiroptera and Perissodactyla. Based on morphological grounds, bats (order Chiroptera) had long been classified in the superorder Archonta (e.g. along with primates, treeshrews and the gliding colugos) until genetic research instead showed their kinship with the other laurasiatheres.[9] The studies conflicted in terms of the exact placement of Chiroptera, however, with it being linked most closely to groups such as order Eulipotyphla in the clade Insectiphillia. Two 2013 studies retrieve that bats, carnivorans and euungulates form a clade Scrotifera, indicating that Eulipotyphla might be the sister group to all other Laurasiatheria taxa.[10][11] Laurasiatheria is also posited to include several extinct orders and superorders. At least some of these are considered wastebasket taxa, historically lumping together several lineages based on superficial attributes and assumed relations to modern mammals. In some cases, these orders have turned out to either be paraphyletic assemblages, or to be composed of mammals now understood not to be laurasiatheres at all. ^Burger, Benjamin J., (2015.) \"The systematic position of the saber-toothed and horned giants of the Eocene: the Uintatheres (order Dinocerata)\", Utah State University Uintah Basin Campus, Vernal, Utah"}
{"url": "https://en.wikipedia.org/wiki/Whale", "text": "Whales are fully aquatic, open-ocean animals: they can feed, mate, give birth, suckle and raise their young at sea. Whales range in size from the 2.6 metres (8.5 ft) and 135 kilograms (298 lb) dwarf sperm whale to the 29.9 metres (98 ft) and 190 tonnes (210 short tons) blue whale, which is the largest known animal that has ever lived. The sperm whale is the largest toothed predator on Earth. Several whale species exhibit sexual dimorphism, in that the females are larger than males. Baleen whales have no teeth; instead, they have plates of baleen, fringe-like structures that enable them to expel the huge mouthfuls of water they take in while retaining the krill and plankton they feed on. Because their heads are enormous—making up as much as 40% of their total body mass—and they have throat pleats that enable them to expand their mouths, they are able to take huge quantities of water into their mouth at a time. Baleen whales also have a well-developed sense of smell. Toothed whales, in contrast, have conical teeth adapted to catching fish or squid. They also have such keen hearing—whether above or below the surface of the water—that some can survive even if they are blind. Some species, such as sperm whales, are particularly well adapted for diving to great depths to catch squid and other favoured prey. Whales evolved from land-living mammals, and must regularly surface to breathe air, although they can remain underwater for long periods of time. Some species, such as the sperm whale, can stay underwater for up to 90 minutes.[2] They have blowholes (modified nostrils) located on top of their heads, through which air is taken in and expelled. They are warm-blooded, and have a layer of fat, or blubber, under the skin. With streamlined fusiform bodies and two limbs that are modified into flippers, whales can travel at speeds of up to 20 knots, though they are not as flexible or agile as seals. Whales produce a great variety of vocalizations, notably the extended songs of the humpback whale. Although whales are widespread, most species prefer the colder waters of the Northern and Southern Hemispheres and migrate to the equator to give birth. Species such as humpbacks and blue whales are capable of travelling thousands of miles without feeding. Males typically mate with multiple females every year, but females only mate every two to three years. Calves are typically born in the spring and summer; females bear all the responsibility for raising them. Mothers in some species fast and nurse their young for one to two years. Once relentlessly hunted for their products, whales are now protected by international law. The North Atlantic right whales nearly became extinct in the twentieth century, with a population low of 450, and the North Pacific grey whale population is ranked Critically Endangered by the IUCN. Besides the threat from whalers, they also face threats from bycatch and marine pollution. The meat, blubber and baleen of whales have traditionally been used by indigenous peoples of the Arctic. Whales have been depicted in various cultures worldwide, notably by the Inuit and the coastal peoples of Vietnam and Ghana, who sometimes hold whale funerals. Whales occasionally feature in literature and film. A famous example is the great white whale in Herman Melville's novel Moby-Dick. Small whales, such as belugas, are sometimes kept in captivity and trained to perform tricks, but breeding success has been poor and the animals often die within a few months of capture. Whale watching has become a form of tourism around the world. The term \"whale\" is sometimes used interchangeably with dolphins and porpoises, acting as a synonym for Cetacea. Six species of dolphins have the word \"whale\" in their name, collectively known as blackfish: the orca, or killer whale, the melon-headed whale, the pygmy killer whale, the false killer whale, and the two species of pilot whales, all of which are classified under the family Delphinidae (oceanic dolphins).[6] Each species has a different reason for it, for example, the killer whale was named \"Ballena asesina\" 'killer whale' by Spanish sailors.[7] The term \"Great Whales\" covers those currently regulated by the International Whaling Commission:[8] the Odontoceti family Physeteridae (sperm whales); and the Mysticeti families Balaenidae (right and bowhead whales), Eschrichtiidae (grey whales), and some of the Balaenopteridae (Minke, Bryde's, Sei, Blue and Fin; not Eden's and Omura's whales).[9] Cetaceans are divided into two parvorders. The larger parvorder, Mysticeti (baleen whales), is characterized by the presence of baleen, a sieve-like structure in the upper jaw made of keratin, which it uses to filter plankton, among others, from the water. Odontocetes (toothed whales) are characterized by bearing sharp teeth for hunting, as opposed to their counterparts' baleen.[10] Cetaceans and artiodactyls now are classified under the order Cetartiodactyla, often still referred to as Artiodactyla, which includes both whales and hippopotamuses. The hippopotamus and pygmy hippopotamus are the whales' closest terrestrial living relatives.[11] Mysticetes Mysticetes are also known as baleen whales. They have a pair of blowholes side by side and lack teeth; instead they have baleen plates which form a sieve-like structure in the upper jaw made of keratin, which they use to filter plankton from the water. Some whales, such as the humpback, reside in the polar regions where they feed on a reliable source of schooling fish and krill.[12] These animals rely on their well-developed flippers and tail fin to propel themselves through the water; they swim by moving their fore-flippers and tail fin up and down. Whale ribs loosely articulate with their thoracic vertebrae at the proximal end, but do not form a rigid rib cage. This adaptation allows the chest to compress during deep dives as the pressure increases.[13] Mysticetes consist of four families: rorquals (balaenopterids), cetotheriids, right whales (balaenids), and grey whales (eschrichtiids). The main difference between each family of mysticete is in their feeding adaptations and subsequent behaviour. Balaenopterids are the rorquals. These animals, along with the cetotheriids, rely on their throat pleats to gulp large amounts of water while feeding. The throat pleats extend from the mouth to the navel and allow the mouth to expand to a large volume for more efficient capture of the small animals they feed on. Balaenopterids consist of two genera and eight species.[14] Balaenids are the right whales. These animals have very large heads, which can make up as much as 40% of their body mass, and much of the head is the mouth. This allows them to take in large amounts of water into their mouths, letting them feed more effectively.[15] Eschrichtiids have one living member: the grey whale. They are bottom feeders, mainly eating crustaceans and benthic invertebrates. They feed by turning on their sides and taking in water mixed with sediment, which is then expelled through the baleen, leaving their prey trapped inside. This is an efficient method of hunting, in which the whale has no major competitors.[16] Odontocetes Odontocetes are known as toothed whales; they have teeth and only one blowhole. They rely on their well-developed sonar to find their way in the water. Toothed whales send out ultrasonic clicks using the melon. Sound waves travel through the water. Upon striking an object in the water, the sound waves bounce back at the whale. These vibrations are received through fatty tissues in the jaw, which is then rerouted into the ear-bone and into the brain where the vibrations are interpreted.[17] All toothed whales are opportunistic, meaning they will eat anything they can fit in their throat because they are unable to chew. These animals rely on their well-developed flippers and tail fin to propel themselves through the water; they swim by moving their fore-flippers and tail fin up and down. Whale ribs loosely articulate with their thoracic vertebrae at the proximal end, but they do not form a rigid rib cage. This adaptation allows the chest to compress during deep dives as opposed to resisting the force of water pressure.[13] Excluding dolphins and porpoises, odontocetes consist of four families: belugas and narwhals (monodontids), sperm whales (physeterids), dwarf and pygmy sperm whales (kogiids), and beaked whales (ziphiids).[6] The differences between families of odontocetes include size, feeding adaptations and distribution. Monodontids consist of two species: the beluga and the narwhal. They both reside in the frigid arctic and both have large amounts of blubber. Belugas, being white, hunt in large pods near the surface and around pack ice, their coloration acting as camouflage. Narwhals, being black, hunt in large pods in the aphotic zone, but their underbelly still remains white to remain camouflaged when something is looking directly up or down at them. They have no dorsal fin to prevent collision with pack ice.[18] Physeterids and Kogiids consist of sperm whales. Sperm whales consist the largest and smallest odontocetes, and spend a large portion of their life hunting squid. P. macrocephalus spends most of its life in search of squid in the depths; these animals do not require any degree of light at all, in fact, blind sperm whales have been caught in perfect health. The behaviour of Kogiids remains largely unknown, but, due to their small lungs, they are thought to hunt in the photic zone.[19] Ziphiids consist of 22 species of beaked whale. These vary from size, to coloration, to distribution, but they all share a similar hunting style. They use a suction technique, aided by a pair of grooves on the underside of their head, not unlike the throat pleats on the rorquals, to feed.[20] Evolution Whales are descendants of land-dwelling mammals of the artiodactylorder (even-toed ungulates). They are related to the Indohyus, an extinct chevrotain-like ungulate, from which they split approximately 48 million years ago.[21][22] Primitive cetaceans, or archaeocetes, first took to the sea approximately 49 million years ago and became fully aquatic 5–10 million years later. What defines an archaeocete is the presence of anatomical features exclusive to cetaceans, alongside other primitive features not found in modern cetaceans, such as visible legs or asymmetrical teeth.[23][24][25][11] Their features became adapted for living in the marine environment. Major anatomical changes included their hearing set-up that channeled vibrations from the jaw to the earbone (Ambulocetus 49 mya), a streamlined body and the growth of flukes on the tail (Protocetus 43 mya), the migration of the nostrils toward the top of the cranium (blowholes), and the modification of the forelimbs into flippers (Basilosaurus 35 mya), and the shrinking and eventual disappearance of the hind limbs (the first odontocetes and mysticetes 34 mya).[26][27][28] Whale morphology shows several examples of convergent evolution, the most obvious being the streamlined fish-like body shape.[29] Other examples include the use of echolocation for hunting in low light conditions — which is the same hearing adaptation used by bats — and, in the rorqual whales, jaw adaptations, similar to those found in pelicans, that enable engulfment feeding.[30] Today, the closest living relatives of cetaceans are the hippopotamuses; these share a semi-aquatic ancestor that branched off from other artiodactyls some 60 mya.[11] Around 40 mya, a common ancestor between the two branched off into cetacea and anthracotheres; nearly all anthracotheres became extinct at the end of the Pleistocene 2.5 mya, eventually leaving only one surviving lineage – the hippopotamus.[31] Whales split into two separate parvorders around 34 mya – the baleen whales (Mysticetes) and the toothed whales (Odontocetes).[32][33][34] Biology Anatomy Whales have torpedo-shaped bodies with non-flexible necks, limbs modified into flippers, non-existent external ear flaps, a large tail fin, and flat heads (with the exception of monodontids and ziphiids). Whale skulls have small eye orbits, long snouts (with the exception of monodontids and ziphiids) and eyes placed on the sides of its head. Whales range in size from the 2.6-metre (8.5 ft) and 135-kilogram (298 lb) dwarf sperm whale to the 34-metre (112 ft) and 190-metric-ton (210-short-ton) blue whale. Overall, they tend to dwarf other cetartiodactyls; the blue whale is the largest creature on Earth. Several species have female-biased sexual dimorphism, with the females being larger than the males. One exception is with the sperm whale, which has males larger than the females.[35][36] Odontocetes, such as the sperm whale, possess teeth with cementum cells overlying dentine cells. Unlike human teeth, which are composed mostly of enamel on the portion of the tooth outside of the gum, whale teeth have cementum outside the gum. Only in larger whales, where the cementum is worn away on the tip of the tooth, does enamel show. Mysticetes have large whalebone, as opposed to teeth, made of keratin. Mysticetes have two blowholes, whereas Odontocetes contain only one.[37] Breathing involves expelling stale air from the blowhole, forming an upward, steamy spout, followed by inhaling fresh air into the lungs; a humpback whale's lungs can hold about 5,000 litres (1,300 US gal) of air. Spout shapes differ among species, which facilitates identification.[38][39] All whales have a thick layer of blubber. In species that live near the poles, the blubber can be as thick as 11 inches (28 cm). This blubber can help with buoyancy (which is helpful for a 100-ton whale), protection to some extent as predators would have a hard time getting through a thick layer of fat, and energy for fasting when migrating to the equator; the primary usage for blubber is insulation from the harsh climate. It can constitute as much as 50% of a whale's body weight. Calves are born with only a thin layer of blubber, but some species compensate for this with thick lanugos.[40][41] Whales have a two- to three-chambered stomach that is similar in structure to those of terrestrial carnivores. Mysticetes contain a proventriculus as an extension of the oesophagus; this contains stones that grind up food. They also have fundic and pyloric chambers.[42] Locomotion Whales have two flippers on the front, and a tail fin. These flippers contain four digits. Although whales do not possess fully developed hind limbs, some, such as the sperm whale and bowhead whale, possess discrete rudimentary appendages, which may contain feet and digits. Whales are fast swimmers in comparison to seals, which typically cruise at 5–15 kn, or 9–28 kilometres per hour (5.6–17.4 mph); the fin whale, in comparison, can travel at speeds up to 47 kilometres per hour (29 mph) and the sperm whale can reach speeds of 35 kilometres per hour (22 mph). The fusing of the neck vertebrae, while increasing stability when swimming at high speeds, decreases flexibility; whales are unable to turn their heads. When swimming, whales rely on their tail fin to propel them through the water. Flipper movement is continuous. Whales swim by moving their tail fin and lower body up and down, propelling themselves through vertical movement, while their flippers are mainly used for steering. Some species log out of the water, which may allow them to travel faster. Their skeletal anatomy allows them to be fast swimmers. Most species have a dorsal fin.[28][43] Whales are adapted for diving to great depths. In addition to their streamlined bodies, they can slow their heart rate to conserve oxygen; blood is rerouted from tissue tolerant of water pressure to the heart and brain among other organs; haemoglobin and myoglobin store oxygen in body tissue; and they have twice the concentration of myoglobin than haemoglobin. Before going on long dives, many whales exhibit a behaviour known as sounding; they stay close to the surface for a series of short, shallow dives while building their oxygen reserves, and then make a sounding dive.[13][44] Senses The whale ear has specific adaptations to the marine environment. In humans, the middle ear works as an impedance equalizer between the outside air's low impedance and the cochlear fluid's high impedance. In whales, and other marine mammals, there is no great difference between the outer and inner environments. Instead of sound passing through the outer ear to the middle ear, whales receive sound through the throat, from which it passes through a low-impedance fat-filled cavity to the inner ear.[45] The whale ear is acoustically isolated from the skull by air-filled sinus pockets, which allow for greater directional hearing underwater.[46] Odontocetes send out high-frequency clicks from an organ known as a melon. This melon consists of fat, and the skull of any such creature containing a melon will have a large depression. The melon size varies between species, the bigger the more dependent they are on it. A beaked whale for example has a small bulge sitting on top of its skull, whereas a sperm whale's head is filled up mainly with the melon.[47][48][49][50] The whale eye is relatively small for its size, yet they do retain a good degree of eyesight. As well as this, the eyes of a whale are placed on the sides of its head, so their vision consists of two fields, rather than a binocular view like humans have. When belugas surface, their lens and cornea correct the nearsightedness that results from the refraction of light; they contain both rod and cone cells, meaning they can see in both dim and bright light, but they have far more rod cells than they do cone cells. Whales do, however, lack short wavelength sensitive visual pigments in their cone cells indicating a more limited capacity for colour vision than most mammals.[51] Most whales have slightly flattened eyeballs, enlarged pupils (which shrink as they surface to prevent damage), slightly flattened corneas and a tapetum lucidum; these adaptations allow for large amounts of light to pass through the eye and, therefore, a very clear image of the surrounding area. They also have glands on the eyelids and outer corneal layer that act as protection for the cornea.[52][48] Whales are not thought to have a good sense of taste, as their taste buds are atrophied or missing altogether. However, some toothed whales have preferences between different kinds of fish, indicating some sort of attachment to taste. The presence of the Jacobson's organ indicates that whales can smell food once inside their mouth, which might be similar to the sensation of taste.[54] Communication Whale vocalization is likely to serve several purposes. Some species, such as the humpback whale, communicate using melodic sounds, known as whale song. These sounds may be extremely loud, depending on the species. Humpback whales only have been heard making clicks, while toothed whales use sonar that may generate up to 20,000 watts of sound (+73 dBm or +43 dBw)[55] and be heard for many miles. Captive whales have occasionally been known to mimic human speech. Scientists have suggested this indicates a strong desire on behalf of the whales to communicate with humans, as whales have a very different vocal mechanism, so imitating human speech likely takes considerable effort.[56] Whales emit two distinct kinds of acoustic signals, which are called whistles and clicks:[57] Clicks are quick broadband burst pulses, used for sonar, although some lower-frequency broadband vocalizations may serve a non-echolocative purpose such as communication; for example, the pulsed calls of belugas. Pulses in a click train are emitted at intervals of ≈35–50 milliseconds, and in general these inter-click intervals are slightly greater than the round-trip time of sound to the target. Whistles are narrow-band frequency modulated (FM) signals, used for communicative purposes, such as contact calls. Intelligence Whales are known to teach, learn, cooperate, scheme, and grieve.[58] The neocortex of many species of whale is home to elongated spindle neurons that, prior to 2007, were known only in hominids.[59] In humans, these cells are involved in social conduct, emotions, judgement, and theory of mind. Whale spindle neurons are found in areas of the brain that are homologous to where they are found in humans, suggesting that they perform a similar function.[60] Bubble net feeding Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain-to-body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately the ⅔ or ¾ exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalisation quotient that can be used as another indication of animal intelligence. Sperm whales have the largest brain mass of any animal on Earth, averaging 8,000 cubic centimetres (490 in3) and 7.8 kilograms (17 lb) in mature males, in comparison to the average human brain which averages 1,450 cubic centimetres (88 in3) in mature males.[61] The brain-to-body mass ratio in some odontocetes, such as belugas and narwhals, is second only to humans.[62] Small whales are known to engage in complex play behaviour, which includes such things as producing stable underwater toroidal air-core vortex rings or \"bubble rings\". There are two main methods of bubble ring production: rapid puffing of a burst of air into the water and allowing it to rise to the surface, forming a ring, or swimming repeatedly in a circle and then stopping to inject air into the helical vortex currents thus formed. They also appear to enjoy biting the vortex-rings, so that they burst into many separate bubbles and then rise quickly to the surface.[63] Some believe this is a means of communication.[64] Whales are also known to produce bubble-nets for the purpose of foraging.[65] Larger whales are also thought, to some degree, to engage in play. The southern right whale, for example, elevates their tail fluke above the water, remaining in the same position for a considerable amount of time. This is known as \"sailing\". It appears to be a form of play and is most commonly seen off the coast of Argentina and South Africa. Humpback whales, among others, are also known to display this behaviour.[66] Life cycle Whales are fully aquatic creatures, which means that birth and courtship behaviours are very different from terrestrial and semi-aquatic creatures. Since they are unable to go onto land to calve, they deliver the baby with the fetus positioned for tail-first delivery. This prevents the baby from drowning either upon or during delivery. To feed the newborn, whales, being aquatic, must squirt the milk into the mouth of the calf. Nursing can occur while the mother whale is in the vertical or horizontal position. While nursing in the vertical position, a mother whale may sometimes rest with her tail flukes remaining stationary above the water. This position with the flukes above the water is known as \"whale-tail-sailing.\" Not all whale-tail-sailing includes nursing of the young, as whales have also been observed tail-sailing while no calves were present.[67] Being mammals, they have mammary glands used for nursing calves; they are weaned off at about 11 months of age. This milk contains high amounts of fat which is meant to hasten the development of blubber; it contains so much fat that it has the consistency of toothpaste.[68] As with humans, females whales typically deliver a single calf. The whale pregnancy/ gestation period lasts longer than the typical 9 month gestation period for humans. The whale gestation period is about 12 months. Once born, the absolute dependency of the calf upon the mother lasts from one to two years. Sexual maturity is achieved at around seven to ten years of age. The length of the developmental phases of a whale's early life varies between different whale species.[12] As with humans, the whale mode of reproduction typically produces but one offspring approximately once each year. While whales have fewer offspring over time than most species, the survival probability for each calf is also greater than for most other species. Female whales are referred to as \"cows.\" Female cows assume full responsibility for the care and training of their young. Male whales, referred to as \"bulls,\" typically play no role in the process of calf rearing. Most baleen whales reside at the poles. So, to prevent the unborn baleen whale calves from dying of frostbite, the baleen mother must migrate to warmer calving/mating grounds. They will then stay there for a matter of months until the calf has developed enough blubber to survive the bitter temperatures of the poles. Until then, the baleen calves will feed on the mother's fatty milk.[69] With the exception of the humpback whale, it is largely unknown when whales migrate. Most will travel from the Arctic or Antarctic into the tropics to mate, calve, and raise during the winter and spring; they will migrate back to the poles in the warmer summer months so the calf can continue growing while the mother can continue eating, as they fast in the breeding grounds. One exception to this is the southern right whale, which migrates to Patagonia and western New Zealand to calve; both are well out of the tropic zone.[70] Sleep Unlike most animals, whales are conscious breathers. All mammals sleep, but whales cannot afford to become unconscious for long because they may drown. While knowledge of sleep in wild cetaceans is limited, toothed cetaceans in captivity have been recorded to sleep with one side of their brain at a time, so that they may swim, breathe consciously, and avoid both predators and social contact during their period of rest.[71] A 2008 study found that sperm whales sleep in vertical postures just under the surface in passive shallow 'drift-dives', generally during the day, during which whales do not respond to passing vessels unless they are in contact, leading to the suggestion that whales possibly sleep during such dives.[71] Ecology Foraging and predation All whales are carnivorous and predatory. Odontocetes, as a whole, mostly feed on fish and cephalopods, and then followed by crustaceans and bivalves. All species are generalist and opportunistic feeders. Mysticetes, as a whole, mostly feed on krill and plankton, followed by crustaceans and other invertebrates. A few are specialists. Examples include the blue whale, which eats almost exclusively krill, the minke whale, which eats mainly schooling fish, the sperm whale, which specialize on squid, and the grey whale which feed on bottom-dwelling invertebrates.[14][72][73] The elaborate baleen \"teeth\" of filter-feeding species, mysticetes, allow them to remove water before they swallow their planktonic food by using the teeth as a sieve.[68] Usually, whales hunt solitarily, but they do sometimes hunt cooperatively in small groups. The former behaviour is typical when hunting non-schooling fish, slow-moving or immobile invertebrates or endothermic prey. When large amounts of prey are available, whales such as certain mysticetes hunt cooperatively in small groups.[74] Some cetaceans may forage with other kinds of animals, such as other species of whales or certain species of pinnipeds.[75][76] Large whales, such as mysticetes, are not usually subject to predation, but smaller whales, such as monodontids or ziphiids, are. These species are preyed on by the orca. To subdue and kill whales, orcas continuously ram them with their heads; this can sometimes kill bowhead whales, or severely injure them. Other times they corral the narwhals or belugas before striking. They are typically hunted by groups of 10 or fewer orcas, but they are seldom attacked by an individual. Calves are more commonly taken by orcas, but adults can be targeted as well.[77] These small whales are also targeted by terrestrial and pagophilic predators. The polar bear is well adapted for hunting Arctic whales and calves. Bears are known to use sit-and-wait tactics as well as active stalking and pursuit of prey on ice or water. Whales lessen the chance of predation by gathering in groups. This however means less room around the breathing hole as the ice slowly closes the gap. When out at sea, whales dive out of the reach of surface-hunting orcas. Polar bear attacks on belugas and narwhals are usually successful in winter, but rarely inflict any damage in summer.[78] Whale pump \"Whale pump\" – the role played by whales in recycling ocean nutrients.[79] A 2010 study considered whales to be a positive influence to the productivity of ocean fisheries, in what has been termed a \"whale pump.\" Whales carry nutrients such as nitrogen from the depths back to the surface. This functions as an upward biological pump, reversing an earlier presumption that whales accelerate the loss of nutrients to the bottom. This nitrogen input in the Gulf of Maine is \"more than the input of all rivers combined\" emptying into the gulf, some 23,000 metric tons (25,000 short tons) each year.[80][79]Whales defecate at the ocean's surface; their excrement is important for fisheries because it is rich in iron and nitrogen. The whale faeces are liquid and instead of sinking, they stay at the surface where phytoplankton feed off it.[79][81][82] Whale fall Upon death, whale carcasses fall to the deep ocean and provide a substantial habitat for marine life. Evidence of whale falls in present-day and fossil records shows that deep sea whale falls support a rich assemblage of creatures, with a global diversity of 407 species, comparable to other neritic biodiversity hotspots, such as cold seeps and hydrothermal vents.[83] Deterioration of whale carcasses happens though a series of three stages. Initially, moving organisms such as sharks and hagfish, scavenge the soft tissues at a rapid rate over a period of months, and as long as two years. This is followed by the colonization of bones and surrounding sediments (which contain organic matter) by enrichment opportunists, such as crustaceans and polychaetes, throughout a period of years. Finally, sulfophilic bacteria reduce the bones releasing hydrogen sulfide enabling the growth of chemoautotrophic organisms, which in turn, support other organisms such as mussels, clams, limpets, and sea snails. This stage may last for decades and supports a rich assemblage of species, averaging 185 species per site.[83][84] Relationship with humans Whaling Whaling by humans has existed since the Stone Age. Ancient whalers used harpoons to spear the bigger animals from boats out at sea.[85] People from Norway and Japan started hunting whales around 2000 B.C.[86] Whales are typically hunted for their meat and blubber by aboriginal groups; they used baleen for baskets or roofing, and made tools and masks out of bones.[86] The Inuit hunted whales in the Arctic Ocean.[86] The Basques started whaling as early as the 11th century, sailing as far as Newfoundland in the 16th century in search of right whales.[87][88] 18th- and 19th-century whalers hunted whales mainly for their oil, which was used as lamp fuel and a lubricant, baleen or whalebone, which was used for items such as corsets and skirt hoops,[86] and ambergris, which was used as a fixative for perfumes. The most successful whaling nations at this time were the Netherlands, Japan, and the United States.[89] Commercial whaling was historically important as an industry well throughout the 17th, 18th and 19th centuries. Whaling was at that time a sizeable European industry with ships from Britain, France, Spain, Denmark, the Netherlands and Germany, sometimes collaborating to hunt whales in the Arctic, sometimes in competition leading even to war.[90] By the early 1790s, whalers, namely the Americans and Australians, focused efforts in the South Pacific where they mainly hunted sperm whales and right whales, with catches of up to 39,000 right whales by Americans alone.[87][91] By 1853, US profits reached US$11,000,000 (£6.5m), equivalent to US$348,000,000 (£230m) today, the most profitable year for the American whaling industry.[92] Commonly exploited species included North Atlantic right whales, sperm whales, which were mainly hunted by Americans, bowhead whales, which were mainly hunted by the Dutch, common minke whales, blue whales, and grey whales. The scale of whale harvesting decreased substantially after 1982 when the International Whaling Commission (IWC) placed a moratorium which set a catch limit for each country, excluding aboriginal groups until 2004.[93] Current whaling nations are Norway, Iceland, and Japan, despite their joining to the IWC, as well as the aboriginal communities of Siberia, Alaska, and northern Canada.[94] Subsistence hunters typically use whale products for themselves and depend on them for survival. National and international authorities have given special treatment to aboriginal hunters since their methods of hunting are seen as less destructive and wasteful. This distinction is being questioned as these aboriginal groups are using more modern weaponry and mechanized transport to hunt with, and are selling whale products in the marketplace. Some anthropologists argue that the term \"subsistence\" should also apply to these cash-based exchanges as long as they take place within local production and consumption.[95][96][97] In 1946, the IWC placed a moratorium, limiting the annual whale catch. Since then, yearly profits for these \"subsistence\" hunters have been close to US$31 million (£20m) per year.[93] Other threats Whales can also be threatened by humans more indirectly. They are unintentionally caught in fishing nets by commercial fisheries as bycatch and accidentally swallow fishing hooks. Gillnetting and Seine netting is a significant cause of mortality in whales and other marine mammals.[98] Species commonly entangled include beaked whales. Whales are also affected by marine pollution. High levels of organic chemicals accumulate in these animals since they are high in the food chain. They have large reserves of blubber, more so for toothed whales as they are higher up the food chain than baleen whales. Lactating mothers can pass the toxins on to their young. These pollutants can cause gastrointestinal cancers and greater vulnerability to infectious diseases.[99] They can also be poisoned by swallowing litter, such as plastic bags.[100] Advanced military sonar harms whales. Sonar interferes with the basic biological functions of whales—such as feeding and mating—by impacting their ability to echolocate. Whales swim in response to sonar and sometimes experience decompression sickness due to rapid changes in depth. Mass strandings have been triggered by sonar activity, resulting in injury or death.[101][102][103][104] Whales are sometimes killed or injured during collisions with ships or boats. This is considered to be a significant threat to vulnerable whale populations such as the North Atlantic right whale, whose total population numbers less than 500.[105] Conservation Whaling decreased substantially after 1946 when, in response to the steep decline in whale populations, the International Whaling Commission placed a moratorium which set a catch limit for each country; this excluded aboriginal groups up until 2004.[89][96][106][107] As of 2015, aboriginal communities are allowed to take 280 bowhead whales off Alaska and two from the western coast of Greenland, 620 grey whales off Washington state, three common minke whales off the eastern coast of Greenland and 178 on their western coast, 10 fin whales from the west coast of Greenland, nine humpback whales from the west coast of Greenland and 20 off St. Vincent and the Grenadines each year.[107] Several species that were commercially exploited have rebounded in numbers; for example, grey whales may be as numerous as they were prior to harvesting, but the North Atlantic population is functionally extinct. Conversely, the North Atlantic right whale was extirpated from much of its former range, which stretched across the North Atlantic, and only remains in small fragments along the coast of Canada, Greenland, and is considered functionally extinct along the European coastline.[108] The IWC has designated two whale sanctuaries: the Southern Ocean Whale Sanctuary, and the Indian Ocean Whale Sanctuary. The Southern Ocean whale sanctuary spans 30,560,860 square kilometres (11,799,610 sq mi) and envelopes Antarctica.[109] The Indian Ocean whale sanctuary takes up all of the Indian Ocean south of 55°S.[110] The IWC is a voluntary organization, with no treaty. Any nation may leave as they wish; the IWC cannot enforce any law it makes. Whale watching An estimated 13 million people went whale watching globally in 2008, in all oceans except the Arctic.[120] Rules and codes of conduct have been created to minimize harassment of the whales.[121] Iceland, Japan and Norway have both whaling and whale watching industries. Whale watching lobbyists are concerned that the most inquisitive whales, which approach boats closely and provide much of the entertainment on whale-watching trips, will be the first to be taken if whaling is resumed in the same areas.[122] Whale watching generated US$2.1 billion (£1.4 billion) per annum in tourism revenue worldwide, employing around 13,000 workers.[120] In contrast, the whaling industry, with the moratorium in place, generates US$31 million (£20 million) per year.[93] The size and rapid growth of the industry has led to complex and continuing debates with the whaling industry about the best use of whales as a natural resource. In myth, literature and art As marine creatures that reside in either the depths or the poles, humans knew very little about whales over the course of human history; many feared or revered them. The Vikings and various arctic tribes revered the whale as they were important pieces of their lives. In Inuitcreation myths, when 'Big Raven', a deity in human form, found a stranded whale, he was told by the Great Spirit where to find special mushrooms that would give him the strength to drag the whale back to the sea and thus, return order to the world. In an Icelandic legend, a man threw a stone at a fin whale and hit the blowhole, causing the whale to burst. The man was told not to go to sea for twenty years, but during the nineteenth year he went fishing and a whale came and killed him. Whales played a major part in shaping the art forms of many coastal civilizations, such as the Norse, with some dating to the Stone Age. Petroglyphs off a cliff face in Bangudae, South Korea show 300 depictions of various animals, a third of which are whales. Some show particular detail in which there are throat pleats, typical of rorquals. These petroglyphs show these people, of around 7,000 to 3,500 B.C.E. in South Korea, had a very high dependency on whales.[123] In coastal regions of China, Korea and Vietnam, the worship of whale gods, who were associated with Dragon Kings after the arrival of Buddhism, was present along with related legends.[125]The god of the seas, according to Chinese folklore, was a large whale with human limbs. Illustration by Gustave Doré of Baron Munchausen's tale of being swallowed by a whale. While the Biblical Book of Jonah refers to the Prophet Jonah being swallowed by \"a big fish\", in later derivations that \"fish\" was identified as a whale. Whales have also played a role in sacred texts. The story of Jonah being swallowed by a great fish is told both in the Qur'an[130] and in the biblical Book of Jonah (and is mentioned by Jesus in the New Testament: Matthew 12:40.[131]). This episode was frequently depicted in medieval art (for example, on a 12th-century column capital at the abbey church of Mozac, France). The Bible also mentions whales in Genesis 1:21, Job 7:12, and Ezekiel 32:2. The \"leviathan\" described at length in Job 41:1-34 is generally understood to refer to a whale. The \"sea monsters\" in Lamentations 4:3 have been taken by some to refer to marine mammals, in particular whales, although most modern versions use the word \"jackals\" instead.[132] In 1585, Alessandro Farnese, 1585, and Francois, Duke of Anjou, 1582, were greeted on his ceremonial entry into the port city of Antwerp by floats including \"Neptune and the Whale\", indicating at least the city's dependence on the sea for its wealth.[133] Niki Caro's film the Whale Rider has a Māori girl ride a whale in her journey to be a suitable heir to the chieftain-ship.[136] Walt Disney's film Pinocchio features a showdown with a giant whale named Monstro at the end of the film. In captivity Belugas were the first whales to be kept in captivity. Other species were too rare, too shy, or too big. The first beluga was shown at Barnum's Museum in New York City in 1861.[138] For most of the 20th century, Canada was the predominant source of wild belugas.[139] They were taken from the St. Lawrence River estuary until the late 1960s, after which they were predominantly taken from the Churchill River estuary until capture was banned in 1992.[139] Russia has become the largest provider since it had been banned in Canada.[139] Belugas are caught in the Amur River delta and their eastern coast, and then are either transported domestically to aquariums or dolphinariums in Moscow, St. Petersburg, and Sochi, or exported to other countries, such as Canada.[139] Most captive belugas are caught in the wild, since captive-breeding programs are not very successful.[140] As of 2006, 30 belugas were in Canada and 28 in the United States, and 42 deaths in captivity had been reported up to that time.[139] A single specimen can reportedly fetch up to US$100,000 (£64,160) on the market. The beluga's popularity is due to its unique colour and its facial expressions. The latter is possible because while most cetacean \"smiles\" are fixed, the extra movement afforded by the beluga's unfused cervical vertebrae allows a greater range of apparent expression.[141] Between 1960 and 1992, the Navy carried out a program that included the study of marine mammals' abilities with sonar, with the objective of improving the detection of underwater objects. A large number of belugas were used from 1975 on, the first being dolphins.[141][142] The program also included training them to carry equipment and material to divers working underwater by holding cameras in their mouths to locate lost objects, survey ships and submarines, and underwater monitoring.[142] A similar program was used by the Russian Navy during the Cold War, in which belugas were also trained for antimining operations in the Arctic.[143] Aquariums have tried housing other species of whales in captivity. The success of belugas turned attention to maintaining their relative, the narwhal, in captivity. However, in repeated attempts in the 1960s and 1970s, all narwhals kept in captivity died within months. A pair of pygmy right whales were retained in an enclosed area (with nets); they were eventually released in South Africa. There was one attempt to keep a stranded Sowerby's beaked whale calf in captivity; the calf rammed into the tank wall, breaking its rostrum, which resulted in death. It was thought that Sowerby's beaked whale evolved to swim fast in a straight line, and a 30-metre (98 ft) tank was not big enough.[144] There have been attempts to keep baleen whales in captivity. There were three attempts to keep grey whales in captivity. Gigi was a grey whale calf that died in transport. Gigi II was another grey whale calf that was captured in the Ojo de Liebre Lagoon, and was transported to SeaWorld.[145] The 680-kilogram (1,500 lb) calf was a popular attraction, and behaved normally, despite being separated from his mother. A year later, the 8,000-kilogram (18,000 lb) whale grew too big to keep in captivity and was released; it was the first of two grey whales, the other being another grey whale calf named JJ, to successfully be kept in captivity.[145] There were three attempts to keep minke whales in captivity in Japan. They were kept in a tidal pool with a sea-gate at the Izu Mito Sea Paradise. Another, unsuccessful, attempt was made by the U.S. [146] One stranded humpback whale calf was kept in captivity for rehabilitation, but died days later.[147]"}
{"url": "https://en.m.wikipedia.org/wiki/Chordate", "text": "Although the name Chordata is attributed to William Bateson (1885), it was already in prevalent use by 1880. Ernst Haeckel described a taxon comprising tunicates, cephalochordates, and vertebrates in 1866. Though he used the German vernacular form, it is allowed under the ICZN code because of its subsequent latinization.[4] Pharyngeal slits. The pharynx is the part of the throat immediately behind the mouth. In fish, the slits are modified to form gills, but in some other chordates they are part of a filter-feeding system that extracts food particles from ingested water. In tetrapods, they are only present during embryonic stages of the development. A post-anal tail. A muscular tail that extends backwards behind the anus. In some chordates such as humans, this is only present in the embryonic stage. Superclass Tetrapoda (four-limbed vertebrates; 35,100+ species) (The classification below follows Benton 2004, and uses a synthesis of rank-based Linnaean taxonomy and also reflects evolutionary relationships. Benton included the Superclass Tetrapoda in the Subclass Sarcopterygii in order to reflect the direct descent of tetrapods from lobe-finned fish, despite the former being assigned a higher taxonomic rank.)[16] Cephalochordates, one of the three subdivisions of chordates, are small, \"vaguely fish-shaped\" animals that lack brains, clearly defined heads and specialized sense organs.[21] These burrowing filter-feeders compose the earliest-branching chordate subphylum.[22][23] Most tunicates appear as adults in one of two major forms, known as \"sea squirts\" and salps. Both of these are soft-bodied filter-feeders that lack the standard features of chordates, which are only retained in their larvae. Sea squirts are sessile and consist mainly of water pumps and filter-feeding apparatus;[24]salps float in mid-water, feeding on plankton, and have a two-generation cycle in which one generation is solitary and the next forms chain-like colonies.[25] However, all tunicate larvae have the standard chordate features, including long, tadpole-like tails; they also have rudimentary brains, light sensors and tilt sensors.[24] The third main group of tunicates, Appendicularia (also known as Larvacea), retain tadpole-like shapes and active swimming all their lives, and were for a long time regarded as larvae of sea squirts or salps.[26] The etymology of the term Urochordata (Balfour 1881) is from the ancient Greek οὐρά (oura, \"tail\") + Latin chorda (\"cord\"), because the notochord is only found in the tail.[27] The term Tunicata (Lamarck 1816) is recognised as having precedence and is now more commonly used.[24] Most craniates are vertebrates, in which the notochord is replaced by the vertebral column.[29] It consists of a series of bony or cartilaginous cylindrical vertebrae, generally with neural arches that protect the spinal cord, and with projections that link the vertebrae. Hagfishes have incomplete braincases and no vertebrae, and are therefore not regarded as vertebrates,[30] but they are members of the craniates, the group within which vertebrates are thought to have evolved.[31] However the cladistic exclusion of hagfish from the vertebrates is controversial, as they may instead be degenerate vertebrates who have secondarily lost their vertebral columns.[32] The position of lampreys is ambiguous. They have complete braincases and rudimentary vertebrae, and therefore may be regarded as vertebrates and true fish.[33] However, molecular phylogenetics, which uses biochemical features to classify organisms, has produced both results that group them with vertebrates and others that group them with hagfish.[34] If lampreys are more closely related to the hagfish than the other vertebrates, this would suggest that they form a clade, which has been named the Cyclostomata.[35] There is still much ongoing differential (DNA sequence based) comparison research that is trying to separate out the simplest forms of chordates. As some lineages of the 90% of species that lack a backbone or notochord might have lost these structures over time, this complicates the classification of chordates. Some chordate lineages may only be found by DNA analysis, when there is no physical trace of any chordate-like structures.[37] Attempts to work out the evolutionary relationships of the chordates have produced several hypotheses. The current consensus is that chordates are monophyletic, meaning that the Chordata include all and only the descendants of a single common ancestor, which is itself a chordate, and that craniates' nearest relatives are tunicates. Recent identification of two conserved signature indels (CSIs) in the proteins cyclophilin-like protein and mitochondrial inner membrane protease ATP23, which are exclusively shared by all vertebrates, tunicates and cephalochordates also provide strong evidence of the monophyly of Chordata.[5] All of the earliest chordate fossils have been found in the Early CambrianChengjiang fauna, and include two species that are regarded as fish, which implies that they are vertebrates. Because the fossil record of early chordates is poor, only molecular phylogenetics offers a reasonable prospect of dating their emergence. However, the use of molecular phylogenetics for dating evolutionary transitions is controversial. It has also proved difficult to produce a detailed classification within the living chordates. Attempts to produce evolutionary \"family trees\" shows that many of the traditional classes are paraphyletic. While this has been well known since the 19th century, an insistence on only monophyletic taxa has resulted in vertebrate classification being in a state of flux.[38] The majority of animals more complex than jellyfish and other Cnidarians are split into two groups, the protostomes and deuterostomes, the latter of which contains chordates.[39] It seems very likely the 555 million-year-oldKimberella was a member of the protostomes.[40][41] If so, this means the protostome and deuterostome lineages must have split some time before Kimberella appeared—at least 558 million years ago, and hence well before the start of the Cambrian 538.8 million years ago.[39] The Ediacaran fossil Ernietta, from about 549 to 543 million years ago, may represent a deuterostome animal.[42] A skeleton of the blue whale, the largest animal, extant or extinct, ever discovered. Mounted outside the Long Marine Laboratory at the University of California, Santa Cruz. The largest blue whale ever reliably recorded measured 98ft (30m) long.A peregrine falcon, the world's fastest animal. Peregrines use gravity and aerodynamics to achieve their top speed of around 242mph (390km/h), as opposed to locomotion. Fossils of one major deuterostome group, the echinoderms (whose modern members include starfish, sea urchins and crinoids), are quite common from the start of the Cambrian, 542 million years ago.[43] The Mid Cambrian fossil Rhabdotubus johanssoni has been interpreted as a pterobranch hemichordate.[44] Opinions differ about whether the Chengjiang fauna fossil Yunnanozoon, from the earlier Cambrian, was a hemichordate or chordate.[45][46] Another fossil, Haikouella lanceolata, also from the Chengjiang fauna, is interpreted as a chordate and possibly a craniate, as it shows signs of a heart, arteries, gill filaments, a tail, a neural chord with a brain at the front end, and possibly eyes—although it also had short tentacles round its mouth.[46]Haikouichthys and Myllokunmingia, also from the Chengjiang fauna, are regarded as fish.[36][47]Pikaia, discovered much earlier (1911) but from the Mid Cambrian Burgess Shale (505 Ma), is also regarded as a primitive chordate.[48] On the other hand, fossils of early chordates are very rare, since invertebrate chordates have no bones or teeth, and only one has been reported for the rest of the Cambrian.[49] The evolutionary relationships between the chordate groups and between chordates as a whole and their closest deuterostome relatives have been debated since 1890. Studies based on anatomical, embryological, and paleontological data have produced different \"family trees\". Some closely linked chordates and hemichordates, but that idea is now rejected.[10] Combining such analyses with data from a small set of ribosomeRNA genes eliminated some older ideas, but opened up the possibility that tunicates (urochordates) are \"basal deuterostomes\", surviving members of the group from which echinoderms, hemichordates and chordates evolved.[50] Some researchers believe that, within the chordates, craniates are most closely related to cephalochordates, but there are also reasons for regarding tunicates (urochordates) as craniates' closest relatives.[10][51] Since early chordates have left a poor fossil record, attempts have been made to calculate the key dates in their evolution by molecular phylogenetics techniques—by analyzing biochemical differences, mainly in RNA. One such study suggested that deuterostomes arose before 900 million years ago and the earliest chordates around 896 million years ago.[51] However, molecular estimates of dates often disagree with each other and with the fossil record,[51] and their assumption that the molecular clock runs at a known constant rate has been challenged.[52][53] Traditionally, Cephalochordata and Craniata were grouped into the proposed clade \"Euchordata\", which would have been the sister group to Tunicata/Urochordata. More recently, Cephalochordata has been thought of as a sister group to the \"Olfactores\", which includes the craniates and tunicates. The matter is not yet settled. A specific relationship between Vertebrates and Tunicates is also strongly supported by two CSIs found in the proteins predicted exosome complex RRP44 and serine palmitoyltransferase, that are exclusively shared by species from these two subphyla but not Cephalochordates, indicating Vertebrates are more closely related to Tunicates than Cephalochordates.[5] Hemichordates (\"half chordates\") have some features similar to those of chordates: branchial openings that open into the pharynx and look rather like gill slits; stomochords, similar in composition to notochords, but running in a circle round the \"collar\", which is ahead of the mouth; and a dorsal nerve cord—but also a smaller ventral nerve cord. There are two living groups of hemichordates. The solitary enteropneusts, commonly known as \"acorn worms\", have long proboscises and worm-like bodies with up to 200 branchial slits, are up to 2.5 metres (8.2 ft) long, and burrow though seafloor sediments. Pterobranchs are colonial animals, often less than 1 millimetre (0.039 in) long individually, whose dwellings are interconnected. Each filter feeds by means of a pair of branched tentacles, and has a short, shield-shaped proboscis. The extinct graptolites, colonial animals whose fossils look like tiny hacksaw blades, lived in tubes similar to those of pterobranchs.[58] Echinoderms differ from chordates and their other relatives in three conspicuous ways: they possess bilateral symmetry only as larvae – in adulthood they have radial symmetry, meaning that their body pattern is shaped like a wheel; they have tube feet; and their bodies are supported by skeletons made of calcite, a material not used by chordates. Their hard, calcified shells keep their bodies well protected from the environment, and these skeletons enclose their bodies, but are also covered by thin skins. The feet are powered by another unique feature of echinoderms, a water vascular system of canals that also functions as a \"lung\" and surrounded by muscles that act as pumps. Crinoids look rather like flowers, and use their feather-like arms to filter food particles out of the water; most live anchored to rocks, but a few can move very slowly. Other echinoderms are mobile and take a variety of body shapes, for example starfish, sea urchins and sea cucumbers.[59]"}
{"url": "https://en.m.wikipedia.org/wiki/Rod_cell", "text": "Rod cells are photoreceptor cells in the retina of the eye that can function in lower light better than the other type of visual photoreceptor, cone cells. Rods are usually found concentrated at the outer edges of the retina and are used in peripheral vision. On average, there are approximately 92 million rod cells (vs ~6 million cones) in the human retina.[1] Rod cells are more sensitive than cone cells and are almost entirely responsible for night vision. However, rods have little role in color vision, which is the main reason why colors are much less apparent in dim light. Contents Rods are a little longer and leaner than cones but have the same basic structure. Opsin-containing disks lie at the end of the cell adjacent to the retinal pigment epithelium, which in turn is attached to the inside of the eye. The stacked-disc structure of the detector portion of the cell allows for very high efficiency. Rods are much more common than cones, with about 120 million rod cells compared to 6 to 7 million cone cells.[2] Like cones, rod cells have a synaptic terminal, an inner segment, and an outer segment. The synaptic terminal forms a synapse with another neuron, usually a bipolar cell or a horizontal cell. The inner and outer segments are connected by a cilium,[3] which lines the distal segment.[4] The inner segment contains organelles and the cell's nucleus, while the rod outer segment (abbreviated to ROS), which is pointed toward the back of the eye, contains the light-absorbing materials.[3] A human rod cell is about 2 microns in diameter and 100 microns long.[5] Rods are not all morphologically the same; in mice, rods close to the outer plexiform synaptic layer display a reduced length due to a shortened synaptic terminal.[6] In vertebrates, activation of a photoreceptor cell is a hyperpolarization (inhibition) of the cell. When they are not being stimulated, such as in the dark, rod cells and cone cells depolarize and release a neurotransmitter spontaneously. This neurotransmitter hyperpolarizes the bipolar cell. Bipolar cells exist between photoreceptors and ganglion cells and act to transmit signals from the photoreceptors to the ganglion cells. As a result of the bipolar cell being hyperpolarized, it does not release its transmitter at the bipolar-ganglion synapse and the synapse is not excited. Activation of photopigments by light sends a signal by hyperpolarizing the rod cell, leading to the rod cell not sending its neurotransmitter, which leads to the bipolar cell then releasing its transmitter at the bipolar-ganglion synapse and exciting the synapse. Depolarization of rod cells (causing release of their neurotransmitter) occurs because in the dark, cells have a relatively high concentration of cyclic guanosine 3'-5' monophosphate (cGMP), which opens ion channels (largely sodium channels, though calcium can enter through these channels as well). The positive charges of the ions that enter the cell down its electrochemical gradient change the cell's membrane potential, cause depolarization, and lead to the release of the neurotransmitter glutamate. Glutamate can depolarize some neurons and hyperpolarize others, allowing photoreceptors to interact in an antagonistic manner. When light hits photoreceptive pigments within the photoreceptor cell, the pigment changes shape. The pigment, called rhodopsin (conopsin is found in cone cells) comprises a large protein called opsin (situated in the plasma membrane), attached to which is a covalently bound prosthetic group: an organic molecule called retinal (a derivative of vitamin A). The retinal exists in the 11-cis-retinal form when in the dark, and stimulation by light causes its structure to change to all-trans-retinal. This structural change causes an increased affinity for the regulatory protein called transducin (a type of G protein). Upon binding to rhodopsin, the alpha subunit of the G protein replaces a molecule of GDP with a molecule of GTP and becomes activated. This replacement causes the alpha subunit of the G protein to dissociate from the beta and gamma subunits of the G protein. As a result, the alpha subunit is now free to bind to the cGMP phosphodiesterase (an effector protein).[8] The alpha subunit interacts with the inhibitory PDE gamma subunits and prevents them from blocking catalytic sites on the alpha and beta subunits of PDE, leading to the activation of cGMP phosphodiesterase, which hydrolyzes cGMP (the second messenger), breaking it down into 5'-GMP.[9] Reduction in cGMP allows the ion channels to close, preventing the influx of positive ions, hyperpolarizing the cell, and stopping the release of the neurotransmitter glutamate.[3] Though cone cells primarily use the neurotransmitter substance acetylcholine, rod cells use a variety. The entire process by which light initiates a sensory response is called visual phototransduction. Activation of a single unit of rhodopsin, the photosensitive pigment in rods, can lead to a large reaction in the cell because the signal is amplified. Once activated, rhodopsin can activate hundreds of transducin molecules, each of which in turn activates a phosphodiesterase molecule, which can break down over a thousand cGMP molecules per second.[3] Thus, rods can have a large response to a small amount of light. As the retinal component of rhodopsin is derived from vitamin A, a deficiency of vitamin A causes a deficit in the pigment needed by rod cells. Consequently, fewer rod cells are able to sufficiently respond in darker conditions, and as the cone cells are poorly adapted for sight in the dark, blindness can result. This is night-blindness. Rods make use of three inhibitory mechanisms (negative feedback mechanisms) to allow a rapid revert to the resting state after a flash of light. Firstly, there exists a rhodopsin kinase (RK) which would phosphorylate the cytosolic tail of the activated rhodopsin on the multiple serines, partially inhibiting the activation of transducin. Also, an inhibitory protein - arrestin then binds to the phosphorylated rhodopsins to further inhibit the rhodopsin activity. While arrestin shuts off rhodopsin, an RGS protein (functioning as a GTPase-activating proteins(GAPs)) drives the transducin (G-protein) into an \"off\" state by increasing the rate of hydrolysis of the bounded GTP to GDP. When the cGMP concentration falls, the previously open cGMP sensitive channels close, leading to a reduction in the influx of calcium ions. The associated decrease in the concentration of calcium ions stimulates the calcium ion-sensitive proteins, which then activate the guanylyl cyclase to replenish the cGMP, rapidly restoring it to its original concentration. This opens the cGMP sensitive channels and causes a depolarization of the plasma membrane.[10] When the rods are exposed to a high concentration of photons for a prolonged period, they become desensitized (adapted) to the environment. As rhodopsin is phosphorylated by rhodopsin kinase (a member of the GPCR kinases(GRKs)), it binds with high affinity to the arrestin. The bound arrestin can contribute to the desensitization process in at least two ways. First, it prevents the interaction between the G protein and the activated receptor. Second, it serves as an adaptor protein to aid the receptor to the clathrin-dependent endocytosis machinery (to induce receptor-mediated endocytosis).[10] A rod cell is sensitive enough to respond to a single photon of light[11] and is about 100 times more sensitive to a single photon than cones. Since rods require less light to function than cones, they are the primary source of visual information at night (scotopic vision). Cone cells, on the other hand, require tens to hundreds of photons to become activated. Additionally, multiple rod cells converge on a single interneuron, collecting and amplifying the signals. However, this convergence comes at a cost to visual acuity (or image resolution) because the pooled information from multiple cells is less distinct than it would be if the visual system received information from each rod cell individually. Wavelength absorbance of short (S), medium (M) and long (L) wavelength cones compared to that of rods (R).[12] Rod cells also respond more slowly to light than cones and the stimuli they receive are added over roughly 100 milliseconds. While this makes rods more sensitive to smaller amounts of light, it also means that their ability to sense temporal changes, such as quickly changing images, is less accurate than that of cones.[3] Experiments by George Wald and others showed that rods are most sensitive to wavelengths of light around 498 nm (green-blue), and insensitive to wavelengths longer than about 640 nm (red). This is responsible for the Purkinje effect: as intensity dims at twilight, the rods take over, and before color disappears completely, peak sensitivity of vision shifts towards the rods' peak sensitivity (blue-green).[13]"}
{"url": "https://en.m.wikipedia.org/wiki/Bowhead_whales", "text": "Bowhead whale The bowhead whale (Balaena mysticetus) is a species of baleen whale belonging to the family Balaenidae and is the only living representative of the genusBalaena. It is the only baleen whale endemic to the Arctic and subarctic waters, and is named after its characteristic massive triangular skull, which it uses to break through Arctic ice. Other common names of the species included the Greenland right whale, Arctic whale, steeple-top, and polar whale.[5] Bowheads have the largest mouth of any animal[6] representing almost one-third of the length of the body, the longest baleen plates with a maximum length of 4 metres (13 feet),[7] and may be the longest-lived mammals, with the ability to reach an age of more than 200 years.[8] Carl Linnaeus named this species in the tenth edition of his Systema Naturae (1758).[10] It was seemingly identical to its relatives in the North Atlantic, North Pacific, and Southern Oceans, and as such they were all thought to be a single species, collectively known as the \"right whale\", and given the binomial nameBalaena mysticetus. Today, the bowhead whale occupies a monotypicgenus, separate from the right whales, as proposed by the work of John Edward Gray in 1821.[11] For the next 180 years, the family Balaenidae was the subject of great taxonometric debate. Authorities have repeatedly recategorized the three populations of right whale plus the bowhead whale, as one, two, three or four species, either in a single genus or in two separate genera. Eventually, it was recognized that bowheads and right whales were different, but there was still no strong consensus as to whether they shared a single genus or two. As recently as 1998, Dale Rice listed just two species – B. glacialis (the right whales) and B. mysticetus (the bowheads) – in his comprehensive and otherwise authoritative classification.[12] Studies in the 2000s finally provided clear evidence that the three living right whale species comprise a phylogenetic lineage, distinct from the bowhead, and that the bowhead and the right whales are rightly classified into two separate genera.[13] The right whales were thus confirmed to be in a separate genus, Eubalaena. The relationship is shown in the cladogram below: The bowhead whale, genus Balaena, in the family Balaenidae (extant taxa only)[14] The earlier fossil record shows no related cetacean after Morenocetus, found in a South American deposit dating back 23 million years. An unknown species of right whale, the so-called \"Swedenborg whale\", which was proposed by Emanuel Swedenborg in the 18th century, was once thought to be a North Atlantic right whale. Based on later DNA analysis, those fossil bones claimed to be from Swedenborg whales were confirmed to be from bowhead whales.[15] Skeleton of a bowhead whaleStamp showing drawing of mother and calf from Faroe Islands The bowhead whale has a large, robust, dark-coloured body and a white chin. It has a massive triangular skull, which it uses to break through the Arctic ice to breathe. Inuit hunters have reported bowheads surfacing through 60 cm (24 in) of ice.[16] It also possesses a strongly bowed lower jaw and a narrow upper jaw. Its baleen is the longest of that of any whale, at 3 m (10 ft), and is used to strain tiny prey from the water. The bowhead whale has paired blowholes at the highest point of the head, which can spout a blow 6.1 m (20 ft 0 in) high. The bowhead's blubber is the thickest of any animal's, with a maximum of 43–50 cm (17–19+1⁄2 in).[17] Unlike most cetaceans, the bowhead does not have a dorsal fin—an adaptation for spending much time under sea-surface ice.[18] Like the sperm whale and other cetaceans, the bowhead whale has a vestigial pelvis that is not connected to the spine. Bowhead whales are comparable in size to the three species of right whales. According to whaling captain William Scoresby Jr., the longest bowhead he measured was 17.7 m (58 ft 1 in) long, while the longest measurement he had ever heard of was of a 20.4 m (66 ft 11 in) whale caught at Godhavn, Greenland, in early 1813. He also spoke of one, caught near Spitsbergen around 1800, that was allegedly nearly 21.3 m (69 ft 11 in) long.[19] In 1850, an American vessel claimed to have caught a 24.54 m (80 ft 6 in) individual in the Western Arctic.[20] Whether these lengths were actually measured is questionable. The longest reliably measured were a male of 16.2 m (53 ft 2 in) and a female of 18 m (59 ft), both landed in Alaska.[21] On average, female bowheads are larger than males. The adults would have likely measured 19 metres (62 ft) in length and 80 metric tons (88 short tons) in body mass, but larger individuals (like the one claimed in 1850) may weigh up to 100 metric tons (110 short tons).[22] Analysis of hundreds of DNA samples from living whales and from baleen used in vessels, toys, and housing material has shown that Arctic bowhead whales have lost a significant portion of their genetic diversity in the past 500 years. Bowheads originally crossed ice-covered inlets and straits to exchange genes between Atlantic and Pacific populations. This conclusion was derived from analyzing maternal lineage using mitochondrial DNA. Whaling and climatic cooling during the Little Ice Age, from the 16th century to the 19th, is supposed to have reduced the whales' summer habitats, which explains the loss of genetic diversity.[23] A 2013 discovery has clarified the function of the bowhead's large palatal retial organ. The bulbous ridge of highly vascularized tissue, the corpus cavernosum maxillaris, extends along the centre of the hard plate, forming two large lobes at the rostral palate. The tissue is histologically similar to that of the corpus cavernosum of the mammalian penis. This organ is thought to provide a mechanism of cooling for the whale (which is normally protected from the cold Arctic waters by 40 cm or 16 in or more of fat). During physical exertion, the whale must cool itself to prevent hyperthermia (and ultimately brain damage). This organ becomes engorged with blood, and as the whale opens its mouth cold seawater flows over the organ, thus cooling the blood.[24] The bowhead whale is not a social animal, typically travelling alone or in small pods of up to six. It is able to dive and remain submerged under water for up to an hour. The time spent under water in a single dive is usually limited to 9–18 minutes.[16] The bowhead is not thought to be a deep diver, but can reach a depth down to 150 m (500 ft). It is a slow swimmer, normally travelling around 2–5 km/h (1–3 mph) [0.55–1.39 m/s].[25] When fleeing from danger, it can travel at a speed of 10 km/h (6.2 mph) [2.78 m/s (9 ft/s)]. During periods of feeding, the average swim speed is increased to 1.1–2.5 m/s (4.0–9.0 km/h).[26] The head of the bowhead whale comprises a third of its body length, creating an enormous feeding apparatus.[26] The bowhead whale is a filter feeder, and feeds by swimming forward with its mouth wide open.[16] It has hundreds of overlapping baleen plates consisting of keratin hanging from each side of the upper jaw. The mouth has a large, upturning lip on the lower jaw that helps to reinforce and hold the baleen plates within the mouth. This also prevents buckling or breakage of the plates from the pressure of the water passing through them as the whale advances. To feed, water is filtered through the fine hairs of keratin of the baleen plates, trapping the prey inside near the tongue where it is then swallowed.[27] The diet consists of mostly zooplankton, which includes krill, copepods, mysids, amphipods, and many other crustaceans.[26][28] About 1.8 tonnes (2 short tons) of food are consumed each day.[27] While foraging, bowheads are solitary or occur in groups of two to 10 or more.[17] Bowhead whales are highly vocal[29] and use low frequency (<1000 Hz) sounds to communicate while travelling, feeding, and socialising. Intense calls for communication and navigation are produced especially during migration season. During breeding season, bowheads make long, complex, variable songs for mating calls.[25] Many tens of distinct songs are sung by a population in a single season.[30] From 2010 through to 2014, near Greenland, 184 distinct songs were recorded from a population of around 300 animals.[31] Sexual activity occurs between pairs and in boisterous groups of several males and one or two females. Breeding season is observed from March through August; conception is believed to occur primarily in March when song activity is at its highest.[25] Reproduction can begin when a whale is 10 to 15 years old. The gestation period is 13–14 months with females producing a calf once every three to four years.[21]Lactation typically lasts about a year. To survive in the cold water immediately after birth, calves are born with a thick layer of blubber. Within 30 minutes of birth, bowhead calves are able to swim on their own. A newborn calf is typically 4–4.5 m (13–15 ft) long, weighs roughly 1,000 kg (2,200 lb), and grows to 8.2 m (26 ft 11 in) within the first year.[21] Bowhead whales are considered to be the longest-living mammals, living for over 200 years.[32] In May 2007, a 15 m (49 ft) specimen caught off the Alaskan coast was discovered with the 90 mm (3.5 in) head of an explosive bomb lance of a model manufactured between 1879 and 1885, so the whale was probably bomb lanced sometime between those years, and its age at the time of death was estimated at between 115 and 130 years.[33] Spurred by this discovery, scientists measured the ages of other bowhead whales; one specimen was estimated to be 211 years old.[34] Other bowhead whales were estimated to be between 135 and 172 years old. This discovery showed the longevity of the bowhead whale is much greater than originally thought.[35] Researchers at CSIRO, Australia's national science agency, estimated that bowhead whales' maximum natural lifespan is 268 years based on genetic analysis.[36] A greater number of cells present in an organism was once believed to result in greater chances of mutations that cause age-related diseases and cancer.[37] Although the bowhead whale has thousands of times more cells than other mammals, it has a much higher resistance to cancer and aging. In 2015, scientists from the US and UK were able to successfully map the whale's genome.[38] Through comparative analysis, two alleles that could be responsible for the whale's longevity were identified. These two specific gene mutations linked to the bowhead whale's ability to live longer are the ERCC1 gene and the proliferating cell nuclear antigen (PCNA) gene. ERCC1 is linked to DNA repair and increased cancer resistance. PCNA is also important in DNA repair. These mutations enable bowhead whales to better repair DNA damage, allowing for greater resistance to cancer.[37] The whale's genome may also reveal physiological adaptations such as having low metabolic rates compared to other mammals.[39] Changes in the gene UCP1, a gene involved in thermoregulation, can explain differences in the metabolic rates in cells. The bowhead whale is the only baleen whale to spend its entire life in the Arctic and subarctic waters.[40] The Alaskan population spends the winter months in the southwestern Bering Sea. The group migrates northward in the spring, following openings in the ice, into the Chukchi and Beaufort seas.[41] The whale's range varies depending on climate changes and on the forming/melting of ice.[42] Historically, bowhead whales' range may have been broader and more southerly than currently thought. Bowheads were abundant around Labrador, Newfoundland (Strait of Belle Isle) and the northern Gulf of St Lawrence until at least the 16th and 17th centuries. It is unclear whether this was due to the colder climate during these periods.[43] The distribution of Balaena spp. during the Pleistocene were far more southerly as fossils have been excavated from Italy and North Carolina, thus could have overlapped between those of Eubalaena based on those locations.[44] Generally, five stocks of bowhead whales are recognized: 1) the Western Arctic stock in the Bering, Chukchi, and Beaufort Seas, 2) the Hudson Bay and Foxe Basin stock, 3) the Baffin Bay and Davis Strait stock, 4) the Sea of Okhotsk stock, and 5) the Svalbard-Barents Sea stock. However, recent evidence suggests that the Hudson Bay and Foxe Basin stock and the Baffin Bay and Davis Strait stock should be considered one stock based on genetics and movements of tagged whales.[45] The Western Arctic bowhead population, also known as the Bering-Chukchi-Beaufort population, has recovered since the commercial harvest of this stock ceased in the early 1900s. A 2019 study estimated that the Western Arctic population was 12,505; although it was lower than the 2011 value of 16,820, the surveyors believed there was no significant decline in 2011-2019 due to the unusual conditions of whale migration and observation in 2019.[46] The yearly growth rate of the Western Arctic bowhead population was 3.7% from 1978 to 2011. These data suggest that the Western Arctic bowhead stock may be near its precommercial whaling level.[45] Migration patterns of this population are being affected by climate change.[47] Alaskan Natives continue to hunt small numbers of bowhead whales for subsistence purposes. The Alaska Eskimo Whaling Commission co-manages the bowhead subsistence harvest with the National Oceanic and Atmospheric Administration. The Alaskan villages that participate in the bowhead subsistence harvest include Barrow, Point Hope, Point Lay, Wainwright, Nuiqsut, Kaktovik, Gambell, Savoonga, Kivalina, Wales, and Little Diomede.[48] The annual subsistence harvest of the Western Arctic stock has ranged from 14 to 72, amounting to an estimated 0.1-0.5% of the population.[45] In March 2008, Canada's Department of Fisheries and Oceans stated the previous estimates in the eastern Arctic had undercounted, with a new estimate of 14,400 animals (range 4,800–43,000).[49] These larger numbers correspond to prewhaling estimates, indicating the population has fully recovered. However, if climate change substantially shrinks sea ice, these whales could be threatened by increased shipping traffic.[50] The status of other populations is less well known. About 1,200 were off West Greenland in 2006, while the Svalbard population may only number in the tens. However, the numbers have been increasing in recent years.[51] The Hudson Bay – Foxe Basin population is distinct from the Baffin Bay – Davis Strait group.[52] The original population size of this local group is unclear, but possibly about 500 to 600 whales annually summered in the northwestern part of the bay in the 1860s.[53] It is likely that the number of whales that actually inhabit Hudson Bay is much smaller than the total population size of this group,[54] but reports from local indigenous people indicate that this population is increasing over decades.[55] Larger portions of the bay are used for summering, while wintering is on a smaller scale. Some animals winter in Hudson Strait, most notably north of Igloolik Island and north eastern Hudson Bay. Distribution patterns in these regions are affected by the presence of orca, and bowheads can disappear from normal ranges in the presence of atypical numbers of orca. Increased mortality caused by orca attack is a possible outcome of climate change, as reduced ice coverage is expected to result in fewer areas that the bowheads can use for shelter from attack.[55] Whaling grounds in the 19th century stretched from Marble Island to Roes Welcome Sound and to Lyon Inlet and Fisher Strait, and whales still migrate through most of these areas. Not much is known about the endangered Sea of Okhotsk population. To learn more about the population, these mammals have been regularly observed near the Shantar Islands, very close to the shore, such as at Ongachan Bay.[65][66] Several companies provide whale-watching services, which are mostly land-based. According to Russian scientists, this total population likely does not exceed 400 animals.[64] Scientific research on this population was seldom done before 2009, when researchers studying belugas noticed concentrations of bowheads in the study area. Thus, bowheads in the Sea of Okhotsk were once called \"forgotten whales\" by researchers. The WWF welcomed the creation of a nature sanctuary in the region[67] Possibly, vagrants from this population occasionally reach into Asian nations such as off Japan or the Korean Peninsula (although this record might be of a right whale[68]). The first documented report of the species in Japanese waters was of a strayed infant (7 m or 23 ft) caught in Osaka Bay on 23 June 1969,[69] and the first live sighting was of a 10 m (33 ft) juvenile around Shiretoko Peninsula (the southernmost of ice floe range in the Northern Hemisphere) on 21 to 23 June 2015.[70] Fossils have been excavated on Hokkaido,[71] but it is unclear whether the northern coasts of Japan were once included in seasonal or occasional migration ranges. Genetic studies suggest Okhotsk population share common ancestry with whales in Bering-Chukchi-Beaufort Seas, and repeated mixings had occurred between whales in the two seas.[72] It is unclear whether this population is a remnant of the historic Svalbard group, recolonized individuals from other stocks, or if a mixing of these two or more stocks has taken place. In 2015, discoveries of the refuge along eastern Greenland where whaling ships could not reach due to ice floes[83] and largest numbers of whales (80–100 individuals) ever sighted between Spitsbergen and Greenland[84] indicate that more whales than previously considered survived whaling periods, and flows from the other populations are possible. During expeditions by a tour operator 'Arctic Kingdom', a large group of bowheads seemingly involved in courtship activities was discovered in very shallow bays south of Qikiqtarjuaq in 2012.[85] Floating skins and rubbing behaviours at sea bottom indicated possible moulting had taken place. Moulting behaviours had never or seldom been documented for this species before. This area is an important habitat for whales that were observed to be relatively active and to interact with humans positively, or to rest on sea floors. These whales belong to Davis Strait stock. Isabella Bay in Niginganiq National Wildlife Area is the first wildlife sanctuary in the world to be designed specially for bowhead whales. However, moultings have not been recorded in this area due to environmental factors.[86] In 1978 the International Whaling Commission (IWC) introduced a hunting strike quota for the Bering-Chukchi-Beaufort Sea (BCB) bowhead.[87] The quota has remained at 67 strikes per year since 1998 and represents about 0.5 percent of BCB population.[87] The population of bowheads in West Greenland and Canada is estimated to be 6,000 and rising, and hunts in this are minimal (<0.001 percent).[87] Both stocks are rising, and the indigenous hunts seem to be self-sustaining.[87] Orca are also known predators.[88] There is no consensus on the number of deaths by orca.[87] Bowheads seek the ice and shallow waters' safety when threatened by orca.[25] The Inuit have a traditional word for this behavior to give historical context that this is not a new phenomenon.[87] Global warming is increasing the frequency that orca are observed in the far north. A once-rare event, orca are now seen more frequently.[87] The bowhead whale has been hunted for blubber, meat, oil, bones, and baleen. Like the right whale, it swims slowly, and floats after death, making it ideal for whaling.[89] Before commercial whaling, they were estimated to number 50,000.[90]Paleo-Eskimo sites indicate bowhead whales were eaten in sites from perhaps 4000 BC. Inuit people near the Pacific developed specific hunting tools, with the whales providing food and fuel.[91] Commercial bowhead whaling began in the 16th century when the Basques killed them as they migrated south through the Strait of Belle Isle in the fall and early winter. In 1611, the first whaling expedition sailed to Spitsbergen. The whaling settlement Smeerenburg was founded on Spitsbergen in 1619. By midcentury, the population(s) there had practically been wiped out, forcing whalers to voyage into the \"West Ice\"—the pack ice off Greenland's east coast. By 1719, they had reached the Davis Strait, and by the first quarter of the 19th century, Baffin Bay.[92] In the North Pacific, the first bowheads were taken off the eastern coast of Kamchatka by the Danish whaleship Neptun, Captain Thomas Sodring, in 1845.[20] In 1847, the first bowheads were caught in the Sea of Okhotsk, and the following year, Captain Thomas Welcome Roys, in the bark Superior, of Sag Harbor, caught the first bowheads in the Bering Strait region. By 1849, 50 ships were hunting bowheads in each area; in the Bering Strait, 500 whales were killed that year, and that number jumped to more than 2000 in 1850.[93] By 1852, 220 ships were cruising around the Bering Strait region, which killed over 2,600 whales. Between 1854 and 1857, the fleet shifted to the Sea of Okhotsk, where 100–160 ships cruised annually. During 1858–1860, the ships shifted back to the Bering Strait region, where the majority of the fleet cruised during the summer until the early 20th century.[94] An estimated 18,600 bowheads were killed in the Bering Strait region between 1848 and 1914, with 60% of the total being reached within the first two decades. An estimated 18,000 bowheads were killed in the Sea of Okhotsk during 1847–1867, 80% in the first decade.[95] The Alaska Department of Fish and Game and the USA government list the bowhead whale as federally endangered.[100] The bowhead whale is listed in Appendix I[101] of the Convention on the Conservation of Migratory Species of Wild Animals (CMS), as this species has been categorized as being in danger of extinction throughout all or a significant proportion of its range. CMS Parties strive towards strictly protecting these animals, conserving or restoring the places where they live, mitigating obstacles to migration, and controlling other factors that might endanger them.[89] ^\"Appendix I\" of the Convention on the Conservation of Migratory Species of Wild Animals (CMS). As amended by the Conference of the Parties in 1985, 1988, 1991, 1994, 1997, 1999, 2002, 2005, and 2008. Effective: 5 March 2009."}
{"url": "https://en.m.wikipedia.org/wiki/Inioidea", "text": "River dolphins are relatively small compared to other dolphins, having evolved to survive in warm, shallow water and strong river currents. They range in size from the 5-foot (1.5 m) long South Asian river dolphin to the 8-foot (2.4 m) and 220-pound (100 kg) Amazon river dolphin. Several species exhibit sexual dimorphism, in that the males are larger than the females. They have streamlined bodies and two limbs that are modified into flippers. River dolphins use their conical-shaped teeth and long beaks to capture fast-moving prey in murky water. They have well-developed hearing that is adapted for both air and water; they do not really rely on vision since the water they swim in is usually very muddy. Instead, they tend to rely on echolocation when hunting and navigating. These species are well-adapted to living in warm, shallow waters, and, unlike other cetaceans, have little to no blubber. River dolphins are not very widely distributed; they are all restricted to certain rivers or deltas. This makes them extremely vulnerable to habitat destruction. River dolphins feed primarily on fish. Male river dolphins typically mate with multiple females every year, but females only mate every two to three years. Calves are typically born in the spring and summer months and females bear all the responsibility for raising them. River dolphins produce a variety of vocalizations, usually in the form of clicks and whistles. River dolphins are rarely kept in captivity; breeding success has been poor and the animals often die within a few months of capture. As of 2020[update], there was only one river dolphin in captivity.[2] Four families of river dolphins (Iniidae, Pontoporiidae, Lipotidae and Platanistidae) are currently recognized, comprising three superfamilies (Inioidea, Lipotoidea and Platanistoidea). Platanistidae, containing the two subspecies of South Asian river dolphin, is the only living family in the superfamily Platanistoidea.[3] Previously, many taxonomists had assigned all river dolphins to a single family, Platanistidae, and treated the Ganges and Indus river dolphins as separate species. A December 2006 survey found no members of Lipotes vexillifer (commonly known as the baiji, or Chinese river dolphin) and declared the species functionally extinct. With their disappearance, one of the recently accepted superfamilies, Lipotoidea, has become extinct.[4] In 2012 the Society for Marine Mammalogy began considering the Bolivian (Inia geoffrensis boliviensis) and Amazonian (Inia geoffrensis geoffrensis) subspecies as full species Inia boliviensis and Inia geoffrensis, respectively; however, much of the scientific community, including the IUCN, continue to consider the Bolivian population to be a subspecies of Inia geoffrensis.[7][8] In October 2014, the Society for Marine Mammalogy took Inia boliviensis and Inia araguaiaensis off their list of aquatic mammal species and subspecies and currently does not recognize these species-level separations.[7][9] Phylogeny of cetaceans based on cytochrome b gene sequences, showing the distant relationship between Platanista and other river dolphins. River dolphins are members of the infraorder Cetacea, which are descendants of land-dwelling mammals of the orderArtiodactyla (even-toed ungulates). They are related to the Indohyus, an extinct chevrotain-like ungulate, from which they split approximately 48 million years ago.[10] The primitive cetaceans, or archaeocetes, first took to the sea approximately 49 million years ago and became fully aquatic by 5–10 million years later. It is unknown when river dolphins first ventured back into fresh water.[11] River dolphins are thought to have relictual distributions, that is, their ancestors originally occupied marine habitats, but were then displaced from these habitats by modern dolphin lineages.[12][13] Many of the morphological similarities and adaptations to freshwater habitats arose due to convergent evolution; thus, a grouping of all river dolphins is polyphyletic. Amazon river dolphins are actually more closely related to oceanic dolphins than to South Asian river dolphins.[14]Isthminia panamensis is an extinct genus and species of river dolphin, living 5.8 to 6.1 million years ago. Its fossils were discovered near Piña, Panama.[15][16] River dolphin has been considered a taxonomic description, suggesting an evolutionary relationship among the group, although it is now known that they form two distinct clades. 'True' river dolphins are descendants of ancient evolutionary lineages that evolved in freshwater environments.[12] The tucuxi (Sotalia fluviatilis) in the Amazon River is another species descended from oceanic dolphins; however, it does not perfectly fit the label of 'facultative' either, as it occurs only in fresh water. The tucuxi was until recently considered conspecific with the Guiana dolphin (Sotalia guianensis), which inhabits marine waters. It may also be true for the Irrawaddy dolphin and the finless porpoise that, although the species may be found in both freshwater and marine environments, individual animals found in rivers may not be able to survive in the ocean, and vice versa.[19] The tucuxi is currently classified as an oceanic dolphin (Delphinidae).[20] The Franciscana (Pontoporia blainvillei) has shown a converse evolutionary pattern, and has an ancient evolutionary lineage in freshwater, but inhabits estuarine and coastal waters.[21] River dolphins have a torpedo shaped body with a flexible neck, limbs modified into flippers, non-existent external ear flaps, a tail fin, and a small bulbous head. River dolphin skulls have small eye orbits, a long snout and eyes placed on the sides of the head. River dolphins are rather small, ranging in size from the 5-foot (1.5 m) long South Asian river dolphin to the 8-foot (2.4 m) and 220-pound (100 kg) Amazon river dolphin. They all have female-biased sexual dimorphism apart from Amazon river dolphin, with the females being larger than the males.[22][23] River dolphins are polygynous, meaning male river dolphins typically mate with multiple females every year, but females only mate every two to three years. Calves are typically born in the spring and summer months and females bear all the responsibility for raising them.[23] River dolphins have conical teeth, used to catch swift prey such as small river fish.[23] They also have very long snouts, with some measuring 23 inches (58 cm), four times longer than most of their oceanic counterparts. They have a two-chambered stomach that is similar in structure to that of terrestrial carnivores. They have fundic and pyloric chambers.[24] Breathing involves expelling stale air from their blowhole, followed by inhaling fresh air into their lungs. They do not have the iconic spout, as this only forms when the warm air exhaled from the lungs meets cold external air, which does not occur in their tropical habitats.[23][25] River dolphins have a relatively thin layer of blubber. Blubber can help with buoyancy, protection from predators (they would have a hard time getting through a thick layer of fat), energy for leaner times, and insulation from harsh climates. The habitats of river dolphins lack these needs.[23] River dolphins have two flippers and a tail fin. These flippers contain four digits. Although river dolphins do not possess fully developed hind limbs, some possess discrete rudimentary appendages, which may contain feet and digits. River dolphins are slow swimmers in comparison to oceanic dolphins, which can travel at speeds up to 35 miles per hour (56 km/h); the tucuxi can only travel at about 14 miles per hour (23 km/h).[26] Unlike other cetaceans, their neck vertebrae are not fused together, meaning they have greater flexibility than other non-terrestrial aquatic mammals, at the expense of speed. This means they can turn their head without actually moving their entire body.[27][28] When swimming, river dolphins rely on their tail fins to propel themselves through the water. Flipper movement is continuous. River dolphins swim by moving their tail fins and lower bodies up and down, propelling themselves through vertical movement, while their flippers are mainly used for steering. All species have a dorsal fin.[23] The ears of river dolphins have specific adaptations to their aquatic environment. In humans, the middle ear works as an impedance equalizer between the outside air's low impedance and the cochlear fluid's high impedance. In river dolphins, and other cetaceans, there is no great difference between the outer and inner environments. Instead of sound passing through the outer ear to the middle ear, river dolphins receive sound through the throat, from which it passes through a low-impedance fat-filled cavity to the inner ear. The ear is acoustically isolated from the skull by air-filled sinus pockets, which allows for greater directional hearing underwater.[29] Dolphins send out high frequency clicks from an organ known as a melon. This melon consists of fat, and the skull of any such creature containing a melon will have a large depression. This allows river dolphins to produce biosonar for orientation.[23][30]: 203–427 [31][32] They are so dependent on echolocation that they can survive even if they are blind.[33] Beyond locating an object, echolocation also provides the animal with an idea on the object's shape and size, though how exactly this works is not yet understood. The small hairs on the rostrum of the Amazon river dolphin are believed to function as a tactile sense, possibly to compensate for their poor eyesight.[34] River dolphins have very small eyes for their size, and do not have a very good sense of sight.[3] In addition, the eyes are placed on the sides of the head, so the vision consists of two fields, rather than a binocular view like humans have. When river dolphins surface, their lens and cornea correct the nearsightedness that results from the refraction of light.[35] They have both rod and cone cells, meaning they can see in both dim and bright light.[35] Most river dolphins have slightly flattened eyeballs, enlarged pupils (which shrink as they surface to prevent damage), slightly flattened corneas and a tapetum lucidum; these adaptations allow for large amounts of light to pass through the eye and, therefore, a very clear image of the surrounding area. They also have glands on their eyelids and an outer corneal layer that act as protection for the cornea.[30]: 505–519 Olfactory lobes are absent in river dolphins, suggesting that they have no sense of smell.[30]: 481–505 River dolphins are not thought to have a sense of taste, as their taste buds are atrophied or missing altogether. However, some dolphins have preferences between different kinds of fish, indicating some sort of attachment to taste.[30]: 447–454 Development and agriculture have had devastating impacts on the habitats on river dolphins. The total population of Araguaian river dolphins is estimated to be between 600 and 1,500 individuals, and genetic diversity is limited.[14] The ecology of their habitat has been adversely affected by agricultural, ranching and industrial activities, as well as by the use of dams for hydroelectric power. The inhabited section of the Araguaia River probably extends over about 900 miles (1,400 km) out of a total length of 1,300 miles (2,100 km). The Tocantins river habitat is fragmented by six hydroelectric dams, so the population there is at particular risk.[14] Its probable eventual IUCN status is vulnerable or worse.[14][36]: 54–58 Both subspecies of South Asian river dolphins have been very adversely affected by human use of the river systems in the subcontinent. Irrigation has lowered water levels throughout both subspecies' ranges. Poisoning of the water supply from industrial and agricultural chemicals may have also contributed to population decline. Perhaps the most significant issue is the building of more than 50 dams along many rivers, causing the segregation of populations and a narrowed gene pool in which the dolphins can breed. Currently, three subpopulations of Indus river dolphins are considered capable of long-term survival if protected.[36]: 31–32, 37–38 [37] As China developed economically, pressure on the baiji river dolphin grew significantly.[36]: 41–46 Industrial and residential waste flowed into the Yangtze. The riverbed was dredged and reinforced with concrete in many locations. Ship traffic multiplied, boats grew in size, and fishermen employed wider and more lethal nets. Noise pollution caused the nearly blind animal to collide with propellers. Stocks of the dolphin's prey declined drastically in the late 20th century, with some fish populations declining to one thousandth of their pre-industrial levels.[38] In the 1950s, the population was estimated at 6,000 animals,[39] but declined rapidly over the subsequent five decades. Only a few hundred were left by 1970. Then the number dropped down to 400 by the 1980s and then to 13 in 1997 when a full-fledged search was conducted. On December 13, 2006, the baiji (Lipotes vexillifer) was declared \"functionally extinct\", after a 45-day search by leading experts in the field failed to find a single specimen. The last verified sighting was in September 2004.[4] The region of the Amazon in Brazil has an extension of 3,100,000 sq mi (8,000,000 km2) containing diverse fundamental ecosystems.[40][41] One of these ecosystems is a floodplain, or a várzea forest, and is home to a large number of fish species which are an essential resource for human consumption.[42] The várzea is also a major source of income through excessive local commercialized fishing.[40][43][44] Várzea consist of muddy river waters containing a vast number and diversity of nutrient-rich species.[45] The abundance of distinct fish species lures the Amazon River dolphin into the várzea areas of high water occurrences during the seasonal flooding.[46] In addition to attracting predators such as the Amazon river dolphin, these high-water occurrences are an ideal location to draw in the local fisheries.[36]: 54–58 Human fishing activities directly compete with the dolphins for the same fish species, the tambaqui (Colossoma macropomum) and the pirapitinga (Piaractus brachypomus), resulting in deliberate or unintentional catches of the Amazon river dolphin.[47][48][49][40][50][51][52][53] The local fishermen overfish, and when the Amazon river dolphins remove the commercialized fish from the nets and lines, it damages the equipment and the capture and causes a negative reaction from the local fishermen.[49][51][52] The Brazilian Institute of Environment and Renewable Natural Resources prohibit fishermen from killing the Amazon river dolphin, yet they are not compensated for the damage to their equipment and the loss of their catch.[53] During the process of catching the commercialized fish, the Amazon river dolphins get caught in the nets and exhaust themselves until they die, or the local fishermen deliberately kill the dolphins that become entangled in their nets.[42] The carcasses are discarded, consumed, or used as bait to attract a scavenger catfish, the piracatinga (Calophysus macropterus).[42][54] The use of the Amazon river dolphin carcass as bait for the piracatinga dates back from 2000.[54] The increasing consumption demand by the local inhabitants and Colombia for the piracatinga has created a market for distribution of the Amazon river dolphin carcasses to be used as bait throughout these regions.[36]: 54–58 [53] For example, of the 15 dolphin carcasses found in the Japurá River in 2010–2011 surveys, 73% of the dolphins were killed for bait, disposed of, or abandoned in entangled gillnets.[42] The data does not fully represent the actual overall number of deaths of the Amazon river dolphins, whether accidental or intentional, because a variety of factors make it extremely complicated to record and medically examine all the carcasses.[42][48][51] Scavenger species feed upon them and the complexity of the river currents makes it nearly impossible to locate all the carcasses.[42] More importantly, the local fishermen do not report these deaths out of fear that legal action will be taken against them,[42] as the Amazon river dolphin and other cetaceans are protected under the Brazilian federal law, prohibiting any takes, harassments, and kills of the species.[55] This section needs expansion. You can help by adding to it. (January 2024) The Global Declaration for River Dolphins was signed by nine countries on 24 October 2023, a date chosen as it is known as the International River Dolphin Day. This pact is intended to promote research and cooperation between countries with river dolphin populations. It is hoped that five further countries will join.[56] A baiji conservation dolphinarium was established at the Institute of Hydrobiology (IHB) in Wuhan in 1992. This was planned as a backup to any other conservation efforts by producing an area completely protected from any threats, and where the baiji could be easily observed. The site includes an indoor and outdoor holding pool, a water filtration system, food storage and preparation facilities, research labs and a small museum. The aim is to also generate income from tourism which can be put towards the baiji plight. The pools are not very large, only kidney shaped tanks with dimensions of 82 feet (25 m) arc 23 feet (7.0 m) width and 11 feet (3.4 m) depth, 33 feet (10 m) diameter, 6.6 feet (2.0 m) deep and 39 feet (12 m) diameter, 11 feet (3.4 m) deep, and are not capable of holding many baijis at one time. Douglas Adams and Mark Carwardine documented their encounters with the endangered animals on their conservation travels for the BBC programme Last Chance to See. The book by the same name, published in 1990, included pictures of a captive specimen, a male named Qi Qi (淇淇) that lived in the Wuhan Institute of Hydrobiology dolphinarium from 1980 to July 14, 2002. Discovered by a fisherman in Dongting Lake, he became the sole resident of the Baiji Dolphinarium (白鱀豚水族馆) beside East Lake. A sexually mature female was captured in late 1995, but died after half a year in 1996 when the Shishou Tian-e-Zhou Baiji Semi-natural Reserve (石首半自然白鱀豚保护区), which had contained only finless porpoises since 1990, was flooded.[57] The Amazon river dolphin has historically been kept in dolphinariums. Today, only three exist in captivity: one in Acuario de Valencia in Venezuela, one in Zoologico de Guistochoca in Peru, and one in Duisburg Zoo in Germany. Several hundred were captured between the 1950s and 1970s, and were distributed in dolphinariums throughout the US, Europe, and Japan. Around 100 went to US dolphinariums, and of that, only 20 survived; the last (named Chuckles) died in Pittsburgh Zoo in 2002.[36]: 58–59 In Chinese mythology, the baiji has many origin stories. For example, near the mouth of the Yangtze, the baiji was a princess that had lost her parents and had lived with her stepfather, whom she had longed to get away from. The stepfather wanted to trade her since she would be sold for a great sum of money, but as they were crossing the river to get to the trader, a storm rolled in. The enraged stepfather tried to take her, but she plunged herself into the river, was transformed into a dolphin before she drowned, and swam away from her abusive stepfather, who also fell in and was transformed into a porpoise.[59] In another story, the baiji was the daughter of a general deported from the city of Wuhan during a war who ran away while her father was in duty. Later, the general met a woman who told him how her father was a general. When he realized that she was his daughter, he threw himself into the river out of shame, and his daughter ran after him and also fell into the river. Before they were drowned, the daughter was transformed into a dolphin and the general into a porpoise.[59] Amazon river dolphins, known by the natives as the boto, encantados or toninhas, are very prevalent in the mythology of the native South Americans. They are often characterized in mythology with superior musical ability, seductiveness and love of sex that often results in illegitimate children, and attraction to parties. Despite the fact that the Encante are said to come from a utopia full of wealth and without pain or death, the encantados crave the pleasures and hardships of human societies.[60] Transformation into human form is said to be rare, and usually occurs at night. The encantado will often be seen running from a festa, despite protests from the others for it to stay, and can be seen by pursuers as it hurries to the river and reverts to dolphin form. When it is under human form, it wears a hat to hide its blowhole, which does not disappear with the shapeshift.[60] Besides the ability to shapeshift into human form, encantados frequently wield other magical abilities, such as controlling storms, hypnotizing humans into doing their will, transforming humans into encantados, and inflicting illness, insanity, and even death. Shamans often intervene in these situations.[60] Kidnapping is also a common theme in such folklore. Encantados are said to be fond of abducting humans with whom they fall in love, children born of their illicit love affairs, or just about anyone near the river who can keep them company, and taking them back to the Encante. The fear of this is so great among people who live near the Amazon River that both children and adults are terrified of going near the water between dusk and dawn, or entering water alone. Some who supposedly have encountered encantados while out in their canoes have been said to have gone insane, but the creatures seem to have done little more than follow their boats and nudge them from time to time.[60]"}
{"url": "https://en.m.wikipedia.org/wiki/Crayfish", "text": "Some kinds of crayfish are known locally as lobsters,[4] crawdads,[5] mudbugs,[5] and yabbies. In the Eastern United States, \"crayfish\" is more common in the north, while \"crawdad\" is heard more in central and southwestern regions, and \"crawfish\" farther south, although considerable overlaps exist.[6] The body of a decapodcrustacean, such as a crab, lobster, or prawn (shrimp), is made up of twenty body segments grouped into two main body parts, the cephalothorax and the abdomen. Each segment may possess one pair of appendages, although in various groups, these may be reduced or missing. On average, crayfish grow to 17.5 cm (6.9 in) in length. Walking legs have a small claw at the end.[8] Crayfish are opportunistic omnivorous scavengers, with the ability to filter and process mud.[9] In aquaculture ponds using isotope analysis they were shown to build body tissue selectively from the animal protein portion of pelleted food and not the other components of the pellet.[10] They have the potential to eat most foods, even nutrient poor material such as grass, leaves, and paper, but can be highly selective and need variety to balance their diet. The personalities of the individual crayfish can be a key determinant in the food preference behaviour in aquaria.[citation needed] Crayfish all over the world can be seen in an ecological role of benthic dwellers, so this is where most of their food is obtained - at the sediment/water interface in ponds, lakes, swamps, or burrows. When the gut contents are analysed, most of the contents is mud: fine particulate organic matter (FPOM) and mixed particles of lignin and cellulose (roots, leaves, bark, wood).[11] Some animal material can also be identified, but this only contributes a small portion of the diet by volume. They feed on submerged vegetable material at times, but their ability to catch large living animal material is restricted. They can feed on interstitial organisms if they can be grasped in the small feeding claws. They can be lured into traps with an array of baits from dog biscuits, fish heads, meat, etc., all of which reinforces the fact that they are generalist feeders. On a day-to-day basis, they consume what they can acquire in their immediate environment in limited space and time available - detritus. At a microbial level, the FPOM has a high surface area of organic particles and consists of a plethora of substrate and bacteria, fungi, micro-algae, meiofauna, partially decomposed organic material and mucus. This mucus or \"slime\" is a biofilm and can be felt on the surface of leaves and sticks. Also crayfish have been shown to be coprophagic - eating their own faeces, they also eat their own exuviae (moultedcarapace) and each other.[10] They have even been observed leaving the water to graze.[citation needed] Detritus or mud is a mixture of dead plankton (plant and animal), organic wastes from the water column, and debris derived from the aquatic and terrestrial environments. Mostly detritus is in the end phase of decomposition and is recognised as black organic mud. The crayfish usually ingest the material in only a few minutes, as distinct from grazing for many hours. The material is mixed with digestive fluids and sorted by size. The finer particles follow a slower and more exacting route through to the hindgut, compared to the coarser material. The coarser material is eliminated first and often reappears in approximately 10 to 12 hours, whereas the finer material is usually eliminated from 16 to 26 hours after ingestion.[12] All waste products coming out through the hindgut are wrapped in a peritrophic membrane, so they look like a tube. Such an investment in the wrapping of the microbial free faeces in a protein rich membrane is most likely the reason they are coprophagic. Such feeding behaviour based on selection, ingestion, and extreme processing ensures periodic feeding, as distinct from continuous grazing. They tend to eat to satiation and then take many hours to process the material, leaving minimal chance of having more room to ingest other items. Crayfish usually have limited home range and so they rest, digest, and eliminate their waste, most commonly in the same location each day. Feeding exposes the crayfish to risk of predation, and so feeding behaviour is often rapid and synchronised with feeding processes that reduce such risks — eat, hide, process and eliminate. Knowledge of the diet of these creatures was considered too complex since the first book ever written in the field of zoology, The Crayfish by T.H. Huxley (1879), where they were described as \"detritivores\". This is why most researchers have not attempted to understand the diet of freshwater crayfish. The most complex study which matched the structure and function of the whole digestive track with ingested material was performed in the 1990s by Brett O'Brien on marron,[12] the least aggressive of the larger freshwater crayfish with aquaculture potential, similar to redclaw and yabbies. The greatest diversity of crayfish species is found in southeastern North America, with over 330 species in 15 genera, all in the family Cambaridae. A further genus of astacid crayfish is found in the Pacific Northwest and the headwaters of some rivers east of the Continental Divide. Many crayfish are also found in lowland areas where the water is abundant in calcium, and oxygen rises from underground springs.[17] Crayfish are also found in some non-coastal wetlands; eight species of crayfish live in Iowa,[18] for example. In 1983, Louisiana designated the crayfish, or crawfish as they are commonly called, as its official state crustacean.[19] Louisiana produces 100 million pounds (45 million kilograms) of crawfish per year with the red swamp and white river crawfish being the main species harvested.[20] Crawfish are a part of Cajun culture dating back hundreds of years.[21] A variety of cottage industries have developed as a result of commercialized crawfish iconography. Their products include crawfish attached to wooden plaques, T-shirts with crawfish logos, and crawfish pendants, earrings, and necklaces made of gold or silver.[22] In the United Kingdom and Ireland, the terms crayfish or crawfish commonly refer to the European spiny lobster, a saltwater species found in much of the East Atlantic and Mediterranean.[33] The only true crayfish species native to the British Isles is the endangered white clawed crayfish.[34][35] Crayfish are susceptible to infections such as crayfish plague and to environmental stressors including acidification. In Europe, they are particularly threatened by crayfish plague, which is caused by the North American water moldAphanomyces astaci. This water mold was transmitted to Europe when North American species of crayfish were introduced.[40] Species of the genus Astacus are particularly susceptible to infection, allowing the plague-coevolved signal crayfish (native to western North America) to invade parts of Europe.[41] In several countries, particularly in Europe, native species of crayfish are under threat by imported species, particularly the signal crayfish (Pacifastacus leniusculus).[43][44] Crayfish are also considered an invasive predatory species, endangering native European species such as the Italian agile frog. Crayfish are eaten worldwide. Like other edible crustaceans, only a small portion of the body of a crayfish is eaten. In most prepared dishes, such as soups, bisques and étouffées, only the tail portion is served. At crawfish boils or other meals where the entire body of the crayfish is presented, other portions, such as the claw meat, may be eaten.[citation needed] Research shows that crayfish do not die immediately when boiled alive, and respond to pain in a similar way to mammals. Then the stress hormone cortisol is released and this leads to the formation of lactic acid in the muscles, which makes the meat taste sour. Crayfish can be cooked more humanely by first freezing them unconscious for a few hours, then destroying the central nervous system along their abdomen by cutting the crayfish lengthwise with a long knife down the center of the crayfish before cooking it.[45] Global crayfish production is centered in Asia, primarily China. In 2018, Asian production accounted for 95% of the world's crawfish supply. [46] Crayfish is part of Swedish cuisine and is usually eaten in August at special crayfish parties (Swedish Kräftskiva). Documentation of the consumption of crayfish dates to at least the 16th century. On the Swedish west coast, Nephrops norvegicus (Swedish Havskräfta, sea crayfish) is more commonly eaten while various freshwater crayfish are consumed in the rest of the country. Prior to the 1960s, crayfish was largely inaccessible to the urban population in Sweden and consumption was largely limited to the upper classes or farmers holding fishing rights in fresh water lakes. With the introduction of import of frozen crayfish the crayfish party is now widely practiced across all spheres in Sweden and among the Swedish-speaking population of Finland.[47] In the United States, crayfish production is strongly centered in Louisiana, with 93% of crayfish farms located in the state as of 2018.[48] In 1987, Louisiana produced 90% of the crayfish harvested in the world, 70% of which were consumed locally.[49] In 2007, the Louisiana crayfish harvest was about 54,800 tons, almost all of it from aquaculture.[50] About 70–80% of crayfish produced in Louisiana are Procambarus clarkii (red swamp crawfish), with the remaining 20–30% being Procambarus zonangulus (white river crawfish).[51] Optimum dietary nutritional requirement of freshwater crayfish, or crayfish nutrient specifications are now available for aquaculture feed producers [52] Like all crustaceans, crayfish are not kosher because they are aquatic animals that do not have both fins and scales.[53] They are therefore not eaten by observant Jews.[54] When using crayfish as bait, it is important to fish in the same environment where they were caught. An Illinois State University report that focused on studies conducted on the Fox River and Des Plaines River watershed stated that rusty crayfish, initially caught as bait in a different environment, were dumped into the water and \"outcompeted the native clearwater crayfish\".[60] Other studies confirmed that transporting crayfish to different environments has led to various ecological problems, including the elimination of native species.[61] Transporting crayfish as live bait has also contributed to the spread of zebra mussels in various waterways throughout Europe and North America, as they are known to attach themselves to exoskeleton of crayfishes.[62][63][64] Crayfish are kept as pets in freshwater aquariums. They prefer foods like shrimp pellets or various vegetables, but will also eat tropical fish food, regular fish food, algae wafers, and small fish that can be captured with their claws. A report by the National Park Service[65] as well as video and anecdotal reports by aquarium owners[66] indicate that crayfish will eat their moulted exoskeleton \"to recover the calcium and phosphates contained in it.\"[65] As omnivores, crayfish will eat almost anything; therefore, they may explore the edibility of aquarium plants in a fish tank. However, most species of dwarf crayfish, such as Cambarellus patzcuarensis, will not destructively dig or eat live aquarium plants.[67] In some nations, such as the United Kingdom, United States, Australia, and New Zealand, imported alien crayfish are a danger to local rivers. The three most widespread American species invasive in Europe are Faxonius limosus, Pacifastacus leniusculus and Procambarus clarkii.[40] Crayfish may spread into different bodies of water because specimens captured for pets in one river are often released into a different catchment. There is a potential for ecological damage when crayfish are introduced into non-native bodies of water: e.g., crayfish plague in Europe, or the introduction of the common yabby (Cherax destructor) into drainages east of the Great Dividing Range in Australia.[68] The Protivin brewery in the Czech Republic uses crayfish outfitted with sensors to detect any changes in their bodies or pulse activity in order to monitor the purity of the water used in their product. The creatures are kept in a fish tank that is fed with the same local natural source water used in their brewing. If three or more of the crayfish have changes to their pulses, employees know there is a change in the water and examine the parameters.[69] Scientists also monitor crayfish in the wild in natural bodies of water to study the levels of pollutants there.[69][70][71]"}
{"url": "https://en.m.wikipedia.org/wiki/Odontoceti", "text": "Toothed whales range in size from the 1.4 m (4 ft 7 in) and 54 kg (119 lb) vaquita to the 20 m (66 ft) and 100 t (98 long tons; 110 short tons) sperm whale. Several species of odontocetes exhibit sexual dimorphism, in that there are size or other morphological differences between females and males. They have streamlined bodies and two limbs that are modified into flippers. Some can travel at up to 20 knots. Odontocetes have conical teeth designed for catching fish or squid. They have well-developed hearing that is well adapted for both air and water, so much so that some can survive even if they are blind. Some species are well adapted for diving to great depths. Almost all have a layer of fat, or blubber, under the skin to keep warm in the cold water, with the exception of river dolphins. Toothed whales consist of some of the most widespread mammals, but some, as with the vaquita, are restricted to certain areas. Odontocetes feed largely on fish and squid, but a few, like the orca, feed on mammals, such as pinnipeds. Males typically mate with multiple females every year, making them polygynous. Females mate every two to three years. Calves are typically born in the spring and summer, and females bear the responsibility for raising them, but more sociable species rely on the family group to care for calves. Many species, mainly dolphins, are highly sociable, with some pods reaching over a thousand individuals. The tube in the head, through which this kind fish takes its breath and spitting water, located in front of the brain and ends outwardly in a simple hole, but inside it is divided by a downward bony septum, as if it were two nostrils; but underneath it opens up again in the mouth in a void. In Aristotle's time, the fourth century BC, whales were regarded as fish due to their superficial similarity. Aristotle, however, could already see many physiological and anatomical similarities with the terrestrial vertebrates, such as blood (circulation), lungs, uterus, and fin anatomy.[citation needed] His detailed descriptions were assimilated by the Romans, but mixed with a more accurate knowledge of the dolphins, as mentioned by Pliny the Elder in his Natural history. In the art of this and subsequent periods, dolphins are portrayed with a high-arched head (typical of porpoises) and a long snout. The harbor porpoise is one of the most accessible species for early cetologists, because it could be seen very close to land, inhabiting shallow coastal areas of Europe. Many of the findings that apply to all cetaceans were therefore first discovered in the porpoises.[2] One of the first anatomical descriptions of the airways of the whales on the basis of a harbor porpoise dates from 1671 by John Ray. It nevertheless referred to the porpoise as a fish.[3][4] Toothed whales, as well as baleen whales, are descendants of land-dwelling mammals of the artiodactylorder (even-toed ungulates). They are closely related to the hippopotamus, sharing a common ancestor that lived around 54 million years ago (mya).[6] The primitive cetaceans, or archaeocetes, first took to the sea approximately 49 mya and became fully aquatic by 5–10 million years later.[7] The ancestors of toothed whales and baleen whales diverged in the early Oligocene. This was due to a change in the climate of the southern oceans that affected where the environment of the plankton that these whales ate.[8] The adaptation of echolocation and enhanced fat synthesis in blubber occurred when toothed whales split apart from baleen whales, and distinguishes modern toothed whales from fully aquatic archaeocetes. This happened around 34 mya.[9] Unlike toothed whales, baleen whales do not have wax ester deposits nor branched fatty chain acids in their blubber. Thus, more recent evolution of these complex blubber traits occurred after baleen whales and toothed whales split, and only in the toothed whale lineage.[10] [11][12] Modern toothed whales do not rely on their sense of sight, but rather on their sonar to hunt prey. Echolocation also allowed toothed whales to dive deeper in search of food, with light no longer necessary for navigation, which opened up new food sources.[13][14] Toothed whales (Odontocetes) echolocate by creating a series of clicks emitted at various frequencies. Sound pulses are emitted through their melon-shaped foreheads, reflected off objects, and retrieved through the lower jaw. Skulls of Squalodon show evidence for the first hypothesized appearance of echolocation.[15]Squalodon lived from the early to middle Oligocene to the middle Miocene, around 33-14 mya. Squalodon featured several commonalities with modern Odontocetes. The cranium was well compressed, the rostrum telescoped outward (a characteristic of the modern parvorder Odontoceti), giving Squalodon an appearance similar to that of modern toothed whales. However, it is thought unlikely that squalodontids are direct ancestors of living dolphins.[16] Toothed whales have torpedo-shaped bodies with usually inflexible necks, limbs modified into flippers, nonexistent external ear flaps, a large tail fin, and bulbous heads (with the exception of sperm whales). Their skulls have small eye orbits, long beaks (with the exception sperm whales), and eyes placed on the sides of their heads. Toothed whales range in size from the 4.5 ft (1.4 m) and 120 lb (54 kg) vaquita to the 20 m (66 ft) and 55 t (61-short-ton) sperm whale. Overall, they tend to be dwarfed by their relatives, the baleen whales (Mysticeti). Several species have sexual dimorphism, with the females being larger than the males. One exception is with the sperm whale, which has males larger than the females.[17][18] Odontocetes possess teeth with cementum cells overlying dentine cells. Unlike human teeth, which are composed mostly of enamel on the portion of the tooth outside of the gum, whale teeth have cementum outside the gum. Only in larger whales, where the cementum is worn away on the tip of the tooth, does enamel show.[17] There is only a single set of functional teeth (monophyodont dentition).[19] Except for the sperm whale, most toothed whales are smaller than the baleen whales. The teeth differ considerably among the species. They may be numerous, with some dolphins bearing over 100 teeth in their jaws. At the other extreme are the narwhals with their single long tusks and the almost toothless beaked whales with tusk-like teeth only in males.[20] Not all species are believed to use their teeth for feeding. For instance, the sperm whale likely uses its teeth for aggression and showmanship.[17] Breathing involves expelling stale air from their one blowhole, forming an upward, steamy spout, followed by inhaling fresh air into the lungs. Spout shapes differ among species, which facilitates identification. The spout only forms when warm air from the lungs meets cold air, so it does not form in warmer climates, as with river dolphins.[17][21][22] Almost all cetaceans have a thick layer of blubber, with the exception of river dolphins. In species that live near the poles, the blubber can be as thick as 11 in (28 cm). This blubber can help with buoyancy, protection to some extent as predators would have a hard time getting through a thick layer of fat, energy for fasting during leaner times, and insulation from the harsh climates. Calves are born with only a thin layer of blubber, but some species compensate for this with thick lanugos.[17][23] Toothed whales have also evolved the ability to store large amounts of wax esters in their adipose tissue as an addition to or in complete replacement of other fats in their blubber. They can produce isovaleric acid from branched chain fatty acids (BCFA). These adaptations are unique, are only in more recent, derived lineages and were likely part of the transition for species to become deeper divers as the families of toothed whales (Physeteridae, Kogiidae, and Ziphiidae) that have the highest quantities of wax esters and BCFAs in their blubber are also the species that dive the deepest and for the longest amount of time.[10] Toothed whales have a two-chambered stomach similar in structure to terrestrial carnivores. They have fundic and pyloric chambers.[24] Cetaceans have two flippers on the front, and a tail fin. These flippers contain four digits. Although toothed whales do not possess fully developed hind limbs, some, such as the sperm whale, possess discrete rudimentary appendages, which may contain feet and digits.[citation needed] Toothed whales are fast swimmers in comparison to seals, which typically cruise at 5–15 knots, or 9–28 km/h (5.6–17.4 mph); the sperm whale, in comparison, can travel at speeds of up to 35 km/h (22 mph). The fusing of the neck vertebrae, while increasing stability when swimming at high speeds, decreases flexibility, rendering them incapable of turning their heads; river dolphins, however, have unfused neck vertebrae and can turn their heads. When swimming, toothed whales rely on their tail fins to propel them through the water. Flipper movement is continuous. They swim by moving their tail fin and lower body up and down, propelling themselves through vertical movement, while their flippers are mainly used for steering. Some species log out of the water, which may allow them to travel faster.[further explanation needed] Their skeletal anatomy allows them to be fast swimmers. Most species have a dorsal fin.[17][23] Most toothed whales are adapted for diving to great depths, porpoises are one exception. In addition to their streamlined bodies, they can slow their heart rate to conserve oxygen; blood is rerouted from tissue tolerant of water pressure to the heart and brain among other organs; haemoglobin and myoglobin store oxygen in body tissue; and they have twice the concentration of myoglobin than haemoglobin. Before going on long dives, many toothed whales exhibit a behaviour known as sounding; they stay close to the surface for a series of short, shallow dives while building their oxygen reserves, and then make a sounding dive.[25] Toothed whale eyes are relatively small for their size, yet they do retain a good degree of eyesight. As well as this, the eyes are placed on the sides of its head, so their vision consists of two fields, rather than a binocular view as humans have. When a beluga surfaces, its lenses and corneas correct the nearsightedness that results from the refraction of light; they contain both rod and cone cells, meaning they can see in both dim and bright light. They do, however, lack short wavelength-sensitive visual pigments in their cone cells, indicating a more limited capacity for colour vision than most mammals.[26] Most toothed whales have slightly flattened eyeballs, enlarged pupils (which shrink as they surface to prevent damage), slightly flattened corneas, and a tapetum lucidum; these adaptations allow for large amounts of light to pass through the eye, and, therefore, a very clear image of the surrounding area. In water, a whale can see around 10.7 m (35 ft) ahead of itself, but they have a smaller range above water. They also have glands on the eyelids and outer corneal layer that act as protection for the cornea.[17][27]: 505–519 Toothed whales are not thought to have a good sense of taste, as their taste buds are atrophied or missing altogether. However, some dolphins have preferences between different kinds of fish, indicating some sort of attachment to taste.[27]: 447–455 Diagram illustrating sound generation, propagation and reception in a toothed whale. Outgoing sounds are red and incoming ones are green Toothed whales are capable of making a broad range of sounds using nasal airsacs located just below the blowhole.[28] Clicks are directional and are used for echolocation, often occurring in a short series called a click train. The click rate increases when approaching an object of interest. Toothed whale biosonar clicks are amongst the loudest sounds made by marine animals.[29] The cetacean ear has specific adaptations to the marine environment. In humans, the middle ear works as an impedance equalizer between the outside air's low impedance and the cochlear fluid's high impedance. In whales, and other marine mammals, no great difference exists between the outer and inner environments. Instead of sound passing through the outer ear to the middle ear, whales receive sound through the throat, from which it passes through a low-impedance, fat-filled cavity to the inner ear.[30] The ear is acoustically isolated from the skull by air-filled sinus pockets, which allow for greater directional hearing underwater.[31] Odontocetes send out high-frequency clicks from an organ known as a melon. This melon consists of fat, and the skull of any such creature containing a melon will have a large depression. The melon size varies between species, the bigger it is, the more dependent they are on it. A beaked whale, for example, has a small bulge sitting on top of its skull, whereas a sperm whale's head is filled mainly with the melon.[17][27]: 1–19 [32][33] Odontocetes are well adapted to hear sounds at ultrasonic frequencies, as opposed to mysticetes who generally hear sounds within the range of infrasonic frequencies.[34] Bottlenose dolphins have been found to have signature whistles unique to a specific individual. These whistles are used for dolphins to communicate with one another by identifying an individual. It can be seen as the dolphin equivalent of a name for humans.[35] Because dolphins are generally associated in groups, communication is necessary. Signal masking is when other similar sounds (conspecific sounds) interfere with the original acoustic sound.[36] In larger groups, individual whistle sounds are less prominent. Dolphins tend to travel in pods, in some instances including up to 600 members. [37] Cetaceans are known to communicate and therefore are able to teach, learn, cooperate, scheme, and grieve.[38] The neocortex of many species of dolphins is home to elongated spindle neurons that, prior to 2007, were known only in hominids.[39] In humans, these cells are involved in social conduct, emotions, judgement, and theory of mind. Dolphin spindle neurons are found in areas of the brain homologous to where they are found in humans, suggesting they perform a similar function.[17] Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales around the two-thirds or three-quarters exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalisation quotient that can be used as another indication of animal intelligence. Sperm whales have the largest brain mass of any animal on earth, averaging 8,000 cm3 (490 in3) and 7.8 kg (17 lb) in mature males, in comparison to the average human brain which averages 1,450 cm3 (88 in3) in mature males.[40] The brain to body mass ratio in some odontocetes, such as belugas and narwhals, is second only to humans.[41] Researchers pushed a pole with a sponge attached along the substrate to simulate the sponging behavior by dolphins Dolphins are known to engage in complex play behaviour, which includes such things as producing stable underwater toroidal air-core vortex rings or \"bubble rings\". Two main methods of bubble ring production are: rapid puffing of a burst of air into the water and allowing it to rise to the surface, forming a ring, or swimming repeatedly in a circle and then stopping to inject air into the helical vortex currents thus formed. They also appear to enjoy biting the vortex rings, so that they burst into many separate bubbles and then rise quickly to the surface. Dolphins are known to use this method during hunting.[42] Dolphins have also been known to use tools. In Shark Bay, a population of Indo-Pacific bottlenose dolphins put sponges on their beak to protect them from abrasions and sting ray barbs while foraging in the seafloor.[43] This behaviour is passed on from mother to daughter, and it is only observed in 54 female individuals.[44] Self-awareness is seen, by some, to be a sign of highly developed, abstract thinking. Self-awareness, though not well-defined scientifically, is believed to be the precursor to more advanced processes like metacognitive reasoning (thinking about thinking) that are typical of humans. Research in this field has suggested that cetaceans, among others,[45] possess self-awareness.[46] The most widely used test for self-awareness in animals is the mirror test, in which a temporary dye is placed on an animal's body, and the animal is then presented with a mirror; then whether the animal shows signs of self-recognition is determined.[46] In 1995, Marten and Psarakos used television to test dolphin self-awareness.[47] They showed dolphins real-time footage of themselves, recorded footage, and another dolphin. They concluded that their evidence suggested self-awareness rather than social behavior. While this particular study has not been repeated since then, dolphins have since \"passed\" the mirror test.[46] Spectrogram of dolphin vocalizations. Whistles, whines, and clicks are visible as upside down V's, horizontal striations, and vertical lines, respectively. Dolphins are capable of making a broad range of sounds using nasal airsacs located just below the blowhole. Roughly three categories of sounds can be identified: frequency modulated whistles, burst-pulsed sounds and clicks. Dolphins communicate with whistle-like sounds produced by vibrating connective tissue, similar to the way human vocal cords function,[28] and through burst-pulsed sounds, though the nature and extent of that ability is not known. The clicks are directional and are for echolocation, often occurring in a short series called a click train. The click rate increases when approaching an object of interest. Dolphin echolocation clicks are amongst the loudest sounds made by marine animals.[48] Bottlenose dolphins have been found to have signature whistles, a whistle that is unique to a specific individual. These whistles are used in order for dolphins to communicate with one another by identifying an individual. It can be seen as the dolphin equivalent of a name for humans.[35] These signature whistles are developed during a dolphin's first year; it continues to maintain the same sound throughout its lifetime.[49] An auditory experience influences the whistle development of each dolphin. Dolphins are able to communicate to one another by addressing another dolphin through mimicking their whistle. The signature whistle of a male bottlenose dolphin tends to be similar to that of his mother, while the signature whistle of a female bottlenose dolphin tends to be more identifying.[50] Bottlenose dolphins have a strong memory when it comes to these signature whistles, as they are able to relate to a signature whistle of an individual they have not encountered for over twenty years.[51] Research done on signature whistle usage by other dolphin species is relatively limited. The research on other species done so far has yielded varied outcomes and inconclusive results.[52][53][54][55] Sperm whales can produce three specific vocalisations: creaks, codas, and slow clicks. A creak is a rapid series of high-frequency clicks that sounds somewhat like a creaky door hinge. It is typically used when homing in on prey.[56]: 135 A coda is a short pattern of 3 to 20 clicks that is used in social situations to identify one another (like a signature whistle), but it is still unknown whether sperm whales possess individually specific coda repertoires or whether individuals make codas at different rates.[57] Slow clicks are heard only in the presence of males (it is not certain whether females occasionally make them). Males make a lot of slow clicks in breeding grounds (74% of the time), both near the surface and at depth, which suggests they are primarily mating signals. Outside breeding grounds, slow clicks are rarely heard, and usually near the surface.[56]: 144 All whales are carnivorous and predatory. Odontocetes, as a whole, mostly feed on fish and cephalopods, and then followed by crustaceans and bivalves. All species are generalist and opportunistic feeders. Some may forage with other kinds of animals, such as other species of whales or certain species of pinnipeds.[23][58] One common feeding method is herding, where a pod squeezes a school of fish into a small volume, known as a bait ball. Individual members then take turns plowing through the ball, feeding on the stunned fish.[59] Coralling is a method where dolphins chase fish into shallow water to catch them more easily.[59] Orcas and bottlenose dolphins have also been known to drive their prey onto a beach to feed on it, a behaviour known as beach or strand feeding.[60][61] The shape of the snout may correlate with tooth number and thus feeding mechanisms. The narwhal, with its blunt snout and reduced dentition, relies on suction feeding.[62] Sperm whales usually dive between 300 and 800 metres (980 and 2,620 ft), and sometimes 1 to 2 kilometres (3,300 to 6,600 ft), in search of food.[56]: 79 Such dives can last more than an hour.[56]: 79 They feed on several species, notably the giant squid, but also the colossal squid, octopuses, and fish like demersalrays, but their diet is mainly medium-sized squid.[56]: 43–55 Some prey may be taken accidentally while eating other items.[56]: 43–55 A study in the Galápagos found that squid from the genera Histioteuthis (62%), Ancistrocheirus (16%), and Octopoteuthis (7%) weighing between 12 and 650 grams (0.026 and 1.433 lb) were the most commonly taken.[63] Battles between sperm whales and giant squid or colossal squid have never been observed by humans; however, white scars are believed to be caused by the large squid. A 2010 study suggests that female sperm whales may collaborate when hunting Humboldt squid.[64] The orca is known to prey on numerous other toothed whale species. One example is the false killer whale.[65] To subdue and kill whales, orcas continually ram them with their heads; this can sometimes kill bowhead whales, or severely injure them. Other times, they corral their prey before striking. They are typically hunted by groups of 10 or fewer orca, but they are seldom attacked by an individual. Calves are more commonly taken by orca, but adults can be targeted, as well.[66] Groups even attack larger cetaceans such as minke whales, gray whales, and rarely sperm whales or blue whales.[67][68] Other marine mammal prey species include nearly 20 species of seal, sea lion and fur seal.[69] These cetaceans are targeted by terrestrial and pagophilic predators. The polar bear is well-adapted for hunting Arctic whales and calves. Bears are known to use sit-and-wait tactics, as well as active stalking and pursuit of prey on ice or water. Whales lessen the chance of predation by gathering in groups. This, however, means less room around the breathing hole as the ice slowly closes the gap. When out at sea, whales dive out of the reach of surface-hunting orca. Polar bear attacks on belugas and narwhals are usually successful in winter, but rarely inflict any damage in summer.[70] For most of the smaller species of dolphins, only a few of the larger sharks, such as the bull shark, dusky shark, tiger shark, and great white shark, are a potential risk, especially for calves.[71] Dolphins can tolerate and recover from extreme injuries (including shark bites) although the exact methods used to achieve this are not known. The healing process is rapid and even very deep wounds do not cause dolphins to hemorrhage to death. Even gaping wounds restore in such a way that the animal's body shape is restored, and infection of such large wounds are rare.[72] Toothed whales are fully aquatic creatures, which means their birth and courtship behaviours are very different from terrestrial and semiaquatic creatures. Since they are unable to go onto land to calve, they deliver their young with the fetus positioned for tail-first delivery. This prevents the calf from drowning either upon or during delivery. To feed the newborn, toothed whales, being aquatic, must squirt the milk into the mouth of the calf. Being mammals, they have mammary glands used for nursing calves; they are weaned around 11 months of age. This milk contains high amounts of fat which is meant to hasten the development of blubber; it contains so much fat, it has the consistency of toothpaste.[73] Females deliver a single calf, with gestation lasting about a year, dependency until one to two years, and maturity around seven to 10 years, all varying between the species. This mode of reproduction produces few offspring, but increases the survival probability of each one. Females, referred to as \"cows\", carry the responsibility of childcare, as males, referred to as \"bulls\", play no part in raising calves. In orcas, false killer whales, short-finned pilot whales, narwhals, and belugas, there is an unusually long post-reproductive lifespan (menopause) in females. Older females, though unable to have their own children, play a key role in the rearing of other calves in the pod, and in this sense, given the costs of pregnancy especially at an advanced age, extended menopause is advantageous.[74][75] The nose of the whale is filled with a waxy substance that was widely used in candles, oil lamps, and lubricants The head of the sperm whale is filled with a waxy liquid called spermaceti. This liquid can be refined into spermaceti wax and sperm oil. These were much sought after by 18th-, 19th-, and 20th-century whalers. These substances found a variety of commercial applications, such as candles, soap, cosmetics, machine oil, other specialized lubricants, lamp oil, pencils, crayons, leather waterproofing, rustproofing materials, and many pharmaceutical compounds.[76][77][78][79]Ambergris, a solid, waxy, flammable substance produced in the digestive system of sperm whales, was also sought as a fixative in perfumery. Sperm whaling in the 18th century began with small sloops carrying only a pair of whaleboats (sometimes only one). As the scope and size of the fleet increased, so did the rig of the vessels change, as brigs, schooners, and finally ships and barks were introduced. In the 19th-century stubby, square-rigged ships (and later barks) dominated the fleet, being sent to the Pacific (the first being the British whaleship Emilia, in 1788),[80] the Indian Ocean (1780s), and as far away as the Japan grounds (1820) and the coast of Arabia (1820s), as well as Australia (1790s) and New Zealand (1790s).[81][82] A sperm whale is killed and stripped of its blubber and spermaceti Hunting for sperm whales during this period was a notoriously dangerous affair for the crews of the 19th-century whaleboats. Although a properly harpooned sperm whale generally exhibited a fairly consistent pattern of attempting to flee underwater to the point of exhaustion (at which point it would surface and offer no further resistance), it was not uncommon for bull whales to become enraged and turn to attack pursuing whaleboats on the surface, particularly if it had already been wounded by repeated harpooning attempts. A commonly reported tactic was for the whale to invert itself and violently thrash the surface of the water with its fluke, flipping and crushing nearby boats. The estimated historic worldwide sperm whale population numbered 1,100,000 before commercial sperm whaling began in the early 18th century.[83] By 1880, it had declined an estimated 29%.[83] From that date until 1946, the population appears to have recovered somewhat as whaling pressure lessened, but after the Second World War, with the industry's focus again on sperm whales, the population declined even further to only 33%.[83] In the 19th century, between 184,000 and 236,000 sperm whales were estimated to have been killed by the various whaling nations,[84] while in the modern era, at least 770,000 were taken, the majority between 1946 and 1980.[85] Remaining sperm whale populations are large enough so that the species' conservation status is vulnerable, rather than endangered.[83] However, the recovery from the whaling years is a slow process, particularly in the South Pacific, where the toll on males of breeding age was severe.[86] Dolphins and porpoises are hunted in an activity known as dolphin drive hunting. This is accomplished by driving a pod together with boats and usually into a bay or onto a beach. Their escape is prevented by closing off the route to the ocean with other boats or nets. Dolphins are hunted this way in several places around the world, including the Solomon Islands, the Faroe Islands, Peru, and Japan, the most well-known practitioner of this method. By numbers, dolphins are mostly hunted for their meat, though some end up in dolphinariums.[87] Despite the controversial nature of the hunt resulting in international criticism, and the possible health risk that the often polluted meat causes,[88] thousands of dolphins are caught in drive hunts each year.[89] In Japan, the hunting is done by a select group of fishermen.[90] When a pod of dolphins has been spotted, they are driven into a bay by the fishermen while banging on metal rods in the water to scare and confuse the dolphins. When the dolphins are in the bay, it is quickly closed off with nets so the dolphins cannot escape. The dolphins are usually not caught and killed immediately, but instead left to calm down over night. The following day, the dolphins are caught one by one and killed. The killing of the animals used to be done by slitting their throats, but the Japanese government banned this method, and now dolphins may officially only be killed by driving a metal pin into the neck of the dolphin, which causes them to die within seconds according to a memo from Senzo Uchida, the executive secretary of the Japan Cetacean Conference on Zoological Gardens and Aquariums.[91] A veterinary team's analysis of a 2011 video footage of Japanese hunters killing striped dolphins using this method suggested that, in one case, death took over four minutes.[92] Since much of the criticism is the result of photos and videos taken during the hunt and slaughter, it is now common for the final capture and slaughter to take place on site inside a tent or under a plastic cover, out of sight from the public. The most circulated footage is probably that of the drive and subsequent capture and slaughter process taken in Futo, Japan, in October 1999, shot by the Japanese animal welfare organization Elsa Nature Conservancy.[93] Part of this footage was, amongst others, shown on CNN. In recent years, the video has also become widespread on the internet and was featured in the animal welfare documentary Earthlings, though the method of killing dolphins as shown in this video is now officially banned. In 2009, a critical documentary on the hunts in Japan titled The Cove was released and shown amongst others at the Sundance Film Festival.[94] Toothed whales can also be threatened by humans more indirectly. They are unintentionally caught in fishing nets by commercial fisheries as bycatch and accidentally swallow fishing hooks. Gillnetting and Seine netting are significant causes of mortality in cetaceans and other marine mammals.[95] Porpoises are commonly entangled in fishing nets. Whales are also affected by marine pollution. High levels of organic chemicals accumulate in these animals since they are high in the food chain. They have large reserves of blubber, more so for toothed whales, as they are higher up the food chain than baleen whales. Lactating mothers can pass the toxins on to their young. These pollutants can cause gastrointestinal cancers and greater vulnerability to infectious diseases.[96] They can also be poisoned by swallowing litter, such as plastic bags.[97] Pollution of the Yangtze river has led to the extinction of the baiji.[98] Environmentalists speculate that advanced naval sonar endangers some whales. Some scientists suggest that sonar may trigger whale beachings, and they point to signs that such whales have experienced decompression sickness.[99][100][101][102] Currently, no international convention gives universal coverage to all small whales, although the International Whaling Commission has attempted to extend its jurisdiction over them. ASCOBANS was negotiated to protect all small whales in the North and Baltic Seas and in the northeast Atlantic. ACCOBAMS protects all whales in the Mediterranean and Black Seas. The global UNEPConvention on Migratory Species currently covers seven toothed whale species or populations on its Appendix I, and 37 species or populations on Appendix II. All oceanic cetaceans are listed in CITES appendices, meaning international trade in them and products derived from them is very limited.[103][104] Various species of toothed whales, mainly dolphins, are kept in captivity, as well as several other species of porpoise such as harbour porpoises and finless porpoises. These small cetaceans are more often than not kept in theme parks, such as SeaWorld, commonly known as a dolphinarium. Bottlenose dolphins are the most common species kept in dolphinariums, as they are relatively easy to train, have a long lifespan in captivity, and have a friendly appearance. Hundreds if not thousands of bottlenose dolphins live in captivity across the world, though exact numbers are hard to determine. Orca are well known for their performances in shows, but the number kept in captivity is very small, especially when compared to the number of bottlenose dolphins, with only 44 captives being held in aquaria as of 2012.[107] Other species kept in captivity are spotted dolphins, false killer whales, and common dolphins, Commerson's dolphins, as well as rough-toothed dolphins, but all in much lower numbers than the bottlenose dolphin. Also, fewer than ten pilot whales, Amazon river dolphins, Risso's dolphins, spinner dolphins, or tucuxi are in captivity. Two unusual and very rare hybrid dolphins, known as wolphins, are kept at the Sea Life Park in Hawaii, which is a cross between a bottlenose dolphin and a false killer whale. Also, two common/bottlenose hybrids reside in captivity: one at Discovery Cove and the other at SeaWorld San Diego.[108] Aggression among captive orca is common. In August 1989, a dominant female orca, Kandu V, attempted to rake a newcomer whale, Corky II, with her mouth during a live show, and smashed her head into a wall. Kandu V broke her jaw, which severed an artery, and then bled to death.[111] In November 2006, a dominant female killer whale, Kasatka, repeatedly dragged experienced trainer Ken Peters to the bottom of the stadium pool during a show after hearing her calf crying for her in the back pools.[112] In February 2010, an experienced female trainer at SeaWorld Orlando, Dawn Brancheau, was killed by orca Tilikum shortly after a show in Shamu Stadium.[113] Tilikum had been associated with the deaths of two people previously.[111][114] In May 2012, Occupational Safety and Health Administration administrative law judge Ken Welsch cited SeaWorld for two violations in the death of Dawn Brancheau and fined the company a total of US$12,000.[115] Trainers were banned from making close contact with the orca.[116] In April 2014, the US Court of Appeals for the District of Columbia denied an appeal by SeaWorld.[117] In 2013, SeaWorld's treatment of orca in captivity was the basis of the movie Blackfish, which documents the history of Tilikum, an orca captured by SeaLand of the Pacific, later transported to SeaWorld Orlando, which has been involved in the deaths of three people.[118] In the aftermath of the release of the film, Martina McBride, 38 Special, REO Speedwagon, Cheap Trick, Heart, Trisha Yearwood, and Willie Nelson cancelled scheduled concerts at SeaWorld parks.[119] SeaWorld disputes the accuracy of the film, and in December 2013 released an ad countering the allegations and emphasizing its contributions to the study of cetaceans and their conservation.[120] ^Susanne Prahl (2007). \"Studies for the construction of epicranialen airway when porpoise (Phocoena phocoena Linnaeus, 1758)\". Dissertation for the Doctoral Degree of the Department of Biology of the Faculty of Mathematics, Computer Science and Natural Sciences at the University of Hamburg: 6."}
{"url": "https://en.m.wikipedia.org/wiki/List_of_extinct_cetaceans", "text": "List of extinct cetaceans The list of extinct cetaceans features the extinctgenera and species of the order Cetacea. The cetaceans (whales, dolphins and porpoises) are descendants of land-living mammals, the even-toed ungulates. The earliest cetaceans were still hoofed mammals. These early cetaceans became gradually better adapted for swimming than for walking on land, finally evolving into fully marine cetaceans. This list currently includes only fossil genera and species. However, the Atlantic population of gray whales (Eschrichtius robustus) became extinct in the 18th century, and the baiji (or Chinese river dolphin, Lipotes vexillifer) was declared \"functionally extinct\" after an expedition in late 2006 failed to find any in the Yangtze River. ^Remington, Kellogg (1927). \"Study of the skull of a fossil sperm-whale from the Temblor Miocene of Southern California\". Contributions to Palaeontology from the Carnegie Institution of Washington: 3–24."}
{"url": "https://en.m.wikipedia.org/wiki/Pinniped", "text": "Seals range in size from the 1 m (3 ft 3 in) and 45 kg (100 lb) Baikal seal to the 5 m (16 ft) and 3,200 kg (7,100 lb) southern elephant seal. Several species exhibit sexual dimorphism. They have streamlined bodies and four limbs that are modified into flippers. Though not as fast in the water as dolphins, seals are more flexible and agile. Otariids primarily use their front limbs to propel themselves through the water, while phocids and walruses primarily use their hind limbs for this purpose. Otariids and walruses have hind limbs that can be pulled under the body and used as legs on land. By comparison, terrestrial locomotion by phocids is more cumbersome. Otariids have visible external ears, while phocids and walruses lack these. Pinnipeds have well-developed senses—their eyesight and hearing are adapted for both air and water, and they have an advanced tactile system in their whiskers or vibrissae. Some species are well adapted for diving to great depths. They have a layer of fat, or blubber, under the skin to keep warm in cold water, and, other than the walrus, all species are covered in fur. Although pinnipeds are widespread, most species prefer the colder waters of the Northern and Southern Hemispheres. They spend most of their lives in water, but come ashore to mate, give birth, molt or to avoid ocean predators, such as sharks and orcas. Seals mainly live in marine environments but can also be found in fresh water. They feed largely on fish and marine invertebrates; a few, such as the leopard seal, feed on large vertebrates, such as penguins and other seals. Walruses are specialized for feeding on bottom-dwelling mollusks. Male pinnipeds typically mate with more than one female (polygyny), although the degree of polygyny varies with the species. The males of land-breeding species tend to mate with a greater number of females than those of ice breeding species. Male pinniped strategies for reproductive success vary between defending females, defending territories that attract females and performing ritual displays or lek mating. Pups are typically born in the spring and summer months and females bear almost all the responsibility for raising them. Mothers of some species fast and nurse their young for a relatively short period of time while others take foraging trips at sea between nursing bouts. Walruses are known to nurse their young while at sea. Seals produce a number of vocalizations, notably the barks of California sea lions, the gong-like calls of walruses and the complex songs of Weddell seals. The name \"pinniped\" derives from the Latin words pinna'fin' and pes, pedis'foot'.[2] The common name \"seal\" originates from the Old English word seolh, which is in turn derived from the Proto-Germanic*selkhaz.[3] Cladogram showing relationships among the living pinnipeds, found in Berta, Churchill and Boessenecker (2018). The Southern Hemisphere eared seal clade is not fully resolved.[4] The German naturalist Johann Karl Wilhelm Illiger was the first to recognize the pinnipeds as a distinct taxonomic unit; in 1811 he gave the name Pinnipedia to both a family and an order.[5] American zoologist Joel Asaph Allen reviewed the world's pinnipeds in an 1880 monograph, History of North American pinnipeds, a monograph of the walruses, sea-lions, sea-bears and seals of North America. In this publication, he traced the history of names, gave keys to families and genera, described North American species and provided synopses of species in other parts of the world.[6] In 1989, Annalisa Berta and colleagues proposed the unranked cladePinnipedimorpha to contain the fossil genus Enaliarctos and modern seals as a sister group.[7] Pinnipeds belong to the order Carnivora and the suborder Caniformia (known as dog-like carnivorans).[8] Of the three extant families, the Otariidae and Odobenidae are grouped in the superfamily Otarioidea,[9] while the Phocidae belong to the superfamily Phocoidea.[10] There are 34 extant species of pinnipeds,[4] and more than 50 fossil species of pinnipedimorphs.[11] Otariids are also known as eared seals due to the presence of pinnae. These animals swim mainly using their well-developed fore-flippers. They can also \"walk\" on land by shifting their hind-flippers forward under the body.[12] The front end of an otariid's frontal bone protrudes between the nasal bones, with a large and flattened supraorbital foramen. An extra spine splits the supraspinatous fossa and bronchi that are divided in the front.[13] Otariids consist of two types: sea lions and fur seals; the latter typically being smaller in size with pointier snouts, longer fore-flippers and heavier fur coats.[14] Five genera and seven species (one now extinct) of sea lion are known to exist, while two genera and nine species of fur seal exist. While sea lions and fur seals have historically been considered separate subfamilies (Otariinae and Arctocephalinae respectively), genetic and molecular evidence has refuted this, indicating that the northern fur seal is basal to other otariids and the Australian sea lion and New Zealand sea lion are more closely related to Arctocephalus than to other sea lions.[4] Odobenidae consists of only one living member: the modern walrus. This animal is noticeable from its larger size (exceeded only by the elephant seals), nearly hairless skin, flattened snout and long upper canines, known as tusks. Like otariids, walruses are capable of walking on land with their hind-flippers. When moving in water, the walrus relies on its hind-flippers for locomotion, while its fore-flippers are used for steering. In addition, visible ear flaps are not present in the species.[15][16] The epipterygoid of the jaw is well developed and the back of the nasal bones are horizontal. In the feet, the calcaneuses protrude in the middle.[13] Phocids are known as true or \"earless\" seals. These animals lack external ear flaps and are incapable of positioning their hind-flippers to move on land, making them more cumbersome. This is because of their massive ankle bones and flatter heels. In water, true seals rely on the side-to-side motion of their hind-flippers and lower body to move forward.[12] The phocid's skull has thickenedmastoids, puffed up entotympanic bones, nasal bones with a pointed tip in the back and a non-existent supraorbital foramen. The hip has a more converse ilium.[13] A 2006 molecular study supports the division of phocids into two monophyletic subfamilies: Monachinae, which consists of elephant seals, monk seals and Antarctic seals; and Phocinae, which consists of all the rest.[4][16] One popular hypothesis suggested that pinnipeds are diphyletic (descended from two ancestral lines), with walruses and otariids sharing a recent common ancestor with bears; and phocids sharing one with Musteloidea. However, morphological and molecular evidence support a monophyletic origin.[13] A 2021 genetic study found that pinnipeds are more closely related to musteloids.[17] Pinnipeds split from other caniforms 50 million years ago (mya) during the Eocene.[18] Fossil animals representing basal lineages include Puijila, of the early Miocene in Arctic Canada. It resembled a modern otter, but shows evidence of quadrupedal swimming—retaining a form of aquatic locomotion that led to those employed by modern pinnipeds. Potamotherium, which lived in the same period in Europe, was similar to Puijila but more aquatic.[19] The braincase of Potamotherium shows evidence that it used its whiskers to hunt, like modern seals.[20] Both Puijila and Potamotherium fossils have been found in lake deposits, suggesting that seal ancestors were originally adapted for fresh water.[19] Fossil of Enaliarctos Enaliarctos, a fossil species of late Oligocene/early Miocene (24–22 mya) California, closely resembled modern pinnipeds; it was adapted to an aquatic life with flippers and a flexible spine. Its teeth were more like land predators in that they were more adapted for shearing. Its hind-flippers may have allowed it to walk on land, and it probably did not leave coastal areas as much as its modern relatives. Enaliarctos was likely more of a fore-flipper swimmer, but could probably swim with either pair.[13] One species, Enaliarctos emlongi, exhibited notable sexual dimorphism, suggesting that this physical characteristic may have been an important driver of pinniped evolution.[21] A closer relative of extant pinnipeds was Pteronarctos, which lived in Oregon 19–15 mya. As in modern seals, the maxilla or upper jaw bone of Pteroarctos intersects with the orbital wall. The extinct family Desmatophocidae lived 23–10 mya in the North Pacific. They had long skulls that with large orbits, interlocked zygomatic bones and rounded molars and premolars. They also were sexually dimorphic and may have been capable of swimming with both or either pair of flippers.[13] They are grouped with modern pinnipeds, but there is debate as to whether they are more closely related to phocids or to otariids and walruses.[22][4] The ancestors of the Otarioidea and Phocidea diverged around 25 mya.[23] Phocids are known to have existed for at least 15 million years,[13] and molecular evidence supports a divergence of the Monachinae and Phocinae lineages around this time.[4] The fossil genera Monotherium and Leptophoca of southeastern North America represent the earliest members of Monachinae and Phocinae respectively.[13] Both lineages may have originated in the North Atlantic, and likely reached the Pacific via the Central American Seaway. Phocines mainly stayed in the Northern Hemisphere, while the monachines diversified southward.[4] The lineages of Otariidae and Odobenidae split around 20 mya.[23] The earliest fossil records of otariids are in North Pacific and dated to around 11 mya. Early fossil genera include Pithanotaria and Thalassoleon.[13] The Callorhinus lineage split the earliest, followed by the Eumetopias/Zalophus lineage and then the rest, which colonized the Southern Hemisphere.[13][4] The earliest fossils of Odobenidae—Prototaria of Japan and Proneotherium of Oregon—date to 18–16 mya. These primitive walruses had normal sized canines and fed on fish instead of mollusks. Later taxa like Gomphotaria, Pontolis and Dusignathus had longer canines on both the upper and lower jaw. The familiar long upper tusks developed in the genera Valenictus and Odobenus. The lineage of the modern walrus may have spread from the North Pacific to the North Atlantic through the Caribbean and Central American Seaway 8–5 mya, and then back to the North Pacific via the Arctic 1 mya, or to the Arctic and subsequently the North Atlantic during the Pleistocene.[13] Pinnipeds have streamlined, spindle-shaped bodies with small or non-existent ear flaps, rounded heads, short muzzles, flexible necks, limbs modified into flippers and small tails.[24][25][26] The mammary glands and genitals can withdraw into the body.[24] Seals are unique among carnivorans in that their orbital walls are mostly shaped by the maxilla and are not contained by certain facial bones.[13] Compared to land carnivores, pinnipeds have fewer teeth, which are pointed and cone-shaped. They are adapted for holding onto slippery prey rather than shearing meat like the carnassials of other carnivorans. The walrus has unique tusks which are long upper canines.[27] Pinnipeds range in size from the 1 m (3 ft 3 in) and 45 kg (100 lb) Baikal seal to the 5 m (16 ft) and 3,200 kg (7,100 lb) southern elephant seal. Overall, they tend to be larger than other carnivores.[24] Several species have male-biased sexual dimorphism that depends on how polygynous a species is: highly polygynous species like elephant seals are extremely sexually dimorphic, while less polygynous species have males and females that are closer in size, or, in the case of Antarctic seals, females are moderately bigger. Males of sexually dimorphic species also tend to have secondary sex characteristics, such as larger or more prominent heads, necks, chests, crests, noses/proboscis and canine teeth as well as thicker fur and manes.[28][29] Though more polygynous species tend to be sexually dimorphic, some evidence suggests that size differences between the sexes originated due to ecological differences, with polygyny developing later.[30][31] Almost all pinnipeds have fur coats, the exception being the walrus, which is only sparsely covered. Even some fully furred species (particularly sea lions) are less furry than land mammals. Fur seals have lush coats consisting of an undercoat and guard hairs.[32] In species that live on ice, young pups have thicker coats than adults. The individual hairs on the coat, known collectively as lanugo, can trap heat from sunlight and keep the pup warm.[33] Pinnipeds are typically countershaded, and are darker colored dorsally and lighter colored ventrally, which serves to counter the effects of self-shadowing caused by light shining over the ocean water. The pure white fur of harp seal pups conceals them in their Arctic environment.[34] Several species have clashing patterns of light and dark pigmentation.[24][34] All fully furred species molt; the process of which may be quick or gradual depending on the species.[35] Seals have a layer of subcutaneous fat, known as blubber, that is particularly thick in phocids and walruses.[24][33] Blubber serves both to keep the animals warm and to provide energy and nourishment when they are fasting. It can constitute as much as 50% of a pinniped's mass. Newborn pups have a thin layer of blubber, but some species compensate for this with thick lanugos.[33] The simple stomach of pinnipeds is typical of carnivores. Most species have neither a cecum nor a clear demarcation between the small and large intestines; the large intestine is comparatively short and only slightly wider than the small intestine. Small intestine lengths range from 8 times (California sea lion) to 25 times (elephant seal) the body length. The length of the intestine may be an adaptation to frequent deep diving, allowing for more room in the digestive tract for partially digested food. An appendix is absent in seals.[36] As in most marine mammals, the kidneys are divided into lobes and filter out excess salt.[37] Harbor seal (top) and California sea lion swimming. The former swims with its hind-flippers, the latter with its fore-flippers. Pinnipeds have two pairs of flippers on the front and back, the fore-flippers and hind-flippers. Their elbows and ankles are not externally visible.[34] Pinnipeds are not as fast as cetaceans, typically swimming at 5–15 kn (9–28 km/h; 6–17 mph) compared to around 20 kn (37 km/h; 23 mph) for several species of dolphin. Seals are more agile and flexible,[38] and some otariids, such as the California sea lion, can make dorsal turns as the back of their heads can touch their hind flippers.[39] Pinnipeds have several adaptions for reducing drag. In addition to their streamlined bodies, they have smooth networks of muscle bundles in their skin that may increase laminar flow and cut through the water. The hair erector muscles are absent, so their fur can be streamlined as they swim.[40] When swimming, otariids rely on their fore-flippers for locomotion in a wing-like manner similar to penguins and sea turtles. Fore-flipper movement is not continuous, and the animal glides between each stroke.[41][42] Compared to terrestrial carnivorans, the fore-limb bones of otariids are reduced in length, giving them less resistance at the elbow joint as the flippers flap;[43] the hind-flippers maneuver them.[44] Phocids and walruses swim by moving their hind-flippers and lower body from side to side, while their fore-flippers are mainly used for maneuvering.[42][45][16] Some species leap out of the water, and \"ride\" waves.[46] Pinnipeds can move around on land, though not as well as terrestrial animals. Otariids and walruses are capable of turning their hind-flippers forward and under the body so they can \"walk\" on all fours.[47] The fore-flippers move along a transverse plane, rather than the sagittal plane like the limbs of land mammals.[48] Otariids create momentum by laterally swaying their heads and necks.[49][48] Sea lions have been recorded climbing up flights of stairs. Phocids lack the ability to walk on their hind-flippers, and must flop and wriggle their bodies forward as their fore-flippers keep them stable. In some species, the fore-flippers may act like oars pushing against the ground. Phocids can move faster on ice, as they are able to slide.[50] The eyes of pinnipeds are relatively large for their size and are positioned near the front of the head. Only the smaller eyes of the walruses are located on each side of the head;[51][52] since they forage at the bottom for sedentary mollusks.[51] A seal's eye is suited for seeing both underwater and in air. Most of retina is equidistant around the spherical lens. The cornea has a flattened center where refraction does not change between air and water. The vascular iris has a strong dilator muscle. A contracted pupil is typically pear-shaped, although the bearded seal's is more horizontal. Compared to deep-diving elephant seals, the iris of shallower species, such as harbor seals and California sea lions, does not change much in size between contraction and expansion.[53] Seals are able to see in darkness with a tapetum lucidum, a reflecting layer that increases sensitivity by reflecting light back through the rods.[54] On land, pinnipeds are near-sighted in dim light. This is reduced in bright light as the retracted pupil decreases the ability of the lens and the cornea to refract (bend) light.[55] Polar living seals like the harp seal have corneas that are adapted to the bright light that reflects off snow and ice. As such, they do not suffer snow blindness.[56][55] Pinnipeds appear to have limited color vision as they lack S-cones.[57] Flexible eye movement has been documented in seals.[58] The walrus can project its eyes out from its sockets in both a forward and upward direction due to its advanced extraocular muscles and absence of an orbital roof.[16] The seal eye is durable as the corneal epithelium is hardened by keratin, and the sclera is thick enough to withstand the pressures of diving. Seals also secrete mucus from the lacrimal gland to protect their eyes. As in many mammals and birds, pinnipeds possess nictitating membranes.[59] The pinniped ear is adapted for hearing underwater, where it can hear sound frequencies of up to 70,000 Hz. In air, hearing is somewhat reduced in pinnipeds compared to many terrestrial mammals. While their airborne hearing sensitivity is generally weaker than humans', they still have a wide frequency range.[60] One study of three species—the harbor seal, California sea lion and northern elephant seal—found that the sea lion was best adapted for airborne hearing, the elephant seal for underwater hearing and the harbor seal was equally adapted for both.[61] Although pinnipeds have a fairly good sense of smell on land,[62] it is useless under water as their nostrils are closed.[63] Pinnipeds have well-developed tactile senses. Compared to terrestrial mammals, the moustache-like whiskers or vibrissae of pinnipeds have ten times more nerve connections, allowing them to effectively detect vibrations in the water.[64] These vibrations are generated, for example, when a fish swims through water. Detecting vibrations is useful when the animals are foraging, and may add to or even replace vision, particularly in darkness.[65][66] Harbor seals can follow hydrodynamic paths made by other animals minutes earlier, similar to a dog following a scent trail,[67][68] and even to discriminate the size and type of object responsible for the trail.[69] Unlike terrestrial mammals, such as rodents, pinnipeds do not sweep their whiskers over an object when examining it, but can protract the hairs forward while holding them steady, maximizing their detection.[65][70] The vibrissa's angle relative to the flow seems to be the most important contributor to detection ability.[70] The whiskers of some otariids grow quite long—those of the Antarctic fur seal can reach 41 cm (16 in).[71] Walruses have the most vibrissae, at 600–700 individual hairs. These are important when searching for prey along the bottom. In addition to foraging, whiskers may also play a role in navigation; spotted seals appear to use them to detect breathing holes in the ice.[72] To dive, a pinniped must first exhale much of the air out of its lungs and shut its nostrils and throat cartilages to protect the trachea.[73][74] The airways are supported by cartilaginous rings and smooth muscle, and the chest muscles and alveoli can completely deflate during deeper dives.[75][76] While terrestrial mammals are generally unable to empty their lungs, pinnipeds can reinflate their lungs even after respiratory collapse.[76] The middle ear contains sinuses that probably fill with blood during dives, preventing middle ear squeeze.[77] The heart of a seal is moderately flattened to allow the lungs to deflate. The trachea is flexible enough to collapse under pressure.[73] During deep dives, any remaining air in their bodies is stored in the bronchioles and trachea, which prevents them from experiencing decompression sickness, oxygen toxicity and nitrogen narcosis. In addition, seals can tolerate large amounts of lactic acid, which reduces skeletal muscle fatigue during intense physical activity.[77] The circulatory system of pinnipeds is large and elaborate; retia mirabilia line the inside of the trunk and limbs, allowing for greater oxygen storage during diving.[78] As with other diving mammals, pinnipeds have large amounts of hemoglobin and myoglobin stored in their blood and muscles. This allows them to stay submerged for long periods of time while still having enough oxygen. Deep-diving species such as elephant seals have blood volumes that represent up to 20% of their body weight. When diving, they reduce their heart rate, and blood flow is mostly restricted to the heart, brain and lungs. To keep their blood pressure stable, phocids have an elastic aorta that dissipates some of the energy of each heartbeat.[77] Pinnipeds keep warm by having large, thick bodies, insulating blubber and fur, and quick-burning metabolism.[79] In addition, the blood vessels in their flippers are adapted for countercurrent exchange; small veins surround arteries transporting blood from the body core, capturing heat from them.[80] While blubber and fur keep the seal warm in water, they can also overheat the animal when it is on land. To counteract overheating, many species cool off by covering themselves in sand. Monk seals may even dig up the cooler layers. The northern fur seal cools off by panting.[81] Pinnipeds spend many months at a time at sea, so they must sleep in the water. Scientists have recorded them sleeping for minutes at a time while slowly drifting downward in a belly-up orientation.[82] Like other marine mammals, seals sleep in water with half of their brain awake so that they can detect and escape from predators, as well as surface for air without fully waking. When they are asleep on land, both sides of their brain go into sleep mode.[83] Walrus on ice off Alaska. This species has a discontinuous distribution around the Arctic Circle. Living pinnipeds are widespread in cold oceanic waters; particularly in the North Atlantic, the North Pacific and the Southern Ocean. By contrast, the consistently warm Indomalayan waters have no seals.[84] Monk seals and some otariids live in tropical and subtropical waters. Seals usually require cool, nutrient-rich waters with temperatures lower than 20 °C (68 °F). Even in more tropical climates, lower temperatures and biological productivity may be provided by currents.[84][85] Only monk seals live in waters that generally lack these features.[84] The Caspian seal and Baikal seal are found in large landlocked bodies of water (the Caspian Sea and Lake Baikal respectively).[13] Pinnipeds have an amphibious lifestyle; they are mostly aquatic, but haul out to breed, molt, rest, sun or to avoid aquatic predators. Several species are known to migrate over vast distances, particularly in response to environmental changes. Elephant seals are at sea for most of the year and there are vast distances between their breeding and molting sites. The northern elephant seal is one of farthest mammalian migraters, traveling 18,000–21,000 km (11,000–13,000 mi). Otariids tend to migrate less than phocids, especially tropical species.[90] Traveling seals may reach their destination using geomagnetic fields, water and wind currents, solar and lunar positions and the temperature and chemical makeup of the water.[91] Pinnipeds may dive during foraging or to avoid predators. When foraging, for example, the Weddell seal typically dives for no more than 15 minutes and 400 m (1,300 ft) deep, but can dive for as long as 73 minutes and reach 600 m (2,000 ft) deep. Northern elephant seals often dive 350–650 m (1,100–2,100 ft) for as long as 20 minutes. They can also dive 1,500 m (4,900 ft) and for over an hour. The dives of otariids tend to be shorter and less deep. They typically last 5–7 minutes with average depths to 30–45 m (100–150 ft). However, the New Zealand sea lion has been recorded diving to a maximum of 460 m (1,510 ft) and have submerged for as long as 12 minutes.[92] The diet of walruses does not require them to dive very deep or very long. Pinnipeds generally live 25–30 years.[93] Pinnipeds may hunt solitarily or cooperatively. The former behavior is typical when hunting non-schooling fish, immobile or sluggish invertebrates and endothermic prey. Solitary foraging species usually hunt in coastal or shallow water. An exception to this is the northern elephant seal, which hunts deep in the open ocean for fish. In addition, walruses feed solitarily but are often near other walruses in small or large groups. For large schools of fish or squid, pinnipeds such as certain otariids hunt cooperatively in large groups, locating and herding their prey. Some species, such as California and South American sea lions, will hunt alongside sea birds and cetaceans.[95] Seals typically swallow their food whole, and will rip apart prey that is too big.[96][97] The leopard seal, a prolific predator of penguins, is known to violently shake its prey to death.[98] Complex serrations in the teeth of filter-feeding species, such as crabeater seals, allow water to leak out as they swallow their planktonic food.[84] The walrus is unique in that it consumes its prey by suction feeding, using its tongue to suck the meat of a bivalve out of the shell.[52] While pinnipeds mostly hunt in the water, South American sea lions are known to chase down penguins on land.[99] Some species may swallow stones or pebbles for reasons not understood.[100] Though they can drink seawater, pinnipeds get most of their fluid intake from their food.[101] Pinnipeds themselves are subject to predation. Most species are preyed on by the orca. To subdue and kill seals, orcas strike them with their heads or tails—the latter causing them to fly in the air—or simply bite into them and rip them apart. They are typically hunted by groups of 10 or fewer whales, but they are occasionally hunted by larger groups or by lone individuals. All age classes may be targeted, but pups most of all. Large sharks are another major predator of pinnipeds—usually the great white shark but also the tiger shark and mako shark. Sharks usually attack by ambushing them from below. Injured seals that escape are usually able to recover from their wounds. Otariids that have been targeted in the hindquarters are more likely to survive, while phocids are more likely to survive with forequarters injures. Pinnipeds are also preyed on by terrestrial and pagophilic predators. The polar bear is a major predator of Arctic seals and walruses, particularly pups. Bears may seek out seals, or simply wait for them to come by. Other terrestrial predators include cougars, brown hyenas and various species of canids, which mostly target the young.[102] Pinnipeds lessen the chance of predation by gathering in groups.[103] Some species are capable of inflicting damaging wounds on their attackers with their sharp canines. Adult walruses are particularly risky prey for polar bears.[102] When out at sea, northern elephant seals dive out of the reach of surface-hunting orcas and white sharks.[82] In the Antarctic, which lacks terrestrial predators, pinniped species spend more time on the ice than their Arctic counterparts.[104] Interspecific predation among pinnipeds does occur. The leopard seal is known to prey on numerous other species, especially the crabeater seal. Leopard seals typically target crabeater pups, particularly from November to January. Older crabeater seals commonly bear scars from failed leopard seal attacks; a 1977 study found that 75% of a sample of 85 individual crabeaters had these scars.[102][105] Walruses, despite being specialized for feeding on bottom-dwelling invertebrates, occasionally prey on Arctic seals. They kill their prey with their long tusks and eat their blubber and skin. Steller sea lions have been recorded eating harbor seals, northern fur seals and California sea lions, particularly pups and small adults. New Zealand sea lions feed on pups of some fur seal species, and the South American sea lion may prey on South American fur seals.[102] Harbor seals on sandy beach. This species breeds on land but mates in the water.[106] The mating system of pinnipeds varies from extreme polygyny to serial monogamy.[107] Of the 33 species, 20 breed on land, and the remaining 13 breed on ice.[108] Species that breed on land are usually polygynous, as females gather in large aggregations and males are able to mate with them as well as defend them from rivals. Polygynous species include elephant seals, grey seals and most otariids.[28] Land-breeding pinnipeds prefer to mate on islands where there are fewer land predators. Suitable islands are in short supply and tend to be crowded. Since the land they breed on is fixed, females return to the same sites for many years. The males arrive earlier in the season and wait for them. The males stay on land to monopolize females; and may fast for months as they would lose their position if they went to feed at sea.[109] Polygynous species also tend to be extremely sexual dimorphic in favor of males. This dimorphism manifests itself in larger chests and necks, longer canines and denser fur—all traits that equip males for combat. Larger males have more blubber and thus more energy reserves for fasting.[28] Other seals, like the walrus and most phocids, breed on ice and copulate in the water—a few land-breeding species also mate in water.[28][106] Females of these species tend to be more spaced out and there is less site fidelity, since ice is less stable than solid land. Hence polygyny tends to be weaker in ice-breeding species. An exception to this is the walrus, whose distribution of food forces females closer together. Pinnipeds that breed on fast ice tend to cluster together more than those that breed on drift ice.[110] Seals that breed on ice tend to have little or no sexual dimorphism. In Antarctic seals, there is some size bias in favor of females. Walruses and hooded seals are unique among ice-breeding species in that they have pronounced sexual dimorphism in favor of males.[28][111] Northern fur seal breeding colony Adult male pinnipeds have several strategies to ensure reproductive success. Otariid males gain access to females by establishing territories where females can bask and give birth and contain valuable resources such as shade, tide pools or access to water. Territories are usually marked by natural barriers,[112] and some may be fully or partially underwater.[113] Males defend their territorial boundaries with threatening vocalizations and postures, but physical fights are usually not very violent, and are mostly limited to early in the season.[114] Individuals also return to the same territorial site each breeding season. In certain species, like the Steller sea lion and northern fur seal, a dominant male can maintain a territory for as long as 2–3 months. Females can usually move freely between territories and males are unable to coerce them, but in some species such as the northern fur seal, South American sea lion and Australian sea lion, males can successfully keep females in their territories with threatening displays and even violence. In some phocid species, like the harbor seal, Weddell seal and bearded seal, the males establish \"maritories\" and patrol and defend the waters bordering female haul-out areas, waiting for a female to enter.[112] These are also maintained by vocalizations.[115] The maritories of Weddell seal males include entries to female breathing holes in the ice.[116] Male northern elephant seals fighting for dominance and females Lek systems are known to exist among some populations of walruses.[112] These males gather near female herds and try to attract them with elaborate courtship displays and vocalizations.[112][117] Lekking may also exist among California sea lions, South American fur seals, New Zealand sea lions and harbor seals.[112][118] In some species, including elephant seals, grey seals and non-lekking walruses, males will try to lay claim to the desired females and defend them from rivals. Elephant seal males, in particular, establish dominance hierarchies via displays and fights, with the highest ranking males having a near monopoly on reproductive success.[112] An alpha male can have a harem of 100 females.[119] Grey seal males usually place themselves among a cluster of females whose members may change over time,[120] while males of some walrus populations guard female herds.[112] Male ringed, crabeater, spotted and hooded seals follow and defend nearby females and mate with them when they reach estrus. These may be lone females or small groups.[121][112] Younger or subdominant male pinnipeds may attempt to achieve reproductive success in other ways including sneakiness, harassment of females or even coordinated disruption of the colony. Female pinnipeds do appear to have some choice in mates, particularly in lek-breeding species like the walrus, but also in elephant seals where the males try to dominate all the females that they want to mate with.[122] When a female elephant seal or grey seal is mounted by an unwanted male, she tries to resist and get away. This commotion attracts other males to the scene, and the most dominant will take over and mate with female himself.[123][124] Dominant female elephant seals stay in the center of the colony where they in the domain of a more dominant male, while marginal females are left with subordinates.[125] Female Steller sea lions may solicit their territorial males for mating.[126] With the exception of the walrus, which has five- to six-year gaps between births, female pinnipeds enter estrus shortly after they give birth, and can thus produce pups every year. All species go through delayed implantation, in which the embryo does not enter the uterus for weeks or months.[127] Delayed implantation allows the female to wait until conditions are right for birthing.[128][129]Gestation in seals (including delayed implantation) typically lasts a year.[130] For most species, birthing takes place in spring and summer.[131] Typically, single pups are born;[130] twins are uncommon and have high mortality rates.[132] Pups of most species are born relatively developed and precocial.[130] Pinniped milk has \"little to no lactose\".[133] Mother pinnipeds have different strategies for maternal care and lactation. Phocids such as elephant seals, grey seals and hooded seals have a lactation period that lasts days or weeks, during which they fast and nurse their pups on land or ice. The milk of these species consists of up to 60% fat, allowing the young to grow fairly quickly. Each day until they are weaned, northern elephant seal pups gain 4 kg (9 lb). Some pups gain weight more quickly than others by stealing extra milk from other mothers. Alloparenting occurs in these fasting species;[134] while most northern elephant seal mothers nurse their own pups and reject nursings from alien pups, some do accept alien pups with their own.[135] For otariids and some phocids like the harbor seal, mothers fast and nurse their pups for a few days at a time. In between nursing bouts, the females forage at sea while the young stay behind onshore. If there is enough food close to shore, a female can be gone for as little as a day, but otherwise may be at sea for as long as three weeks.[136] Lactation in otariids may last 6–11 months; in the Galápagos fur seal it can last up to 3 years. Pups of these species are weaned at heavier weights than their phocid counterparts, but the latter grow quicker.[137] Walruses are unique in that mothers nurse their young at sea.[138] Young pinnipeds typically start swimming on their own and some species can even swim as newborns. Young may wait days or weeks before entering the water; elephant seals start swimming weeks after weaning.[139] Male pinnipeds generally play little role in raising the young.[140] Male walruses may help inexperienced young as they learn to swim, and have even been recorded caring for orphans. When a group is threatened, all the adults may protect the young.[141] Male California sea lions have been observed to help shield swimming pups from predators.[142] Males can also pose threats to the safety of pups, particularly during fights.[140] Pups of some species may be abducted, assaulted and killed by males.[143] Pinnipeds can produce a number of vocalizations. While most vocals are audible to the human ear, Weddell seals have been recorded in Antarctica making ultrasonic calls underwater.[144] In addition, the vocals of northern elephant seals may produce infrasonic vibrations. Vocals are produced both in air and underwater; the former are more common among otariids and the latter among phocids. Antarctic seals are noisier on land or ice than Arctic seals due to the absence of polar bears.[115] Male vocals are usually deeper than those of the females. Vocalizations are particularly important during the breeding seasons. Dominant male elephant seals display their status and power with \"clap-threats\" and loud drum-like calls[145] that may be modified by the proboscis.[146] Male otariids have strong barks, growls and roars. Male walruses are known to produce gong-like calls when attempting to attract females, these are amplified underwater with inflatable throat sacs.[147] The Weddell seal has perhaps the most extensive vocal repertoire, producing both airborne and underwater sounds. Trilling, gluping, chirping, chugging and knocking are some examples of the calls produced under water. When warning other seals, the calls may be pronounced by \"prefixes\" and \"suffixes\".[115] The underwater vocals of Weddell seals can last 70 seconds, which is long for a marine mammal call. Some calls have around seven rhythm patterns and could be categorized as \"songs\".[148] Similar calls have been recorded in other Antarctic seals[149] and in bearded seals. In some pinniped species, there appear to be regional dialects or even individual variations in vocalizations. These differences are likely important for territorial males becoming accustomed to their neighbors (dear enemy effect) and mothers and pups who need to remain in contact on crowded beaches. Female seals emit a \"pulsed, bawling\" contact call, while pups respond by squawking. Contact calls are particularly important for otariid mothers returning from sea.[150] Other vocalizations produced by seals include grunts, rasps, rattles, creaks, warbles, clicks and whistles.[115] Sea lion balancing a ball Non-vocal communication is not as common in pinnipeds as in cetaceans. Nevertheless, when they feel threatened, hauled-out harbor seals and Baikal seals may slap themselves with their flippers to create a warning sound. Teeth chattering, hisses and exhalations are also made as aggressive warnings by pinnipeds. Visual displays also occur: Ross seals resting on the ice will show the stripes on their chests and bare their teeth to a perceived threat, while swimming Weddell seals will make an S-shaped posture to intimidate rivals under the ice.[115] Male hooded seals use their inflatable nasal membranes to display to and attract females.[29] In a match-to-sample task study, a single California sea lion was able to demonstrate an understanding of symmetry, transitivity and equivalence; a second seal was unable to complete the tasks.[151] They demonstrate the ability to understand simple syntax and commands when taught an artificial sign language, though they only rarely used the signs semantically or logically.[152] In 2011, a captive California sea lion named Ronan was recorded bobbing its head in synchrony to musical rhythms. This \"rhythmic entrainment\" was previously seen only in humans, parrots and other birds possessing vocal mimicry.[153] Adult male elephant seals can recognize each other's vocalizations by remembering the rhythm and timbre.[154] In the 1970s, a captive harbor seal named Hoover was trained to imitate human speech and laughter.[155] For sea lions used in entertainment, trainers toss a ball at the animal or simply place the object on its nose, so it will eventually understand the behavior desired. A sea lion may need a year of training before it can publicly perform. Its long-term memory allows it to perform a trick after as much as three months of non-performance.[142] Various human cultures have for millennia depicted pinnipeds. In Homer's Odyssey, the sea god Proteus shepherds a colony of seals.[b][156] In northern Scotland, Celts of Orkney and the Hebrides believed in selkies—seals that could change into humans and walk on land.[157] In Inuit mythology, they are associated with the goddess Sedna, who sometimes transformed into a seal. It was believed that marine mammals, including seals, came from her severed fingers.[158] In modern culture, pinnipeds are thought of as cute, playful and comical figures.[159] Pinnipeds can be found in facilities around the world, as their size and playfulness make them popular attractions.[160] Seals have been kept in captivity since at least Ancient Rome and their trainability was noticed by Pliny the Elder.[c] Zoologist Georges Cuvier noted during the 19th century that wild seals show considerable affection for humans and stated that they are second only to some monkeys among wild animals in their easy tamability. Francis Galton noted in his seminal work on domestication that seals were a spectacular example of an animal that would most likely never be domesticated, despite their friendliness, survivability and \"desire for comfort\", because they serve no practical use for humans.[161] Some modern exhibits have a pool with artificial haul-out sites and a rocky background, while others have seals housed in shelters located above a pool which they can jump into. More elaborate exhibits contain deep pools that can be viewed underwater with rock-mimicking cement as haul-out areas. The most popular captive pinniped is the California sea lion, due to its trainability and adaptability. Other commonly kept species include the grey seal and harbor seal. Larger animals like walruses and Steller sea lions are much less common.[160] Some organizations, such as the Humane Society of the United States and World Animal Protection, object to keeping pinnipeds and other marine mammals in captivity. They state that the exhibits could not be large enough to house animals that have evolved to be migratory, and a pool could never replace the size and biodiversity of the ocean. They also state that the tricks performed for audiences are \"exaggerated variations of their natural behaviors\" and distract the people from the animal's unnatural environment.[162] California sea lions are used in military applications by the U.S. Navy Marine Mammal Program, including detecting naval mines and enemy divers. In the Persian Gulf, the animals have been trained to swim behind divers approaching a U.S. naval ship and attach a clamp with a rope to the diver's leg. Navy officials say that the sea lions can do this in seconds, before the enemy realizes what happened.[163] Organizations like PETA believe that such operations put the animals in danger.[164] The Navy insists that the sea lions are removed once their job is done.[165] Humans have hunted seals since the Stone Age. Originally, seals were merely hit with clubs during haul-out. Eventually, more lethal weapons were used, like spears and harpoons. They were also trapped in nets. The use of firearms in seal hunting during the modern era drastically increased the number of killings. Pinnipeds are typically hunted for their meat and blubber. The skins of fur seals and phocids are made into coats, and the tusks of walruses have been used as ivory.[166] There is a distinction made between the subsistence hunting of seals by indigenous peoples of the Arctic and commercial hunting: subsistence hunters depend on seal products for survival.[167] National and international authorities have given special treatment to aboriginal hunters since their methods of killing are seen as more sustainable and smaller in scope. However indigenous people have recently used more modern technology and are profiting more from seal products in the marketplace. Some anthropologists argue that the term \"subsistence\" should also apply to these activities, as long as they are local in scale. More than 100,000 phocids (especially ringed seals) as well as around 10,000 walruses are harvested annually by native hunters.[166] Commercial sealing rivaled whaling as an important industry throughout history. Harvested species included harp seals, hooded seals, Caspian seals, elephant seals, walruses and all species of fur seal.[168] After the 1960s, the harvesting of seals decreased substantially as an industry[166] after the Canadian government implemented measures to protect female seals and restrict the hunting season.[169] Several species that were commercially exploited have rebounded in numbers; for example, Antarctic fur seals may have reached their pre-harvesting numbers. The northern elephant seal nearly went extinct in the late 19th century, with only a small population remaining on Guadalupe Island. It has since recolonized much of its historic range, but has a population bottleneck.[168] Conversely, the Mediterranean monk seal was extirpated from much of the Mediterranean and its current range is still limited.[170] Several species of pinniped continue to be exploited. The Convention for the Conservation of Antarctic Seals protects species within the Antarctic and surrounding waters, but allows restricted hunting of crabeater seals, leopard seals and Weddell seals. Weddell seal hunting is forbidden between September and February if the animal is older than a year, to ensure healthy population growth.[171] The Government of Canada permits the hunting of harp seals. This has been met with controversy and debate. Proponents of seal hunts insist that the animals are killed humanely and the white-coated pups are not taken, while opponents argue that it is irresponsible to kill harp seals as they are already threatened by declining habitat.[172][173] The Caribbean monk seal has been killed and exploited by European settlers and their descendants since 1494, starting with Christopher Columbus himself. The seals were easy targets for organized sealers, fishermen, turtle hunters and buccaneers because they evolved with little pressure from terrestrial predators and were thus \"genetically tame\". In the Bahamas, as many as 100 seals were slaughtered in one night. The species was considered to be already extinct by the mid-nineteenth century until a small colony was found near the Yucatán Peninsula in 1866. Seal killings continued, and the last reliable report of the animal alive was in 1952 at Serranilla Bank. The IUCN declared it extinct in 1996.[174] The Japanese sea lion was common around the Japanese islands, but overexploitation and competition from fisheries drastically decreased the population in the 1930s. The last recorded individual was a juvenile in 1974.[175] Some species have become so numerous that they conflict with local people. In the United States, pinnipeds are protected under the Marine Mammal Protection Act of 1972 (MMPA). Since that year, California sea lion populations have risen to 250,000. These animals began exploiting more man-made environments, like docks, for haul-out sites. Many docks are not designed to withstand the weight of several resting sea lions. Wildlife managers have used various methods to control the animals, and some city officials have redesigned docks so they can better resist sea lion use.[184][185] Inland-living New Zealand sea lions face unique human conflicts such as road mortality and run-ins with human infrastructure.[186] Seals also conflict with fisherman.[187] In 2007, MMPA was amended to permit the lethal removal of sea lions from salmon runs at Bonneville Dam.[188] In the 1980s and 1990s, South African politicians and fishermen demanded that brown fur seals be culled, believing that the animals competed with commercial fisheries. Scientific studies found that culling fur seals would actually have a negative effect on the fishing industry, and the culling option was dropped in 1993.[189] ^Boessenecker, R. W.; Churchill, M. (2018). \"The last of the desmatophocid seals: a new species of Allodesmus from the upper Miocene of Washington, USA, and a revision of the taxonomy of Desmatophocidae\". Zoological Journal of the Linnean Society. 184 (1): 211–235. doi:10.1093/zoolinnean/zlx098. ^\"Zalophus californianus japonicus (CR)\". Japan Integrated Biodiversity Information System. Red Data Book (in Japanese). Ministry of the Environment (Japan). Archived from the original on 5 June 2011. Retrieved 20 August 2013. \"The Japanese sea lion (Zalophus californianus japonicus) was common in the past around the coast of the Japanese Archipelago, but declined rapidly after the 1930s from overhunting and increased competition with commercial fisheries. The last record in Japan was a juvenile, captured in 1974 off the coast of Rebun Island, northern Hokkaido.\""}
{"url": "https://web.archive.org/web/20171001191231/https://www.theguardian.com/environment/2013/jul/03/whales-flee-military-sonar-strandings", "text": "Wednesday 3 July 2013 01.00 EDT First published on Wednesday 3 July 2013 01.00 EDT Whales flee from the loud military sonar used by navies to hunt submarines, new research has proven for the first time. The studies provide a missing link in the puzzle that has connected naval exercises around the world to unusual mass strandings of whales and dolphins. The strong response observed in the beaked whales occurred at noise levels well below those allowed for US navy exercises. \"This result has to be taken into consideration by regulators and those planning naval exercises,\" said Stacy DeRuiter, at the University of St Andrews in Scotland, who led one of the teams. \"For whales and dolphins, listening is as important as seeing is for humans – they communicate, locate food, and navigate using sound,\" said Sarah Dolman, at charity Whale and Dolphin Conservation. \"Noise pollution threatens vulnerable populations, driving them away from areas important to their survival, and at worst injuring or even causing the deaths of some whales and dolphins.\" Dolman said there were no accepted international standards regarding noise pollution and there was an urgent need to re-evaluate the environmental impacts of military activities. The US Navy part-funded the new studies but said the findings only showed behavioural responses to sonar, not actual harm. Nonetheless, Kenneth Hess, a US Navy spokesman, said permit conditions for naval exercises were reviewed annually and added: \"We will evaluate the effectiveness of our marine mammal protective measures in light of new research findings.\" Beaked whales are the most common species affected by unusual mass strandings, perhaps because their shy nature makes them more easily scared by noises that they may interpret as killer whale sounds. Researchers used suction cups to attach digital devices to Cuvier's beaked whales off the coast of Southern California to measure the noise they were exposed to and their response. When a simulated military sonar signal was sounded at 200dB and between 3km and 10km away, the whales initially stopped feeding and swimming. They then swam rapidly away from the noise and some performed unusually deep and long dives. \"The missing piece of the puzzle was how whales changed their behaviour and how that led to mass strandings,\" said DeRuiter. She added that they also stopped feeding for 6-7 hours, which is unusual. \"If they miss out on food, they will be less healthy,\" she said, noting that where populations have been measured, numbers of Cuvier's beaked whales are declining. A second study, also off Southern California, estimated that a blue whale spooked by the sonar missed out on over a tonne of krill, about a day's worth of food. \"Blue whales rely on large aggregations of dense krill to sustain their extreme body size, so they continuously dive and feed throughout the day when high-density prey patches are present,\" said Jeremy Goldbogen, at Cascadia Research, a non-profit US research organisation in Olympia, Washington. \"Because of this, we suggest that sonar-induced disruption of feeding could have significant and previously undocumented impacts on individual baleen whale fitness and the health of their populations.\" A spokesman for the UK's Royal Navy said: \"The Royal Navy already limits its use of sonar around whales. We are committed to taking all reasonable and practical measures to protect the environment and mitigate effects on marine mammals. This new research will be taken into account in the regular review of MoD active sonar mitigation procedures.\" • This article was amended on 3 July 2013 to clarify the claims made by the US Navy, which part-funded the new studies."}
{"url": "https://en.m.wikipedia.org/wiki/File:Cwall99_lg.jpg", "text": "Summary English: Under certain conditions, en:Emiliania huxleyi can form massive blooms which can be detected by satellite remote sensing. What looks like white clouds in the water, is in fact the reflected light from billions of coccoliths floating in the water-column. Under certain conditions, Emiliania huxleyi can form massive blooms which can be detected by satellite remote sensing. What looks like white clouds in the water, is in fact the reflected light from billions of coccoliths floating in the water-column.< File usage The following pages on the English Wikipedia use this file (pages on other projects are not listed):"}
{"url": "https://en.m.wikipedia.org/wiki/Heterotroph", "text": "Heterotrophs represent one of the two mechanisms of nutrition (trophic levels), the other being autotrophs (auto = self, troph = nutrition). Autotrophs use energy from sunlight (photoautotrophs) or oxidation of inorganic compounds (lithoautotrophs) to convert inorganic carbon dioxide to organic carbon compounds and energy to sustain their life. Comparing the two in basic terms, heterotrophs (such as animals) eat either autotrophs (such as plants) or other heterotrophs, or both. Contents Heterotrophs can be organotrophs or lithotrophs. Organotrophs exploit reduced carbon compounds as electron sources, like carbohydrates, fats, and proteins from plants and animals. On the other hand, lithoheterotrophs use inorganic compounds, such as ammonium, nitrite, or sulfur, to obtain electrons. Another way of classifying different heterotrophs is by assigning them as chemotrophs or phototrophs. Phototrophs utilize light to obtain energy and carry out metabolic processes, whereas chemotrophs use the energy obtained by the oxidation of chemicals from their environment.[9] Photoorganoheterotrophs, such as Rhodospirillaceae and purple non-sulfur bacteria synthesize organic compounds using sunlight coupled with oxidation of organic substances. They use organic compounds to build structures. They do not fix carbon dioxide and apparently do not have the Calvin cycle.[10] Chemolithoheterotrophs like Oceanithermus profundus[11] obtain energy from the oxidation of inorganic compounds, including hydrogen sulfide, elemental sulfur, thiosulfate, and molecular hydrogen. Mixotrophs (or facultative chemolithotroph) can use either carbon dioxide or organic carbon as the carbon source, meaning that mixotrophs have the ability to use both heterotrophic and autotrophic methods.[12][13] Although mixotrophs have the ability to grow under both heterotrophic and autotrophic conditions, C. vulgaris have higher biomass and lipid productivity when growing under heterotrophic compared to autotrophic conditions.[14] Heterotrophs, by consuming reduced carbon compounds, are able to use all the energy that they obtain from food for growth and reproduction, unlike autotrophs, which must use some of their energy for carbon fixation.[10] Both heterotrophs and autotrophs alike are usually dependent on the metabolic activities of other organisms for nutrients other than carbon, including nitrogen, phosphorus, and sulfur, and can die from lack of food that supplies these nutrients.[15] This applies not only to animals and fungi but also to bacteria.[10] The chemical origin of life hypothesis suggests that life originated in a prebiotic soup with heterotrophs.[16] The summary of this theory is as follows: early Earth had a highly reducing atmosphere and energy sources such as electrical energy in the form of lightning, which resulted in reactions that formed simple organic compounds, which further reacted to form more complex compounds and eventually resulted in life.[17][18] Alternative theories of an autotrophic origin of life contradict this theory.[19] The theory of a chemical origin of life beginning with heterotrophic life was first proposed in 1924 by Alexander Ivanovich Oparin, and eventually published “The Origin of Life.” [20] It was independently proposed for the first time in English in 1929 by John Burdon Sanderson Haldane.[21] While these authors agreed on the gasses present and the progression of events to a point, Oparin championed a progressive complexity of organic matter prior to the formation of cells, while Haldane had more considerations about the concept of genes as units of heredity and the possibility of light playing a role in chemical synthesis (autotrophy).[22] Evidence grew to support this theory in 1953, when Stanley Miller conducted an experiment in which he added gasses that were thought to be present on early Earth – water (H 2 O), methane (CH4), ammonia (NH3), and hydrogen (H2) – to a flask and stimulated them with electricity that resembled lightning present on early Earth.[23] The experiment resulted in the discovery that early Earth conditions were supportive of the production of amino acids, with recent re-analyses of the data recognizing that over 40 different amino acids were produced, including several not currently used by life.[16] This experiment heralded the beginning of the field of synthetic prebiotic chemistry, and is now known as the Miller–Urey experiment.[24] On early Earth, oceans and shallow waters were rich with organic molecules that could have been used by primitive heterotrophs.[25] This method of obtaining energy was energetically favorable until organic carbon became more scarce than inorganic carbon, providing a potential evolutionary pressure to become autotrophic.[25][26] Following the evolution of autotrophs, heterotrophs were able to utilize them as a food source instead of relying on the limited nutrients found in their environment.[27] Eventually, autotrophic and heterotrophic cells were engulfed by these early heterotrophs and formed a symbiotic relationship.[27] The endosymbiosis of autotrophic cells is suggested to have evolved into the chloroplasts while the endosymbiosis of smaller heterotrophs developed into the mitochondria, allowing the differentiation of tissues and development into multicellularity. This advancement allowed the further diversification of heterotrophs.[27] Today, many heterotrophs and autotrophs also utilize mutualistic relationships that provide needed resources to both organisms.[28] One example of this is the mutualism between corals and algae, where the former provides protection and necessary compounds for photosynthesis while the latter provides oxygen.[29] However this hypothesis is controversial as CO2 was the main carbon source at the early Earth, suggesting that early cellular life were autotrophs that relied upon inorganic substrates as an energy source and lived at alkaline hydrothermal vents or acidic geothermal ponds.[30] Simple biomolecules transported from space was considered to have been either too reduced to have been fermented or too heterogeneous to support microbial growth.[31] Heterotrophic microbes likely originated at low H2 partial pressures. Bases, amino acids, and ribose are considered to be the first fermentation substrates.[32] Heterotrophs are currently found in each domain of life: Bacteria, Archaea, and Eukarya.[33] Domain Bacteria includes a variety of metabolic activity including photoheterotrophs, chemoheterotrophs, organotrophs, and heterolithotrophs.[33] Within Domain Eukarya, kingdoms Fungi and Animalia are entirely heterotrophic, though most fungi absorb nutrients through their environment.[34][35] Most organisms within Kingdom Protista are heterotrophic while Kingdom Plantae is almost entirely autotrophic, except for myco-heterotrophic plants.[34] Lastly, Domain Archaea varies immensely in metabolic functions and contains many methods of heterotrophy.[33] Many heterotrophs are chemoorganoheterotrophs that use organic carbon (e.g. glucose) as their carbon source, and organic chemicals (e.g. carbohydrates, lipids, proteins) as their electron sources.[36] Heterotrophs function as consumers in food chain: they obtain these nutrients from saprotrophic, parasitic, or holozoic nutrients.[37] They break down complex organic compounds (e.g., carbohydrates, fats, and proteins) produced by autotrophs into simpler compounds (e.g., carbohydrates into glucose, fats into fatty acids and glycerol, and proteins into amino acids). They release the chemical energy of nutrient molecules by oxidizing carbon and hydrogen atoms from carbohydrates, lipids, and proteins to carbon dioxide and water, respectively. They can catabolize organic compounds by respiration, fermentation, or both. Fermenting heterotrophs are either facultative or obligate anaerobes that carry out fermentation in low oxygen environments, in which the production of ATP is commonly coupled with substrate-level phosphorylation and the production of end products (e.g. alcohol, CO2, sulfide).[38] These products can then serve as the substrates for other bacteria in the anaerobic digest, and be converted into CO2 and CH4, which is an important step for the carbon cycle for removing organic fermentation products from anaerobic environments.[38] Heterotrophs can undergo respiration, in which ATP production is coupled with oxidative phosphorylation.[38][39] This leads to the release of oxidized carbon wastes such as CO2 and reduced wastes like H 2 O, H 2 S, or N 2 O into the atmosphere. Heterotrophic microbes’ respiration and fermentation account for a large portion of the release of CO2 into the atmosphere, making it available for autotrophs as a source of nutrient and plants as a cellulose synthesis substrate.[40][39] Respiration in heterotrophs is often accompanied by mineralization, the process of converting organic compounds to inorganic forms.[40] When the organic nutrient source taken in by the heterotroph contains essential elements such as N, S, P in addition to C, H, and O, they are often removed first to proceed with the oxidation of organic nutrient and production of ATP via respiration.[40] S and N in organic carbon source are transformed into H 2 S and NH4+ through desulfurylation and deamination, respectively.[40][39] Heterotrophs also allow for dephosphorylation as part of decomposition.[39] The conversion of N and S from organic form to inorganic form is a critical part of the nitrogen and sulfur cycle. H 2 S formed from desulfurylation is further oxidized by lithotrophs and phototrophs while NH4+ formed from deamination is further oxidized by lithotrophs to the forms available to plants.[40][39] Heterotrophs’ ability to mineralize essential elements is critical to plant survival.[39] Most opisthokonts and prokaryotes are heterotrophic; in particular, all animals and fungi are heterotrophs.[5] Some animals, such as corals, form symbiotic relationships with autotrophs and obtain organic carbon in this way. Furthermore, some parasitic plants have also turned fully or partially heterotrophic, while carnivorous plants consume animals to augment their nitrogen supply while remaining autotrophic. Animals are classified as heterotrophs by ingestion, fungi are classified as heterotrophs by absorption. ^\"The purpose of saprotrophs and their internal nutrition, as well as the main two types of fungi that are most often referred to, as well as describes, visually, the process of saprotrophic nutrition through a diagram of hyphae, referring to the Rhizobium on damp, stale whole-meal bread or rotting fruit.\" Advanced Biology Principles, p 296.[full citation needed]"}
{"url": "https://en.m.wikipedia.org/wiki/Seed", "text": "Seed In botany, a seed is a plant embryo and food reserve enclosed in a protective outer covering called a seed coat (testa). More generally, the term \"seed\" means anything that can be sown, which may include seed and husk or tuber. Seeds are the product of the ripened ovule, after the embryo sac is fertilized by sperm from pollen, forming a zygote. The embryo within a seed develops from the zygote and grows within the mother plant to a certain size before growth is halted. In the flowering plants, the ovary ripens into a fruit which contains the seed and serves to disseminate it. Many structures commonly referred to as \"seeds\" are actually dry fruits. Sunflower seeds are sometimes sold commercially while still enclosed within the hard wall of the fruit, which must be split open to reach the seed. Different groups of plants have other modifications, the so-called stone fruits (such as the peach) have a hardened fruit layer (the endocarp) fused to and surrounding the actual seed. Nuts are the one-seeded, hard-shelled fruit of some plants with an indehiscent seed, such as an acorn or hazelnut. History The first land plants evolved around 468 million years ago,[1] and reproduced using spores. The earliest seed bearing plants to appear were the gymnosperms, which have no ovaries to contain the seeds. They arose during the late Devonian period (416 million to 358 million years ago).[2] From these early gymnosperms, seed ferns evolved during the Carboniferous period (359 to 299 million years ago); they had ovules that were borne in a cupule,[3] which consisted of groups of enclosing branches likely used to protect the developing seed.[4] Published literature about seed storage, viability and its hygrometric dependence began in the early 19th century, influential works being: Angiosperm seeds are \"enclosed seeds\", produced in a hard or fleshy structure called a fruit that encloses them for protection. Some fruits have layers of both hard and fleshy material. In gymnosperms, no special structure develops to enclose the seeds, which begin their development \"naked\" on the bracts of cones. However, the seeds do become covered by the cone scales as they develop in some species of conifer. Angiosperm (flowering plants) seeds consist of three genetically distinct constituents: (1) the embryo formed from the zygote, (2) the endosperm, which is normally triploid, (3) the seed coat from tissue derived from the maternal tissue of the ovule. In angiosperms, the process of seed development begins with double fertilization, which involves the fusion of two male gametes with the egg cell and the central cell to form the primary endosperm and the zygote. Right after fertilization, the zygote is mostly inactive, but the primary endosperm divides rapidly to form the endosperm tissue. This tissue becomes the food the young plant will consume until the roots have developed after germination. Ovule After fertilization, the ovules develop into the seeds. The ovule consists of a number of components: The funicle (funiculus, funiculi) or seed stalk which attaches the ovule to the placenta and hence ovary or fruit wall, at the pericarp. The nucellus, the remnant of the megasporangium and main region of the ovule where the megagametophyte develops. The micropyle, a small pore or opening in the apex of the integument of the ovule where the pollen tube usually enters during the process of fertilization. The chalaza, the base of the ovule opposite the micropyle, where integument and nucellus are joined.[8] The shape of the ovules as they develop often affects the final shape of the seeds. Plants generally produce ovules of four shapes: the most common shape is called anatropous, with a curved shape. Orthotropous ovules are straight with all the parts of the ovule lined up in a long row producing an uncurved seed. Campylotropous ovules have a curved megagametophyte often giving the seed a tight \"C\" shape. The last ovule shape is called amphitropous, where the ovule is partly inverted and turned back 90 degrees on its stalk (the funicle or funiculus). In the majority of flowering plants, the zygote's first division is transversely oriented in regards to the long axis, and this establishes the polarity of the embryo. The upper or chalazal pole becomes the main area of growth of the embryo, while the lower or micropylar pole produces the stalk-like suspensor that attaches to the micropyle. The suspensor absorbs and manufactures nutrients from the endosperm that are used during the embryo's growth.[9] Embryo The inside of a Ginkgo seed, showing a well-developed embryo, nutritive tissue (megagametophyte), and a bit of the surrounding seed coat The main components of the embryo are: The cotyledons, the seed leaves, attached to the embryonic axis. There may be one (Monocotyledons), or two (Dicotyledons). The cotyledons are also the source of nutrients in the non-endospermic dicotyledons, in which case they replace the endosperm, and are thick and leathery. In endospermic seeds, the cotyledons are thin and papery. Dicotyledons have the point of attachment opposite one another on the axis. The epicotyl, the embryonic axis above the point of attachment of the cotyledon(s). The plumule, the tip of the epicotyl, and has a feathery appearance due to the presence of young leaf primordia at the apex, and will become the shoot upon germination. The hypocotyl, the embryonic axis below the point of attachment of the cotyledon(s), connecting the epicotyl and the radicle, being the stem-root transition zone. The radicle, the basal tip of the hypocotyl, grows into the primary root. Monocotyledonous plants have two additional structures in the form of sheaths. The plumule is covered with a coleoptile that forms the first leaf while the radicle is covered with a coleorhiza that connects to the primary root and adventitious roots form the sides. Here the hypocotyl is a rudimentary axis between radicle and plumule. The seeds of corn are constructed with these structures; pericarp, scutellum (single large cotyledon) that absorbs nutrients from the endosperm, plumule, radicle, coleoptile, and coleorhiza – these last two structures are sheath-like and enclose the plumule and radicle, acting as a protective covering. Seed coat The maturing ovule undergoes marked changes in the integuments, generally a reduction and disorganization but occasionally a thickening. The seed coat forms from the two integuments or outer layers of cells of the ovule, which derive from tissue from the mother plant, the inner integument forms the tegmen and the outer forms the testa. (The seed coats of some monocotyledon plants, such as the grasses, are not distinct structures, but are fused with the fruit wall to form a pericarp.) The testae of both monocots and dicots are often marked with patterns and textured markings, or have wings or tufts of hair. When the seed coat forms from only one layer, it is also called the testa, though not all such testae are homologous from one species to the next. The funiculus abscisses (detaches at fixed point – abscission zone), the scar forming an oval depression, the hilum. Anatropous ovules have a portion of the funiculus that is adnate (fused to the seed coat), and which forms a longitudinal ridge, or raphe, just above the hilum. In bitegmic ovules (e.g. Gossypium described here) both inner and outer integuments contribute to the seed coat formation. With continuing maturation the cells enlarge in the outer integument. While the inner epidermis may remain a single layer, it may also divide to produce two to three layers and accumulates starch, and is referred to as the colourless layer. By contrast, the outer epidermis becomes tanniferous. The inner integument may consist of eight to fifteen layers.[10] As the cells enlarge, and starch is deposited in the outer layers of the pigmented zone below the outer epidermis, this zone begins to lignify, while the cells of the outer epidermis enlarge radially and their walls thicken, with nucleus and cytoplasm compressed into the outer layer. these cells which are broader on their inner surface are called palisade cells. In the inner epidermis, the cells also enlarge radially with plate like thickening of the walls. The mature inner integument has a palisade layer, a pigmented zone with 15–20 layers, while the innermost layer is known as the fringe layer.[10] Gymnosperms In gymnosperms, which do not form ovaries, the ovules and hence the seeds are exposed. This is the basis for their nomenclature – naked seeded plants. Two sperm cells transferred from the pollen do not develop the seed by double fertilization, but one sperm nucleus unites with the egg nucleus and the other sperm is not used.[11] Sometimes each sperm fertilizes an egg cell and one zygote is then aborted or absorbed during early development.[12] The seed is composed of the embryo (the result of fertilization) and tissue from the mother plant, which also form a cone around the seed in coniferous plants such as pine and spruce. Shape and appearance Seeds are very diverse, and as such there are many terms are used to describe them. Terms to describe shape Bean-shaped (reniform) – resembling a kidney, with lobed ends on either side of the hilum Square or Oblong – angular, with all sides being either equal, or longer-than-wide Triangular – three-sided, broadest below the middle Elliptic or Ovate or Obovate – rounded at both ends, or egg shaped (ovate or obovate, broader at one end), being rounded but either symmetrical about the middle, or broader below the middle, or broader above the middle[13] Discoid– resembling a disc or plate, having both thickness and parallel faces and with a rounded margin) Other common descriptors for seeds focus on color, texture, and form. Striate seeds are striped with parallel, longitudinal lines or ridges. The most common colours are brown and black, with other colours appearing less frequently. The surface texture varies from highly polished to considerably roughened. The surface may also have a variety of appendages (see Seed coat), and be described by terms such as papillate or digitiform (finger-like).[14] A seed coat with the consistency of cork is referred to as suberose. Other terms include crustaceous (hard, thin or brittle). In addition, the endosperm forms a supply of nutrients for the embryo in most monocotyledons and the endospermic dicotyledons. Seed types Seeds have been considered to occur in many structurally different types (Martin 1946).[15] These are based on a number of criteria, of which the dominant one is the embryo-to-seed size ratio. This reflects the degree to which the developing cotyledons absorb the nutrients of the endosperm, and thus obliterate it.[15] Six types occur amongst the monocotyledons, ten in the dicotyledons, and two in the gymnosperms (linear and spatulate).[16] This classification is based on three characteristics: embryo morphology, amount of endosperm and the position of the embryo relative to the endosperm. Embryo In endospermic seeds, there are two distinct regions inside the seed coat, an upper and larger endosperm and a lower smaller embryo. The embryo is the fertilised ovule, an immature plant from which a new plant will grow under proper conditions. The embryo has one cotyledon or seed leaf in monocotyledons, two cotyledons in almost all dicotyledons and two or more in gymnosperms. In the fruit of grains (caryopses) the single monocotyledon is shield shaped and hence called a scutellum. The scutellum is pressed closely against the endosperm from which it absorbs food and passes it to the growing parts. Embryo descriptors include small, straight, bent, curved, and curled. Nutrient storage Within the seed, there usually is a store of nutrients for the seedling that will grow from the embryo. The form of the stored nutrition varies depending on the kind of plant. In angiosperms, the stored food begins as a tissue called the endosperm, which is derived from the mother plant and the pollen via double fertilization. It is usually triploid, and is rich in oil or starch, and protein. In gymnosperms, such as conifers, the food storage tissue (also called endosperm) is part of the female gametophyte, a haploid tissue. The endosperm is surrounded by the aleurone layer (peripheral endosperm), filled with proteinaceous aleurone grains. Originally, by analogy with the animal ovum, the outer nucellus layer (perisperm) was referred to as albumen, and the inner endosperm layer as vitellus. Although misleading, the term began to be applied to all the nutrient matter. This terminology persists in referring to endospermic seeds as \"albuminous\". The nature of this material is used in both describing and classifying seeds, in addition to the embryo to endosperm size ratio. The endosperm may be considered to be farinaceous (or mealy) in which the cells are filled with starch, as for instance cereal grains, or not (non-farinaceous). The endosperm may also be referred to as \"fleshy\" or \"cartilaginous\" with thicker soft cells such as coconut, but may also be oily as in Ricinus (castor oil), Croton and Poppy. The endosperm is called \"horny\" when the cell walls are thicker such as date and coffee, or \"ruminated\" if mottled, as in nutmeg, palms and Annonaceae.[17] In most monocotyledons (such as grasses and palms) and some (endospermic or albuminous) dicotyledons (such as castor beans) the embryo is embedded in the endosperm (and nucellus), which the seedling will use upon germination. In the non-endospermic dicotyledons the endosperm is absorbed by the embryo as the latter grows within the developing seed, and the cotyledons of the embryo become filled with stored food. At maturity, seeds of these species have no endosperm and are also referred to as exalbuminous seeds. The exalbuminous seeds include the legumes (such as beans and peas), trees such as the oak and walnut, vegetables such as squash and radish, and sunflowers. According to Bewley and Black (1978), Brazil nut storage is in hypocotyl and this place of storage is uncommon among seeds.[18] All gymnosperm seeds are albuminous. Seed coat The seed coat develops from the maternal tissue, the integuments, originally surrounding the ovule. The seed coat in the mature seed can be a paper-thin layer (e.g. peanut) or something more substantial (e.g. thick and hard in honey locust and coconut), or fleshy as in the sarcotesta of pomegranate. The seed coat helps protect the embryo from mechanical injury, predators, and drying out. Depending on its development, the seed coat is either bitegmic or unitegmic. Bitegmic seeds form a testa from the outer integument and a tegmen from the inner integument while unitegmic seeds have only one integument. Usually, parts of the testa or tegmen form a hard protective mechanical layer. The mechanical layer may prevent water penetration and germination. Amongst the barriers may be the presence of lignifiedsclereids.[19] The outer integument has a number of layers, generally between four and eight organised into three layers: (a) outer epidermis, (b) outer pigmented zone of two to five layers containing tannin and starch, and (c) inner epidermis. The endotegmen is derived from the inner epidermis of the inner integument, the exotegmen from the outer surface of the inner integument. The endotesta is derived from the inner epidermis of the outer integument, and the outer layer of the testa from the outer surface of the outer integument is referred to as the exotesta. If the exotesta is also the mechanical layer, this is called an exotestal seed, but if the mechanical layer is the endotegmen, then the seed is endotestal. The exotesta may consist of one or more rows of cells that are elongated and pallisade like (e.g. Fabaceae), hence 'palisade exotesta'.[20][21] In addition to the three basic seed parts, some seeds have an appendage, an aril, a fleshy outgrowth of the funicle (funiculus), (as in yew and nutmeg) or an oily appendage, an elaiosome (as in Corydalis), or hairs (trichomes). In the latter example these hairs are the source of the textile crop cotton. Other seed appendages include the raphe (a ridge), wings, caruncles (a soft spongy outgrowth from the outer integument in the vicinity of the micropyle), spines, or tubercles. A scar also may remain on the seed coat, called the hilum, where the seed was attached to the ovary wall by the funicle. Just below it is a small pore, representing the micropyle of the ovule. Size and seed set A collection of various vegetable and herb seeds Seeds are very diverse in size. The dust-like orchid seeds are the smallest, with about one million seeds per gram; they are often embryonic seeds with immature embryos and no significant energy reserves. Orchids and a few other groups of plants are mycoheterotrophs which depend on mycorrhizal fungi for nutrition during germination and the early growth of the seedling. Some terrestrial orchid seedlings, in fact, spend the first few years of their lives deriving energy from the fungi and do not produce green leaves.[22] At up to 55 pounds (25 kilograms) the largest seed is the coco de mer(Lodoicea maldivica).[23] This indicates a 25 Billion fold difference in seed weight. Plants that produce smaller seeds can generate many more seeds per flower, while plants with larger seeds invest more resources into those seeds and normally produce fewer seeds. Small seeds are quicker to ripen and can be dispersed sooner, so autumn all blooming plants often have small seeds. Many annual plants produce great quantities of smaller seeds; this helps to ensure at least a few will end in a favorable place for growth. Herbaceous perennials and woody plants often have larger seeds; they can produce seeds over many years, and larger seeds have more energy reserves for germination and seedling growth and produce larger, more established seedlings after germination.[24][25] Functions Seeds serve several functions for the plants that produce them. Key among these functions are nourishment of the embryo, dispersal to a new location, and dormancy during unfavorable conditions. Seeds fundamentally are means of reproduction, and most seeds are the product of sexual reproduction which produces a remixing of genetic material and phenotype variability on which natural selection acts. Plant seeds hold endophytic microorganisms that can perform various functions, the most important of which is protection against disease.[26] Embryo nourishment Seeds protect and nourish the embryo or young plant. They usually give a seedling a faster start than a sporeling from a spore, because of the larger food reserves in the seed and the multicellularity of the enclosed embryo. Dispersal Unlike animals, plants are limited in their ability to seek out favorable conditions for life and growth. As a result, plants have evolved many ways to disperse their offspring by dispersing their seeds (see also vegetative reproduction). A seed must somehow \"arrive\" at a location and be there at a time favorable for germination and growth. When the fruits open and release their seeds in a regular way, it is called dehiscent, which is often distinctive for related groups of plants; these fruits include capsules, follicles, legumes, silicles and siliques. When fruits do not open and release their seeds in a regular fashion, they are called indehiscent, which include the fruits achenes, caryopses, nuts, samaras, and utricles.[27] By wind (anemochory) Dandelion seeds are contained within achenes, which can be carried long distances by the wind.The seed pod of milkweed (Asclepias syriaca) Seeds (nuts) are attractive long-term storable food resources for animals (e.g. acorns, hazelnut, walnut); the seeds are stored some distance from the parent plant, and some escape being eaten if the animal forgets them. Myrmecochory is the dispersal of seeds by ants. Foraging ants disperse seeds which have appendages called elaiosomes[30] (e.g. bloodroot, trilliums, acacias, and many species of Proteaceae). Elaiosomes are soft, fleshy structures that contain nutrients for animals that eat them. The ants carry such seeds back to their nest, where the elaiosomes are eaten. The remainder of the seed, which is hard and inedible to the ants, then germinates either within the nest or at a removal site where the seed has been discarded by the ants.[31] This dispersal relationship is an example of mutualism, since the plants depend upon the ants to disperse seeds, while the ants depend upon the plants seeds for food. As a result, a drop in numbers of one partner can reduce success of the other. In South Africa, the Argentine ant (Linepithema humile) has invaded and displaced native species of ants. Unlike the native ant species, Argentine ants do not collect the seeds of Mimetes cucullatus or eat the elaiosomes. In areas where these ants have invaded, the numbers of Mimetes seedlings have dropped.[32] Dormancy Seed dormancy has two main functions: the first is synchronizing germination with the optimal conditions for survival of the resulting seedling; the second is spreading germination of a batch of seeds over time so a catastrophe (e.g. late frosts, drought, herbivory) does not result in the death of all offspring of a plant (bet-hedging).[33] Seed dormancy is defined as a seed failing to germinate under environmental conditions optimal for germination, normally when the environment is at a suitable temperature with proper soil moisture. This true dormancy or innate dormancy is therefore caused by conditions within the seed that prevent germination. Thus dormancy is a state of the seed, not of the environment.[34] Induced dormancy, enforced dormancy or seed quiescence occurs when a seed fails to germinate because the external environmental conditions are inappropriate for germination, mostly in response to conditions being too dark or light, too cold or hot, or too dry. Seed dormancy is not the same as seed persistence in the soil or on the plant, though even in scientific publications dormancy and persistence are often confused or used as synonyms.[35] Often, seed dormancy is divided into four major categories: exogenous; endogenous; combinational; and secondary. A more recent system distinguishes five classes: morphological, physiological, morphophysiological, physical, and combinational dormancy.[36] Exogenous dormancy is caused by conditions outside the embryo, including: Physical dormancy or hard seed coats occurs when seeds are impermeable to water. At dormancy break, a specialized structure, the 'water gap', is disrupted in response to environmental cues, especially temperature, so water can enter the seed and germination can occur. Plant families where physical dormancy occurs include Anacardiaceae, Cannaceae, Convulvulaceae, Fabaceae and Malvaceae.[37] Chemical dormancy considers species that lack physiological dormancy, but where a chemical prevents germination. This chemical can be leached out of the seed by rainwater or snow melt or be deactivated somehow.[38] Leaching of chemical inhibitors from the seed by rain water is often cited as an important cause of dormancy release in seeds of desert plants, but little evidence exists to support this claim.[39] Endogenous dormancy is caused by conditions within the embryo itself, including: In morphological dormancy, germination is prevented due to morphological characteristics of the embryo. In some species, the embryo is just a mass of cells when seeds are dispersed; it is not differentiated. Before germination can take place, both differentiation and growth of the embryo have to occur. In other species, the embryo is differentiated but not fully grown (underdeveloped) at dispersal, and embryo growth up to a species specific length is required before germination can occur. Examples of plant families where morphological dormancy occurs are Apiaceae, Cycadaceae, Liliaceae, Magnoliaceae and Ranunculaceae.[40][41] Morphophysiological dormancy includes seeds with underdeveloped embryos, and also have physiological components to dormancy. These seeds, therefore, require a dormancy-breaking treatments, as well as a period of time to develop fully grown embryos. Plant families where morphophysiological dormancy occurs include Apiaceae, Aquifoliaceae, Liliaceae, Magnoliaceae, Papaveraceae and Ranunculaceae.[40] Some plants with morphophysiological dormancy, such as Asarum or Trillium species, have multiple types of dormancy, one affects radicle (root) growth, while the other affects plumule (shoot) growth. The terms \"double dormancy\" and \"two-year seeds\" are used for species whose seeds need two years to complete germination or at least two winters and one summer. Dormancy of the radicle (seedling root) is broken during the first winter after dispersal while dormancy of the shoot bud is broken during the second winter.[40] Physiological dormancy means the embryo, due to physiological causes, cannot generate enough power to break through the seed coat, endosperm or other covering structures. Dormancy is typically broken at cool wet, warm wet, or warm dry conditions. Abscisic acid is usually the growth inhibitor in seeds, and its production can be affected by light. Drying, in some plants, including a number of grasses and those from seasonally arid regions, is needed before they will germinate. The seeds are released, but need to have a lower moisture content before germination can begin. If the seeds remain moist after dispersal, germination can be delayed for many months or even years. Many herbaceous plants from temperate climate zones have physiological dormancy that disappears with drying of the seeds. Other species will germinate after dispersal only under very narrow temperature ranges, but as the seeds dry, they are able to germinate over a wider temperature range.[42] In seeds with combinational dormancy, the seed or fruit coat is impermeable to water and the embryo has physiological dormancy. Depending on the species, physical dormancy can be broken before or after physiological dormancy is broken.[41] Secondary dormancy* is caused by conditions after the seed has been dispersed and occurs in some seeds when nondormant seed is exposed to conditions that are not favorable to germination, very often high temperatures. The mechanisms of secondary dormancy are not yet fully understood, but might involve the loss of sensitivity in receptors in the plasma membrane.[43] The following types of seed dormancy do not involve seed dormancy, strictly speaking, as lack of germination is prevented by the environment, not by characteristics of the seed itself (see Germination): Photodormancy or light sensitivity affects germination of some seeds. These photoblastic seeds need a period of darkness or light to germinate. In species with thin seed coats, light may be able to penetrate into the dormant embryo. The presence of light or the absence of light may trigger the germination process, inhibiting germination in some seeds buried too deeply or in others not buried in the soil. Thermodormancy is seed sensitivity to heat or cold. Some seeds, including cocklebur and amaranth, germinate only at high temperatures (30 °C or 86 °F); many plants that have seeds that germinate in early to midsummer have thermodormancy, so germinate only when the soil temperature is warm. Other seeds need cool soils to germinate, while others, such as celery, are inhibited when soil temperatures are too warm. Often, thermodormancy requirements disappear as the seed ages or dries. Not all seeds undergo a period of dormancy. Seeds of some mangroves are viviparous; they begin to germinate while still attached to the parent. The large, heavy root allows the seed to penetrate into the ground when it falls. Many garden plant seeds will germinate readily as soon as they have water and are warm enough; though their wild ancestors may have had dormancy, these cultivated plants lack it. After many generations of selective pressure by plant breeders and gardeners, dormancy has been selected out. For annuals, seeds are a way for the species to survive dry or cold seasons. Ephemeral plants are usually annuals that can go from seed to seed in as few as six weeks.[44] Persistence and seed banks Germination Seed germination is a process by which a seed embryo develops into a seedling. It involves the reactivation of the metabolic pathways that lead to growth and the emergence of the radicle or seed root and plumule or shoot. The emergence of the seedling above the soil surface is the next phase of the plant's growth and is called seedling establishment.[45] Three fundamental conditions must exist before germination can occur. (1) The embryo must be alive, called seed viability. (2) Any dormancy requirements that prevent germination must be overcome. (3) The proper environmental conditions must exist for germination. Seed viability is the ability of the embryo to germinate and is affected by a number of different conditions. Some plants do not produce seeds that have functional complete embryos, or the seed may have no embryo at all, often called empty seeds. Predators and pathogens can damage or kill the seed while it is still in the fruit or after it is dispersed. Environmental conditions like flooding or heat can kill the seed before or during germination. The age of the seed affects its health and germination ability: since the seed has a living embryo, over time cells die and cannot be replaced. Some seeds can live for a long time before germination, while others can only survive for a short period after dispersal before they die. Seed vigor is a measure of the quality of seed, and involves the viability of the seed, the germination percentage, germination rate, and the strength of the seedlings produced.[47] The germination percentage is simply the proportion of seeds that germinate from all seeds subject to the right conditions for growth. The germination rate is the length of time it takes for the seeds to germinate. Germination percentages and rates are affected by seed viability, dormancy and environmental effects that impact on the seed and seedling. In agriculture and horticulture quality seeds have high viability, measured by germination percentage plus the rate of germination. This is given as a percent of germination over a certain amount of time, 90% germination in 20 days, for example. 'Dormancy' is covered above; many plants produce seeds with varying degrees of dormancy, and different seeds from the same fruit can have different degrees of dormancy.[48] It's possible to have seeds with no dormancy if they are dispersed right away and do not dry (if the seeds dry they go into physiological dormancy). There is great variation amongst plants and a dormant seed is still a viable seed even though the germination rate might be very low. In order for the seed coat to split, the embryo must imbibe (soak up water), which causes it to swell, splitting the seed coat. However, the nature of the seed coat determines how rapidly water can penetrate and subsequently initiate germination. The rate of imbibition is dependent on the permeability of the seed coat, amount of water in the environment and the area of contact the seed has to the source of water. For some seeds, imbibing too much water too quickly can kill the seed. For some seeds, once water is imbibed the germination process cannot be stopped, and drying then becomes fatal. Other seeds can imbibe and lose water a few times without causing ill effects, but drying can cause secondary dormancy. Repair of DNA damage During seed dormancy, often associated with unpredictable and stressful environments, DNA damage accumulates as the seeds age.[49][50][51] In rye seeds, the reduction of DNA integrity due to damage is associated with loss of seed viability during storage.[49] Upon germination, seeds of Vicia faba undergo DNA repair.[50] A plant DNA ligase that is involved in repair of single- and double-strand breaks during seed germination is an important determinant of seed longevity.[52] Also, in Arabidopsis seeds, the activities of the DNA repair enzymes Poly ADP ribose polymerases (PARP) are likely needed for successful germination.[53] Thus DNA damages that accumulate during dormancy appear to be a problem for seed survival, and the enzymatic repair of DNA damages during germination appears to be important for seed viability. Inducing germination A number of different strategies are used by gardeners and horticulturists to break seed dormancy. Scarification allows water and gases to penetrate into the seed; it includes methods to physically break the hard seed coats or soften them by chemicals, such as soaking in hot water or poking holes in the seed with a pin or rubbing them on sandpaper or cracking with a press or hammer. Sometimes fruits are harvested while the seeds are still immature and the seed coat is not fully developed and sown right away before the seed coat become impermeable. Under natural conditions, seed coats are worn down by rodents chewing on the seed, the seeds rubbing against rocks (seeds are moved by the wind or water currents), by undergoing freezing and thawing of surface water, or passing through an animal's digestive tract. In the latter case, the seed coat protects the seed from digestion, while often weakening the seed coat such that the embryo is ready to sprout when it is deposited, along with a bit of fecal matter that acts as fertilizer, far from the parent plant. Microorganisms are often effective in breaking down hard seed coats and are sometimes used by people as a treatment; the seeds are stored in a moist warm sandy medium for several months under nonsterile conditions. Stratification, also called moist-chilling, breaks down physiological dormancy, and involves the addition of moisture to the seeds so they absorb water, and they are then subjected to a period of moist chilling to after-ripen the embryo. Sowing in late summer and fall and allowing to overwinter under cool conditions is an effective way to stratify seeds; some seeds respond more favorably to periods of oscillating temperatures which are a part of the natural environment. Leaching or the soaking in water removes chemical inhibitors in some seeds that prevent germination. Rain and melting snow naturally accomplish this task. For seeds planted in gardens, running water is best – if soaked in a container, 12 to 24 hours of soaking is sufficient. Soaking longer, especially in stagnant water, can result in oxygen starvation and seed death. Seeds with hard seed coats can be soaked in hot water to break open the impermeable cell layers that prevent water intake. Other methods used to assist in the germination of seeds that have dormancy include prechilling, predrying, daily alternation of temperature, light exposure, potassium nitrate, the use of plant growth regulators, such as gibberellins, cytokinins, ethylene, thiourea, sodium hypochlorite, and others.[54] Some seeds germinate best after a fire. For some seeds, fire cracks hard seed coats, while in others, chemical dormancy is broken in reaction to the presence of smoke. Liquid smoke is often used by gardeners to assist in the germination of these species.[55] Sterile seeds Seeds may be sterile for few reasons: they may have been irradiated, unpollinated, cells lived past expectancy, or bred for the purpose. Evolution and origin of seeds The issue of the origin of seed plants remains unsolved. However, more and more data tends to place this origin in the middle Devonian. The description in 2004 of the proto-seed Runcaria heinzelinii in the Givetian of Belgium is an indication of that ancient origin of seed-plants. As with modern ferns, most land plants before this time reproduced by sending into the air spores that would land and become whole new plants. Taxonomists have described early \"true\" seeds from the upper Devonian, which probably became the theater of their true first evolutionary radiation. With this radiation came an evolution of seed size, shape, dispersal and eventually the radiation of gymnosperms and angiosperms and monocotyledons and dicotyledons. Seed plants progressively became one of the major elements of nearly all ecosystems. True to the seed Also called growing true, refers to plants whose seed will yield the same type of plant as the original plant. Open pollinated plants, which include heirlooms, will almost always grow true to seed if another variety does not cross-pollinate them. Economic importance Seed market In the United States farmers spent $22 billion on seeds in 2018, a 35 percent increase since 2010. DowDuPont and Monsanto account for 72 percent of corn and soybean seed sales in the U.S. with the average price of a bag of GMO corn seed is priced at $270.[59] Seed production Seed production in natural plant populations varies widely from year to year in response to weather variables, insects and diseases, and internal cycles within the plants themselves. Over a 20-year period, for example, forests composed of loblolly pine and shortleaf pine produced from 0 to nearly 5.5 million sound pine seeds per hectare.[60] Over this period, there were six bumper, five poor, and nine good seed crops, when evaluated for production of adequate seedlings for natural forest reproduction. Seeds are used to propagate many crops such as cereals, legumes, forest trees, turfgrasses, and pasture grasses. Particularly in developing countries, a major constraint faced is the inadequacy of the marketing channels to get the seed to poor farmers.[62] Thus the use of farmer-retained seed remains quite common. Poison and food safety While some seeds are edible, others are harmful, poisonous or deadly.[63] Plants and seeds often contain chemical compounds to discourage herbivores and seed predators. In some cases, these compounds simply taste bad (such as in mustard), but other compounds are toxic or break down into toxic compounds within the digestive system. Children, being smaller than adults, are more susceptible to poisoning by plants and seeds.[64] A deadly poison, ricin, comes from seeds of the castor bean. Reported lethal doses are anywhere from two to eight seeds,[65][66] though only a few deaths have been reported when castor beans have been ingested by animals.[67] The seeds of many legumes, including the common bean (Phaseolus vulgaris), contain proteins called lectins which can cause gastric distress if the beans are eaten without cooking. The common bean and many others, including the soybean, also contain trypsin inhibitors which interfere with the action of the digestive enzyme trypsin. Normal cooking processes degrade lectins and trypsin inhibitors to harmless forms.[71] In religion The Book of Genesis in the Old Testament begins with an explanation of how all plant forms began: And God said, Let the earth bring forth grass, the herb yielding seed, and the fruit tree yielding fruit after his kind, whose seed is in itself, upon the earth: and it was so. And the earth brought forth grass, and herb yielding seed after its kind, and the tree yielding fruit, whose seed was in itself, after its kind: and God saw that it was good. And the evening and the morning were the third day.[79] It is Allah Who causeth the seed-grain and the date-stone to split and sprout. He causeth the living to issue from the dead, and He is the one to cause the dead to issue from the living. That is Allah: then how are ye deluded away from the truth?[80]"}
{"url": "https://en.m.wikipedia.org/wiki/Coccolithophore", "text": "Coccolithophores are the most productive calcifying organisms on the planet, covering themselves with a calcium carbonate shell called a coccosphere. However, the reasons they calcify remain elusive. One key function may be that the coccosphere offers protection against microzooplankton predation, which is one of the main causes of phytoplankton death in the ocean.[1] Coccolithophores are ecologically important, and biogeochemically they play significant roles in the marine biological pump and the carbon cycle.[2][1] Depending on habitat, they can produce up to 40 percent of the local marine primary production.[3] They are of particular interest to those studying global climate change because, as ocean acidity increases, their coccoliths may become even more important as a carbon sink.[4] Management strategies are being employed to prevent eutrophication-related coccolithophore blooms, as these blooms lead to a decrease in nutrient flow to lower levels of the ocean.[5] Coccolithophores are single-celled phytoplankton that produce small calcium carbonate (CaCO3) scales (coccoliths) which cover the cell surface in the form of a spherical coating, called a coccosphere. They have been an integral part of marine plankton communities since the Jurassic.[15][16] Today, coccolithophores contribute ~1–10% to inorganic carbon fixation (calcification) to total carbon fixation (calcification plus photosynthesis) in the surface ocean[17] and ~50% to pelagic CaCO3 sediments.[18] Their calcareous shell increases the sinking velocity of photosynthetically fixed CO2 into the deep ocean by ballasting organic matter.[19][20] At the same time, the biogenic precipitation of calcium carbonate during coccolith formation reduces the total alkalinity of seawater and releases CO2.[21][22] Thus, coccolithophores play an important role in the marine carbon cycle by influencing the efficiency of the biological carbon pump and the oceanic uptake of atmospheric CO2.[1] As of 2021, it is not known why coccolithophores calcify and how their ability to produce coccoliths is associated with their ecological success.[23][24][25][26][27] The most plausible benefit of having a coccosphere seems to be a protection against predators or viruses.[28][26] Viral infection is an important cause of phytoplankton death in the oceans,[29] and it has recently been shown that calcification can influence the interaction between a coccolithophore and its virus.[30][31] The major predators of marine phytoplankton are microzooplankton like ciliates and dinoflagellates. These are estimated to consume about two-thirds of the primary production in the ocean[32] and microzooplankton can exert a strong grazing pressure on coccolithophore populations.[33] Although calcification does not prevent predation, it has been argued that the coccosphere reduces the grazing efficiency by making it more difficult for the predator to utilise the organic content of coccolithophores.[34]Heterotrophicprotists are able to selectively choose prey on the basis of its size or shape and through chemical signals[35][36] and may thus favor other prey that is available and not protected by coccoliths.[1] Coccolithophores are spherical cells about 5–100 micrometres across, enclosed by calcareous plates called coccoliths, which are about 2–25 micrometres across. Each cell contains two brown chloroplasts which surround the nucleus.[39] (a) dinoflagellates tend to utilize a haplontic (asexual) life cycle, (b) diatoms tend to utilize a diplontic (sexual) life cycle, and (c) coccolithophores tend to utilize a haplo-diplontic life cycle. Note that not all coccolithophores calcify in their haploid phase.[3] The complex life cycle of coccolithophores is known as a haplodiplontic life cycle, and is characterized by an alternation of both asexual and sexual phases. The asexual phase is known as the haploid phase, while the sexual phase is known as the diploid phase. During the haploid phase, coccolithophores produce haploid cells through mitosis. These haploid cells can then divide further through mitosis or undergo sexual reproduction with other haploid cells. The resulting diploid cell goes through meiosis to produce haploid cells again, starting the cycle over. With coccolithophores, asexual reproduction by mitosis is possible in both phases of the life cycle, which is a contrast with most other organisms that have alternating life cycles.[42] Both abiotic and biotic factors may affect the frequency with which each phase occurs.[43] Coccolithophores reproduce asexually through binary fission. In this process the coccoliths from the parent cell are divided between the two daughter cells. There have been suggestions stating the possible presence of a sexual reproduction process due to the diploid stages of the coccolithophores, but this process has never been observed.[44] K or r- selected strategies of coccolithophores depend on their life cycle stage. When coccolithophores are diploid, they are r-selected. In this phase they tolerate a wider range of nutrient compositions. When they are haploid they are K- selected and are often more competitive in stable low nutrient environments.[44] Most coccolithophores are K strategist and are usually found on nutrient-poor surface waters. They are poor competitors when compared to other phytoplankton and thrive in habitats where other phytoplankton would not survive.[45] These two stages in the life cycle of coccolithophores occur seasonally, where more nutrition is available in warmer seasons and less is available in cooler seasons. This type of life cycle is known as a complex heteromorphic life cycle.[44] Coccolithophores occur throughout the world's oceans. Their distribution varies vertically by stratified layers in the ocean and geographically by different temporal zones.[46] While most modern coccolithophores can be located in their associated stratified oligotrophic conditions, the most abundant areas of coccolithophores where there is the highest species diversity are located in subtropical zones with a temperate climate.[47] While water temperature and the amount of light intensity entering the water's surface are the more influential factors in determining where species are located, the ocean currents also can determine the location where certain species of coccolithophores are found.[48] Although motility and colony formation vary according to the life cycle of different coccolithophore species, there is often alternation between a motile, haploid phase, and a non-motile diploid phase. In both phases, the organism's dispersal is largely due to ocean currents and circulation patterns.[49] Within the Pacific Ocean, approximately 90 species have been identified with six separate zones relating to different Pacific currents that contain unique groupings of different species of coccolithophores.[50] The highest diversity of coccolithophores in the Pacific Ocean was in an area of the ocean considered the Central North Zone which is an area between 30 oN and 5 oN, composed of the North Equatorial Current and the Equatorial Countercurrent. These two currents move in opposite directions, east and west, allowing for a strong mixing of waters and allowing a large variety of species to populate the area.[50] In the Atlantic Ocean, the most abundant species are E. huxleyi and Florisphaera profunda with smaller concentrations of the species Umbellosphaerairregularis, Umbellosphaera tenuis and different species of Gephyrocapsa.[50] Deep-dwelling coccolithophore species abundance is greatly affected by nutricline and thermocline depths. These coccolithophores increase in abundance when the nutricline and thermocline are deep and decrease when they are shallow.[51] Size comparison between the relatively large coccolithophore Scyphosphaera apsteinii and the relatively small but ubiquitous coccolithophore Emiliania huxleyi[52] The complete distribution of coccolithophores is currently not known and some regions, such as the Indian Ocean, are not as well studied as other locations in the Pacific and Atlantic Oceans. It is also very hard to explain distributions due to multiple constantly changing factors involving the ocean's properties, such as coastal and equatorial upwelling, frontal systems, benthic environments, unique oceanic topography, and pockets of isolated high or low water temperatures.[53] The upper photic zone is low in nutrient concentration, high in light intensity and penetration, and usually higher in temperature. The lower photic zone is high in nutrient concentration, low in light intensity and penetration and relatively cool. The middle photic zone is an area that contains the same values in between that of the lower and upper photic zones.[47] Larger coccolithophores such as the species above are less numerous than the smaller but ubiquitous Emiliania huxleyi, but they are heavily calcified and make important contributions to global calcification.[54][55] Unmarked scale bars 5 μm. The Great Calcite Belt of the Southern Ocean is a region of elevated summertime upper ocean calcite concentration derived from coccolithophores, despite the region being known for its diatom predominance. The overlap of two major phytoplankton groups, coccolithophores and diatoms, in the dynamic frontal systems characteristic of this region provides an ideal setting to study environmental influences on the distribution of different species within these taxonomic groups.[56] The Great Calcite Belt, defined as an elevated particulate inorganic carbon (PIC) feature occurring alongside seasonally elevated chlorophyll a in austral spring and summer in the Southern Ocean,[57] plays an important role in climate fluctuations,[58][59] accounting for over 60% of the Southern Ocean area (30–60° S).[60] The region between 30° and 50° S has the highest uptake of anthropogenic carbon dioxide (CO2) alongside the North Atlantic and North Pacific oceans.[61] Recent studies show that climate change has direct and indirect impacts on Coccolithophore distribution and productivity. They will inevitably be affected by the increasing temperatures and thermal stratification of the top layer of the ocean, since these are prime controls on their ecology, although it is not clear whether global warming would result in net increase or decrease of coccolithophores. As they are calcifying organisms, it has been suggested that ocean acidification due to increasing carbon dioxide could severely affect coccolithophores.[51] Recent CO2 increases have seen a sharp increase in the population of coccolithophores.[62] Coccolithophores are one of the more abundant primary producers in the ocean. As such, they are a large contributor to the primary productivity of the tropical and subtropical oceans, however, exactly how much has yet to have been recorded.[66] The ratio between the concentrations of nitrogen, phosphorus and silicate in particular areas of the ocean dictates competitive dominance within phytoplankton communities. Each ratio essentially tips the odds in favor of either diatoms or other groups of phytoplankton, such as coccolithophores. A low silicate to nitrogen and phosphorus ratio allows coccolithophores to outcompete other phytoplankton species; however, when silicate to phosphorus to nitrogen ratios are high coccolithophores are outcompeted by diatoms. The increase in agricultural processes lead to eutrophication of waters and thus, coccolithophore blooms in these high nitrogen and phosphorus, low silicate environments.[5] The calcite in calcium carbonate allows coccoliths to scatter more light than they absorb. This has two important consequences: 1) Surface waters become brighter, meaning they have a higher albedo, and 2) there is induced photoinhibition, meaning photosythetic production is diminished due to an excess of light. In case 1), a high concentration of coccoliths leads to a simultaneous increase in surface water temperature and decrease in the temperature of deeper waters. This results in more stratification in the water column and a decrease in the vertical mixing of nutrients. However, a 2012 study estimated that the overall effect of coccolithophores on the increase in radiative forcing of the ocean is less than that from anthropogenic factors.[67] Therefore, the overall result of large blooms of coccolithophores is a decrease in water column productivity, rather than a contribution to global warming. Their predators include the common predators of all phytoplankton including small fish, zooplankton, and shellfish larvae.[45][68] Viruses specific to this species have been isolated from several locations worldwide and appear to play a major role in spring bloom dynamics. No environmental evidence of coccolithophore toxicity has been reported, but they belong to the class Prymnesiophyceae which contain orders with toxic species. Toxic species have been found in the genera Prymnesium Massart and Chrysochromulina Lackey. Members of the genus Prymnesium have been found to produce haemolytic compounds, the agent responsible for toxicity. Some of these toxic species are responsible for large fish kills and can be accumulated in organisms such as shellfish; transferring it through the food chain. In laboratory tests for toxicity members of the oceanic coccolithophore genera Emiliania, Gephyrocapsa, Calcidiscus and Coccolithus were shown to be non-toxic as were species of the coastal genus Hymenomonas, however several species of Pleurochrysis and Jomonlithus, both coastal genera were toxic to Artemia.[68] Most phytoplankton need sunlight and nutrients from the ocean to survive, so they thrive in areas with large inputs of nutrient rich water upwelling from the lower levels of the ocean. Most coccolithophores require sunlight only for energy production, and have a higher ratio of nitrate uptake over ammonium uptake (nitrogen is required for growth and can be used directly from nitrate but not ammonium). Because of this they thrive in still, nutrient-poor environments where other phytoplankton are starving.[69]Trade-offs associated with these faster growth rates include a smaller cell radius and lower cell volume than other types of phytoplankton. Giant DNA-containing viruses are known to lytically infect coccolithophores, particularly E. huxleyi. These viruses, known as E. huxleyi viruses (EhVs), appear to infect the coccosphere coated diploid phase of the life cycle almost exclusively. It has been proposed that as the haploid organism is not infected and therefore not affected by the virus, the co-evolutionary \"arms race\" between coccolithophores and these viruses does not follow the classic Red Queen evolutionary framework, but instead a \"Cheshire Cat\" ecological dynamic.[70] More recent work has suggested that viral synthesis of sphingolipids and induction of programmed cell death provides a more direct link to study a Red Queen-like coevolutionary arms race at least between the coccolithoviruses and diploid organism.[43] Coccolithophores are members of the clade Haptophyta, which is a sister clade to Centrohelida, which are both in Haptista.[71] The oldest known coccolithophores are known from the Late Triassic, around the Norian-Rhaetian boundary.[72] Diversity steadily increased over the course of the Mesozoic, reaching its apex during the Late Cretaceous. However, there was a sharp drop during the Cretaceous-Paleogene extinction event, when more than 90% of coccolithophore species became extinct. Coccoliths reached another, lower apex of diversity during the Paleocene-Eocene thermal maximum, but have subsequently declined since the Oligocene due to decreasing global temperatures, with species that produced large and heavily calcified coccoliths most heavily affected.[26] Each coccolithophore encloses itself in a protective shell of coccoliths, calcified scales which make up its exoskeleton or coccosphere.[73] The coccoliths are created inside the coccolithophore cell and while some species maintain a single layer throughout life only producing new coccoliths as the cell grows, others continually produce and shed coccoliths. The primary constituent of coccoliths is calcium carbonate, or chalk. Calcium carbonate is transparent, so the organisms' photosynthetic activity is not compromised by encapsulation in a coccosphere.[45] Coccoliths are produced by a biomineralization process known as coccolithogenesis.[39] Generally, calcification of coccoliths occurs in the presence of light, and these scales are produced much more during the exponential phase of growth than the stationary phase.[74] Although not yet entirely understood, the biomineralization process is tightly regulated by calcium signaling. Calcite formation begins in the golgi complex where protein templates nucleate the formation of CaCO3 crystals and complex acidic polysaccharides control the shape and growth of these crystals.[49] As each scale is produced, it is exported in a Golgi-derived vesicle and added to the inner surface of the coccosphere. This means that the most recently produced coccoliths may lie beneath older coccoliths.[42] Depending upon the phytoplankton's stage in the life cycle, two different types of coccoliths may be formed. Holococcoliths are produced only in the haploid phase, lack radial symmetry, and are composed of anywhere from hundreds to thousands of similar minute (ca 0.1 μm) rhombic calcite crystals. These crystals are thought to form at least partially outside the cell. Heterococcoliths occur only in the diploid phase, have radial symmetry, and are composed of relatively few complex crystal units (fewer than 100). Although they are rare, combination coccospheres, which contain both holococcoliths and heterococcoliths, have been observed in the plankton recording coccolithophore life cycle transitions. Finally, the coccospheres of some species are highly modified with various appendages made of specialized coccoliths.[53] While the exact function of the coccosphere is unclear, many potential functions have been proposed. Most obviously coccoliths may protect the phytoplankton from predators. It also appears that it helps them to create a more stable pH. During photosynthesis carbon dioxide is removed from the water, making it more basic. Also calcification removes carbon dioxide, but chemistry behind it leads to the opposite pH reaction; it makes the water more acidic. The combination of photosynthesis and calcification therefore even out each other regarding pH changes.[75] In addition, these exoskeletons may confer an advantage in energy production, as coccolithogenesis seems highly coupled with photosynthesis. Organic precipitation of calcium carbonate from bicarbonate solution produces free carbon dioxide directly within the cellular body of the alga, this additional source of gas is then available to the Coccolithophore for photosynthesis. It has been suggested that they may provide a cell-wall like barrier to isolate intracellular chemistry from the marine environment.[76] More specific, defensive properties of coccoliths may include protection from osmotic changes, chemical or mechanical shock, and short-wavelength light.[41] It has also been proposed that the added weight of multiple layers of coccoliths allows the organism to sink to lower, more nutrient rich layers of the water and conversely, that coccoliths add buoyancy, stopping the cell from sinking to dangerous depths.[77] Coccolith appendages have also been proposed to serve several functions, such as inhibiting grazing by zooplankton.[53] Coccoliths are the main component of the Chalk, a Late Cretaceous rock formation which outcrops widely in southern England and forms the White Cliffs of Dover, and of other similar rocks in many other parts of the world.[10] At the present day sedimented coccoliths are a major component of the calcareous oozes that cover up to 35% of the ocean floor and is kilometres thick in places.[49] Because of their abundance and wide geographic ranges, the coccoliths which make up the layers of this ooze and the chalky sediment formed as it is compacted serve as valuable microfossils. Energetic costs of coccolithophore calcification.[26] Energetic costs reported as a percentage of total photosynthetic budget. Calcification, the biological production of calcium carbonate (CaCO3), is a key process in the marine carbon cycle. Coccolithophores are the major planktonic group responsible for pelagic CaCO3 production.[78][79] The diagram on the right shows the energetic costs of coccolithophore calcification: (A) Transport processes include the transport into the cell from the surrounding seawater of primary calcification substrates Ca2+ and HCO3− (black arrows) and the removal of the end product H+ from the cell (gray arrow). The transport of Ca2+ through the cytoplasm to the CV is the dominant cost associated with calcification.[26] (B) Metabolic processes include the synthesis of CAPs (gray rectangles) by the Golgi complex (white rectangles) that regulate the nucleation and geometry of CaCO3 crystals. The completed coccolith (gray plate) is a complex structure of intricately arranged CAPs and CaCO3 crystals.[26] (C) Mechanical and structural processes account for the secretion of the completed coccoliths that are transported from their original position adjacent to the nucleus to the cell periphery, where they are transferred to the surface of the cell. The costs associated with these processes are likely to be comparable to organic-scale exocytosis in noncalcifying haptophyte algae.[26] The diagram on the left shows the benefits of coccolithophore calcification. (A) Accelerated photosynthesis includes CCM (1) and enhanced light uptake via scattering of scarce photons for deep-dwelling species (2). (B) Protection from photodamage includes sunshade protection from ultraviolet (UV) light and photosynthetic active radiation (PAR) (1) and energy dissipation under high-light conditions (2). (C) Armor protection includes protection against viral/bacterial infections (1) and grazing by selective (2) and nonselective (3) grazers.[26] The degree by which calcification can adapt to ocean acidification is presently unknown. Cell physiological examinations found the essential H+ efflux (stemming from the use of HCO3− for intra-cellular calcification) to become more costly with ongoing ocean acidification as the electrochemical H+ inside-out gradient is reduced and passive proton outflow impeded.[80] Adapted cells would have to activate proton channels more frequently, adjust their membrane potential, and/or lower their internal pH.[81] Reduced intra-cellular pH would severely affect the entire cellular machinery and require other processes (e.g. photosynthesis) to co-adapt in order to keep H+ efflux alive.[82][83] The obligatory H+ efflux associated with calcification may therefore pose a fundamental constraint on adaptation which may potentially explain why \"calcification crisis\" were possible during long-lasting (thousands of years) CO2 perturbation events[84][85] even though evolutionary adaption to changing carbonate chemistry conditions is possible within one year.[84][85] Unraveling these fundamental constraints and the limits of adaptation should be a focus in future coccolithophore studies because knowing them is the key information required to understand to what extent the calcification response to carbonate chemistry perturbations can be compensated by evolution.[86] Silicate- or cellulose-armored functional groups such as diatoms and dinoflagellates do not need to sustain the calcification-related H+ efflux. Thus, they probably do not need to adapt in order to keep costs for the production of structural elements low. On the contrary, dinoflagellates (except for calcifying species;[87] with generally inefficient CO2-fixing RuBisCO enzymes[88] may even profit from chemical changes since photosynthetic carbon fixation as their source of structural elements in the form of cellulose should be facilitated by the ocean acidification-associated CO2 fertilization.[89][90] Under the assumption that any form of shell/exoskeleton protects phytoplankton against predation[28] non-calcareous armors may be the preferable solution to realize protection in a future ocean.[86] Representation of comparative energetic effort for armor construction in three major shell-forming phytoplankton taxa as a function of carbonate chemistry conditions[86] The diagram on the right is a representation of how the comparative energetic effort for armor construction in diatoms, dinoflagellates and coccolithophores appear to operate. The frustule (diatom shell) seems to be the most inexpensive armor under all circumstances because diatoms typically outcompete all other groups when silicate is available. The coccosphere is relatively inexpensive under sufficient [CO2], high [HCO3−], and low [H+] because the substrate is saturating and protons are easily released into seawater.[80] In contrast, the construction of thecal elements, which are organic (cellulose) plates that constitute the dinoflagellate shell, should rather be favored at high H+ concentrations because these usually coincide with high [CO2]. Under these conditions dinoflagellates could down-regulate the energy-consuming operation of carbon concentrating mechanisms to fuel the production of organic source material for their shell. Therefore, a shift in carbonate chemistry conditions toward high [CO2] may promote their competitiveness relative to coccolithophores. However, such a hypothetical gain in competitiveness due to altered carbonate chemistry conditions would not automatically lead to dinoflagellate dominance because a huge number of factors other than carbonate chemistry have an influence on species composition as well.[86][91] Currently, the evidence supporting or refuting a protective function of the coccosphere against predation is limited. Some researchers found that overall microzooplankton predation rates were reduced during blooms of the coccolithophore Emiliania huxleyi,[92][93] while others found high microzooplankton grazing rates on natural coccolithophore communities.[94] In 2020, researchers found that in situ ingestion rates of microzooplankton on E. huxleyi did not differ significantly from those on similar sized non-calcifying phytoplankton.[95] In laboratory experiments the heterotrophic dinoflagellate Oxyrrhis marina preferred calcified over non-calcified cells of E. huxleyi, which was hypothesised to be due to size selective feeding behaviour, since calcified cells are larger than non-calcified E. huxleyi.[96] In 2015, Harvey et al. investigated predation by the dinoflagellate O. marina on different genotypes of non-calcifying E. huxleyi as well as calcified strains that differed in the degree of calcification.[97] They found that the ingestion rate of O. marina was dependent on the genotype of E. huxleyi that was offered, rather than on their degree of calcification. In the same study, however, the authors found that predators which preyed on non-calcifying genotypes grew faster than those fed with calcified cells.[97] In 2018, Strom et al. compared predation rates of the dinoflagellate Amphidinium longum on calcified relative to naked E. huxleyi prey and found no evidence that the coccosphere prevents ingestion by the grazer.[98] Instead, ingestion rates were dependent on the offered genotype of E. huxleyi.[98] Altogether, these two studies suggest that the genotype has a strong influence on ingestion by the microzooplankton species, but if and how calcification protects coccolithophores from microzooplankton predation could not be fully clarified.[1] Because coccolithophores are photosynthetic organisms, they are able to use some of the CO2 released in the calcification reaction for photosynthesis.[100] However, the production of calcium carbonate drives surface alkalinity down, and in conditions of low alkalinity the CO2 is instead released back into the atmosphere.[101] As a result of this, researchers have postulated that large blooms of coccolithophores may contribute to global warming in the short term.[102] A more widely accepted idea, however, is that over the long term coccolithophores contribute to an overall decrease in atmospheric CO2 concentrations. During calcification two carbon atoms are taken up and one of them becomes trapped as calcium carbonate. This calcium carbonate sinks to the bottom of the ocean in the form of coccoliths and becomes part of sediment; thus, coccolithophores provide a sink for emitted carbon, mediating the effects of greenhouse gas emissions.[102] Research also suggests that ocean acidification due to increasing concentrations of CO2 in the atmosphere may affect the calcification machinery of coccolithophores. This may not only affect immediate events such as increases in population or coccolith production, but also may induce evolutionary adaptation of coccolithophore species over longer periods of time. For example, coccolithophores use H+ion channels in to constantly pump H+ ions out of the cell during coccolith production. This allows them to avoid acidosis, as coccolith production would otherwise produce a toxic excess of H+ ions. When the function of these ion channels is disrupted, the coccolithophores stop the calcification process to avoid acidosis, thus forming a feedback loop.[103] Low ocean alkalinity, impairs ion channel function and therefore places evolutionary selective pressure on coccolithophores and makes them (and other ocean calcifiers) vulnerable to ocean acidification.[104] In 2008, field evidence indicating an increase in calcification of newly formed ocean sediments containing coccolithophores bolstered the first ever experimental data showing that an increase in ocean CO2 concentration results in an increase in calcification of these organisms. Decreasing coccolith mass is related to both the increasing concentrations of CO2 and decreasing concentrations of CO2−3 in the world's oceans. This lower calcification is assumed to put coccolithophores at ecological disadvantage. Some species like Calcidiscusleptoporus, however, are not affected in this way, while the most abundant coccolithophore species, E. huxleyi might be (study results are mixed).[103][105] Also, highly calcified coccolithophorids have been found in conditions of low CaCO3 saturation contrary to predictions.[4] Understanding the effects of increasing ocean acidification on coccolithophore species is absolutely essential to predicting the future chemical composition of the ocean, particularly its carbonate chemistry. Viable conservation and management measures will come from future research in this area. Groups like the European-based CALMARO[106] are monitoring the responses of coccolithophore populations to varying pH's and working to determine environmentally sound measures of control. Of particular interest are fossils dating back to the Palaeocene-Eocene Thermal Maximum 55 million years ago. This period is thought to correspond most directly to the current levels of CO2 in the ocean.[108] Finally, field evidence of coccolithophore fossils in rock were used to show that the deep-sea fossil record bears a rock record bias similar to the one that is widely accepted to affect the land-based fossil record.[109] The coccolithophorids help in regulating the temperature of the oceans. They thrive in warm seas and release dimethyl sulfide (DMS) into the air whose nuclei help to produce thicker clouds to block the sun.[110] When the oceans cool, the number of coccolithophorids decrease and the amount of clouds also decrease. When there are fewer clouds blocking the sun, the temperature also rises. This, therefore, maintains the balance and equilibrium of nature.[111][112]"}
{"url": "https://en.m.wikipedia.org/wiki/Marine_microorganisms", "text": "Marine microorganisms have been variously estimated to make up about 70%,[4] or about 90%,[5][6] of the biomass in the ocean. Taken together they form the marine microbiome. Over billions of years this microbiome has evolved many life styles and adaptations and come to participate in the global cycling of almost all chemical elements.[7] Microorganisms are crucial to nutrient recycling in ecosystems as they act as decomposers. They are also responsible for nearly all photosynthesis that occurs in the ocean, as well as the cycling of carbon, nitrogen, phosphorus and other nutrients and trace elements.[8] Marine microorganisms sequester large amounts of carbon and produce much of the world's oxygen. A small proportion of marine microorganisms are pathogenic, causing disease and even death in marine plants and animals.[9] However marine microorganisms recycle the major chemical elements, both producing and consuming about half of all organic matter generated on the planet every year. As inhabitants of the largest environment on Earth, microbial marine systems drive changes in every global system. In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all life on the planet, including the marine microorganisms.[10] Despite its diversity, microscopic life in the oceans is still poorly understood. For example, the role of viruses in marine ecosystems has barely been explored even in the beginning of the 21st century.[11] Microorganisms are crucial to nutrient recycling in ecosystems as they act as decomposers. Some microorganisms are pathogenic, causing disease and even death in plants and animals.[9] As inhabitants of the largest environment on Earth, microbial marine systems drive changes in every global system. Microbes are responsible for virtually all the photosynthesis that occurs in the ocean, as well as the cycling of carbon, nitrogen, phosphorus and other nutrients and trace elements.[8] While recent technological developments and scientific discoveries have been substantial, we still lack a major understanding at all levels of the basic ecological questions in relation to the microorganisms in our seas and oceans. These fundamental questions are: 1. What is out there? Which microorganisms are present in our seas and oceans and in what numbers do they occur? 2. What are they doing? What functions do each of these microorganisms perform in the marine environment and how do they contribute to the global cycles of energy and matter? 3. What are the factors that determine the presence or absence of a microorganism and how do they influence biodiversity and function and vice versa? Microscopic life undersea is diverse and still poorly understood, such as for the role of viruses in marine ecosystems.[13] Most marine viruses are bacteriophages, which are harmless to plants and animals, but are essential to the regulation of saltwater and freshwater ecosystems.[14] They infect and destroy bacteria in aquatic microbial communities, and are the most important mechanism of recycling carbon in the marine environment. The organic molecules released from the dead bacterial cells stimulate fresh bacterial and algal growth.[15] Viral activity may also contribute to the biological pump, the process whereby carbon is sequestered in the deep ocean.[16] Sea spray containing marine microorganisms can be swept high into the atmosphere where they become aeroplankton, and can travel the globe before falling back to earth. A stream of airborne microorganisms circles the planet above weather systems but below commercial air lanes.[17] Some peripatetic microorganisms are swept up from terrestrial dust storms, but most originate from marine microorganisms in sea spray. In 2018, scientists reported that hundreds of millions of viruses and tens of millions of bacteria are deposited daily on every square meter around the planet.[18][19] Microscopic organisms live throughout the biosphere. The mass of prokaryote microorganisms — which includes bacteria and archaea, but not the nucleated eukaryote microorganisms — may be as much as 0.8 trillion tons of carbon (of the total biosphere mass, estimated at between 1 and 4 trillion tons).[20] Single-celled barophilic marine microbes have been found at a depth of 10,900 m (35,800 ft) in the Mariana Trench, the deepest spot in the Earth's oceans.[21][22] Microorganisms live inside rocks 580 m (1,900 ft) below the sea floor under 2,590 m (8,500 ft) of ocean off the coast of the northwestern United States,[21][23] as well as 2,400 m (7,900 ft; 1.5 mi) beneath the seabed off Japan.[24] The greatest known temperature at which microbial life can exist is 122 °C (252 °F) (Methanopyrus kandleri).[25] In 2014, scientists confirmed the existence of microorganisms living 800 m (2,600 ft) below the ice of Antarctica.[26][27] According to one researcher, \"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are.\"[21] Marine microorganisms serve as \"the foundation of all marine food webs, recycling major elements and producing and consuming about half the organic matter generated on Earth each year\".[28][29] When not inside an infected cell or in the process of infecting a cell, viruses exist in the form of independent particles. These viral particles, also known as virions, consist of two or three parts: (i) the genetic material (genome) made from either DNA or RNA, long molecules that carry genetic information; (ii) a protein coat called the capsid, which surrounds and protects the genetic material; and in some cases (iii) an envelope of lipids that surrounds the protein coat when they are outside a cell. The shapes of these virus particles range from simple helical and icosahedral forms for some virus species to more complex structures for others. Most virus species have virions that are too small to be seen with an optical microscope. The average virion is about one one-hundredth the size of the average bacterium. The origins of viruses in the evolutionary history of life are unclear: some may have evolved from plasmids—pieces of DNA that can move between cells—while others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity.[32] Viruses are considered by some to be a life form, because they carry genetic material, reproduce, and evolve through natural selection. However, they lack key characteristics (such as cell structure) that are generally considered necessary to count as life. Because they possess some but not all such qualities, viruses have been described as \"organisms at the edge of life\"[33] and as replicators.[34] Viruses are found wherever there is life and have probably existed since living cells first evolved.[35] The origin of viruses is unclear because they do not form fossils, so molecular techniques have been used to compare the DNA or RNA of viruses and are a useful means of investigating how they arose.[36] Viruses are now recognised as ancient and as having origins that pre-date the divergence of life into the three domains.[37] Opinions differ on whether viruses are a form of life or organic structures that interact with living organisms.[34] They are considered by some to be a life form, because they carry genetic material, reproduce by creating multiple copies of themselves through self-assembly, and evolve through natural selection. However they lack key characteristics such as a cellular structure generally considered necessary to count as life. Because they possess some but not all such qualities, viruses have been described as replicators[34] and as \"organisms at the edge of life\".[33] Bacteriophages, often just called phages, are viruses that parasite bacteria and archaea. Marine phages parasite marine bacteria and archaea, such as cyanobacteria.[38] They are a common and diverse group of viruses and are the most abundant biological entity in marine environments, because their hosts, bacteria, are typically the numerically dominant cellular life in the sea. Generally there are about 1 million to 10 million viruses in each mL of seawater, or about ten times more double-stranded DNA viruses than there are cellular organisms,[39][40] although estimates of viral abundance in seawater can vary over a wide range.[41][42] For a long time, tailed phages of the order Caudovirales seemed to dominate marine ecosystems in number and diversity of organisms.[38] However, as a result of more recent research, non-tailed viruses appear to be dominant in multiple depths and oceanic regions, followed by the Caudovirales families of myoviruses, podoviruses, and siphoviruses.[43] Phages belonging to the families: Corticoviridae,[44]Inoviridae,[45]Microviridae,[46] and Autolykiviridae[47][48][49][50] are also known to infect diverse marine bacteria. Microorganisms make up about 70% of the marine biomass.[4] It is estimated viruses kill 20% of this biomass each day and that there are 15 times as many viruses in the oceans as there are bacteria and archaea. Viruses are the main agents responsible for the rapid destruction of harmful algal blooms,[40] which often kill other marine life.[54] The number of viruses in the oceans decreases further offshore and deeper into the water, where there are fewer host organisms.[16] Viruses are an important natural means of transferring genes between different species, which increases genetic diversity and drives evolution.[32] It is thought that viruses played a central role in the early evolution, before the diversification of bacteria, archaea and eukaryotes, at the time of the last universal common ancestor of life on Earth.[55] Viruses are still one of the largest reservoirs of unexplored genetic diversity on Earth.[16] Viruses normally range in length from about 20 to 300 nanometers. This can be contrasted with the length of bacteria, which starts at about 400 nanometers. There are also giant viruses, often called giruses, typically about 1000 nanometers (one micron) in length. All giant viruses belongto phylumNucleocytoviricota (NCLDV), together with poxviruses. The largest known of these is Tupanvirus. This genus of giant virus was discovered in 2018 in the deep ocean as well as a soda lake, and can reach up to 2.3 microns in total length.[56] The discovery and subsequent characterization of giant viruses has triggered some debate concerning their evolutionary origins.[57] The two main hypotheses for their origin are that either they evolved from small viruses, picking up DNA from host organisms, or that they evolved from very complicated organisms into the current form which is not self-sufficient for reproduction.[58] What sort of complicated organism giant viruses might have diverged from is also a topic of debate. One proposal is that the origin point actually represents a fourth domain of life,[59][60] but this has been largely discounted.[61][62] The ancestors of modern bacteria were unicellular microorganisms that were the first forms of life to appear on Earth, about 4 billion years ago. For about 3 billion years, most organisms were microscopic, and bacteria and archaea were the dominant forms of life.[65][66] Although bacterial fossils exist, such as stromatolites, their lack of distinctive morphology prevents them from being used to examine the history of bacterial evolution, or to date the time of origin of a particular bacterial species. However, gene sequences can be used to reconstruct the bacterial phylogeny, and these studies indicate that bacteria diverged first from the archaeal/eukaryotic lineage.[67] Bacteria were also involved in the second great evolutionary divergence, that of the archaea and eukaryotes. Here, eukaryotes resulted from the entering of ancient bacteria into endosymbiotic associations with the ancestors of eukaryotic cells, which were themselves possibly related to the Archaea.[68][69] This involved the engulfment by proto-eukaryotic cells of alphaproteobacterial symbionts to form either mitochondria or hydrogenosomes, which are still found in all known Eukarya. Later on, some eukaryotes that already contained mitochondria also engulfed cyanobacterial-like organisms. This led to the formation of chloroplasts in algae and plants. There are also some algae that originated from even later endosymbiotic events. Here, eukaryotes engulfed a eukaryotic algae that developed into a \"second-generation\" plastid.[70][71] This is known as secondary endosymbiosis. Pelagibacter ubique and its relatives may be the most abundant organisms in the ocean, and it has been claimed that they are possibly the most abundant bacteria in the world. They make up about 25% of all microbial plankton cells, and in the summer they may account for approximately half the cells present in temperate ocean surface water. The total abundance of P. ubique and relatives is estimated to be about 2 × 1028 microbes.[73] However, it was reported in Nature in February 2013 that the bacteriophageHTVC 010 P, which attacks P. ubique, has been discovered and \"it probably really is the commonest organism on the planet\".[74][75] Archaea were initially viewed as extremophiles living in harsh environments, such as the yellow archaea pictured here in a hot spring, but they have since been found in a much broader range of habitats.[78] Archaea were initially classified as bacteria, but this classification is outdated.[80] Archaeal cells have unique properties separating them from the other two domains of life, Bacteria and Eukaryota. The Archaea are further divided into multiple recognized phyla. Classification is difficult because the majority have not been isolated in the laboratory and have only been detected by analysis of their nucleic acids in samples from their environment. Archaea are particularly numerous in the oceans, and the archaea in plankton may be one of the most abundant groups of organisms on the planet. Archaea are a major part of Earth's life and may play roles in both the carbon cycle and the nitrogen cycle. Thermoproteota (also known as eocytes or Crenarchaeota) are a phylum of archaea thought to be very abundant in marine environments and one of the main contributors to the fixation of carbon.[82] All living organisms can be grouped as either prokaryotes or eukaryotes. Life originated as single-celled prokaryotes and later evolved into the more complex eukaryotes. In contrast to prokaryotic cells, eukaryotic cells are highly organised. Prokaryotes are the bacteria and archaea, while eukaryotes are the other life forms — protists, plants, fungi and animals. Protists are usually single-celled, while plants, fungi and animals are usually multi-celled. It seems very plausible that the root of the eukaryotes lie within archaea; the closest relatives nowadays known may be the Heimdallarchaeota phylum of the proposed Asgard superphylum. This theory is a modern version of a scenario originally proposed in 1984 as Eocyte hypothesis, when Thermoproteota were the closest known archaeal relatives of eukaryotes then. A possible transitional form of microorganism between a prokaryote and a eukaryote was discovered in 2012 by Japanese scientists. Parakaryon myojinensis is a unique microorganism larger than a typical prokaryote, but with nuclear material enclosed in a membrane as in a eukaryote, and the presence of endosymbionts. This is seen to be the first plausible evolutionary form of microorganism, showing a stage of development from the prokaryote to the eukaryote.[83][84] Protists are eukaryotes that cannot be classified as plants, fungi or animals. They are usually single-celled and microscopic. Life originated as single-celled prokaryotes (bacteria and archaea) and later evolved into more complex eukaryotes. Eukaryotes are the more developed life forms known as plants, animals, fungi and protists. The term protist came into use historically as a term of convenience for eukaryotes that cannot be strictly classified as plants, animals or fungi. They are not a part of modern cladistics, because they are paraphyletic (lacking a common ancestor). Protists are highly diverse organisms currently organised into 18 phyla, but are not easy to classify.[89][90] Studies have shown high protist diversity exists in oceans, deep sea-vents and river sediments, suggesting a large number of eukaryotic microbial communities have yet to be discovered.[91][92] There has been little research on mixotrophic protists, but recent studies in marine environments found mixotrophic protests contribute a significant part of the protist biomass.[87] Since protists are eukaryotes they possess within their cell at least one nucleus, as well as organelles such as mitochondria and Golgi bodies. Protists are asexual but can reproduce rapidly through mitosis or by fragmentation. Single-celled and microscopic protists Diatoms are a major algae group generating about 20% of world oxygen production.[93] In contrast to the cells of prokaryotes, the cells of eukaryotes are highly organised. Plants, animals and fungi are usually multi-celled and are typically macroscopic. Most protists are single-celled and microscopic. But there are exceptions. Some single-celled marine protists are macroscopic. Some marine slime molds have unique life cycles that involve switching between unicellular, colonial, and multicellular forms.[95] Other marine protist are neither single-celled nor microscopic, such as seaweed. Protists have been described as a taxonomic grab bag of misfits where anything that doesn't fit into one of the main biological kingdoms can be placed.[98] Some modern authors prefer to exclude multicellular organisms from the traditional definition of a protist, restricting protists to unicellular organisms.[99][100] This more constrained definition excludes many brown, multicellular red and green algae, and slime molds.[101] Another way of categorising protists is according to their mode of locomotion. Many unicellular protists, particularly protozoans, are motile and can generate movement using flagella, cilia or pseudopods. Cells which use flagella for movement are usually referred to as flagellates, cells which use cilia are usually referred to as ciliates, and cells which use pseudopods are usually referred to as amoeba or amoeboids. Other protists are not motile, and consequently have no movement mechanism. A flagellum (Latin for whip) is a lash-like appendage that protrudes from the cell body of some protists (as well as some bacteria). Flagellates use from one to several flagella for locomotion and sometimes as feeding and sensory organelle. Flagellates include bacteria as well as protists. The rotary motor model used by bacteria uses the protons of an electrochemical gradient in order to move their flagella. Torque in the flagella of bacteria is created by particles that conduct protons around the base of the flagellum. The direction of rotation of the flagella in bacteria comes from the occupancy of the proton channels along the perimeter of the flagellar motor.[107] Ciliates generally have hundreds to thousands of cilia that are densely packed together in arrays. During movement, an individual cilium deforms using a high-friction power stroke followed by a low-friction recovery stroke. Since there are multiple cilia packed together on an individual organism, they display collective behavior in a metachronal rhythm. This means the deformation of one cilium is in phase with the deformation of its neighbor, causing deformation waves that propagate along the surface of the organism. These propagating waves of cilia are what allow the organism to use the cilia in a coordinated manner to move. A typical example of a ciliated microorganism is the Paramecium, a one-celled, ciliated protozoan covered by thousands of cilia. The cilia beating together allow the Paramecium to propel through the water at speeds of 500 micrometers per second.[108] Over 1500 species of fungi are known from marine environments.[109] These are parasitic on marine algae or animals, or are saprobes feeding on dead organic matter from algae, corals, protozoan cysts, sea grasses, and other substrata.[110] Spores of many species have special appendages which facilitate attachment to the substratum.[111] Marine fungi can also be found in sea foam and around hydrothermal areas of the ocean.[112] A diverse range of unusual secondary metabolites is produced by marine fungi.[113] A typical milliliter of seawater contains about 103 to 104 fungal cells.[119] This number is greater in coastal ecosystems and estuaries due to nutritional runoff from terrestrial communities. A higher diversity of mycoplankton is found around coasts and in surface waters down to 1000 metres, with a vertical profile that depends on how abundant phytoplankton is.[120][121] This profile changes between seasons due to changes in nutrient availability.[122] Marine fungi survive in a constant oxygen deficient environment, and therefore depend on oxygen diffusion by turbulence and oxygen generated by photosynthetic organisms.[123] Lichens are mutualistic associations between a fungus, usually an ascomycete, and an alga or a cyanobacterium. Several lichens are found in marine environments.[124] Many more occur in the splash zone, where they occupy different vertical zones depending on how tolerant they are to submersion.[125] Some lichens live a long time; one species has been dated at 8,600 years.[126] However their lifespan is difficult to measure because what defines the same lichen is not precise.[127] Lichens grow by vegetatively breaking off a piece, which may or may not be defined as the same lichen, and two lichens of different ages can merge, raising the issue of whether it is the same lichen.[127] The sea snailLittoraria irrorata damages plants of Spartina in the sea marshes where it lives, which enables spores of intertidal ascomycetous fungi to colonise the plant. The snail then eats the fungal growth in preference to the grass itself.[128] According to fossil records, fungi date back to the late Proterozoic era 900-570 million years ago. Fossil marine lichens 600 million years old have been discovered in China.[129] It has been hypothesized that mycoplankton evolved from terrestrial fungi, likely in the Paleozoic era (390 million years ago).[130] Composite image showing the global distribution of photosynthesis, including both oceanic phytoplankton and terrestrial vegetation. Dark red and blue-green indicate regions of high photosynthetic activity in the ocean and on land, respectively. Primary producers are the autotroph organisms that make their own food instead of eating other organisms. This means primary producers become the starting point in the food chain for heterotroph organisms that do eat other organisms. Some marine primary producers are specialised bacteria and archaea which are chemotrophs, making their own food by gathering around hydrothermal vents and cold seeps and using chemosynthesis. However most marine primary production comes from organisms which use photosynthesis on the carbon dioxide dissolved in the water. This process uses energy from sunlight to convert water and carbon dioxide[133]: 186–187 into sugars that can be used both as a source of chemical energy and of organic molecules that are used in the structural components of cells.[133]: 1242 Marine primary producers are important because they underpin almost all marine animal life by generating most of the oxygen and food that provide other organisms with the chemical energy they need to exist. Cyanobacteria were the first organisms to evolve an ability to turn sunlight into chemical energy. They form a phylum (division) of bacteria which range from unicellular to filamentous and include colonial species. They are found almost everywhere on earth: in damp soil, in both freshwater and marine environments, and even on Antarctic rocks.[136] In particular, some species occur as drifting cells floating in the ocean, and as such were amongst the first of the phytoplankton. The first primary producers that used photosynthesis were oceanic cyanobacteria about 2.3 billion years ago.[137][138] The release of molecular oxygen by cyanobacteria as a by-product of photosynthesis induced global changes in the Earth's environment. Because oxygen was toxic to most life on Earth at the time, this led to the near-extinction of oxygen-intolerant organisms, a dramatic change which redirected the evolution of the major animal and plant species.[139] The tiny (0.6 µm) marine cyanobacterium Prochlorococcus, discovered in 1986, forms today an important part of the base of the ocean food chain and accounts for much of the photosynthesis of the open ocean[140] and an estimated 20% of the oxygen in the Earth's atmosphere.[141] It is possibly the most plentiful genus on Earth: a single millilitre of surface seawater may contain 100,000 cells or more.[142] Originally, biologists thought cyanobacteria was algae, and referred to it as \"blue-green algae\". The more recent view is that cyanobacteria are bacteria, and hence are not even in the same Kingdom as algae. Most authorities exclude all prokaryotes, and hence cyanobacteria from the definition of algae.[143][144] Dinoflagellates and diatoms are important components of marine algae and have their own sections below. Euglenophytes are a phylum of unicellular flagellates with only a few marine members. Not all algae are microscopic. Green, red and brown algae all have multicellular macroscopic forms that make up the familiar seaweeds. Green algae, an informal group, contains about 8,000 recognised species.[145] Many species live most of their lives as single cells or are filamentous, while others form colonies made up from long chains of cells, or are highly differentiated macroscopic seaweeds. Red algae, a (disputed) phylum contains about 7,000 recognised species,[146] mostly multicellular and including many notable seaweeds.[146][147]Brown algae form a class containing about 2,000 recognised species,[148] mostly multicellular and including many seaweeds such as kelp. Unlike higher plants, algae lack roots, stems, or leaves. They can be classified by size as microalgae or macroalgae. Microalgae are the microscopic types of algae, not visible to the naked eye. They are mostly unicellular species which exist as individuals or in chains or groups, though some are multicellular. Microalgae are important components of the marine protists discussed above, as well as the phytoplankton discussed below. They are very diverse. It has been estimated there are 200,000-800,000 species of which about 50,000 species have been described.[149] Depending on the species, their sizes range from a few micrometers (µm) to a few hundred micrometers. They are specially adapted to an environment dominated by viscous forces. Unicellular organisms are usually microscopic. There are exceptions. Mermaid's wineglass, a genus of subtropical green algae, is single-celled but remarkably large and complex in form with a single large nucleus, making it a model organism for studying cell biology.[150] Another single-celled algae, Caulerpa taxifolia, has the appearance of a vascular plant including \"leaves\" arranged neatly up stalks like a fern. Selective breeding in aquariums to produce hardier strains resulted in an accidental release into the Mediterranean where it has become an invasive species known colloquially as killer algae.[151] Chlamydomonas globosa, a unicellular green alga with two flagella just visible at bottom left Macroalgae are the larger, multicellular and more visible types of algae, commonly called seaweeds. Seaweeds usually grow in shallow coastal waters where they are anchored to the seafloor by a holdfast. Like microalgae, macroalgae (seaweeds) can be regarded as marine protists since they are not true plants. But they are not microorganisms, so they are not within the scope of this article. Plankton (from Greek for wanderers) are a diverse group of organisms that live in the water column of large bodies of water but cannot swim against a current. As a result, they wander or drift with the currents.[153] Plankton are defined by their ecological niche, not by any phylogenetic or taxonomic classification. They are a crucial source of food for many marine animals, from forage fish to whales. Plankton can be divided into a plant-like component and an animal component. Phytoplankton – such as this colony of Chaetoceros socialis – naturally gives off red fluorescent light which dissipates excess solar energy they cannot consume through photosynthesis. This glow can be detected by satellites as an indicator of how efficiently ocean phytoplankton is photosynthesising.[154][155] Phytoplankton are the plant-like components of the plankton community (\"phyto\" comes from the Greek for plant). They are autotrophic (self-feeding), meaning they generate their own food and do not need to consume other organisms. Phytoplankton perform three crucial functions: they generate nearly half of the world atmospheric oxygen, they regulate ocean and atmospheric carbon dioxide levels, and they form the base of the marine food web. When conditions are right, blooms of phytoplankton algae can occur in surface waters. Phytoplankton are r-strategists which grow rapidly and can double their population every day. The blooms can become toxic and deplete the water of oxygen. However, phytoplankton numbers are usually kept in check by the phytoplankton exhausting available nutrients and by grazing zooplankton.[156] Diatoms form a (disputed) phylum containing about 100,000 recognised species of mainly unicellular algae. Diatoms generate about 20 per cent of the oxygen produced on the planet each year,[93] take in over 6.7 billion metric tons of silicon each year from the waters in which they live,[159] and contribute nearly half of the organic material found in the oceans. Diatoms have a silica shell (frustule) with radial (centric) or bilateral (pennate) symmetry Diatoms are enclosed in protective silica (glass) shells called frustules. Each frustule is made from two interlocking parts covered with tiny holes through which the diatom exchanges nutrients and wastes.[156] The frustules of dead diatoms drift to the ocean floor where, over millions of years, they can build up as much as half a mile deep.[160] Silicified frustule of a pennate diatom with two overlapping halves Guinardia delicatula, a diatom responsible for algal blooms in the North Sea and the English Channel[161] Fossil diatom There are over 100,000 species of diatoms which account for 50% of the ocean's primary production Coccolithophores are minute unicellular photosynthetic protists with two flagella for locomotion. Most of them are protected by a shell covered with ornate circular plates or scales called coccoliths. The coccoliths are made from calcium carbonate. The calcite shells are important to the marine carbon cycle.[163] The term coccolithophore derives from the Greek for a seed carrying stone, referring to their small size and the coccolith stones they carry. Under the right conditions they bloom, like other phytoplankton, and can turn the ocean milky white.[164] (1) When sunlight strikes a rhodopsin molecule (2) it changes its configuration so a proton is expelled from the cell (3) the chemical potential causes the proton to flow back to the cell (4) thus generating energy (5) in the form of adenosine triphosphate.[165] Phototrophic metabolism relies on one of three energy-converting pigments: chlorophyll, bacteriochlorophyll, and retinal. Retinal is the chromophore found in rhodopsins. The significance of chlorophyll in converting light energy has been written about for decades, but phototrophy based on retinal pigments is just beginning to be studied.[166] In 2000 a team of microbiologists led by Edward DeLong made a crucial discovery in the understanding of the marine carbon and energy cycles. They discovered a gene in several species of bacteria[168][169] responsible for production of the protein rhodopsin, previously unheard of in bacteria. These proteins found in the cell membranes are capable of converting light energy to biochemical energy due to a change in configuration of the rhodopsin molecule as sunlight strikes it, causing the pumping of a proton from inside out and a subsequent inflow that generates the energy.[170] The archaeal-like rhodopsins have subsequently been found among different taxa, protists as well as in bacteria and archaea, though they are rare in complex multicellular organisms.[171][172][173] Research in 2019 shows these \"sun-snatching bacteria\" are more widespread than previously thought and could change how oceans are affected by global warming. \"The findings break from the traditional interpretation of marine ecology found in textbooks, which states that nearly all sunlight in the ocean is captured by chlorophyll in algae. Instead, rhodopsin-equipped bacteria function like hybrid cars, powered by organic matter when available — as most bacteria are — and by sunlight when nutrients are scarce.\"[174][166] There is an astrobiological conjecture called the Purple Earth hypothesis which surmises that original life forms on Earth were retinal-based rather than chlorophyll-based, which would have made the Earth appear purple instead of green.[175][176] During the 1930s Alfred C. Redfield found similarities between the composition of elements in phytoplankton and the major dissolved nutrients in the deep ocean.[177] Redfield proposed that the ratio of carbon to nitrogen to phosphorus (106:16:1) in the ocean was controlled by the phytoplankton's requirements, as phytoplankton subsequently release nitrogen and phosphorus as they remineralize. This ratio has become known as the Redfield ratio, and is used as a fundamental principle in describing the stoichiometry of seawater and phytoplankton evolution.[178] However, the Redfield ratio is not a universal value and can change with things like geographical latitude.[179] Based on allocation of resources, phytoplankton can be classified into three different growth strategies: survivalist, bloomer and generalist. Survivalist phytoplankton has a high N:P ratio (>30) and contains an abundance of resource-acquisition machinery to sustain growth under scarce resources. Bloomer phytoplankton has a low N:P ratio (<10), contains a high proportion of growth machinery and is adapted to exponential growth. Generalist phytoplankton has similar N:P to the Redfield ratio and contain relatively equal resource-acquisition and growth machinery.[178] Zooplankton are the animal component of the planktonic community (\"zoo\" comes from the Greek for animal). They are heterotrophic (other-feeding), meaning they cannot produce their own food and must consume instead other plants or animals as food. In particular, this means they eat phytoplankton. Radiolarians are unicellular predatory protists encased in elaborate globular shells usually made of silica and pierced with holes. Their name comes from the Latin for \"radius\". They catch prey by extending parts of their body through the holes. As with the silica frustules of diatoms, radiolarian shells can sink to the ocean floor when radiolarians die and become preserved as part of the ocean sediment. These remains, as microfossils, provide valuable information about past oceanic conditions.[185] Like radiolarians, foraminiferans (forams for short) are single-celled predatory protists, also protected with shells that have holes in them. Their name comes from the Latin for \"hole bearers\". Their shells, often called tests, are chambered (forams add more chambers as they grow). The shells are usually made of calcite, but are sometimes made of agglutinated sediment particles or chiton, and (rarely) of silica. Most forams are benthic, but about 40 species are planktic.[187] They are widely researched with well established fossil records which allow scientists to infer a lot about past environments and climates.[185] Foraminiferans ...can have more than one nucleus ...and defensive spines Foraminiferans are important unicellular zooplankton protists, with calcium tests Recent studies of marine microzooplankton found 30–45% of the ciliate abundance was mixotrophic, and up to 65% of the amoeboid, foram and radiolarian biomass was mixotrophic.[87] Phaeocystis is an important algal genus found as part of the marine phytoplankton around the world. It has a polymorphic life cycle, ranging from free-living cells to large colonies.[196] It has the ability to form floating colonies, where hundreds of cells are embedded in a gel matrix, which can increase massively in size during blooms.[197] As a result, Phaeocystis is an important contributor to the marine carbon[198] and sulfur cycles.[199]Phaeocystis species are endosymbionts to acantharian radiolarians.[200][201] Called nonconstitutive mixotrophs by Mitra et al., 2016.[203] Zooplankton that are photosynthetic: microzooplankton or metazoan zooplankton that acquire phototrophy through chloroplast retentiona or maintenance of algal endosymbionts. Generalists Protists that retain chloroplasts and rarely other organelles from many algal taxa Dinoflagellates are part of the algae group, and form a phylum of unicellular flagellates with about 2,000 marine species.[204] The name comes from the Greek \"dinos\" meaning whirling and the Latin \"flagellum\" meaning a whip or lash. This refers to the two whip-like attachments (flagella) used for forward movement. Most dinoflagellates are protected with red-brown, cellulose armour. Like other phytoplankton, dinoflagellates are r-strategists which under right conditions can bloom and create red tides. Excavates may be the most basal flagellate lineage.[102] By trophic orientation dinoflagellates cannot be uniformly categorized. Some dinoflagellates are known to be photosynthetic, but a large fraction of these are in fact mixotrophic, combining photosynthesis with ingestion of prey (phagotrophy).[205] Some species are endosymbionts of marine animals and other protists, and play an important part in the biology of coral reefs. Others predate other protozoa, and a few forms are parasitic. Many dinoflagellates are mixotrophic and could also be classified as phytoplankton. The toxic dinoflagellate Dinophysis acuta acquire chloroplasts from its prey. \"It cannot catch the cryptophytes by itself, and instead relies on ingesting ciliates such as the red Myrionecta rubra, which sequester their chloroplasts from a specific cryptophyte clade (Geminigera/Plagioselmis/Teleaulax)\".[202] Traditionally dinoflagellates have been presented as armoured or unarmoured Dinoflagellates often live in symbiosis with other organisms. Many nassellarian radiolarians house dinoflagellatesymbionts within their tests.[207] The nassellarian provides ammonium and carbon dioxide for the dinoflagellate, while the dinoflagellate provides the nassellarian with a mucous membrane useful for hunting and protection against harmful invaders.[208] There is evidence from DNA analysis that dinoflagellate symbiosis with radiolarians evolved independently from other dinoflagellate symbioses, such as with foraminifera.[209] Some dinoflagellates are bioluminescent. At night, ocean water can light up internally and sparkle with blue light because of these dinoflagellates.[210][211] Bioluminescent dinoflagellates possess scintillons, individual cytoplasmic bodies which contain dinoflagellate luciferase, the main enzyme involved in the luminescence. The luminescence, sometimes called the phosphorescence of the sea, occurs as brief (0.1 sec) blue flashes or sparks when individual scintillons are stimulated, usually by mechanical disturbances from, for example, a boat or a swimmer or surf.[212] Stone dagger of Ötzi the Iceman who lived during the Copper Age. The blade is made of chert containing radiolarians, calcispheres, calpionellids and a few sponge spicules. The presence of calpionellids, which are extinct, was used to date this dagger.[216] Sediments at the bottom of the ocean have two main origins, terrigenous and biogenous. Terrigenous sediments account for about 45% of the total marine sediment, and originate in the erosion of rocks on land, transported by rivers and land runoff, windborne dust, volcanoes, or grinding by glaciers. Biogenous sediments account for the other 55% of the total sediment, and originate in the skeletal remains of marine protists (single-celled plankton and benthos microorganisms). Much smaller amounts of precipitated minerals and meteoric dust can also be present. Ooze, in the context of a marine sediment, does not refer to the consistency of the sediment but to its biological origin. The term ooze was originally used by John Murray, the \"father of modern oceanography\", who proposed the term radiolarian ooze for the silica deposits of radiolarian shells brought to the surface during the Challenger expedition.[217] A biogenic ooze is a pelagic sediment containing at least 30 per cent from the skeletal remains of marine organisms. Coccolithophores are the largest global source of biogenic calcium carbonate, and significantly contribute to the global carbon cycle.[221] They are the main constituent of chalk deposits such as the white cliffs of Dover. Distribution of sediment types on the seafloor Within each colored area, the type of material shown is what dominates, although other materials are also likely to be present. For further information, see here Marine microbenthos are microorganisms that live in the benthic zone of the ocean – that live near or on the seafloor, or within or on surface seafloor sediments. The word benthos comes from Greek, meaning \"depth of the sea\". Microbenthos are found everywhere on or about the seafloor of continental shelves, as well as in deeper waters, with greater diversity in or on seafloor sediments. In shallow waters, seagrass meadows, coral reefs and kelp forests provide particularly rich habitats. In photic zones benthic diatoms dominate as photosynthetic organisms. In intertidal zones changing tides strongly control opportunities for microbenthos. Both foraminifera and diatoms have planktonic and benthic forms, that is, they can drift in the water column or live on sediment at the bottom of the ocean. Either way, their shells end up on the seafloor after they die. These shells are widely used as climate proxies. The chemical composition of the shells are a consequence of the chemical composition of the ocean at the time the shells were formed. Past water temperatures can be also be inferred from the ratios of stable oxygen isotopes in the shells, since lighter isotopes evaporate more readily in warmer water leaving the heavier isotopes in the shells. Information about past climates can be inferred further from the abundance of forams and diatoms, since they tend to be more abundant in warm water.[223] The sudden extinction event which killed the dinosaurs 66 million years ago also rendered extinct three-quarters of all other animal and plant species. However, deep-sea benthic forams flourished in the aftermath. In 2020 it was reported that researchers have examined the chemical composition of thousands of samples of these benthic forams and used their findings to build the most detailed climate record of Earth ever.[224][225] Some endoliths have extremely long lives. In 2013 researchers reported evidence of endoliths in the ocean floor, perhaps millions of years old, with a generation time of 10,000 years.[226] These are slowly metabolizing and not in a dormant state. Some Actinomycetota found in Siberia are estimated to be half a million years old.[227][228][229] (A) Microbial interactions range from mutually beneficial to harmful for one or more partners. Blue double headed arrows highlight that relationships can move between classifications often influenced by environmental conditions. (B) Host-microbe symbioses should be considered within the context of microbial communities where the host participates in multiple and often different symbiotic relationships. (C) Microbial communities are influenced by a variety of microbe-microbe symbioses ranging from cooperation (e.g., syntrophy or co-metabolism) to competition. Arrows depict generally beneficial (blue) and detrimental (red) outcomes for one (single arrowhead) or both (double arrowhead) members. Note as with host-microbe symbioses these relationships can be viewed as fluid and influenced by environmental conditions.[230] The concept of the holobiont was initially defined by Dr. Lynn Margulis in her 1991 book Symbiosis as a Source of Evolutionary Innovation as an assemblage of a host and the many other species living in or around it, which together form a discrete ecological unit.[231] The components of a holobiont are individual species or bionts, while the combined genome of all bionts is the hologenome.[232] The concept has subsequently evolved since this original definition,[233] with the focus moving to the microbial species associated with the host. Thus the holobiont includes the host, virome, microbiome, and other members, all of which contribute in some way to the function of the whole.[234][235] A holobiont typically includes a eukaryotehost and all of the symbioticviruses, bacteria, fungi, etc. that live on or inside it.[236] However, there is controversy over whether holobionts can be viewed as single evolutionary units.[237] The viral shunt pathway is a mechanism that prevents marine microbial particulate organic matter (POM) from migrating up trophic levels by recycling them into dissolved organic matter (DOM), which can be readily taken up by microorganisms.[244] Viral shunting helps maintain diversity within the microbial ecosystem by preventing a single species of marine microbe from dominating the micro-environment.[245] The DOM recycled by the viral shunt pathway is comparable to the amount generated by the other main sources of marine DOM.[246] Sea ice microbial communities (SIMCO) refer to groups of microorganisms living within and at the interfaces of sea ice at the poles. The ice matrix they inhabit has strong vertical gradients of salinity, light, temperature and nutrients. Sea ice chemistry is most influenced by the salinity of the brine which affects the pH and the concentration of dissolved nutrients and gases. The brine formed during the melting sea ice creates pores and channels in the sea ice in which these microbes can live. As a result of these gradients and dynamic conditions, a higher abundance of microbes are found in the lower layer of the ice, although some are found in the middle and upper layers.[251] Hydrothermal vents are located where the tectonic plates are moving apart and spreading. This allows water from the ocean to enter into the crust of the earth where it is heated by the magma. The increasing pressure and temperature forces the water back out of these openings, on the way out, the water accumulates dissolved minerals and chemicals from the rocks that it encounters. Vents can be characterized by temperature and chemical composition as diffuse vents which release clear relatively cool water usually below 30 °C, as white smokers which emit milky coloured water at warmer temperatures, about 200-330 °C, and as black smokers which emit water darkened by accumulated precipitates of sulfide at hot temperatures, about 300-400 °C.[252] Hydrothermal vent microbial communities are microscopic unicellular organisms that live and reproduce in the chemically distinct area around hydrothermal vents. These include organisms in microbial mats, free floating cells, and bacteria in endosymbiotic relationships with animals. Because there is no sunlight at these depths, energy is provided by chemosynthesis where symbiotic bacteria and archaea form the bottom of the food chain and are able to support a variety of organisms such as giant tube worms and Pompeii worms. These organisms utilize this symbiotic relationship in order to utilize and obtain the chemical energy that is released at these hydrothermal vent areas.[253]Chemolithoautotrophic bacteria can derive nutrients and energy from the geological activity at a hydrothermal vent to fix carbon into organic forms.[254] Viruses are also a part of the hydrothermal vent microbial community and their influence on the microbial ecology in these ecosystems is a burgeoning field of research.[255] Viruses are the most abundant life in the ocean, harboring the greatest reservoir of genetic diversity.[256] As their infections are often fatal, they constitute a significant source of mortality and thus have widespread influence on biological oceanographic processes, evolution and biogeochemical cycling within the ocean.[257] Evidence has been found however to indicate that viruses found in vent habitats have adopted a more mutualistic than parasitic evolutionary strategy in order to survive the extreme and volatile environment they exist in.[258] Deep-sea hydrothermal vents were found to have high numbers of viruses indicating high viral production.[259] Like in other marine environments, deep-sea hydrothermal viruses affect abundance and diversity of prokaryotes and therefore impact microbial biogeochemical cycling by lysing their hosts to replicate.[260] However, in contrast to their role as a source of mortality and population control, viruses have also been postulated to enhance survival of prokaryotes in extreme environments, acting as reservoirs of genetic information. The interactions of the virosphere with microorganisms under environmental stresses is therefore thought to aide microorganism survival through dispersal of host genes through horizontal gene transfer.[261] The deep biosphere is that part of the biosphere that resides below the first few meters of the surface. It extends at least 5 kilometers below the continental surface and 10.5 kilometers below the sea surface, with temperatures that may exceed 100 °C. Above the surface living organisms consume organic matter and oxygen. Lower down, these are not available, so they make use of \"edibles\" (electron donors) such as hydrogen released from rocks by various chemical processes, methane, reduced sulfur compounds and ammonium. They \"breathe\" electron acceptors such as nitrates and nitrites, manganese and iron oxides, oxidized sulfur compounds and carbon dioxide. There is very little energy at greater depths, and metabolism can be up to a million times slower than at the surface. Cells may live for thousands of years before dividing and there is no known limit to their age. The subsurface accounts for about 90% of the biomass in bacteria and archaea, and 15% of the total biomass for the biosphere. Eukaryotes are also found, mostly microscopic, but including some multicellular life. Viruses are also present and infect the microbes. Subsurface life environments In 2018, researchers from the Deep Carbon Observatory announced that life forms, including 70% of the bacteria and archaea on Earth, totaling a biomass of 23 billion tonnes carbon, live up to 4.8 km (3.0 mi) deep underground, including 2.5 km (1.6 mi) below the seabed.[262][263][264] In 2019 microbial organisms were discovered living 7,900 feet (2,400 m) below the surface, breathing sulfur and eating rocks such as pyrite as their regular food source.[265][266][267] This discovery occurred in the oldest known water on Earth.[268] These aerobic microorganisms, found deep in organically poor sediments, have been in quasi-suspended animation for maybe 100 million years To date biologists have been unable to culture in the laboratory the vast majority of microorganisms. This applies particularly to bacteria and archaea, and is due to a lack of knowledge or ability to supply the required growth conditions.[271][272] The term microbial dark matter has come to be used to describe microorganisms scientists know are there but have been unable to culture, and whose properties therefore remain elusive.[271] Microbial dark matter is unrelated to the dark matter of physics and cosmology, but is so-called for the difficulty in effectively studying it. It is hard to estimate its relative magnitude, but the accepted gross estimate is that less than one per cent of microbial species in a given ecological niche is culturable. In recent years effort is being put to decipher more of the microbial dark matter by means of learning their genomeDNA sequence from environmental samples[273] and then by gaining insights to their metabolism from their sequenced genome, promoting the knowledge required for their cultivation. Bacteria are the oldest and most biodiverse group, followed by Archaea and Fungi (the most recent groups). In 1998, before awareness of the extent of microbial life had gotten underway, Robert M. May[274] estimated there were 3 million species of living organisms on the planet. But in 2016, Locey and Lennon[275] estimated the number of microorganism species could be as high as 1 trillion.[276] Microbial diversity Comparative representation of the known and estimated (small box) and the yet unknown (large box) microbial diversity, which applies to both marine and terrestrial microorganisms. The text boxes refer to factors that adversely affect the knowledge of the microbial diversity that exists on the planet.[276] Strategies for sampling plankton by size classes and abundance The blue background indicates the filtered volume required to obtain sufficient organism numbers for analysis. Actual volumes from which organisms are sampled are always recorded.[277] Traditionally, the phylogeny of microorganisms was inferred and their taxonomy was established based on studies of morphology. However, developments in molecular phylogenetics have allowed evolutionary relationship of species to be established by analyzing deeper characteristics, such as their DNA and protein sequences, for example ribosomal DNA.[278] The lack of easily accessible morphological features, such as those present in animals and plants, particularly hampered early efforts at classifying bacteria and archaea. This resulted in erroneous, distorted and confused classification, an example of which, noted Carl Woese, is Pseudomonas whose etymology ironically matched its taxonomy, namely \"false unit\".[279] Many bacterial taxa have been reclassified or redefined using molecular phylogenetics. It would be difficult to consistently separate out these two microbes using images alone. However, if their barcodes are aligned to each other and their bases are coloured to see them more clearly, it becomes easy to see which bases are different between these two microbes. In this manner, millions of different kinds of microbes can be distinguished.[280] DNA barcode alignment and comparison between the two species of marine bacteria pictured above[280] Methods used to study phytoplankton Three different possibilities to process the sample are using raw samples, fixation or preservation, and filtration. For microscopy and flow cytometry raw samples either are measured immediately or have to be fixed for later measurements. Since molecular methods, pigment analysis and detection of molecular tracers usually require concentrated cells, filter residues serve for phytoplankton measurements. Molecular characterization and quantification of trace molecules is performed using chromatography, mass spectrometry, and nuclear magnetic resonance spectroscopy.[281] The new sequencing technologies and the accumulation of sequence data have resulted in a paradigm shift, highlighted both the ubiquity of microbial communities in association within higher organisms and the critical roles of microbes in ecosystem health.[283] These new possibilities have revolutionized microbial ecology, because the analysis of genomes and metagenomes in a high-throughput manner provides efficient methods for addressing the functional potential of individual microorganisms as well as of whole communities in their natural habitats.[284][285][286] The discovery process involves marine sampling, DNA sequencing and contig generation. Previously unknown genes, pathways and even whole genomes are being discovered. These genome-editing technologies are used to retrieve and modify valuable microorganisms for production, particularly in marine metagenomics. Organisms may be cultivable or uncultivable. Metagenomics is providing especially valuable information for uncultivable samples.[287] Omics is a term used informally to refer to branches of biology whose names end in the suffix -omics, such as genomics, proteomics, metabolomics, and glycomics. Marine Omics has recently emerged as a field of research of its own.[288] Omics aims at collectively characterising and quantifying pools of biological molecules that translate into the structure, function, and dynamics of an organism or organisms. For example, functional genomics aims at identifying the functions of as many genes as possible of a given organism. It combines different -omics techniques such as transcriptomics and proteomics with saturated mutant collections.[289][290] Many omes beyond the original genome have become useful and have been widely adopted in recent years by research scientists. The suffix -omics can provide an easy shorthand to encapsulate a field; for example, an interactomics study is reasonably recognisable as relating to large-scale analyses of gene-gene, protein-protein, or protein-ligand interactions, while proteomics has become established as a term for studying proteins on a large scale. A schematic conceptual framework for marine biogeochemical modeling from environmental, imaging, and meta-omics data.[294] A semi-automatic computational pipeline is schematized for combining biomarkers with biogeochemical data [295] that can be incorporated into classic biogeochemical models [296] for creating a next generation of biogeochemical trait-based meta-omics models by considering their respective traits. Such novel meta-omics-enabled approaches aim to improve the monitoring and prediction of ocean processes while respecting the complexity of the planktonic system.[297][298] As an example of how omics data can be used with marine phytoplankton to inform Earth system science, metatranscriptome sequences from natural phytoplankton communities were used to help identify physiological traits (cellular concentration of ribosomes and their rRNAs) underpinning adaptation to environmental conditions (temperature). A mechanistic phytoplankton cell model was used to test the significance of the identified physiological trait for cellular stoichiometry. Environmental selection in a trait‐based global marine ecosystem model was then linking emergent growth and cellular allocation strategies to large‐scale patterns in light, nutrients and temperature in the surface marine environment. Global predictions of cellular resource allocation and stoichiometry (N:P ratio) were consistent with patterns in metatranscriptome data[300] and latitudinal patterns in the elemental ratios of marine plankton and organic matter.[301] The three‐dimensional view of ribosome shows rRNA in dark blue and dark red. Lighter colours represent ribosomal proteins. Bands above show temperature‐dependent abundance of the eukaryotic ribosomal protein S14.[300] In marine environments, microbial primary production contributes substantially to CO2 sequestration. Marine microorganisms also recycle nutrients for use in the marine food web and in the process release CO2 to the atmosphere. Microbial biomass and other organic matter (remnants of plants and animals) are converted to fossil fuels over millions of years. By contrast, burning of fossil fuels liberates greenhouse gases in a small fraction of that time. As a result, the carbon cycle is out of balance, and atmospheric CO2 levels will continue to rise as long as fossil fuels continue to be burnt.[6] Microorganisms have key roles in carbon and nutrient cycling, animal (including human) and plant health, agriculture and the global food web. Microorganisms live in all environments on Earth that are occupied by macroscopic organisms, and they are the sole life forms in other environments, such as the deep subsurface and ‘extreme’ environments. Microorganisms date back to the origin of life on Earth at least 3.8 billion years ago, and they will likely exist well beyond any future extinction events... Unless we appreciate the importance of microbial processes, we fundamentally limit our understanding of Earth's biosphere and response to climate change and thus jeopardize efforts to create an environmentally sustainable future.[6] Marine microorganisms known as cyanobacteria first emerged in the oceans during the Precambrian era roughly 2 billion years ago. Over eons, the photosynthesis of marine microorganisms generated by oxygen has helped shape the chemical environment in the evolution of plants, animals and many other life forms. Marine microorganisms were first observed in 1675 by Dutch lensmaker Antonie van Leeuwenhoek. ^Starckx, Senne (31 October 2012). \"A place in the sun\". Flanders Today. Archived from the original on 4 March 2016. Retrieved 8 December 2012. Algae is the crop of the future, according to researchers in Geel ^Mandoli DF (1998). \"Elaboration of Body Plan and Phase Change during Development of Acetabularia: How Is the Complex Architecture of a Giant Unicell Built?\". Annual Review of Plant Physiology and Plant Molecular Biology. 49: 173–198. doi:10.1146/annurev.arplant.49.1.173. PMID15012232. S 2 CID6241264. ^Redfield, Alfred C. (1934). \"On the Proportions of Organic Derivatives in Sea Water and their Relation to the Composition of Plankton\". In Johnstone, James; Daniel, Richard Jellicoe (eds.). James Johnstone Memorial Volume. Liverpool: University Press of Liverpool. pp. 176–92. OCLC13993674."}
{"url": "https://iwc.int/aboriginal", "text": "Aboriginal Subsistence Whaling In some parts of the world, whale products play an important role in the nutritional and cultural life of native peoples. Four IWC member countries conduct aboriginal subsistence hunts today: Denmark (Greenland), Russia (Chukotka), St Vincent and the Grenadines (Bequia) and the United States (Alaska and also potentially a resumption of hunts previously undertaken by the Makah Tribe of Washington State). From the outset, the IWC recognised that indigenous or aboriginal subsistence whaling is not the same as commercial whaling. Aboriginal whaling does not seek to maximise catches or profit. It is categorised differently by the IWC and is not subject to the moratorium. The IWC recognises that its regulations have the potential to impact significantly on traditional cultures, and great care must be taken in discharging this responsibility. In summary, the IWC objectives for management of aboriginal subsistence whaling are to ensure that hunted whale populations are maintained at (or brought back to) healthy levels, and to enable native people to hunt whales at levels that are appropriate to cultural and nutritional requirements in the long term. Click here to read more about IWC objectives for management of aboriginal subsistence whaling The three objectives for management of Aboriginal Subsistence Whaling, as adopted by the IWC in 1981: To ensure that the risks of extinction to individual stocks are not seriously increased by subsistence whaling; To enable aboriginal people to harvest whales in perpetuity at levels appropriate to their cultural and nutritional requirements, subject to the other objectives; To maintain the status of whale stocks at or above the level giving the highest net recruitment and to ensure that stocks below that level are moved towards it so far as the environment permits. ASW catch limits are set in multiple-year blocks. The current block was established in 2018 and catch limits will next be considered in 2024. To support this process the Commission receives information from two sources: the Scientific Committee provide advice on the sustainability of proposed hunts and safe catch limits. At its meeting in 2022, the Commission acknowledged the importance and priority that should be given to the scientific work programmes that support ASW. The Commission also acknowledged the value of traditional knowledge, acquired by indigenous communities over centuries. The Commission supported efforts by the Scientific Committee to incorporate this knowledge into its pioneering programme of assessment and review. At its meeting in 2022, the Commission also endorsed the next steps in a new initiative to conduct an IWC Aboriginal Subsistence Whaling Survey of Indigenous and Human Rights Instruments. This survey was part of a package of measures endorsed in 2018, aiming to inform and assist the Commission in its role as regulator of indigenous whaling (see below). The ASW Working Group and a new approach In 2018, the Commission endorsed proposals made by a working group established six-years earlier to consider some long-standing and complex questions, and improve the way that the Commission considered ASW. A number of new initiatives were endorsed in order to facilitate a more straight-forward process when catch limits are next considered at the 2024 meeting of the Commission. The new initiatives include: a new timetable for sharing information from the hunts, and receiving feedback, maximising discussion time and transparency; agreement that status quo catch limits would be renewed automatically, assuming a series of agreed steps continue to be completed; a commitment to establish closer ties with international and inter-governmental organisations focusing on indigenous rights. Click here to read more about the Aboriginal Subsistence Whaling Working Group The Ad Hoc Aboriginal Subsistence Whaling Working Group (ASWWG) In 2018 the Commission was able to endorse a range of proposals regarding the management of ASW in large part due to the efforts of the ASWWG. The group was formed in 2012, following difficult Commission discussions on ASW. The group's task was to identify and consider a complex range of long-standing issues including removing ASW catches from political discussion and questions related to local consumption v commercialism. The group was chaired by Dr Mike Tillman of the US and comprised of the four ASW member countries (Denmark, Russia, St Vincent and the Grenadines and the United States, and four other IWC member countries (Argentina, Austria, Japan, and Switzerland). Two members of the IWC Scientific Committee (from Australia and Norway) were also members of the group, supported by the IWC Secretariat. Acknowledging the scale of the challenge and extensive efforts of all members of the group, and paying particular tribute to the leadership of Dr Tillman, the Commission thanked the ASWWG for completing its task in 2018. You can read the full Terms of Reference for the ASW Working Group here. You can read the first report of the ASW Working Group to the ASW Sub-committee (2012) here. The Voluntary Fund for Aboriginal Subsistence Whaling (ASW) was established by the Commission at its meeting in 2012. The aim of the fund is to assist subsistence hunts in achieving compliance with IWC measures. These measures span a range of issues including hunter safety, reporting processes and weapons improvement programmes which reduce animal suffering and increase efficiency during these hunts."}
{"url": "https://iopscience.iop.org/article/10.1088/1748-3182/3/1/016001", "text": "Dates Abstract The finite element modeling (FEM) space reported here contains the head of a simulated whale based on CT data sets as well as physical measurements of sound-propagation characteristics of actual tissue samples. Simulated sound sources placed inside and outside of an adult male Cuvier's beaked whale (Ziphius cavirostris) reveal likely sound propagation pathways into and out of the head. Two separate virtual sound sources that were located at the left and right phonic lips produced beams that converged just outside the head. This result supports the notion that dual sound sources can interfere constructively to form a biologically useful and, in fact, excellent sonar beam in front of the animal. The most intriguing FEM results concern pathways by which sounds reach the ears. The simulations reveal a previously undescribed 'gular pathway' for sound reception in Ziphius. Propagated sound pressure waves enter the head from below and between the lower jaws, pass through an opening created by the absence of the medial bony wall of the posterior mandibles, and continue toward the bony ear complexes through the internal mandibular fat bodies. This new pathway has implications for understanding the evolution of underwater hearing in odontocetes. Our model also provides evidence for receive beam directionality, off-axis acoustic shadowing and a plausible mechanism for the long-standing orthodox sound reception pathway in odontocetes. The techniques developed for this study can be used to study acoustic perturbation in a wide variety of marine organisms."}
{"url": "https://en.m.wikipedia.org/wiki/Plastid", "text": "In land plants, the plastids that contain chlorophyll can perform photosynthesis, thereby creating internal chemical energy from external sunlight energy while capturing carbon from Earth's atmosphere and furnishing the atmosphere with life-giving oxygen. These are the chlorophyll-plastids—and they are named chloroplasts; (see top graphic). All plastids are derived from proplastids, which are present in the meristematic regions of the plant. Proplastids and young chloroplasts typically divide by binary fission, but more mature chloroplasts also have this capacity. Plant proplastids (undifferentiated plastids) may differentiate into several forms, depending upon which function they perform in the cell, (see top graphic). They may develop into any of the following variants:[10] Each plastid creates multiple copies of its own unique genome, or plastome, (from 'plastid genome')—which for a chlorophyll plastid (or chloroplast) is equivalent to a 'chloroplast genome', or a 'chloroplast DNA'.[11][12] The number of genome copies produced per plastid is variable, ranging from 1000 or more in rapidly dividing new cells, encompassing only a few plastids, down to 100 or less in mature cells, encompassing numerous plastids. A plastome typically contains a genome that encodes transferribonucleic acids (tRNA)s and ribosomalribonucleic acids (rRNAs). It also contains proteins involved in photosynthesis and plastid gene transcription and translation. But these proteins represent only a small fraction of the total protein set-up necessary to build and maintain any particular type of plastid. Nuclear genes (in the cell nucleus of a plant) encode the vast majority of plastid proteins; and the expression of nuclear and plastid genes is co-regulated to coordinate the development and differention of plastids. Many plastids, particularly those responsible for photosynthesis, possess numerous internal membrane layers. Plastid DNA exists as protein-DNA complexes associated as localized regions within the plastid's inner envelope membrane; and these complexes are called 'plastid nucleoids'. Unlike the nucleus of a eukaryotic cell, a plastid nucleoid is not surrounded by a nuclear membrane. The region of each nucleoid may contain more than 10 copies of the plastid DNA. Where the proplastid (undifferentiated plastid) contains a single nucleoid region located near the centre of the proplastid, the developing (or differentiating) plastid has many nucleoids localized at the periphery of the plastid and bound to the inner envelope membrane. During the development/ differentiation of proplastids to chloroplasts—and when plastids are differentiating from one type to another—nucleoids change in morphology, size, and location within the organelle. The remodelling of plastid nucleoids is believed to occur by modifications to the abundance of and the composition of nucleoid proteins. In normal plant cells long thin protuberances called stromules sometimes form—extending from the plastid body into the cell cytosol while interconnecting several plastids. Proteins and smaller molecules can move around and through the stromules. Comparatively, in the laboratory, most cultured cells—which are large compared to normal plant cells—produce very long and abundant stromules that extend to the cell periphery. In 2014, evidence was found of the possible loss of plastid genome in Rafflesia lagascae, a non-photosynthetic parasitic flowering plant, and in Polytomella, a genus of non-photosynthetic green algae. Extensive searches for plastid genes in both taxons yielded no results, but concluding that their plastomes are entirely missing is still disputed.[13] Some scientists argue that plastid genome loss is unlikely since even these non-photosynthetic plastids contain genes necessary to complete various biosynthetic pathways including heme biosynthesis.[13][14] Even with any loss of plastid genome in Rafflesiaceae, the plastids still occur there as \"shells\" without DNA content,[15] which is reminiscent of hydrogenosomes in various organisms. Rhodoplasts: the red plastids found in red algae, which allows them to photosynthesize down to marine depths of 268 m.[10] The chloroplasts of plants differ from rhodoplasts in their ability to synthesize starch, which is stored in the form of granules within the plastids. In red algae, floridean starch is synthesized and stored outside the plastids in the cytosol.[16] The plastid of photosynthetic Paulinella species is often referred to as the 'cyanelle' or chromatophore, and is used in photosynthesis.[17][18] It had a much more recent endosymbiotic event, in the range of 140–90 million years ago, which is the only other known primary endosymbiosis event of cyanobacteria.[19][20] In reproducing, most plants inherit their plastids from only one parent. In general, angiosperms inherit plastids from the female gamete, where many gymnosperms inherit plastids from the male pollen. Algae also inherit plastids from just one parent. Thus the plastid DNA of the other parent is completely lost. In normal intraspecific crossings—resulting in normal hybrids of one species—the inheriting of plastid DNA appears to be strictly uniparental; i.e., from the female. In interspecific hybridisations, however, the inheriting is apparently more erratic. Although plastids are inherited mainly from the female in interspecific hybridisations, there are many reports of hybrids of flowering plants producing plastids from the male. Approximately 20% of angiosperms, including alfalfa (Medicago sativa), normally show biparental inheriting of plastids.[21] DNA repair proteins are encoded by the cell's nuclear genome and then translocated to plastids where they maintain genome stability/ integrity by repairing the plastid's DNA.[23] For example, in chloroplasts of the moss Physcomitrella patens, a protein employed in DNA mismatch repair (Msh1) interacts with proteins employed in recombinational repair (RecA and RecG) to maintain plastid genome stability.[24] Plastids are thought to be descended from endosymbioticcyanobacteria. The primary endosymbiotic event of the Archaeplastida is hypothesized to have occurred around 1.5 billion years ago[25] and enabled eukaryotes to carry out oxygenic photosynthesis.[26] Three evolutionary lineages in the Archaeplastida have since emerged in which the plastids are named differently: chloroplasts in green algae and/or plants, rhodoplasts in red algae, and muroplasts in the glaucophytes. The plastids differ both in their pigmentation and in their ultrastructure. For example, chloroplasts in plants and green algae have lost all phycobilisomes, the light harvesting complexes found in cyanobacteria, red algae and glaucophytes, but instead contain stroma and grana thylakoids. The glaucocystophycean plastid—in contrast to chloroplasts and rhodoplasts—is still surrounded by the remains of the cyanobacterial cell wall. All these primary plastids are surrounded by two membranes. The plastid of photosynthetic Paulinella species is often referred to as the 'cyanelle' or chromatophore, and had a much more recent endosymbiotic event about 90–140 million years ago; it is the only known primary endosymbiosis event of cyanobacteria outside of the Archaeplastida.[17][18] The plastid belongs to the \"PS-clade\" (of the cyanobacteria genera Prochlorococcus and Synechococcus), which is a different sister clade to the plastids belonging to the Archaeplastida.[4][5] Some dinoflagellates and sea slugs, in particular of the genus Elysia, take up algae as food and keep the plastid of the digested alga to profit from the photosynthesis; after a while, the plastids are also digested. This process is known as kleptoplasty, from the Greek, kleptes (κλέπτης), thief. In 1977 J.M Whatley proposed a plastid development cycle which said that plastid development is not always unidirectional but is instead a complicated cyclic process. Proplastids are the precursor of the more differentiated forms of plastids, as shown in the diagram to the right.[28] ^Sometimes Ernst Haeckel is credited to coin the term plastid, but his \"plastid\" includes nucleated cells and anucleated \"cytodes\"[7] and thus totally different concept from the plastid in modern literature."}
{"url": "https://en.m.wikipedia.org/wiki/File:Cycling_of_marine_phytoplankton.png", "text": "Summary English: Cycling of marine phytoplankton. Phytoplankton live in the photic zone of the ocean, where photosynthesis is possible. During photosynthesis, they assimilate carbon dioxide and release oxygen. If solar radiation is too high, phytoplankton may fall victim to photodegradation. For growth, phytoplankton cells depend on nutrients, which enter the ocean by rivers, continental weathering, and glacial ice meltwater on the poles. Phytoplankton release dissolved organic carbon (DOC) into the ocean. Since phytoplankton are the basis of marine food webs, they serve as prey for zooplankton, fish larvae and other heterotrophic organisms. They can also be degraded by bacteria or by viral lysis. Although some phytoplankton cells, such as dinoflagellates, are able to migrate vertically, they are still incapable of actively moving against currents, so they slowly sink and ultimately fertilize the seafloor with dead cells and detritus. attribution – You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use."}
{"url": "https://en.m.wikipedia.org/wiki/Metazoans", "text": "Over 1.5 million living animal species have been described, of which around 1.05 million are insects, over 85,000 are molluscs, and around 65,000 are vertebrates. It has been estimated there are as many as 7.77 million animal species on Earth. Animal body lengths range from 8.5 μm (0.00033 in) to 33.6 m (110 ft). They have complex ecologies and interactions with each other and their environments, forming intricate food webs. The scientific study of animals is known as zoology, and the study of animal behaviors is known as ethology. Etymology The word animal comes from the Latin noun animal of the same meaning, which is itself derived from Latin animalis 'having breath or soul'.[5] The biological definition includes all members of the kingdom Animalia.[6] In colloquial usage, the term animal is often used to refer only to nonhuman animals.[7][8][9][10] The term metazoa is derived from Ancient Greek μετα (meta) 'after' (in biology, the prefix meta- stands for 'later') and ζῷᾰ (zōia) 'animals', plural of ζῷον zōion 'animal'.[11][12] Characteristics Animals are unique in having the ball of cells of the early embryo (1) develop into a hollow ball or blastula (2). Structure All animals are composed of cells, surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins.[22] During development, the animal extracellular matrix forms a relatively flexible framework upon which cells can move about and be reorganised, making the formation of complex structures possible. This may be calcified, forming structures such as shells, bones, and spicules.[23] In contrast, the cells of other multicellular organisms (primarily algae, plants, and fungi) are held in place by cell walls, and so develop by progressive growth.[24] Animal cells uniquely possess the cell junctions called tight junctions, gap junctions, and desmosomes.[25] With few exceptions—in particular, the sponges and placozoans—animal bodies are differentiated into tissues.[26] These include muscles, which enable locomotion, and nerve tissues, which transmit signals and coordinate the body. Typically, there is also an internal digestive chamber with either one opening (in Ctenophora, Cnidaria, and flatworms) or two openings (in most bilaterians).[27] Reproduction and development Nearly all animals make use of some form of sexual reproduction.[28] They produce haploidgametes by meiosis; the smaller, motile gametes are spermatozoa and the larger, non-motile gametes are ova.[29] These fuse to form zygotes,[30] which develop via mitosis into a hollow sphere, called a blastula. In sponges, blastula larvae swim to a new location, attach to the seabed, and develop into a new sponge.[31] In most other groups, the blastula undergoes more complicated rearrangement.[32] It first invaginates to form a gastrula with a digestive chamber and two separate germ layers, an external ectoderm and an internal endoderm.[33] In most cases, a third germ layer, the mesoderm, also develops between them.[34] These germ layers then differentiate to form tissues and organs.[35] Animals evolved in the sea. Lineages of arthropods colonised land around the same time as land plants, probably between 510 and 471 million years ago during the Late Cambrian or Early Ordovician.[54]Vertebrates such as the lobe-finned fishTiktaalik started to move on to land in the late Devonian, about 375 million years ago.[55][56] Animals occupy virtually all of earth's habitats and microhabitats, with faunas adapted to salt water, hydrothermal vents, fresh water, hot springs, swamps, forests, pastures, deserts, air, and the interiors of other organisms.[57] Animals are however not particularly heat tolerant; very few of them can survive at constant temperatures above 50 °C (122 °F)[58] or in the most extreme cold deserts of continental Antarctica.[59] Diversity Size The blue whale (Balaenoptera musculus) is the largest animal that has ever lived, weighing up to 190 tonnes and measuring up to 33.6 metres (110 ft) long.[60][61][62] The largest extant terrestrial animal is the African bush elephant (Loxodonta africana), weighing up to 12.25 tonnes[60] and measuring up to 10.67 metres (35.0 ft) long.[60] The largest terrestrial animals that ever lived were titanosaursauropod dinosaurs such as Argentinosaurus, which may have weighed as much as 73 tonnes, and Supersaurus which may have reached 39 meters.[63][64] Several animals are microscopic; some Myxozoa (obligate parasites within the Cnidaria) never grow larger than 20 μm,[65] and one of the smallest species (Myxobolus shekel) is no more than 8.5 μm when fully grown.[66] Numbers and habitats of major phyla The following table lists estimated numbers of described extant species for the major animal phyla,[67] along with their principal habitats (terrestrial, fresh water,[68] and marine),[69] and free-living or parasitic ways of life.[70] Species estimates shown here are based on numbers described scientifically; much larger estimates have been calculated based on various means of prediction, and these can vary wildly. For instance, around 25,000–27,000 species of nematodes have been described, while published estimates of the total number of nematode species include 10,000–20,000; 500,000; 10 million; and 100 million.[71] Using patterns within the taxonomic hierarchy, the total number of animal species—including those not yet described—was calculated to be about 7.77 million in 2011.[72][73][b] Evolutionary origin Evidence of animals is found as long ago as the Cryogenian period. 24-Isopropylcholestane (24-ipc) has been found in rocks from roughly 650 million years ago; it is only produced by sponges and pelagophyte algae. Its likely origin is from sponges based on molecular clock estimates for the origin of 24-ipc production in both groups. Analyses of pelagophyte algae consistently recover a Phanerozoic origin, while analyses of sponges recover a Neoproterozoic origin, consistent with the appearance of 24-ipc in the fossil record.[88][89] The first body fossils of animals appear in the Ediacaran, represented by forms such as Charnia and Spriggina. It had long been doubted whether these fossils truly represented animals,[90][91][92] but the discovery of the animal lipid cholesterol in fossils of Dickinsonia establishes their nature.[93] Animals are thought to have originated under low-oxygen conditions, suggesting that they were capable of living entirely by anaerobic respiration, but as they became specialized for aerobic metabolism they became fully dependent on oxygen in their environments.[94] Some palaeontologists have suggested that animals appeared much earlier than the Cambrian explosion, possibly as early as 1 billion years ago.[101] Early fossils that might represent animals appear for example in the 665-million-year-old rocks of the Trezona Formation of South Australia. These fossils are interpreted as most probably being early sponges.[102]Trace fossils such as tracks and burrows found in the Tonian period (from 1 gya) may indicate the presence of triploblastic worm-like animals, roughly as large (about 5 mm wide) and complex as earthworms.[103] However, similar tracks are produced by the giant single-celled protist Gromia sphaerica, so the Tonian trace fossils may not indicate early animal evolution.[104][105] Around the same time, the layered mats of microorganisms called stromatolites decreased in diversity, perhaps due to grazing by newly evolved animals.[106] Objects such as sediment-filled tubes that resemble trace fossils of the burrows of wormlike animals have been found in 1.2 gya rocks in North America, in 1.5 gya rocks in Australia and North America, and in 1.7 gya rocks in Australia. Their interpretation as having an animal origin is disputed, as they might be water-escape or other structures.[107][108] Ros-Rocher and colleagues (2021) trace the origins of animals to unicellular ancestors, providing the external phylogeny shown in the cladogram. Uncertainty of relationships is indicated with dashed lines.[115] An alternative phylogeny, from Kapli and colleagues (2021), proposes a clade Xenambulacraria for the Xenacoelamorpha + Ambulacraria; this is either within Deuterostomia, as sister to Chordata, or the Deuterostomia are recovered as paraphyletic, and Xenambulacraria is sister to the proposed clade Centroneuralia, consisting of Chordata + Protostomia.[126] Sponges are physically very distinct from other animals, and were long thought to have diverged first, representing the oldest animal phylum and forming a sister clade to all other animals.[127] Despite their morphological dissimilarity with all other animals, genetic evidence suggests sponges may be more closely related to other animals than the comb jellies are.[128][129] Sponges lack the complex organization found in most other animal phyla;[130] their cells are differentiated, but in most cases not organised into distinct tissues, unlike all other animals.[131] They typically feed by drawing in water through pores, filtering out food and nutrients.[132] The comb jellies and Cnidaria are radially symmetric and have digestive chambers with a single opening, which serves as both mouth and anus.[133] Animals in both phyla have distinct tissues, but these are not organised into discrete organs.[134] They are diploblastic, having only two main germ layers, ectoderm and endoderm.[135] The tiny placozoans have no permanent digestive chamber and no symmetry; they superficially resemble amoebae.[136][137] Their phylogeny is poorly defined, and under active research.[128][138] Bilateria Idealised bilaterian body plan.[d] With an elongated body and a direction of movement the animal has head and tail ends. Sense organs and mouth form the basis of the head. Opposed circular and longitudinal muscles enable peristaltic motion. The remaining animals, the great majority—comprising some 29 phyla and over a million species—form a clade, the Bilateria, which have a bilaterally symmetric body plan. The Bilateria are triploblastic, with three well-developed germ layers, and their tissues form distinct organs. The digestive chamber has two openings, a mouth and an anus, and there is an internal body cavity, a coelom or pseudocoelom. These animals have a head end (anterior) and a tail end (posterior), a back (dorsal) surface and a belly (ventral) surface, and a left and a right side.[139][140] Having a front end means that this part of the body encounters stimuli, such as food, favouring cephalisation, the development of a head with sense organs and a mouth. Many bilaterians have a combination of circular muscles that constrict the body, making it longer, and an opposing set of longitudinal muscles, that shorten the body;[140] these enable soft-bodied animals with a hydrostatic skeleton to move by peristalsis.[141] They also have a gut that extends through the basically cylindrical body from mouth to anus. Many bilaterian phyla have primary larvae which swim with cilia and have an apical organ containing sensory cells. However, over evolutionary time, descendant spaces have evolved which have lost one or more of each of these characteristics. For example, adult echinoderms are radially symmetric (unlike their larvae), while some parasitic worms have extremely simplified body structures.[139][140] Genetic studies have considerably changed zoologists' understanding of the relationships within the Bilateria. Most appear to belong to two major lineages, the protostomes and the deuterostomes.[142] It is often suggested that the basalmost bilaterians are the Xenacoelomorpha, with all other bilaterians belonging to the subclade Nephrozoa[143][144][145] However, this suggestion has been contested, with other studies finding that xenacoelomorphs are more closely related to Ambulacraria than to other bilaterians.[126] Protostomes and deuterostomes The bilaterian gut develops in two ways. In many protostomes, the blastopore develops into the mouth, while in deuterostomes it becomes the anus. Protostomes and deuterostomes differ in several ways. Early in development, deuterostome embryos undergo radial cleavage during cell division, while many protostomes (the Spiralia) undergo spiral cleavage.[146] Animals from both groups possess a complete digestive tract, but in protostomes the first opening of the embryonic gut develops into the mouth, and the anus forms secondarily. In deuterostomes, the anus forms first while the mouth develops secondarily.[147][148] Most protostomes have schizocoelous development, where cells simply fill in the interior of the gastrula to form the mesoderm. In deuterostomes, the mesoderm forms by enterocoelic pouching, through invagination of the endoderm.[149] Ecdysozoa The Ecdysozoa are protostomes, named after their shared trait of ecdysis, growth by moulting.[156] They include the largest animal phylum, the Arthropoda, which contains insects, spiders, crabs, and their kin. All of these have a body divided into repeating segments, typically with paired appendages. Two smaller phyla, the Onychophora and Tardigrada, are close relatives of the arthropods and share these traits. The ecdysozoans also include the Nematoda or roundworms, perhaps the second largest animal phylum. Roundworms are typically microscopic, and occur in nearly every environment where there is water;[157] some are important parasites.[158] Smaller phyla related to them are the Nematomorpha or horsehair worms, and the Kinorhyncha, Priapulida, and Loricifera. These groups have a reduced coelom, called a pseudocoelom.[159] In the classical era, Aristotle divided animals,[e] based on his own observations, into those with blood (roughly, the vertebrates) and those without. The animals were then arranged on a scale from man (with blood, 2 legs, rational soul) down through the live-bearing tetrapods (with blood, 4 legs, sensitive soul) and other groups such as crustaceans (no blood, many legs, sensitive soul) down to spontaneously generating creatures like sponges (no blood, no legs, vegetable soul). Aristotle was uncertain whether sponges were animals, which in his system ought to have sensation, appetite, and locomotion, or plants, which did not: he knew that sponges could sense touch, and would contract if about to be pulled off their rocks, but that they were rooted like plants and never moved about.[168] In 1758, Carl Linnaeus created the first hierarchical classification in his Systema Naturae.[169] In his original scheme, the animals were one of three kingdoms, divided into the classes of Vermes, Insecta, Pisces, Amphibia, Aves, and Mammalia. Since then the last four have all been subsumed into a single phylum, the Chordata, while his Insecta (which included the crustaceans and arachnids) and Vermes have been renamed or broken up. The process was begun in 1793 by Jean-Baptiste de Lamarck, who called the Vermes une espèce de chaos (a chaotic mess)[f] and split the group into three new phyla: worms, echinoderms, and polyps (which contained corals and jellyfish). By 1809, in his Philosophie Zoologique, Lamarck had created 9 phyla apart from vertebrates (where he still had 4 phyla: mammals, birds, reptiles, and fish) and molluscs, namely cirripedes, annelids, crustaceans, arachnids, insects, worms, radiates, polyps, and infusorians.[167] In 1874, Ernst Haeckel divided the animal kingdom into two subkingdoms: Metazoa (multicellular animals, with five phyla: coelenterates, echinoderms, articulates, molluscs, and vertebrates) and Protozoa (single-celled animals), including a sixth animal phylum, sponges.[173][172] The protozoa were later moved to the former kingdom Protista, leaving only the Metazoa as a synonym of Animalia.[174] See also Notes ^The application of DNA barcoding to taxonomy further complicates this; a 2016 barcoding analysis estimated a total count of nearly 100,000 insect species for Canada alone, and extrapolated that the global insect fauna must be in excess of 10 million species, of which nearly 2 million are in a single fly family known as gall midges (Cecidomyiidae).[74] ^Munro, John H. (2003). \"Medieval Woollens: Textiles, Technology, and Organisation\". In Jenkins, David (ed.). The Cambridge History of Western Textiles. Cambridge University Press. pp. 214–215. ISBN978-0-521-34107-3."}
{"url": "https://en.m.wikipedia.org/wiki/File:Salmonlarvakils_2.jpg", "text": "Summary English: Salmon egg hatching (Salmo salar) - the Alevin (larva) has grown around the remains of the yolk sac - visible are the arteries spinning around the yolk and little oildrops, also the gut, the spine, the main caudal blood vessel, the bladder and the arcs of the gills. In about 24hrs it will be a fry without yolk sac. attribution – You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. share alike – If you remix, transform, or build upon the material, you must distribute your contributions under the same or compatible license as the original."}
{"url": "https://en.m.wikipedia.org/wiki/HNLC", "text": "High-nutrient, low-chlorophyll regions High-nutrient, low-chlorophyll (HNLC) regions are regions of the ocean where the abundance of phytoplankton is low and fairly constant despite the availability of macronutrients. Phytoplankton rely on a suite of nutrients for cellular function. Macronutrients (e.g., nitrate, phosphate, silicic acid) are generally available in higher quantities in surface ocean waters, and are the typical components of common garden fertilizers. Micronutrients (e.g., iron, zinc, cobalt) are generally available in lower quantities and include trace metals. Macronutrients are typically available in millimolar concentrations, while micronutrients are generally available in micro- to nanomolar concentrations. In general, nitrogen tends to be a limiting ocean nutrient, but in HNLC regions it is never significantly depleted.[1][2] Instead, these regions tend to be limited by low concentrations of metabolizable iron.[1] Iron is a critical phytoplankton micronutrient necessary for enzyme catalysis and electron transport.[3][4] Between the 1930s and '80s, it was hypothesized that iron is a limiting ocean micronutrient, but there were not sufficient methods reliably to detect iron in seawater to confirm this hypothesis.[5] In 1989, high concentrations of iron-rich sediments in nearshore coastal waters off the Gulf of Alaska were detected.[6] However, offshore waters had lower iron concentrations and lower productivity despite macronutrient availability for phytoplankton growth.[6] This pattern was observed in other oceanic regions and led to the naming of three major HNLC zones: the North Pacific Ocean, the Equatorial Pacific Ocean, and the Southern Ocean.[1][2] The discovery of HNLC regions has fostered scientific debate about the ethics and efficacy of iron fertilization experiments which attempt to draw down atmospheric carbon dioxide by stimulating surface-level photosynthesis. It has also led to the development of hypotheses such as grazing control which poses that HNLC regions are formed, in part, from the grazing of phytoplankton (e.g. dinoflagellates, ciliates) by smaller organisms (e.g. protists). Global distribution of surface chlorophyll levels. Chlorophyll (a proxy for phytoplankton mass) is relatively low in the three HNLC regions (North Pacific, Equatorial Pacific, and Southern Ocean).Global nitrogen to phosphorus ratio is plotted for the global surface ocean. Nutrients are available in the three HNLC regions in sufficient RKR Ratios for biological activity. Primary production is the process by which autotrophs use light to convert carbon from aqueous carbon dioxide to sugar for cellular growth.[7] Light provides the energy for the photosynthetic process and nutrients are incorporated into organic material. For photosynthesis to occur, macronutrients such as nitrate and phosphate must be available in sufficient ratios and bioavailable forms for biological utilization. The molecular ratio of 106(Carbon):16(Nitrogen):1(Phosphorus) was deduced by Redfield, Ketcham, and Richards (RKR) and is known as the Redfield Ratio.[8] Photosynthesis (forward) and respiration (reverse) is represented by the equation: Photosynthesis can be limited by deficiencies of certain macronutrients. However, in the North Pacific, the Equatorial Pacific, and the Southern Ocean macronutrients are found in sufficient ratios, quantities and bioavailable forms to support greater levels of primary production than found. Macronutrient availability in HNLC regions in tandem with low standing stocks of phytoplankton suggests that some other biogeochemical process limits phytoplankton growth.[7] Since primary production and phytoplankton biomass cannot currently be measured over entire ocean basins, scientists use chlorophyll α as a proxy for primary production. Modern satellite observations monitor and track global chlorophyll α abundances in the ocean via remote sensing. Higher chlorophyll concentrations generally indicate areas of enhanced primary production, and conversely lower chlorophyll levels indicate low primary production. This co-occurrence of low chlorophyll and high macronutrient availability is why these regions are deemed \"high-nutrient, low-chlorophyll.\" In addition to the macronutrients needed for organic matter synthesis, phytoplankton need micronutrients such as trace metals for cellular functions.[7] Micronutrient availability can constrain primary production because trace metals are sometimes limiting nutrients. Iron has been determined to be a primary limiting micronutrient in HNLC provinces.[5] Recent studies have indicated that zinc and cobalt may be secondary and/or co-limiting micronutrients.[10][11] HNLC regions cover 20% of the world’s oceans and are characterized by varying physical, chemical, and biological patterns. These surface waters have annually varying, yet relatively abundant macronutrient concentrations compared to other oceanic provinces.[5] While HNLC broadly describes the biogeochemical trends of these large ocean regions, all three zones experience seasonal phytoplankton blooms in response to global atmospheric patterns. On average, HNLC regions tend to be growth-limited by iron and variably, zinc.[11][12] This trace metal limitation leads to communities of smaller sized phytoplankton. Compared to more productive regions of the ocean, HNLC zones have higher ratios of silicic acid to nitrate because larger diatoms, that require silicic acid to make their opal silica shells, are less prevalent.[10][11][12] Unlike the Southern Ocean and the North Pacific, the Equatorial Pacific experiences temporal silicate availability which leads to large seasonal diatom blooms.[13][14] The distribution of trace metals and relative abundance of macronutrients are reflected in the plankton community structure. For example, the selection of phytoplankton with a high surface area to volume ratio results in HNLC regions being dominated by nano- and picoplankton. This ratio allows for optimal utilization of available dissolved nutrients. Larger phytoplankton, such as diatoms, cannot energetically sustain themselves in these regions. Common picoplankton within these regions include genera such as prochlorococcus (not generally found in the North Pacific), synechococcus, and various eukaryotes. Grazing protists likely control the abundance and distribution of these small phytoplankton.[15][16] The generally lower net primary production in HNLC zones results in lower biological draw-down of atmospheric carbon dioxide and thus these regions are generally considered a net source of carbon dioxide to the atmosphere.[14] HNLC areas are of interest to geoengineers and some in the scientific community who believe fertilizing large patches of these waters with iron could potentially lower dissolved carbon dioxide and offset increased anthropogenic carbon emissions.[6] Analysis of Antarctic ice core data over the last million years shows correlation between high levels of dust and low temperature, indicating that addition of diffuse iron-rich dust to the sea has been a natural amplifier of climate cooling.[17] Dust blown off the Alaskan coast into the North Pacific.Currents in the North Pacific Ocean. The discovery and naming of the first HNLC region, the North Pacific, was formalized in a seminal paper published in 1988.[6] The study concluded that surface waters of the eastern North Pacific are generally dominated by picoplankton despite the relative abundance of macronutrients.[6] In other words, larger phytoplankton, such as diatoms which thrive in nutrient-rich waters, were not found. Instead, the surface waters were replete with smaller pico- and nanoplankton.[6] Based on laboratory nutrient experiments, iron was hypothesized to be a key limiting micronutrient.[6] The Pacific Ocean is the largest and oldest body of water on Earth. The North Pacific is characterized by the general clockwise rotation of the North Pacific gyre, which is driven by trade winds. Spatial variations in tradewinds result in cooler air temperatures in the western North Pacific and milder air temperatures in the eastern North Pacific (i.e., Subarctic Pacific).[18] Iron is supplied to the North Pacific by dust storms that occur in Asia and Alaska as well as iron-rich waters advected from the continental margin, sometimes by eddies such as Haida Eddies.[19][20] Concentrations of iron however vary throughout the year. Ocean currents are driven by seasonal atmospheric patterns which transport iron from the Kuril-Kamchatka margin into the western Subarctic Pacific. This introduction of iron provides a subsurface supply of micronutrients, which can be used by primary producers during upwelling of deeper waters to the surface.[21] Seafloor depth may also stimulate phytoplankton blooms in HNLC regions as iron diffuses from the seafloor and alleviates iron limitation in shallow waters.[22] Research conducted in the Gulf of Alaska showed that areas with shallow waters, such as the south shelf of Alaska, have more intense phytoplankton blooms than offshore waters.[22] Volcanic ash from the eruption of the Kasatochi volcano in August 2008 provided an example of natural iron fertilization in the Northeast Pacific Ocean.[23] The region was fertilized by raining volcanic dust containing soluble iron. In the days following, phytoplankton blooms were visible from space.[23] Limitations in trace metal concentrations in the North Pacific limit diatom blooms throughout the entire year.[24] Even though the North Pacific is an HNLC region, it produces and exports to the ocean interior a relatively high amount of particulate biogenic silica compared to the North Atlantic, which supports significant diatom growth.[24] The Equatorial Pacific is an oceanic province characterized by nearly year-round upwelling due to the convergence of trade winds from the northeast and southeast at the Intertropical Convergence Zone. The Equatorial Pacific spans nearly half of Earth’s circumference and plays a major role in global marine new primary production.[25] New production is a term used in biological oceanography to describe the way in which nitrogen is recycled within the ocean.[18] In regions of enhanced new production, nitrate from the aphotic zone makes its way into surface waters, replenishing surface nitrate supply. Despite nitrogen availability in Equatorial Pacific waters, primary production and observed surface ocean biomass are considerably lower compared to other major upwelling regions of the ocean.[26] Thus the Equatorial Pacific is considered one of the three major HNLC regions. Like other major HNLC provinces, the Equatorial Pacific is considered nutrient-limited due to lack of trace metals such as iron. The Equatorial Pacific receives approximately 7-10 times more iron from Equatorial Undercurrent (EUC) upwelling than from inputs due to settling atmospheric dust.[27] Climate reconstructions of glacial periods using sediment proxy records have revealed that the Equatorial Pacific may have been 2.5 times more productive than the modern equatorial ocean.[27] During these glacial periods, the Equatorial Pacific increased its export of marine new production,[27] thus providing a sink of atmospheric carbon dioxide. The science of paleoceanography attempts to understand the interplay of glacial cycles with ocean dynamics. Paleo-oceanographers currently challenge the Aeolian Dust hypothesis which suggests that the atmospheric transport of iron-rich dust off Central and South America controls the intensity of primary production in the Equatorial Pacific.[27] One study suggests that because EUC upwelling provides most of the bioavailable iron to the equatorial surface waters, the only method to reverse HNLC conditions is by enhancing upwelling.[27][28] In other words, enhanced regional upwelling, rather than iron-rich atmospheric dust deposition, may explain why this region experiences higher primary productivity during glacial periods. Compared to the North Pacific and Southern Ocean, Equatorial Pacific waters have relatively low levels of biogenic silica and thus do not support significant standing stocks of diatoms.[14] Picoplankton are the most abundant marine primary producers in these regions due mainly to their ability to assimilate low concentrations of trace metals.[14] Various phytoplankton communities within the Equatorial Pacific are grazed at the same rate as their growth rate, which further limits primary production.[28] There is no current consensus regarding which of the two main hypotheses (grazing or micronutrients) controls production in these equatorial waters. It is likely that trace metal limitations select for smaller-celled organisms, which thereby increases the grazing pressure of protists.[28] While the Equatorial Pacific maintains HNLC characteristics, productivity can be high at times. Productivity leads to an abundance of seabirds such as storm petrels near the convergence of subtropical water and the equatorial \"cold tongue.\" The Equatorial Pacific contains the world's largest yellowfin tuna fisheries[18] and is home to the spotted dolphin. A schematic of Antarctic currents. Movement of surface water away from the continent pulls water up from depth. The Southern Ocean is the largest HNLC region in the global ocean. The surface waters of the Southern Ocean have been widely identified as being rich in macronutrients despite low phytoplankton stocks.[29][30][31] Iron deposited in the North Atlantic is incorporated into North Atlantic Deep Water and is transported to the Southern Ocean via thermohaline circulation.[32] Eventually mixing with the Antarctic Circumpolar Water, upwelling provides iron and macronutrients to the Southern Ocean surface waters. Therefore, iron inputs and primary production in the Southern Ocean are sensitive to iron-rich Saharan dust deposited over the Atlantic. Because of low atmospheric dust inputs directly onto Southern Ocean surface waters,[33][34] chlorophyll α concentrations are low. Light availability in the Southern Ocean changes dramatically seasonally, but it does not seem to be a significant constraint on phytoplankton growth.[3] Macronutrients present in Southern Ocean surface waters come from upwelled deep water. While micronutrients such as zinc and cobalt may possibly co-limit phytoplankton growth in the Southern Ocean, iron appears to be a critical limiting micronutrient.[4] Some regions of the Southern Ocean experience both adequate bioavailable iron and macronutrient concentrations yet phytoplankton growth is limited. Hydrographic studies[35][36] and explorations of the Southern Drake Passage region[37] have observed this phenomenon around the Crozet Islands, Kerguelen Islands, and South Georgia and the South Sandwich Islands.[37][38] These areas are adjacent to shelf regions of Antarctica and islands of the Southern Ocean. The micronutrients required for algal growth are believed to be supplied from the shelves themselves.[37] Except in areas close to the Antarctic shelf, micronutrient deficiency severely limits productivity in the Southern Ocean. Iron availability is not the only regulator of phytoplankton productivity and biomass.[39][40] In the Southern Ocean, prevailing low temperatures are believed to have a negative impact on phytoplankton growth rates.[40] Phytoplankton growth rate is very intense and short lived in open areas surrounded by sea ice and permanent sea-ice zones. Grazing by herbivores such as krill, copepods and salps is believed to suppress phytoplankton standing stock. Unlike the open waters of the Southern Ocean, grazing along continental shelf margins is low, so most phytoplankton that are not consumed sink to the sea floor which provides nutrients to benthic organisms.[39] Given the remote location of HNLC areas, scientists have combined modeling and observational data in order to study limits on primary production. Combining these two data sources allows for comparison between the North Pacific, Equatorial Pacific, and Southern Ocean. Two current explanations for global HNLC regions are growth limitations due to iron availability and phytoplankton grazing controls. In 1988, John Martin confirmed the hypothesis that iron limits phytoplankton blooms and growth rates in the North Pacific. His work was extrapolated to other HNLC regions through evidence which linked low surface iron concentration with low chlorophyll.[6] In response to iron fertilization experiments (IronEx, SOIREE, SEEDS, etc.) in HNLC areas, large phytoplankton responses such as decreased surface nutrient concentration and increased biological activity were observed.[41][42][43][44][45] Iron fertilization studies conducted at repeated intervals over the span of a week have produced a larger biological response than a single fertilization event.[42][44][46] The biological response size tends to depend on a site’s biological, chemical, and physical characteristics. In the Equatorial and North Pacific, silica is thought to constrain additional production after iron fertilization, while light limits additional production in the Southern Ocean.[42] Native, smaller phytoplankton were initial responders to increased iron, but were quickly outcompeted by larger, coastal phytoplankton such as diatoms.[44][47][48][49] The large bloom response and community shift has led to environmental concerns about fertilizing large sections of HNLC regions. One study suggests that diatoms grow preferentially during fertilization experiments. Some diatoms, such as pseudo-nitzschia, release the neurotoxindomoic acid, poisoning grazing fish.[48] If diatoms grow preferentially during iron fertilization experiments, sustained fertilizations could enhance domoic acid poisoning in the marine food web near fertilized patches.[48] Iron enters remote HNLC regions through two primary methods: upwelling of nutrient-rich water and atmospheric dust deposition. Iron needs to be replenished frequently and in bioavailable forms because of its insolubility, rapid uptake through biological systems, and binding affinity with ligands.[50][51] Dust deposition might not result in phytoplankton blooms unless settling dust is in the correct bioavailable form of iron. Additionally, iron must be deposited during productive seasons and coincide with the appropriate RKR-ratios of surface nutrients.[19][52] Aeolian dust has a larger influence on Northern Hemisphere HNLC regions because more land mass contributes to more dust deposition.[53] Due to the Southern Ocean's isolation from land, upwelling related to eddy diffusivity provides iron to HNLC regions.[54] Formulated by John Walsh in 1976, the grazing hypothesis states that grazing by heterotrophs suppresses primary productivity in areas of high nutrient concentrations.[41][55] Predation by microzooplankton primarily accounts for phytoplankton loss in HNLC regions. Grazing by larger zooplankton and advective mixing are also responsible for a small proportion of losses to phytoplankton communities.[6][56][57] Constant grazing limits phytoplankton to a low, constant standing stock. Without this grazing pressure, some scientists believe small phytoplankton would produce blooms despite micronutrient depletion because smaller phytoplankton typically have lower iron requirements and can absorb nutrients at a slower rate.[50][56] Current scientific consensus agrees that HNLC areas lack high productivity because of a combination of iron and physiological limitations, grazing pressure, and physical forcings.[2][6][43][49][56][58] The extent to which each factor contributes to low production may differ in each HNLC region. Iron limitation allows for smaller, more iron-frugal phytoplankton to grow at rapid rates, while grazing by microzooplankton maintains stable stocks of these smaller phytoplankton.[6][51][56] Once micronutrients become available, grazing may then limit bloom sizes.[41][43][44][46][49] Additional micronutrient limitations from trace metals like zinc or cobalt may suppress phytoplankton blooms.[12] Turbulent mixing at higher-latitude HNLC regions (North Pacific and Southern Ocean) may mix phytoplankton below the critical depth needed to have community growth.[41] The marine carbon cycle. Carbon dioxide is taken up by phytoplankton for photosynthesis and incorporated into the marine food web. When plankton or predator dies, sedimentation of organic matter reaches the seafloor, where carbon can be buried and sequestered. Stimulating phytoplankton blooms is thought to increase sedimentation of particulate organic carbon after the blooms die off, leading to increased carbon sequestration. Since past iron fertilization experiments have resulted in large phytoplankton blooms, some have suggested that large-scale ocean fertilization experiments should be conducted to draw down inorganic anthropogenic carbon dioxide in the form of particulate organic carbon. Fertilization would stimulate biological productivity, leading to a decrease in the amount of inorganic surface carbon dioxide within a fertilized patch. The bloom would then die off and presumably sink to the deep ocean, taking much of the absorbed carbon dioxide to the seafloor and sequestering it from the short-term carbon cycle in the deep ocean or ocean sediments.[46][59][60][61][62] To effectively remove anthropogenic carbon from the atmosphere, iron fertilization would need to result in significant removal of particulate carbon from the surface ocean and transport it to the deep ocean.[42][43][60][61] Various studies estimated that less than 7-10% of carbon taken up during a bloom would be sequestered,[63] and only a 15-25 ppm decrease in atmospheric carbon dioxide would result with sustained global iron fertilization.[7][60] The amount of carbon dioxide removed may be offset by the fuel cost of acquiring, transporting, and releasing significant amounts of iron into remote HNLC regions.[61] Many environmental concerns exist for large-scale iron fertilization. While blooms can be studied and traced, scientists still do not know if the extra production gets incorporated into the food chain or falls to the deep ocean after a bloom dies off.[42][43] Even if carbon is exported to depth, raining organic matter can be respired, potentially creating mid-column anoxic zones or causing acidification of deep ocean water.[61][64] Pronounced community shifts to diatoms have been observed during fertilization, and it's still unclear if the change in species composition has any long-term environmental effects.[48][61] The following is completely theoretical. Testing would be required to determine feasibility, optimum iron concentration per unit area, carbon sequestration by area over time, need for other micro-nutrients, amount of energy required to maintain such a system, and the potential amount of energy produced by the system. This system considers economic feasibility (profitability of bio-fuel products and carbon credits) and risk management. Grazing results in algae being consumed by micro-zooplankton. This predation results in less than 7-10% of carbon being taken to the bottom of the ocean. Growing algae in floating farms could allow these HNLC areas to grow algae for harvest without the problem of predation. Algae grown in floating farms would be recycled through grazing if there was a catastrophic failure of a floating farm, which would limit any environmental damage. Algae grown in floating farms could be harvested and used for food or fuel. All biological life is made up of lipids, carbohydrates, amino acids, and nucleic acids. Whole algae could be turned into animal feed, fertilizer, or bio-char. Separating the lipids from the algae could also create bio-diesel from the lipid content and bio-char from the rest. Of course, the algae could be pumped to the bottom of the ocean, below any grazing pressure for sequestration. In a controlled floating farm, the harvest could be sampled to record the amount of algae per unit volume which will indicate the amount of carbon being sequestered. If this carbon is sequestered at the bottom of the ocean, this figure could be used to accurately create carbon credits. Sequestering carbon dioxide on the ocean floor could destroy the unstudied ecosystem and cause undiscovered lifeforms to go extinct. Carbon sequestration on land does so with desiccated algae. Without sufficient sources of water, bacteria and other life will have a difficult time digesting the sequestered algae. Biofuels, not sold and used as renewable fuel, could be sequestered in abandoned oil wells and coal mines. The volume of bio-diesel and mass of bio-char would provide an accurate figure for producing (when sequestering) and selling (when removing from wells or mines) carbon credits."}
{"url": "https://en.m.wikipedia.org/wiki/Africa", "text": "Africa Africa is the world's second-largest and second-most populous continent after Asia. At about 30.3 million km2 (11.7 million square miles) including adjacent islands, it covers 20% of Earth's land area and 6% of its total surface area.[7] With 1.4 billion people[1][2] as of 2021, it accounts for about 18% of the world's human population. Africa's population is the youngest amongst all the continents;[8][9] the median age in 2012 was 19.7, when the worldwide median age was 30.4.[10] Despite a wide range of natural resources, Africa is the least wealthy continent per capita and second-least wealthy by total wealth, ahead of Oceania. Scholars have attributed this to different factors including geography, climate, tribalism,[11]colonialism, the Cold War,[12][13]neocolonialism, lack of democracy, and corruption.[11] Despite this low concentration of wealth, recent economic expansion and the large and young population make Africa an important economic market in the broader global context. Etymology Afri was a Latin name used to refer to the inhabitants of then-known northern Africa to the west of the Nile river, and in its widest sense referred to all lands south of the Mediterranean (Ancient Libya).[29][30] This name seems to have originally referred to a native Libyan tribe, an ancestor of modern Berbers; see Terence for discussion. The name had usually been connected with the Phoenician word ʿafar meaning \"dust\",[31] but a 1981 hypothesis[32] has asserted that it stems from the Berber word ifri (plural ifran) meaning \"cave\", in reference to cave dwellers.[33] The same word[33] may be found in the name of the Banu Ifran from Algeria and Tripolitania, a Berber tribe originally from Yafran (also known as Ifrane) in northwestern Libya,[34] as well as the city of Ifrane in Morocco. According to the Romans, Africa lies to the west of Egypt, while \"Asia\" was used to refer to Anatolia and lands to the east. A definite line was drawn between the two continents by the geographer Ptolemy (85–165 CE), indicating Alexandria along the Prime Meridian and making the isthmus of Suez and the Red Sea the boundary between Asia and Africa. As Europeans came to understand the real extent of the continent, the idea of \"Africa\" expanded with their knowledge. Other etymological hypotheses have been postulated for the ancient name \"Africa\": The 1st-century Jewish historian Flavius Josephus (Ant. 1.15) asserted that it was named for Epher, grandson of Abraham according to Gen. 25:4, whose descendants, he claimed, had invaded Libya. Massey, in 1881, stated that Africa is derived from the Egyptian af-rui-ka, meaning \"to turn toward the opening of the Ka.\" The Ka is the energetic double of every person and the \"opening of the Ka\" refers to a womb or birthplace. Africa would be, for the Egyptians, \"the birthplace.\"[36] Michèle Fruyt in 1976 proposed[37] linking the Latin word with africus \"south wind\", which would be of Umbrian origin and mean originally \"rainy wind\". Robert R. Stieglitz of Rutgers University in 1984 proposed: \"The name Africa, derived from the Latin *Aphir-ic-a, is cognate to Hebrew Ophir ['rich'].\"[38] Ibn Khallikan and some other historians claim that the name of Africa came from a Himyarite king called Afrikin ibn Kais ibn Saifi also called \"Afrikus son of Abraham\" who subdued Ifriqiya.[39][40][41] Arabic afrīqā (feminine noun) and ifrīqiyā, now usually pronounced afrīqiyā (feminine) 'Africa', from ‘afara [‘ = ‘ain, not ’alif] 'to be dusty' from ‘afar 'dust, powder' and ‘afir 'dried, dried up by the sun, withered' and ‘affara 'to dry in the sun on hot sand' or 'to sprinkle with dust'.[42] Other migrations of modern humans within the African continent have been dated to that time, with evidence of early human settlement found in Southern Africa, Southeast Africa, North Africa, and the Sahara.[54] Emergence of civilization The size of the Sahara has historically been extremely variable, with its area rapidly fluctuating and at times disappearing depending on global climatic conditions.[55] At the end of the Ice ages, estimated to have been around 10,500 BCE, the Sahara had again become a green fertile valley, and its African populations returned from the interior and coastal highlands in sub-Saharan Africa, with rock art paintings depicting a fertile Sahara and large populations discovered in Tassili n'Ajjer dating back perhaps 10 millennia.[56] However, the warming and drying climate meant that by 5000 BC, the Sahara region was becoming increasingly dry and hostile. Around 3500 BC, due to a tilt in the Earth's orbit, the Sahara experienced a period of rapid desertification.[57] The population trekked out of the Sahara region towards the Nile Valley below the Second Cataract where they made permanent or semi-permanent settlements. A major climatic recession occurred, lessening the heavy and persistent rains in Central and Eastern Africa. Since this time, dry conditions have prevailed in Eastern Africa and, increasingly during the last 200 years, in Ethiopia. The domestication of cattle in Africa preceded agriculture and seems to have existed alongside hunter-gatherer cultures. It is speculated that by 6000 BC, cattle were domesticated in North Africa.[58] In the Sahara-Nile complex, people domesticated many animals, including the donkey and a small screw-horned goat which was common from Algeria to Nubia. Between 10,000 and 9,000 BC, pottery was independently invented in the region of Mali in the savannah of West Africa.[59][60] In the steppes and savannahs of the Sahara and Sahel in Northern West Africa, people possibly ancestral to modern Nilo-Saharan and Mandé cultures started to collect wild millet,[61] around 8000 to 6000 BCE. Later, gourds, watermelons, castor beans, and cotton were also collected.[62] Sorghum was first domesticated in Eastern Sudan around 4000 BC, in one of the earliest instances of agriculture in human history. Its cultivation would gradually spread across Africa, before spreading to India around 2000 BC.[63][64] Around 4000 BC, the Saharan climate started to become drier at an exceedingly fast pace.[67] This climate change caused lakes and rivers to shrink significantly and caused increasing desertification. This, in turn, decreased the amount of land conducive to settlements and encouraged migrations of farming communities to the more tropical climate of West Africa.[67] During the first millennium BC, a reduction in wild grain populations related to changing climate conditions facilitated the expansion of farming communities and the rapid adoption of rice cultivation around the Niger River.[68][69] By the first millennium BC, ironworking had been introduced in Northern Africa. Around that time it also became established in parts of sub-Saharan Africa, either through independent invention there or diffusion from the north[70][71] and vanished under unknown circumstances around 500 AD, having lasted approximately 2,000 years,[72] and by 500 BC, metalworking began to become commonplace in West Africa. Ironworking was fully established by roughly 500 BC in many areas of East and West Africa, although other regions did not begin ironworking until the early centuries CE. Copper objects from Egypt, North Africa, Nubia, and Ethiopia dating from around 500 BC have been excavated in West Africa, suggesting that Trans-Saharan trade networks had been established by this date.[67] Early civilizations At about 3300 BC, the historical record opens in Northern Africa with the rise of literacy in the Pharaonic civilization of ancient Egypt.[73] One of the world's earliest and longest-lasting civilizations, the Egyptian state continued, with varying levels of influence over other areas, until 343 BC.[74][75] Egyptian influence reached deep into modern-day Libya and Nubia, and, according to Martin Bernal, as far north as Crete.[76] Christianity spread across these areas at an early date, from Judaea via Egypt and beyond the borders of the Roman world into Nubia;[84] by 340 AD at the latest, it had become the state religion of the Aksumite Empire. Syro-Greek missionaries, who arrived by way of the Red Sea, were responsible for this theological development.[85] In the early 7th century, the newly formed Arabian Islamic Caliphate expanded into Egypt, and then into North Africa. In a short while, the local Berber elite had been integrated into Muslim Arab tribes. When the Umayyad capital Damascus fell in the 8th century, the Islamic centre of the Mediterranean shifted from Syria to Qayrawan in North Africa. Islamic North Africa had become diverse, and a hub for mystics, scholars, jurists, and philosophers. During the above-mentioned period, Islam spread to sub-Saharan Africa, mainly through trade routes and migration.[86] In West Africa, Dhar Tichitt and Oualata in present-day Mauritania figure prominently among the early urban centers, dated to 2,000 BC. About 500 stone settlements litter the region in the former savannah of the Sahara. Its inhabitants fished and grew millet. It has been found by Augustin Holl that the Soninke of the Mandé peoples were likely responsible for constructing such settlements. Around 300 BCE, the region became more desiccated and the settlements began to decline, most likely relocating to Koumbi Saleh.[87] Architectural evidence and the comparison of pottery styles suggest that Dhar Tichitt was related to the subsequent Ghana Empire. Djenné-Djenno (in present-day Mali) was settled around 300 BC, and the town grew to house a sizable Iron Age population, as evidenced by crowded cemeteries. Living structures were made of sun-dried mud. By 250 BCE, Djenné-Djenno had become a large, thriving market town.[88][89] Further south, in central Nigeria, around 1,500 BC, the Nok culture developed on the Jos Plateau. It was a highly centralized community. The Nok people produced lifelike representations in terracotta, including human heads and human figures, elephants, and other animals. By 500 BC, and possibly earlier, they were smelting iron. By 200 AD, the Nok culture had vanished.[71] and vanished under unknown circumstances around 500 AD, having lasted approximately 2,000 years. Based on stylistic similarities with the Nok terracottas, the bronze figurines of the Yoruba kingdom of Ife and those of the Bini kingdom of Benin are suggested to be continuations of the traditions of the earlier Nok culture.[90][72] Ninth to eighteenth centuries Pre-colonial Africa possessed perhaps as many as 10,000 different states and polities[91] characterized by many different sorts of political organization and rule. These included small family groups of hunter-gatherers such as the San people of southern Africa; larger, more structured groups such as the family clan groupings of the Bantu-speakingpeoples of central, southern, and eastern Africa; heavily structured clan groups in the Horn of Africa; the large Sahelian kingdoms; and autonomous city-states and kingdoms such as those of the Akan; Edo, Yoruba, and Igbo people in West Africa; and the Swahili coastal trading towns of Southeast Africa. The intricate 9th-century bronzes from Igbo-Ukwu, in Nigeria displayed a level of technical accomplishment that was notably more advanced than European bronze casting of the same period.[92] By the ninth century AD, a string of dynastic states, including the earliest Hausa states, stretched across the sub-Saharan savannah from the western regions to central Sudan. The most powerful of these states were Ghana, Gao, and the Kanem-Bornu Empire. Ghana declined in the eleventh century, but was succeeded by the Mali Empire which consolidated much of western Sudan in the thirteenth century. Kanem accepted Islam in the eleventh century. In the forested regions of the West African coast, independent kingdoms grew with little influence from the Muslim north. The Kingdom of Nri was established around the ninth century and was one of the first. It is also one of the oldest kingdoms in present-day Nigeria and was ruled by the Eze Nri. The Nri kingdom is famous for its elaborate bronzes, found at the town of Igbo-Ukwu. The bronzes have been dated from as far back as the ninth century.[93] The Kingdom of Ife, historically the first of these Yoruba city-states or kingdoms, established government under a priestly oba ('king' or 'ruler' in the Yoruba language), called the Ooni of Ife. Ife was noted as a major religious and cultural centre in West Africa, and for its unique naturalistic tradition of bronze sculpture. The Ife model of government was adapted at the Oyo Empire, where its obas or kings, called the Alaafins of Oyo, once controlled a large number of other Yoruba and non-Yoruba city-states and kingdoms; the FonKingdom of Dahomey was one of the non-Yoruba domains under Oyo control. thirteenth centuries. Their migration resulted in the fusion of the Arabs and Berbers, where the locals were Arabized,[95] and Arab culture absorbed elements of the local culture, under the unifying framework of Islam.[96] Following the breakup of Mali, a local leader named Sonni Ali (1464–1492) founded the Songhai Empire in the region of middle Niger and the western Sudan and took control of the trans-Saharan trade. Sonni Ali seized Timbuktu in 1468 and Jenne in 1473, building his regime on trade revenues and the cooperation of Muslim merchants. His successor Askia Mohammad I (1493–1528) made Islam the official religion, built mosques, and brought to Gao Muslim scholars, including al-Maghili (d.1504), the founder of an important tradition of Sudanic African Muslim scholarship.[97] By the eleventh century, some Hausa states – such as Kano, jigawa, Katsina, and Gobir – had developed into walled towns engaging in trade, servicing caravans, and the manufacture of goods. Until the fifteenth century, these small states were on the periphery of the major Sudanic empires of the era, paying tribute to Songhai to the west and Kanem-Borno to the east. Height of the slave trade Slavery had long been practiced in Africa.[98][99] Between the 15th and the 19th centuries, the Atlantic slave trade took an estimated 7–12 million slaves to the New World.[100][101][102] In addition, more than 1 million Europeans were captured by Barbary pirates and sold as slaves in North Africa between the 16th and 19th centuries.[103] In West Africa, the decline of the Atlantic slave trade in the 1820s caused dramatic economic shifts in local polities. The gradual decline of slave-trading, prompted by a lack of demand for slaves in the New World, increasing anti-slavery legislation in Europe and America, and the British Royal Navy's increasing presence off the West African coast, obliged African states to adopt new economies. Between 1808 and 1860, the British West Africa Squadron seized approximately 1,600 slave ships and freed 150,000 Africans who were aboard.[104] Action was also taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against \"the usurping King of Lagos\", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers.[105] The largest powers of West Africa (the Asante Confederacy, the Kingdom of Dahomey, and the Oyo Empire) adopted different ways of adapting to the shift. Asante and Dahomey concentrated on the development of \"legitimate commerce\" in the form of palm oil, cocoa, timber and gold, forming the bedrock of West Africa's modern export trade. The Oyo Empire, unable to adapt, collapsed into civil wars.[106] Colonialism The Scramble for Africa[b] was the invasion, colonization, and partition of most of Africa among seven Western European powers during the era of \"New Imperialism\" (1833–1914). In 1870, 10% of the continent was formally under European control. By 1914, this figure had risen to almost 90%, with only Liberia and Ethiopia retaining their full sovereignty.[c] With the decline of the European colonial empires in the wake of both world wars, most of their African possessions achieved independence during the Cold War. However, the old imperial boundaries and economic systems imposed by the Scramble continue to affect the politics and economies of African countries.[111] Independence struggles European control in 1939 Imperial rule by Europeans would continue until after the conclusion of World War II, when almost all remaining colonial territories gradually obtained formal independence. Independence movements in Africa gained momentum following World War II, which left the major European powers weakened. In 1951, Libya, a former Italian colony, gained independence. In 1956, Tunisia and Morocco won their independence from France.[112]Ghana followed suit the next year (March 1957),[113] becoming the first of the sub-Saharan colonies to be granted independence. Most of the rest of the continent became independent over the next decade. Post-colonial Africa Today, Africa contains 54 sovereign countries, most of which have borders that were drawn during the era of European colonialism. Since independence, African states have frequently been hampered by instability, corruption, violence, and authoritarianism. The vast majority of African states are republics that operate under some form of the presidential system of rule. However, few of them have been able to sustain democratic governments on a permanent basis – per the criteria laid out by Lührmann et al. (2018), only Botswana and Mauritius have been consistently democratic for the entirety of their post-colonial history. Most African countries have experienced several coups or periods of military dictatorship. Between 1990 and 2018, though, the continent as a whole has trended towards more democratic governance.[114] Upon independence an overwhelming majority of Africans lived in extreme poverty. The continent suffered from the lack of infrastructural or industrial development under colonial rule, along with political instability. With limited financial resources or access to global markets, relatively stable countries such as Kenya still experienced only very slow economic development. Only a handful of African countries succeeded in obtaining rapid economic growth prior to 1990. Exceptions include Libya and Equatorial Guinea, both of which possess large oil reserves. Instability throughout the continent after decolonization resulted primarily from marginalization of ethnic groups, and corruption. In pursuit of personal political gain, many leaders deliberately promoted ethnic conflicts, some of which had originated during the colonial period, such as from the grouping of multiple unrelated ethnic groups into a single colony, the splitting of a distinct ethnic group between multiple colonies, or existing conflicts being exacerbated by colonial rule (for instance, the preferential treatment given to ethnic Hutus over Tutsis in Rwanda during German and Belgian rule). Faced with increasingly frequent and severe violence, military rule was widely accepted by the population of many countries as means to maintain order, and during the 1970s and 1980s a majority of African countries were controlled by military dictatorships. Territorial disputes between nations and rebellions by groups seeking independence were also common in independent African states. The most devastating of these was the Nigerian Civil War, fought between government forces and an Igboseparatist republic, which resulted in a famine that killed 1–2 million people. Two civil wars in Sudan, the first lasting from 1955 to 1972 and the second from 1983 to 2005, collectively killed around 3 million. Both were fought primarily on ethnic and religious lines. Cold War conflicts between the United States and the Soviet Union also contributed to instability. Both the Soviet Union and the United States offered considerable incentives to African political and military leaders who aligned themselves with the superpowers' foreign policy. As an example, during the Angolan Civil War, the Soviet and Cuban aligned MPLA and the American aligned UNITA received the vast majority of their military and political support from these countries. Many African countries became highly dependent on foreign aid. The sudden loss of both Soviet and American aid at the end of the Cold War and fall of the USSR resulted in severe economic and political turmoil in the countries most dependent on foreign support. Various conflicts between various insurgent groups and governments continue. Since 2003 there has been an ongoing conflict in Darfur (Sudan) which peaked in intensity from 2003 to 2005 with notable spikes in violence in 2007 and 2013–15, killing around 300,000 people total. The Boko Haram Insurgency primarily within Nigeria (with considerable fighting in Niger, Chad, and Cameroon as well) has killed around 350,000 people since 2009. Most African conflicts have been reduced to low-intensity conflicts as of 2022. However, the Tigray War which began in 2020 has killed an estimated 300,000–500,000 people, primarily due to famine. Overall though, violence across Africa has greatly declined in the 21st century, with the end of civil wars in Angola, Sierra Leone, and Algeria in 2002, Liberia in 2003, and Sudan and Burundi in 2005. The Second Congo War, which involved 9 countries and several insurgent groups, ended in 2003. This decline in violence coincided with many countries abandoning communist-style command economies and opening up for market reforms, which over the course of the 1990s and 2000s promoted the establishment of permanent, peaceful trade between neighboring countries (see Capitalist peace). Improved stability and economic reforms have led to a great increase in foreign investment into many African nations, mainly from China,[121] which further spurred economic growth. Between 2000 and 2014, annual GDP growth in sub-Saharan Africa averaged 5.02%, doubling its total GDP from $811 Billion to $1.63 Trillion (Constant 2015 USD).[122] North Africa experienced comparable growth rates.[123] A significant part of this growth can also be attributed to the facilitated diffusion of information technologies and specifically the mobile telephone.[124] While several individual countries have maintained high growth rates, since 2014 overall growth has considerably slowed, primarily as a result of falling commodity prices, continued lack of industrialization, and epidemics of Ebola and COVID-19.[125][126] The coastline is 26,000 km (16,000 mi) long, and the absence of deep indentations of the shore is illustrated by the fact that Europe, which covers only 10,400,000 km2 (4,000,000 sq mi) – about a third of the surface of Africa – has a coastline of 32,000 km (20,000 mi).[129] From the most northerly point, Ras ben Sakka in Tunisia (37°21' N), to the most southerly point, Cape Agulhas in South Africa (34°51'15\" S), is a distance of approximately 8,000 km (5,000 mi).[130]Cape Verde, 17°33'22\" W, the westernmost point, is a distance of approximately 7,400 km (4,600 mi) to Ras Hafun, 51°27'52\" E, the most easterly projection that neighbours Cape Guardafui, the tip of the Horn of Africa.[129] Between 60 million years ago and 10 million years ago, the Somali Plate began rifting from the African Plate along the East African Rift.[132] Since the continent of Africa consists of crust from both the African and the Somali plates, some literature refers to the African Plate as the Nubian Plate to distinguish it from the continent as a whole.[133] Climate The climate of Africa ranges from tropical to subarctic on its highest peaks. Its northern half is primarily desert, or arid, while its central and southern areas contain both savanna plains and dense jungle (rainforest) regions. In between, there is a convergence, where vegetation patterns such as sahel and steppe dominate. Africa is the hottest continent on Earth and 60% of the entire land surface consists of drylands and deserts.[134] The record for the highest-ever recorded temperature, in Libya in 1922 (58 °C (136 °F)), was discredited in 2013.[135][136] Climate change Graph showing temperature change in Africa between 1901 and 2021, with red colour being warmer and blue being colder than average (the average temperature during 1971–2000 is taken as the reference point for these changes). Over the coming decades, warming from climate change is expected across almost all the Earth's surface, and global mean rainfall will increase.[143] Currently, Africa is warming faster than the rest of the world on average. Large portions of the continent may become uninhabitable as a result of the rapid effects of climate change, which would have disastrous effects on human health, food security, and poverty.[144][145][146] Regional effects on rainfall in the tropics are expected to be much more spatially variable. The direction of change at any one location is often less certain. Ecology and biodiversity The main biomes in Africa. Africa has over 3,000 protected areas, with 198 marine protected areas, 50 biosphere reserves, and 80 wetlands reserves. Significant habitat destruction, increases in human population and poaching are reducing Africa's biological diversity and arable land. Human encroachment, civil unrest and the introduction of non-native species threaten biodiversity in Africa. This has been exacerbated by administrative problems, inadequate personnel and funding problems.[134] Deforestation is affecting Africa at twice the world rate, according to the United Nations Environment Programme (UNEP).[147] According to the University of Pennsylvania African Studies Center, 31% of Africa's pasture lands and 19% of its forests and woodlands are classified as degraded, and Africa is losing over four million hectares of forest per year, which is twice the average deforestation rate for the rest of the world.[134] Some sources claim that approximately 90% of the original, virgin forests in West Africa have been destroyed.[148] Over 90% of Madagascar's original forests have been destroyed since the arrival of humans 2000 years ago.[149] About 65% of Africa's agricultural land suffers from soil degradation.[150] Infrastructure Water resources Water development and management are complex in Africa due to the multiplicity of trans-boundary water resources (rivers, lakes and aquifers).[153] Around 75% of sub-Saharan Africa falls within 53 international river basin catchments that traverse multiple borders.[154][153] This particular constraint can also be converted into an opportunity if the potential for trans-boundary cooperation is harnessed in the development of the area's water resources.[153] A multi-sectoral analysis of the Zambezi River, for example, shows that riparian cooperation could lead to a 23% increase in firm energy production without any additional investments.[154][153] A number of institutional and legal frameworks for transboundary cooperation exist, such as the Zambezi River Authority, the Southern African Development Community (SADC) Protocol, Volta River Authority and the Nile Basin Commission.[153] However, additional efforts are required to further develop political will, as well as the financial capacities and institutional frameworks needed for win-win multilateral cooperative actions and optimal solutions for all riparians.[153] The African Union, not to be confused with the AU Commission, is formed by the Constitutive Act of the African Union, which aims to transform the African Economic Community, a federated commonwealth, into a state under established international conventions. The African Union has a parliamentary government, known as the African Union Government, consisting of legislative, judicial and executive organs. It is led by the African Union President and Head of State, who is also the President of the Pan-African Parliament. A person becomes AU President by being elected to the PAP, and subsequently gaining majority support in the PAP. The powers and authority of the President of the African Parliament derive from the Constitutive Act and the Protocol of the Pan-African Parliament, as well as the inheritance of presidential authority stipulated by African treaties and by international treaties, including those subordinating the Secretary General of the OAU Secretariat (AU Commission) to the PAP. The government of the AU consists of all-union, regional, state, and municipal authorities, as well as hundreds of institutions, that together manage the day-to-day affairs of the institution. Boundary conflicts African nations have made great efforts to respect international borders as inviolate for a long time. For example, the Organization of African Unity (OAU), which was established in 1963 and replaced by the African Union in 2002, set the respect for the territorial integrity of each country as one of its principles in OAU Charter.[156] Indeed, compared with the formation of European countries, there have been fewer international conflicts in Africa for changing the borders, which has influenced country formation there and has enabled some countries to survive that might have been defeated and absorbed by others.[157] Yet international conflicts have played out by support for proxy armies or rebel movements. Many states have experienced civil wars: including Rwanda, Sudan, Angola, Sierra Leone, Congo, Liberia, Ethiopia and Somalia.[158] Although it has abundant natural resources, Africa remains the world's poorest and least-developed continent (other than Antarctica), the result of a variety of causes that may include corrupt governments that have often committed serious human rights violations, failed central planning, high levels of illiteracy, low self-esteem, lack of access to foreign capital, legacies of colonialism, the slave trade, and the Cold War, and frequent tribal and military conflict (ranging from guerrilla warfare to genocide).[159] Its total nominal GDP remains behind that of the United States, China, Japan, Germany, the United Kingdom, India and France. According to the United Nations' Human Development Report in 2003, the bottom 24 ranked nations (151st to 175th) were all African.[160] Poverty, illiteracy, malnutrition and inadequate water supply and sanitation, as well as poor health, affect a large proportion of the people who reside in the African continent. In August 2008, the World Bank[161] announced revised global poverty estimates based on a new international poverty line of $1.25 per day (versus the previous measure of $1.00). Eighty-one percent of the sub-Saharan African population was living on less than $2.50 (PPP) per day in 2005, compared with 86% for India.[162] Sub-Saharan Africa is the least successful region of the world in reducing poverty ($1.25 per day); some 50% of the population living in poverty in 1981 (200 million people), a figure that rose to 58% in 1996 before dropping to 50% in 2005 (380 million people). The average poor person in sub-Saharan Africa is estimated to live on only 70 cents per day, and was poorer in 2003 than in 1973,[163] indicating increasing poverty in some areas. Some of it is attributed to unsuccessful economic liberalization programmes spearheaded by foreign companies and governments, but other studies have cited bad domestic government policies more than external factors.[164][165] Africa is now at risk of being in debt once again, particularly in sub-Saharan African countries. The last debt crisis in 2005 was resolved with help from the heavily indebted poor countries scheme (HIPC). The HIPC resulted in some positive and negative effects on the economy in Africa. About ten years after the 2005 debt crisis in sub-Saharan Africa was resolved, Zambia fell back into debt. A small reason was due to the fall in copper prices in 2011, but the bigger reason was that a large amount of the money Zambia borrowed was wasted or pocketed by the elite.[166] From 1995 to 2005, Africa's rate of economic growth increased, averaging 5% in 2005. Some countries experienced still higher growth rates, notably Angola, Sudan and Equatorial Guinea, all of which had recently begun extracting their petroleum reserves or had expanded their oil extraction capacity. In a recently published analysis based on World Values Survey data, the Austrian political scientist Arno Tausch maintained that several African countries, most notably Ghana, perform quite well on scales of mass support for democracy and the market economy.[167] Tausch's global value comparison based on the World Values Survey derived the following factor analytical scales: 1. The non-violent and law-abiding society 2. Democracy movement 3. Climate of personal non-violence 4. Trust in institutions 5. Happiness, good health 6. No redistributive religious fundamentalism 7. Accepting the market 8. Feminism 9. Involvement in politics 10. Optimism and engagement 11. No welfare mentality, acceptancy of the Calvinist work ethics. The spread in the performance of African countries with complete data, Tausch concluded \"is really amazing\". While one should be especially hopeful about the development of future democracy and the market economy in Ghana, the article suggests pessimistic tendencies for Egypt and Algeria, and especially for Africa's leading economy, South Africa. High Human Inequality, as measured by the UNDP's Human Development Report's Index of Human Inequality, further impairs the development of human security. Tausch also maintains that the certain recent optimism, corresponding to economic and human rights data, emerging from Africa, is reflected in the development of a civil society. In recent years, the People's Republic of China has built increasingly stronger ties with African nations and is Africa's largest trading partner. In 2007, Chinese companies invested a total of US$1 billion in Africa.[121] A Harvard University study led by professor Calestous Juma showed that Africa could feed itself by making the transition from importer to self-sufficiency. \"African agriculture is at the crossroads; we have come to the end of a century of policies that favoured Africa's export of raw materials and importation of food. Africa is starting to focus on agricultural innovation as its new engine for regional trade and prosperity.\"[174] Electricity generation The main source of electricity is hydropower, which contributes significantly to the current installed capacity for energy.[153] The Kainji Dam is a typical hydropower resource generating electricity for all the large cities in Nigeria as well as their neighbouring country, Niger.[175] Hence, the continuous investment in the last decade, which has increased the amount of power generated.[153] Africa's population has rapidly increased over the last 40 years, and is consequently relatively young. In some African states, more than half the population is under 25 years of age.[176] The total number of people in Africa increased from 229 million in 1950 to 630 million in 1990.[177] As of 2021, the population of Africa is estimated at 1.4 billion [1][2]. Africa's total population surpassing other continents is fairly recent; African population surpassed Europe in the 1990s, while the Americas was overtaken sometime around the year 2000; Africa's rapid population growth is expected to overtake the only two nations currently larger than its population, at roughly the same time – India and China's 1.4 billion people each will swap ranking around the year 2022.[178] This increase in number of babies born in Africa compared to the rest of the world is expected to reach approximately 37% in the year 2050; while in 1990 sub-Saharan Africa accounted for only 16% of the world's births.[179] The total fertility rate (children per woman) for Sub-Saharan Africa is 4.7 as of 2018, the highest in the world.[180] All countries in sub-Saharan Africa had TFRs (average number of children) above replacement level in 2019 and accounted for 27.1% of global livebirths.[181] In 2021, sub-Saharan Africa accounted for 29% of global births.[182] Speakers of Bantu languages (part of the Niger–Congo family) are the majority in southern, central and southeast Africa. The Bantu-speaking peoples from the Sahel progressively expanded over most of sub-Saharan Africa.[183] But there are also several Nilotic groups in South Sudan and East Africa, the mixed Swahili people on the Swahili Coast, and a few remaining indigenous Khoisan (\"San\" or \"Bushmen\") and Pygmy peoples in Southern and Central Africa, respectively. Bantu-speaking Africans also predominate in Gabon and Equatorial Guinea, and are found in parts of southern Cameroon. In the Kalahari Desert of Southern Africa, the distinct people known as the Bushmen (also \"San\", closely related to, but distinct from \"Hottentots\") have long been present. The San are physically distinct from other Africans and are the indigenous people of southern Africa.[citation needed] Pygmies are the pre-Bantu indigenous peoples of central Africa.[184] The peoples of West Africa primarily speak Niger–Congo languages, belonging mostly to its non-Bantu branches, though some Nilo-Saharan and Afro-Asiatic speaking groups are also found. The Niger–Congo-speaking Yoruba, Igbo, Fulani, Akan, and Wolof ethnic groups are the largest and most influential. In the central Sahara, Mandinka or Mande groups are most significant. Chadic-speaking groups, including the Hausa, are found in more northerly parts of the region nearest to the Sahara, and Nilo-Saharan communities, such as the Songhai, Kanuri and Zarma, are found in the eastern parts of West Africa bordering Central Africa. The peoples of North Africa consist of three main indigenous groups: Berbers in the northwest, Egyptians in the northeast, and Nilo-Saharan-speaking peoples in the east. The Arabs who arrived in the 7th century CE introduced the Arabic language and Islam to North Africa. The Semitic Phoenicians (who founded Carthage) and Hyksos, the Indo-Iranian Alans, the Indo- European Greeks, Romans, and Vandals settled in North Africa as well. Significant Berber communities remain within Morocco and Algeria in the 21st century, while, to a lesser extent, Berber speakers are also present in some regions of Tunisia and Libya.[185] The Berber-speaking Tuareg and other often-nomadic peoples are the principal inhabitants of the Saharan interior of North Africa. In Mauritania, there is a small but near-extinct Berber community in the north and Niger–Congo-speaking peoples in the south, though in both regions Arabic and Arab culture predominates. In Sudan, although Arabic and Arab culture predominate, it is mostly inhabited by groups that originally spoke Nilo-Saharan, such as the Nubians, Fur, Masalit and Zaghawa, who, over the centuries, have variously intermixed with migrants from the Arabian peninsula. Small communities of Afro-Asiatic-speaking Beja nomads can also be found in Egypt and Sudan.[186] Prior to the decolonization movements of the post-World War II era, Europeans were represented in every part of Africa.[187] Decolonization during the 1960s and 1970s often resulted in the mass emigration of white settlers – especially from Algeria and Morocco (1.6 million pieds-noirs in North Africa),[188] Kenya, Congo,[189] Rhodesia, Mozambique and Angola.[190] Between 1975 and 1977, over a million colonials returned to Portugal alone.[191] Nevertheless, white Africans remain an important minority in many African states, particularly Zimbabwe, Namibia, Réunion, and South Africa.[192] The country with the largest white African population is South Africa.[193]Dutch and Britishdiasporas represent the largest communities of European ancestry on the continent today.[194] European colonization also brought sizable groups of Asians, particularly from the Indian subcontinent, to British colonies. Large Indian communities are found in South Africa, and smaller ones are present in Kenya, Tanzania, and some other southern and southeast African countries. The large Indian community in Uganda was expelled by the dictator Idi Amin in 1972, though many have since returned. The islands in the Indian Ocean are also populated primarily by people of Asian origin, often mixed with Africans and Europeans. The Malagasy people of Madagascar are an Austronesian people, but those along the coast are generally mixed with Bantu, Arab, Indian and European origins. Malay and Indian ancestries are also important components in the group of people known in South Africa as Cape Coloureds (people with origins in two or more races and continents). During the 20th century, small but economically important communities of Lebanese and Chinese[121] have also developed in the larger coastal cities of West and East Africa, respectively.[195] Religion While Africans profess a wide variety of religious beliefs, the majority of the people respect African religions or parts of them. However, in formal surveys or census, most people will identify with major religions that came from outside the continent, mainly through colonisation. There are several reasons for this, the main one being the colonial idea that African religious beliefs and practices are not good enough. Religious beliefs and statistics on religious affiliation are difficult to come by since they are often a sensitive topic for governments with mixed religious populations.[201][202] According to the World Book Encyclopedia, Islam and Christianity are the two largest religions in Africa. Islam is most prevalent in Northern Africa, and is the state religion of many North African countries, such as Algeria, where 99% of the population practices Islam.[203] The majority of people in most governments in Southern, Southeast, and Central Africa, as well as in a sizable portion of the Horn of Africa and West Africa, identify as Christians. The Coptic Christians constitute a sizable minority in Egypt, and the Ethiopian Orthodox Church is the largest church in Ethiopia, with 36 million and 51 million adherents.[204] According to Encyclopædia Britannica, 45% of the population are Christians, 40% are Muslims, and 10% follow traditional religions.[citation needed] A small number of Africans are Hindu, Buddhist, Confucianist, Baháʼí, or Jewish. There is also a minority of people in Africa who are irreligious. Languages By most estimates, well over a thousand languages (UNESCO has estimated around two thousand) are spoken in Africa.[205] Most are of African origin, though some are of European or Asian origin. Africa is the most multilingual continent in the world, and it is not rare for individuals to fluently speak not only multiple African languages, but one or more European ones as well.[further explanation needed] There are four major groups indigenous to Africa: A simplistic view of language families spoken in Africa The Afroasiatic languages are a language family of about 240 languages and 285 million people widespread throughout the Horn of Africa, North Africa, the Sahel, and Southwest Asia. The Niger-Congo language family covers much of sub-Saharan Africa. In terms of number of languages, it is the largest language family in Africa and perhaps one of the largest in the world. The Khoisan languages form a group of three unrelated[207] families and two isolates and number about fifty in total. They are mainly spoken in Southern Africa by approximately 400,000 people.[208] Many of the Khoisan languages are endangered. The Khoi and San peoples are considered the original inhabitants of this part of Africa. Following the end of colonialism, nearly all African countries adopted official languages that originated outside the continent, although several countries also granted legal recognition to indigenous languages (such as Swahili, Yoruba, Igbo and Hausa). In numerous countries, English and French (see African French) are used for communication in the public sphere such as government, commerce, education and the media. Arabic, Portuguese, Afrikaans and Spanish are examples of languages that trace their origin to outside of Africa, and that are used by millions of Africans today, both in the public and private spheres. Italian is spoken by some in former Italian colonies in Africa. German is spoken in Namibia, as it was a former German protectorate. In total, at least a fifth of Africans speak the former colonial languages.[209][210][211][d] Health Prevalence of HIV/AIDS in Africa, total (% of population ages 15–49), in 2011 (World Bank) over 15% 5–15% 2–5% 1–2% 0.5-1% 0.1–0.5% not available More than 85% of individuals in Africa use traditional medicine as an alternative to often expensive allopathic medical health care and costly pharmaceutical products. The Organization of African Unity (OAU) Heads of State and Government declared the 2000s decade as the African Decade on African traditional medicine in an effort to promote The WHO African Region's adopted resolution for institutionalizing traditional medicine in health care systems across the continent.[212] Public policy makers in the region are challenged with consideration of the importance of traditional/indigenous health systems and whether their coexistence with the modern medical and health sub-sector would improve the equitability and accessibility of health care distribution, the health status of populations, and the social-economic development of nations within sub-Saharan Africa.[213] AIDS in post-colonial Africa is a prevalent issue. Although the continent is home to about 15.2 percent of the world's population,[214] more than two-thirds of the total infected worldwide – some 35 million people – were Africans, of whom 15 million have already died.[215]Sub-Saharan Africa alone accounted for an estimated 69 percent of all people living with HIV[216] and 70 percent of all AIDS deaths in 2011.[217] In the countries of sub-Saharan Africa most affected, AIDS has raised death rates and lowered life expectancy among adults between the ages of 20 and 49 by about twenty years.[215] Furthermore, the life expectancy in many parts of Africa has declined, largely as a result of the HIV/AIDS epidemic with life-expectancy in some countries reaching as low as thirty-four years.[218] Culture Some aspects of traditional African cultures have become less practised in recent years as a result of neglect and suppression by colonial and post-colonial regimes. For example, African customs were discouraged, and African languages were prohibited in mission schools.[219] Leopold II of Belgium attempted to \"civilize\" Africans by discouraging polygamy and witchcraft.[219] Obidoh Freeborn posits that colonialism is one element that has created the character of modern African art.[220] According to authors Douglas Fraser and Herbert M. Cole, \"The precipitous alterations in the power structure wrought by colonialism were quickly followed by drastic iconographic changes in the art.\"[221] Fraser and Cole assert that, in Igboland, some art objects \"lack the vigor and careful craftsmanship of the earlier art objects that served traditional functions.[221] Author Chika Okeke-Agulu states that \"the racist infrastructure of British imperial enterprise forced upon the political and cultural guardians of empire a denial and suppression of an emergent sovereign Africa and modernist art.\"[222] Editors F. Abiola Irele and Simon Gikandi comment that the current identity of African literature had its genesis in the \"traumatic encounter between Africa and Europe.\"[223] On the other hand, Mhoze Chikowero believes that Africans deployed music, dance, spirituality, and other performative cultures to (re)assert themselves as active agents and indigenous intellectuals, to unmake their colonial marginalization and reshape their own destinies.\"[224] There is now a resurgence in the attempts to rediscover and revalue African traditional cultures, under such movements as the African Renaissance, led by Thabo Mbeki, Afrocentrism, led by a group of scholars, including Molefi Asante, as well as the increasing recognition of traditional spiritualism through decriminalization of Vodou and other forms of spirituality. Pottery, metalwork, sculpture, architecture, textile art and fiber art are important visual art forms across Africa and may be included in the study of African art. The term \"African art\" does not usually include the art of the North African areas along the Mediterranean coast, as such areas had long been part of different traditions. For more than a millennium, the art of such areas had formed part of Berber or Islamic art, although with many particular local characteristics. The Art of Ethiopia, with a long Christian tradition,[227] is also different from that of most of Africa, where the Traditional African religion (with Islam in the north) was dominant until the 20th century.[228] African art includes prehistoric and ancient art, the Islamic art of West Africa, the Christian art of East Africa, and the traditional artifacts of these and other regions. Many African sculptures were historically made of wood and other natural materials that have not survived from earlier than a few centuries ago, although rare older pottery and metal figures can be found in some areas.[229] Some of the earliest decorative objects, such as shell beads and evidence of paint, have been discovered in Africa, dating to the Middle Stone Age.[230][231][232]Masks are important elements in the art of many peoples, along with human figures, and are often highly Stylized. There is a vast variety of styles, often varying within the same context of origin and depending on the use of the object, but wide regional trends are apparent; sculpture is most common among \"groups of settled cultivators in the areas drained by the Niger and Congo rivers\" in West Africa.[233] Direct images of deities are relatively infrequent, but masks in particular are or were often made for ritual ceremonies. Since the late 19th century there has been an increasing amount of African art in Western collections, the finest pieces of which are displayed as part of the history of colonization. African art has had an important influence on European Modernist art,[234] which was inspired by their interest in abstract depiction. It was this appreciation of African sculpture that has been attributed to the very concept of \"African art\", as seen by European and American artists and art historians.[235] West African cultures developed bronze casting for reliefs, like the famous Benin Bronzes, to decorate palaces and for highly naturalistic royal heads from around the Bini town of Benin City, Edo State, as well as in terracotta or metal, from the 12th–14th centuries. Akan gold weights are a form of small metal sculptures produced over the period 1400–1900; some represent proverbs, contributing a narrative element rare in African sculpture; and royal regalia included gold sculptured elements.[236] Many West African figures are used in religious rituals and are often coated with materials placed on them for ceremonial offerings. The Mande-speaking peoples of the same region make pieces from wood with broad, flat surfaces and arms and legs shaped like cylinders. In Central Africa, however, the main distinguishing characteristics include heart-shaped faces that are curved inward and display patterns of circles and dots. African architecture in some areas has been influenced by external cultures for centuries, according to available evidence. Western architecture has influenced coastal areas since the late 15th century and is now an important source of inspiration for many larger buildings, particularly in major cities. African architecture uses a wide range of materials, including thatch, stick/wood, mud, mudbrick, rammed earth, and stone. These material preferences vary by region: North Africa for stone and rammed earth, the Horn of Africa for stone and mortar, West Africa for mud/adobe, Central Africa for thatch/wood and more perishable materials, Southeast and Southern Africa for stone and thatch/wood. Cinema of Africa covers both the history and present of the making or screening of films on the African continent, and also refers to the persons involved in this form of audiovisual culture. It dates back to the early 20th century, when film reels were the primary cinematic technology in use. During the colonial era, African life was shown only by the work of white, colonial, Western filmmakers, who depicted Africans in a negative fashion, as exotic \"others\".[238] As there are more than 50 countries with audiovisual traditions, there is no one single 'African cinema'. Both historically and culturally, there are major regional differences between North African and sub-Saharan cinemas, and between the cinemas of different countries.[238] Dance African dance (also Afro dance, Afrodance and Afro-dance)[249][250][251][252][253] refers to the various dance styles of sub-Saharan Africa. These dances are closely connected with the traditional rhythms and music traditions of the region. Music and dancing is an integral part of many traditional African societies. Songs and dances facilitate teaching and promoting social values, celebrating special events and major life milestones, performing oral history and other recitations, and spiritual experiences.[254] African dance uses the concepts of polyrhythm and total body articulation.[255] African dances are a collective activity performed in large groups, with significant interaction between dancers and onlookers in the majority of styles.[256] In recent years, the continent has made major progress in terms of state-of-the-art basketball facilities which have been built in cites as diverse as Cairo, Dakar, Johannesburg, Kigali, Luanda and Rades.[257] The number of African basketball players who drafted into the NBA has experienced major growth in the 2010s.[258] Rugby is popular in several southern African nations. Namibia and Zimbabwe both have appeared on multiple occasions at the Rugby World Cup, while South Africa is the most successful national team at the Rugby World Cup, having won the tournament on four occasions, in 1995, 2007, 2019, and 2023. [259] The countries in this table are categorized according to the scheme for geographic subregions used by the United Nations, and data included are per sources in cross-referenced articles. Where they differ, provisos are clearly indicated. ^The previous three references show that there a total of 130 million English speakers, 120 million French speakers, and over 30 million Portuguese speakers in Africa, making them about 20% of Africa's 2022 population of 1.4 billion people. ^Desfayes, Michel (25 January 2011). \"The Names of Countries\". michel-desfayes.org. Archived from the original on 27 June 2019. Retrieved 9 April 2019. Africa. From the name of an ancient tribe in Tunisia, the Afri (adjective: Afer). The name is still extant today as Ifira and Ifri-n-Dellal in Greater Kabylia (Algeria). A Berber tribe was called Beni-Ifren in the Middle Ages and Ifurace was the name of a Tripolitan people in the 6th century. The name is from the Berber language ifri 'cave'. Troglodytism was frequent in northern Africa and still occurs today in southern Tunisia. Herodote wrote that the Garamantes, a North African people, used to live in caves. The Ancient Greek called troglodytēs an African people who lived in caves. Africa was coined by the Romans and 'Ifriqiyeh' is the arabized Latin name. (Most details from Decret & Fantar, 1981). ^McBrearty, Sally; Brooks, Allison (2000). \"The revolution that wasn't: a new interpretation of the origin of modern human behavior\". Journal of Human Evolution. 39 (5): 453–563. doi:10.1006/jhev.2000.0435. PMID11102266."}
{"url": "https://en.m.wikipedia.org/wiki/Kleptoplasty", "text": "Kleptoplasty Kleptoplasty or kleptoplastidy is a process in symbiotic relationships whereby plastids, notably chloroplasts from algae, are sequestered by the host. The word is derived from Kleptes (κλέπτης) which is Greek for thief. The alga is eaten normally and partially digested, leaving the plastid intact. The plastids are maintained within the host, temporarily continuing photosynthesis and benefiting the host. Kleptoplasty is a process in symbiotic relationships whereby plastids, notably chloroplasts from algae, are sequestered by the host. The alga is eaten normally and partially digested, leaving the plastid intact. The plastids are maintained within the host, temporarily continuing photosynthesis and benefiting the host.[1] The term was coined in 1990 to describe chloroplast symbiosis.[2][3] The stability of transient plastids varies considerably across plastid-retaining species. In the dinoflagellatesGymnodinium spp. and Pfisteria piscicida, kleptoplastids are photosynthetically active for only a few days, while kleptoplastids in Dinophysis spp. can be stable for 2 months.[1] In other dinoflagellates, kleptoplasty has been hypothesized to represent either a mechanism permitting functional flexibility, or perhaps an early evolutionary stage in the permanent acquisition of chloroplasts.[5] Two species of rhabdocoel marine flatworms, Baicalellia solaris and Pogaina paranygulgus, make use of kleptoplasty. The group was previously classified as having algal endosymbionts, though it was already discovered that the endosymbionts did not contain nuclei.[9] While consuming diatoms, B. solaris and P. paranygulus, in a process not yet discovered, extract plastids from their prey, incorporating them subepidermally, while separating and digesting the frustule and remainder of the diatom. In B. solaris the extracted plastids, or kleptoplasts, continue to exhibit functional photosynthesis for a short period of roughly 7 days. As the two groups are not sister taxa, and the trait is not shared among groups more closely related, there is evidence that kleptoplasty evolved independently within the two taxa.[10] Sea slugs in the clade Sacoglossa practise kleptoplasty.[11] Several species of Sacoglossan sea slugs capture intact, functional chloroplasts from algal food sources, retaining them within specialized cells lining the mollusc's digestive diverticula. The longest known kleptoplastic association, which can last up to ten months, is found in Elysia chlorotica,[2] which acquires chloroplasts by eating the alga Vaucheria litorea, storing the chloroplasts in the cells that line its gut.[12] Juvenile sea slugs establish the kleptoplastic endosymbiosis when feeding on algal cells, sucking out the cell contents, and discarding everything except the chloroplasts. The chloroplasts are phagocytosed by digestive cells, filling extensively branched digestive tubules, providing their host with the products of photosynthesis.[13] It is not resolved, however, whether the stolen plastids actively secrete photosynthate or whether the slugs profit indirectly from slowly degrading kleptoplasts.[14] Due to this unusual ability, the sacoglossans are sometimes referred to as \"solar-powered sea slugs,\" though the actual benefit from photosynthesis on the survival of some of the species that have been analyzed seems to be marginal at best.[15] In fact, some species may even die in the presence of the carbon dioxide-fixing kleptoplasts as a result of elevated levels of reactive oxygen species.[16] Changes in temperature have been shown to negatively affect kleptoplastic abilities in sacoglossans. Rates of photosynthetic efficiency as well as kleptoplast abundance have been shown to decrease in correlation to a decrease in temperature. The patterns and rate of these changes, however, varies between different species of sea slug.[17]"}
{"url": "https://en.m.wikipedia.org/wiki/Paradox_of_the_plankton", "text": "Paradox of the plankton In aquatic biology, the paradox of the plankton describes the situation in which a limited range of resources supports an unexpectedly wide range of plankton species, apparently flouting the competitive exclusion principle, which holds that when two species compete for the same resource, one will be driven to extinction. While it was long assumed that turbulence disrupts plankton patches at spatial scales less than a few metres, researchers using small-scale analysis of plankton distribution found that these exhibited patches of aggregation (on the order of 10cm) that had sufficient lifetimes (more than 10 minutes) to enable plankton grazing, competition, and infection.[12] In the lytic cycle, viruses reproduce in host cells to manufacture more viruses; the viruses then burst out of the cell. One potential resolution to the paradox is the control on plankton populations by marine lytic viruses. Marine viruses play an important role in bacteria and plankton ecology. They are a significant component of biogeochemical cycling[13] and horizontal gene transfer in both bacterial and plankton communities. Viruses are the most abundant organisms in the ocean, and have the capacity to deplete host populations very rapidly. Marine viruses infect specific host species, and therefore an abundance of a virus can quickly and effectively alter the structure of the phytoplankton and bacterial communities. Via the lytic cycle, a virus encounters a host and reproduces until the cell bursts, releasing viruses. Viruses can also enter a lysogenic cycle, in which the virus writes its DNA into the host genome. When a phytoplankton species enters a bloom period, cell concentration increases and many viral targets suddenly become available. [14] One explanation to the paradox of the plankton is the \"Boom-and-busted dynamic\" hypothesis, also called \"Kill the winner.\" In a phytoplankton bloom, an individual species multiplies rapidly in ideal conditions, which increases its cell concentration in an area, outcompeting other phytoplankton. This \"boom\" in host cells creates an opportunity for rapid infection by viruses, leading to a \"bust\" in which the phytoplankton population rapidly diminishes. This creates a large gap in the local phytoplankton ecology and allows other species to fill in and continue growing. Such population control by viruses creates temporal and spatial diversity in phytoplankton communities. Long term control results, as the virus prevents the formerly dominant species from booming during future bloom events.[15]"}
{"url": "https://books.google.com/books?id=hKvrCAAAQBAJ", "text": "Coral Reef Ecology Coral reef communities are among the most complex, mature and productive ecosystems on earth. Their activity resulted in the creation of vast lime constructions. Being extremely productive and having the function of a powerful biofilter, coral reefs play an important role in global biogeochemical processes and in the reproduction of food resources in tropical marine regions. All aspects of coral reef science are covered systematically and on the basis of a holistic ecosystem approach. The geological history of coral reefs, their geomorphology as well as biology including community structure of reef biota, their functional characteristics, physiological aspects, biogeochemical metabolism, energy balance, environmental problems and management of resources are treated in detail."}
{"url": "https://www.semanticscholar.org/paper/Anatomical-adaptations-of-aquatic-mammals-Reidenberg/382d07fd926a30cd4a3015ffec385e3c365a8900", "text": "The articles in this issue are a blend of literature review and new, hypothesis‐driven anatomical research, which highlight the special nature of anatomical form and function in aquatic mammals that enables their exquisite adaptation for life in such a challenging environment. It is demonstrated that, with several exceptions, the anatomical characteristics of the digestive system of the Arctocephalus australis are similar to those in other carnivores, which is an important contribution for clinical diagnostic and conservation purposes, for both veterinarians and biologists. The results demonstrate that, due to its capacity to stay under water, the respiratory apparatus of the South American fur seal shows specific characteristics, which are of great importance for clinical diagnostic and wildlife conservation purposes. The comparison of the species of Thalassocnus with each other suggests a progressive shift to a particular ecology from the earliest to the latest species of the genus, a conclusion in agreement with those of the studies of craniomandibular, dental, and forelimb gross morphology, and bone internal microstructure. This special issue of The Anatomical Record explores extravagant adaptions that vertebrates have evolved from their base groups to survive in the most challenging environments and becomes a tool for a pioneer‐like diversification of vertebrates. Dolphins possess a unique microbiota profile within the Mammalia class, highly resembling that of carnivorous marine fishes, and the breast-fed calf showed a distinctive compositional structure of the gut microbial ecosystem, which partially overlaps with the mother's milk microbiota. The results show that the Franciscana dolphin presents differential characteristics in relation to several parts of the digestive system, including, specifically, the tongue, the teeth, the stomach, and the small intestine. It would still be prudent to follow Macphail’s caution that it is premature to make strong comparative statements without more empirical evidence, but an approach that includes learning more about how animals flexibly link information across multiple representations could be a productive way of comparing species by allowing them to use their specific strengths within comparative tasks. 43 References In general, the visual system of marine mammals demonstrates a high degree of development and several specific features associated with adaptation for vision in both the aquatic and aerial environments. Comparisons of musculoskeletal anatomy of the hyolingual apparatus in cetacean specimens reveal specific adaptations for aquatic life, including vascular retial adaptations for thermoregulation and large amounts of submucosal adipose tissue for nutritional storage. The earliest representatives of these clades all show morphological features that indicate they were feeding while in the water, suggesting that feeding ecology is a key factor in the evolution of marine mammals. The ziphiids clearly form a distinct group of cetaceans in their utilization of differences in stomach morphology, and further study is necessary to establish whether these differences correlate with specialized adaptations related to an aquatic environment. Comparisons with sirenians and pinnipeds provide no evidence for the idea that the odontocete's large brain, high encephalization level, and extreme neocortical gyrification is an adaptation to a fully aquatic lifestyle, and results indicate that the high enphalization level of odontOCetes is likely related to their socially complex lifestyle patterns that transcend the influence of an aquatic environment. There is a significant positive relationship between maximum relative prey size consumed and average asymmetry relative to skull size in odontocete species, and this finding provides support for the hypothesis that the directional asymmetry found in odonocete skulls is related to an aquatic adaptation enabling swallowing large, whole prey while maintaining respiratory tract protection. Analysis of the microstructural features of bone in early and late archaic cetaceans, and in a comparative sample of modern terrestrial, semiaquatic, and aquatic mammals, shows that high bone density is an aquatic specialization that provides static buoyancy control (ballast) for animals living in shallow water, while low bonedensity is associated with dynamic buoyancy Control for animalsliving in deep water. A review of recent advances in understanding of water and electrolyte balance and of renal function in marine mammals concludes that the sensitivity of the renin-angiotensin-aldosterone system appears to be influenced by the availability of Na+. Modifications of the size, attachments and fascicle architecture of the muscles and the structure and range of possible movement of the joints suggest that in fur seals and sea lions these movements contribute to the generation of massive forward thrust via the cooperative activity of muscles capable of generating large amounts of force throughout the range of movement."}
{"url": "https://phys.org/news/2019-08-acid-oceans-plankton-fueling-faster.html", "text": "Our research into the effects of CO₂-induced changes to microscopic ocean algae—called phytoplankton—was published today in Nature Climate Change. It has uncovered a previously unrecognized threat from ocean acidification. In our study we discovered increased seawater acidity reduced Antarctic phytoplanktons' ability to build strong cell walls, making them smaller and less effective at storing carbon. At current rates of seawater acidification, we could see this effect before the end of the century. What is ocean acidification? Carbon dioxide emissions are not just altering our atmosphere. More than 40% of CO₂ emitted by people is absorbed by our oceans. While reducing the CO₂ in our atmosphere is generally a good thing, the ugly consequence is this process makes seawater more acidic. Just as placing a tooth in a jar of cola will (eventually) dissolve it, increasingly acidic seawater has a devastating effect on organisms that build their bodies out of calcium, like corals and shellfish. Many studies to date have therefore taken the perfectly logical step of studying the effects of seawater acidification on these \"calcifying\" creatures. However, we wanted to know if other, non-calcifying, species are at risk. Diatoms in our oceans Phytoplankton use photosynthesis to turn carbon in the atmosphere into carbon in their bodies. We looked at diatoms, a key group of phytoplankton responsible for 40% of this process in the ocean. Not only do they remove huge amounts of carbon, they also fuel entire marine food webs. Diatoms use dissolved silica to build the walls of their cells. These dense, glass-like structures mean diatoms sink more quickly than other phytoplankton and therefore increase the transfer of carbon to the sea floor where it may be stored for millennia. This makes diatoms major players in the global carbon cycle. That's why our team decided to look at how climate-change-driven ocean acidification might affect this process. We exposed a natural Antarctic phytoplankton community to increasing levels of acidity. We then measured the rate at which the whole community used dissolved silica to build their cells, as well as the rates of individual species within the community. More acid means less silicone The more acidic the seawater, the more the diatom communities were made up of smaller species, reducing the total amount of silica they produced. Less silica means the diatoms aren't heavy enough to sink quickly, reducing the rate at which they float down to the sea bed, safely storing carbon away from the atmosphere. On examining individual cells, we found many of the species were highly sensitive to increased acidity, reducing their individual silicification rates by 35-80%. These results revealed not only are communities changing, but species that remain in the community are building less-dense cell walls. Most alarming, many of the species were affected at ocean pH levels predicted for the end of this century, adding to a growing body of evidence showing significant ecological implications of climate change will take effect much sooner than previously anticipated. Marine diversity is in decline These losses in silica production could have far reaching consequences for the biology and chemistry of our oceans. Many species affected are also an important component of the diet of the Antarctic krill, which is central to the Antarctic marine food web. Fewer diatoms sinking to the ocean floor mean significant changes in silicon cycling and carbon burial. In a time when carbon drawn down by our ocean is crucial to helping sustain our atmospheric systems, any loss from this process will exacerbate CO₂ pollution. Our new research adds yet another group of organisms to the list of climate change casualties. It emphasises the urgent need to reduce our dependency on fossil fuels. The only course of action to prevent catastrophic climate change is to stop emitting CO₂. We need to cut our emissions soon, if we hope to keep our oceans from becoming too acidic to sustain healthy marine ecosystems. This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no part may be reproduced without the written permission. The content is provided for information purposes only. Let us know if there is a problem with our content Use this form if you have come across a typo, inaccuracy or would like to send an edit request for the content on this page. For general inquiries, please use our contact form. For general feedback, use the public comments section below (please adhere to guidelines). Please select the most appropriate category to facilitate processing of your request Your message to the editors Your email (only if you want to be contacted back) Thank you for taking time to provide your feedback to the editors. Your feedback is important to us. However, we do not guarantee individual replies due to the high volume of messages. E-mail the story Acid oceans are shrinking plankton, fueling faster climate change Note Your email address is used only to let the recipient know who sent the email. Neither your address nor the recipient's address will be used for any other purpose. The information you enter will appear in your e-mail message and is not retained by Phys.org in any form. Your message Newsletter sign up Get weekly and/or daily updates delivered to your inbox. You can unsubscribe at any time and we'll never share your details to third parties."}
{"url": "https://www.biodiversitylibrary.org/item/108760", "text": "Review My PDF Generate My PDF If you are generating a PDF of a journal article or book chapter, please feel free to enter the title and author information. The information you enter here will be stored in the downloaded file to assist you in managing your downloaded PDFs locally. Thank you for your request. Please wait for an email containing a link to download the PDF. For your reference, the confirmation number for this request is . Join Our Mailing List Sign up to receive the latest BHL news, content highlights, and promotions."}
{"url": "https://en.m.wikipedia.org/wiki/Scarab_(artifact)", "text": "Scarab (artifact) Scarabs are beetle-shaped amulets and impression seals which were widely popular throughout ancient Egypt. They still survive in large numbers today, and are popular among collectors of ancient artifacts. Through their inscriptions and typology, they prove to be an important source of information for archaeologists and historians of the ancient world, and represent a significant body of ancient Egyptian art.[1] Though primarily worn as amulets and sometimes rings, scarabs were also inscribed for use as personal or administrative seals or were incorporated into other kinds of jewelry. Some scarabs were created for political or diplomatic purposes to commemorate or advertise royal achievements. Additionally, scarabs held religious significance and played a role in Egyptian funerary practices.[2] Likely due to their connections to the Egyptian god Khepri, amulets in the form of scarab beetles became enormously popular in Ancient Egypt by the early Middle Kingdom (approx. 2000 BC) and remained popular for the rest of the pharaonic period and beyond.[3] By the end of the First Intermediate Period (about 2055 BC) scarabs had become extremely common. They largely replaced cylinder seals and circular \"button seals\" with simple geometric designs. Throughout the period in which they were made, scarabs were often engraved with the names of pharaohs and other royal figures. In the Middle Kingdom, scarabs were also engraved with the names and titles of officials, to be used as official seals. During the New Kingdom and Third Intermediate Period, scarabs with short prayers or mottos became popular, though these scarabs are somewhat difficult to translate.[5] Scarabs were typically carved or molded in the form of a scarab beetle (usually identified as Scarabaeus sacer) with varying degrees of naturalism but usually at least indicating the head, wing case and legs but with a flat base. The base was usually inscribed with designs or hieroglyphs to form an impression seal. They were usually drilled from end to end to allow them to be strung on a thread or incorporated into a swivel ring. The common length for standard scarabs is between 6 mm and 40 mm and most are between 10 mm and 20 mm. Larger scarabs were made from time to time for particular purposes, such as the commemorative scarabs of Amenhotep III.[6] Scarabs were generally either carved from stone, or molded from Egyptian faience, a type of Ancient Egyptian sintered-quartz ceramic. Once carved, they would typically be glazed blue or green and then fired. The most common stone used for scarabs was a form of steatite, a soft stone that becomes hard when fired (forming enstatite), or porcelain.[7] In contrast, hardstone scarabs most commonly were composed of green jasper, amethyst and carnelian. From the late Old Kingdom onwards, scarab rings developed from simple scarabs tied to fingers with threads into rings with scarab bezels in the Middle Kingdom, and further into rings with cast scarabs in the New Kingdom, typically strung on gold wire rather than string. Bezels emerged during the Old Kingdom period, often as amulets which were meant to represent Ra, the Egyptian solar god. Scarabs used for jewelry and rings were often composed of glazed steatite, which was a popular medium in ancient Egypt, though the glaze on many of these rings has been eroded over time due to weathering.[8] While the majority of scarabs would originally have been green or blue, much of the colored glazes have become discolored or erased by the elements over time, leaving most steatite scarabs appearing white or brown. Scarabs are identified as the dung beetle Scarabaeus sacer, pictured here rolling a ball of dung. In ancient Egypt, the Scarab Beetle was a highly significant symbolic representation of the divine manifestation of the morning sun. The Egyptian god Khepri was believed to roll the sun across the sky each day at daybreak. In a similar fashion, some beetles of the family Scarabaeidae use their legs to roll dung into balls. Ancient Egyptians believed this action was symbolic of the sun's east to west journey across the sky.[9] Thus, the scarab was seen as a reflection of the eternal cycle of life and was characterized as representing the idea of rebirth and regeneration.[10][11] The scarab has ties to themes of manifestation and growth, and scarabs have been found all across Egypt which originate from many different periods in Egyptian history. Scarabs have also been found inside of sunken ships, like one discovered in Uluburun, Turkey, which was inscribed with the name of the Egyptian queen Nefertiti. This scarab was among many luxury items excavated from the wreckage. Its unique inscription provides a framework of time for when the sinkage took place. This discovery gives ancient historians insight into the nature of Bronze Age trading goods and commercial networks of exchange within the Mediterranean.[12] Scarab amulets were sometimes placed in tombs as part of the deceased's personal effects or jewelry, though not all scarabs had an association with ancient Egyptian funerary practices. There are, however, three types of scarabs that seem to be specifically related to ancient funerary practices: heart scarabs, pectoral scarabs and naturalistic scarabs. Heart scarabs became popular in the early New Kingdom and remained in use until the Third Intermediate Period. They are typically 4 cm-12 cm long, and are often made from dark green or black stone not pierced for suspension. The heart was the most significant internal organ to ancient Egyptians, as they believed it to be the center of intellect and the mind. Therefore, the heart was left inside the deceased's body during the mummification process, while the other viscera were removed for separate preservation.[13] To determine safe passage into the underworld, ancient Egyptians performed the \"weighing of the heart\" rite, which utilized heart scarabs. Heart scarabs were often hung around the mummy's neck with a gold wire and the scarab itself was held in a gold frame. The base of a heart scarab was usually carved, either directly or on a gold plate fixed to the base, with hieroglyphs which name the deceased and repeat some or all of spell 30B from the Book of the Dead. The spell commands the deceased's heart not to give evidence against the deceased when he/she is being judged by the gods of the underworld.[14][15] From the Twenty-fifth Dynasty onwards, large (typically 3–8 cm long), relatively flat uninscribed pectoral scarabs were sewn together with a pair of separately made outstretched wings, onto the chests of mummies via holes formed at the edge of the scarab. Pectoral scarabs appear to be associated with the god Khepri, who is often depicted in the same form.[16] Naturalistic scarabs are relatively small (typically 2 cm to 3 cm long), made from a wide variety of hardstones and Egyptian Faience, and are distinguished from other scarabs by their naturalistic carved three dimensional bases, which often also include an integral suspension loop running widthways. Groups of these funerary scarabs, often made from different materials, formed part of the battery of amulets which were believed by ancient Egyptians to protect mummies throughout the Late Period. Ancient Egyptians believed that when a person died and underwent their final judgement, the gods of the underworld would ask many detailed and intricate questions which had to be answered precisely and ritually, according to the Book of the Dead. Since many ancient Egyptians were illiterate, even placing a copy of this scroll in their coffin would not be enough to protect them from judgment for giving a wrong answer. As a result, the priests would read the questions and their appropriate answers to the beetle, which would then be killed, mummified, and placed in the ear of the deceased. It was believed that when the gods then asked their questions, the ghostly scarab would whisper the correct answer into the ear of the supplicant, who could then answer the gods wisely and correctly. Amenhotep III (the immediate predecessor of Akhenaten) is famed for having commemorative scarabs manufactured. These were large (mostly between 3.5 cm and 10 cm long) and made of steatite, a grayish-green or brown colored talc. These scarabs were intricately crafted, created under royal supervision, and carried lengthy inscriptions describing one of five important events in his reign (all of which mention his queen, Tiye). More than 200 of these have survived, and the locations in which they have been discovered suggest they were sent out as royal gifts and propaganda in support of Egyptian diplomatic activities. The crafting of these large scarabs was a continuation of an earlier Eighteenth Dynasty tradition of making scarabs to celebrate specific royal achievements, such as the erection of obelisks at major temples during the reign of Thuthmosis III. This tradition was revived centuries later during the Twenty-fifth Dynasty, when the Kushite pharaoh Shabaka (721–707 BC) had large scarabs made to commemorate his victories in imitation of those previously produced for Amenhotep III.[17] Scarabs are often found inscribed with the names of pharaohs and more rarely with the names of their queens and other members of the royal family. Generally, there is a correlation between how long a king or queen ruled and how many scarabs have been found bearing one or more of their names. Famously, a golden scarab of Nefertiti was discovered in the Uluburun ship wreck. Most scarabs bearing a royal name can reasonably be dated to the period in which the person named lived. However, there are a number of important exceptions. Scarabs have been found bearing the names of pharaohs of the Old Kingdom (particularly of well-known kings such as Khufu, Khafre and Unas). It is now believed these were produced in later periods, most probably during the Twenty-fifth Dynasty or Twenty-sixth Dynasty, when there was considerable interest in and imitation of the works of well-established kings of the past. Scarabs have also been found in vast numbers bearing the throne name of the New Kingdom King Thutmose III (1504–1450 BC) Men Kheper Re. Many of these scarabs date from the long and successful reign of this warrior pharaoh or shortly thereafter, but the majority do not. Like all pharaohs, Thuthmosis was regarded as a god after his death. Unlike most pharaohs, his cult, centered on his mortuary temple, seems to have continued for years, if not centuries. As a result, many scarabs bearing the inscription Men Kheper Re are likely to commemorate Thuthmosis III but may have been produced hundreds of years later. Later pharaohs adopted the same throne name (including Piye of the Twenty-fifth Dynasty, 747–716 BC) leading to some confusion. The hieroglyphs making Men Kheper Re seem to have become regarded as a protective charm in themselves and were inscribed on scarabs without any specific reference to Thuthmosis III. It can be doubted that in many cases the carver understood the meaning of the inscription but reproduced it blindly. On a lesser scale the same may be true of the throne name of Rameses II (1279–1212 BC) User Maat Re (\"the justice of Ra is powerful\"), which is commonly found on scarabs which otherwise do not appear to date from his reign. The birth names of pharaohs were also popular names among private individuals and so, for example, a scarab simply bearing the name \"Amenhotep\" need not be associated with any particular king who also bore that name. The significance of a scarab bearing a royal name is unclear and probably changed over time and from scarab to scarab. Many may simply have been made privately in honor of a ruler during or after his lifetime. Some may also have been royal gifts. In some cases, scarabs with royal names may have been official seals or badges of office, perhaps connected with the royal estates or household. Others, although relatively few, may have been personal seals owned by the royal individual named on them. As the king fulfilled many different roles in ancient Egyptian society, so scarabs naming a pharaoh may have had a direct or indirect connection with a wide range of private and public activities. During the late Middle Kingdom (1850-1782 BC), significant cultural and political developments led to scarabs being inscribed with the names and titles of non-royal individuals, usually officials within the bureaucracy.[18] These scarabs exhibit extreme hieroglyphic precision unmatched in other periods, including early Middle Kingdom, Second Intermediate Period, and start of the 18th Dynasty.[19] Although the scarab ceased its utilitarian use as a personal seal soon after the collapse of the Middle Kingdom, it retained its religious and magical importance throughout the dynastic period.[20] Phoenician seal engravers adopted the scarab from the Egyptians in the period of the Achaemenid Empire empire, from the later sixth century BC to the mid-fourth century BC. The majority of these scarabs have been unearthed in the western Phoenician (Punic) burial grounds of Carthage, Sardinia, and Ibiza, with numerous others originating in the Eastern Mediterranean.[21] The city of Tharros on Sardinia was a major center of production and distribution, and scarabs were transported to the Etruscans in the 5th century by Greek/Phoenician merchants.[22] The Etruscan scarab was most popular in Vulci and Tarquinia from the last decades of the 6th century BC. By the 5th century BC, Phoenician scarabs were carved with not only Egyptian themes but also Etruscan and western Greek imagery. The innovations include Egyptianizing (the standard of Phoenicia), native Levantine (more Syrian in style and subject matter), and Hellenizing (mainly following late Archaic Greek subject matter and styles, also called Graeco-Phoenician). Canaanite scarabs imitate contemporary Egyptian late Middle Kingdom designs whilst also introducing new decorative elements and symbols.[23] Scarabs made by Canaanite artisans show extensive use of hatching and cross-hatching on the bodies of the various figures, representations of animals and humans, and the use of the palm branch.[24][25] Anra scarabs are scarab seals dating to the Second Intermediate Period found in the Levant, Nubia, and Egypt. As anra scarabs have overwhelmingly been found in Palestine (~80%), it has been suggested it was marketed by the contemporaneous 15th Dynasty for the Canaanites.[26]: 277 Anra scarabs are identified by an undeciphered and variable sequence of Egyptian hieroglyphs on the base of the scarab. In Stephen Sommers' The Mummy (1999), the scarab is used as a deadly, ancient beetle that eats the internal and external organs, killing whom ever it comes into contact with. In The Twilight Zone episode Queen of the Nile, the main character Pamela Morris has an ancient scarab beetle amulet that can drain the youth of anyone she places it on, enabling her to remain young forever. Morris tells her final victim that she got it from \"the pharaohs, who understood its power.\" In Disney's animated movie Aladdin, the location of the Cave of Wonders is revealed when two halves of a scarab beetle are joined. Scarabs are used as the monetary unit of planet Sauria (originally known as Dinosaur Planet) in the 2002 video game Star Fox Adventures. In Dungeons and Dragons, there is a magic item called the Scarab of Protection. It protects its wearer against deathly curses and similar effects, usually caused by undead monsters and necromancy. However, each scarab can only stop so many of these attacks before it is destroyed, crumbling to dust. Budge, 1977, (1926). The Dwellers on the Nile,E. A. Wallis Budge, (Dover Publications), c 1977, (originally, c 1926, by Religious Tract Society, titled as: The Dwellers on the Nile: Chapter of the Life, History, Religion and Literature of the Ancient-Egyptians); pp 265–268: \"account of the hunting of wild cattle by Amenhetep III\", \"taken from a great Scarab\"; (there are 16 registers-(lines) of hieroglyphs); (softcover, ISBN0-486-23501-7) Ward, John, and F. L. Griffith. The Sacred Beetle: A Popular Treatise on Egyptian Scarabs in Art and History. Five hundred examples of Scarabs and cylinders, the translations by F. Llewellyn Griffith. London: John Murray, 1902. OCLC1853124"}
{"url": "https://en.m.wikipedia.org/wiki/Aristotle%27s_biology", "text": "The theory describes five major biological processes, namely metabolism, temperature regulation, information processing, embryogenesis, and inheritance. Each was defined in some detail, in some cases sufficient to enable modern biologists to create mathematical models of the mechanisms described. Aristotle's method, too, resembled the style of science used by modern biologists when exploring a new area, with systematic data collection, discovery of patterns, and inference of possible causal explanations from these. He did not perform experiments in the modern sense, but made observations of living animals and carried out dissections. He names some 500 species of bird, mammal, and fish; and he distinguishes dozens of insects and other invertebrates. He describes the internal anatomy of over a hundred animals, and dissected around 35 of these. Apart from his pupil, Theophrastus, who wrote a matching Enquiry into Plants, no research of comparable scope was carried out in ancient Greece, though Hellenistic medicine in Egypt continued Aristotle's inquiry into the mechanisms of the human body. Aristotle's biology was influential in the medieval Islamic world. Translation of Arabic versions and commentaries into Latin brought knowledge of Aristotle back into Western Europe, but the only biological work widely taught in medieval universities was On the Soul. The association of his work with medieval scholasticism, as well as errors in his theories, caused Early Modern scientists such as Galileo and William Harvey to reject Aristotle. Criticism of his errors and secondhand reports continued for centuries. He has found better acceptance among zoologists, and some of his long-derided observations in marine biology have been found in modern times to be true. Aristotle (384–322 BC) studied at Plato's Academy in Athens, remaining there for about 20 years. Like Plato, he sought universals in his philosophy, but unlike Plato he backed up his views with detailed and systematic observation, notably of the natural history of the island of Lesbos, where he spent about two years, and the marine life in the seas around it, especially of the Pyrrha lagoon in the island's centre.[1] This study made him the earliest scientist whose written work survives. No similarly detailed work on zoology was attempted until the sixteenth century; accordingly Aristotle remained highly influential for some two thousand years. He returned to Athens and founded his own school, the Lycaeum, where he taught for the last dozen years of his life. His writings on zoology form about a quarter of his surviving work.[2] Aristotle's pupil Theophrastus later wrote a similar book on botany, Enquiry into Plants.[3] Aristotle's biology is constructed on the basis of his theory of form, which is derived from Plato's theory of Forms, but significantly different from it. Plato's Forms were eternal and fixed, being \"blueprints in the mind of God\".[4] Real things in the world could, in Plato's view, at best be approximations to these perfect Forms. Aristotle heard Plato's view and developed it into a set of three biological concepts. He uses the same Greek word, εἶδος (eidos), to mean first of all the set of visible features that uniquely characterised a kind of animal. Aristotle used the word γένος (génos) to mean a kind.[a] For example, the kind of animal called a bird has feathers, a beak, wings, a hard-shelled egg, and warm blood.[4] Aristotle further noted that there are many bird forms within the bird kind – cranes, eagles, crows, bustards, sparrows, and so on, just as there are many forms of fishes within the fish kind. He sometimes called these atoma eidē, indivisible forms.[b]Human is one of these indivisible forms: Socrates and the rest of us are all different individually, but we all have human form.[4] More recent studies have shown that Aristotle used the terms γένος (génos) and εἶδος (eidos) in a relative way. A taxon that is considered an eidos in one context can be considered a génos (which includes various eide) in another.[5] Finally, Aristotle observed that the child does not take just any form, but is given it by the parents' seeds, which combine. These seeds thus contain form, or in modern terms information.[c] Aristotle makes clear that he sometimes intends this third sense by giving the analogy of a woodcarving. It takes its form from wood (its material cause); the tools and carving technique used to make it (its efficient cause); and the design laid out for it (its eidos or embedded information). Aristotle further emphasises the informational nature of form by arguing that a body is compounded of elements like earth and fire, just as a word is compounded of letters in a specific order.[d][4] The five processes formed what Aristotle called the soul: it was not something extra, but the system consisting exactly of these mechanisms. The Aristotelian soul died with the animal and was thus purely biological. Different types of organism possessed different types of soul. Plants had a vegetative soul, responsible for reproduction and growth. Animals had both a vegetative and a sensitive soul, responsible for mobility and sensation. Humans, uniquely, had a vegetative, a sensitive, and a rational soul, capable of thought and reflection.[6][9][10] Metabolism: Leroi's open system model. Food is converted to the body's uniform parts and excreted residues.[11] Aristotle's account of metabolism sought to explain how food was processed by the body to provide both heat and the materials for the body's construction and maintenance. The metabolic system for live-bearing tetrapods[f] described in the Parts of Animals can be modelled as an open system, a branching tree of flows of material through the body.[11] The system worked as follows. The incoming material, food, enters the body and is concocted into blood; waste is excreted as urine, bile, and faeces, and the element fire is released as heat. Blood is made into flesh, the rest forming other earthy tissues such as bones, teeth, cartilages and sinews. Leftover blood is made into fat, whether soft suet or hard lard. Some fat from all around the body is made into semen.[11][12] All the tissues are in Aristotle's view completely uniform parts with no internal structure of any kind; a cartilage for example was the same all the way through, not subdivided into atoms as Democritus (c. 460–c. 370 BC) had argued.[13] The uniform parts can be arranged on a scale of Aristotelian qualities, from the coldest and driest, such as hair, to the hottest and wettest, such as milk.[11][12] At each stage of metabolism, residual materials are excreted as faeces, urine, and bile.[11][12] Temperature regulation: Leroi's model based on Youth and Old Age, Life and Death 26.[11][12] Aristotle's account of temperature regulation sought to explain how an animal maintained a steady temperature and the continued oscillation of the thorax needed for breathing. The system of regulation of temperature and breathing described in Youth and Old Age, Life and Death 26 is sufficiently detailed to permit modelling as a negative feedbackcontrol system (one that maintains a desired property by opposing disturbances to it), with a few assumptions such as a desired temperature to compare the actual temperature against.[14] The system worked as follows. Heat is constantly lost from the body. Food products reach the heart and are processed into new blood, releasing fire during metabolism, which raises the blood temperature too high. That raises the heart temperature, causing lung volume to increase, in turn raising the airflow at the mouth. The cool air brought in through the mouth reduces the heart temperature, so the lung volume accordingly decreases, restoring the temperature to normal.[g][14] The mechanism only works if the air is cooler than the reference temperature. If the air is hotter than that, the system becomes a positive feedback cycle, the body's fire is put out, and death follows. The system as described damps out fluctuations in temperature. Aristotle however predicted that his system would cause lung oscillation (breathing), which is possible given extra assumptions such as of delays or non-linear responses.[14][16] Information processing: Leroi's \"centralized incoming and outgoing motions model\" of an animal's \"sensitive soul\"; the heart is the seat of perception.[17] Aristotle's information processing model has been named the \"centralized incoming and outgoing motions model\". It sought to explain how changes in the world led to appropriate behaviour in the animal.[17] The system worked as follows. The animal's sense organ is altered when it detects an object. This causes a perceptual change in the animal's seat of sensation, which Aristotle believed was the heart (cardiocentrism) rather than the brain. This in turn causes a change in the heart's heat, which causes a quantitative change sufficient to make the heart transmit a mechanical impulse to a limb, which moves, moving the animal's body. The alteration in the heat of the heart also causes a change in the consistency of the joints, which helps the limb to move.[17] There is thus a causal chain which transmits information from a sense organ to an organ capable of making decisions, and onwards to a motor organ. In this respect, the model is analogous to a modern understanding of information processing such as in sensory-motor coupling.[18][17] Aristotle's inheritance model sought to explain how the parents' characteristics are transmitted to the child, subject to influence from the environment.[19][h] The system worked as follows. The father's semen and the mother's menses have movements that encode their parental characteristics.[19][20] The model is partly asymmetric, as only the father's movements define the form or eidos of the species, while the movements of both the father's and the mother's uniform parts define features other than the form, such as the father's eye colour or the mother's nose shape.[19] Aristotle's theory has some symmetry, as semen movements carry maleness while the menses carry femaleness. If the semen is hot enough to overpower the cold menses, the child will be a boy; but if it is too cold to do this, the child will be a girl. Inheritance is thus particulate (definitely one trait or another), as in Mendelian genetics, unlike the Hippocratic model which was continuous and blending.[19] The child's sex can be influenced by factors that affect temperature, including the weather, the wind direction, diet, and the father's age. Features other than sex also depend on whether the semen overpowers the menses, so if a man has strong semen, he will have sons who resemble him, while if the semen is weak, he will have daughters who resemble their mother.[i][19] Aristotle's model of embryogenesis sought to explain how the inherited parental characteristics cause the formation and development of an embryo.[21] The system worked as follows. First, the father's semen curdles the mother's menses, which Aristotle compares with how rennet (an enzyme from a cow's stomach) curdles milk in cheesemaking. This forms the embryo; it is then developed by the action of the pneuma (literally, breath or spirit) in the semen. The pneuma first makes the heart appear; this is vital, as the heart nourishes all other organs. Aristotle observed that the heart is the first organ seen to be active (beating) in a hen's egg. The pneuma then makes the other organs develop.[21] Aristotle asserts in his Physics that according to Empedocles, order \"spontaneously\" appears in the developing embryo. In The Parts of Animals, he argues that what he describes as a theory of Empedocles, that the vertebral column is divided into vertebrae because, as it happens, the embryo twists about and snaps the column into pieces, is wrong. Aristotle argues instead that the process has a predefined goal: that the \"seed\" that develops into the embryo began with an inbuilt \"potential\" to become specific body parts, such as vertebrae. Further, each sort of animal gives rise to animals of its own kind: humans only have human babies.[22] Aristotle has been called unscientific[23] by philosophers from Francis Bacon onwards[23] for at least two reasons: his scientific style,[24] and his use of explanation. His explanations are in turn made cryptic by his complicated system of causes.[23] However, these charges need to be considered in the light of what was known in his own time.[23] His systematic gathering of data, too, is obscured by the lack of modern methods of presentation, such as tables of data: for example, the whole of History of Animals Book VI is taken up with a list of observations of the life histories of birds that \"would now be summarized in a single table in Nature – and in the Online Supplementary Information at that\".[25] Aristotle inferred growth laws from his observations on animals, including that brood size decreases with body mass, whereas gestation period increases. He was correct in these predictions, at least for mammals: data are shown for mouse and elephant. Aristotle did not do experiments in the modern sense.[26] He used the ancient Greek term pepeiramenoi to mean observations, or at most investigative procedures,[27] such as (in Generation of Animals) finding a fertilised hen's egg of a suitable stage and opening it so as to be able to see the embryo's heart inside.[28] Instead, he practised a different style of science: systematically gathering data, discovering patterns common to whole groups of animals, and inferring possible causal explanations from these.[24][29] This style is common in modern biology when large amounts of data become available in a new field, such as genomics. It does not result in the same certainty as experimental science, but it sets out testable hypotheses and constructs a narrative explanation of what is observed. In this sense, Aristotle's biology is scientific.[24] From the data he collected and documented, Aristotle inferred quite a number of rules relating the life-history features of the live-bearing tetrapods (terrestrial placental mammals[j]) that he studied. Among these correct predictions are the following. Brood size decreases with (adult) body mass, so that an elephant has fewer young (usually just one) per brood than a mouse. Lifespan increases with gestation period, and also with body mass, so that elephants live longer than mice, have a longer period of gestation, and are heavier. As a final example, fecundity decreases with lifespan, so long-lived kinds like elephants have fewer young in total than short-lived kinds like mice.[30] Aristotle used the analogy of the movement of water through a porous pot (an oenochoe shown) to help explain biological processes as mechanisms. Aristotle's use of explanation has been considered \"fundamentally unscientific\".[23] The French playwright Molière's 1673 play The Imaginary Invalid portrays the quack Aristotelian doctor Argan blandly explaining that opium causes sleep by virtue of its dormitive [sleep-making] principle, its virtus dormitiva.[k][31] Argan's explanation is at best empty (devoid of mechanism),[23] at worst vitalist. But the real Aristotle did provide biological mechanisms, in the form of the five processes of metabolism, temperature regulation, information processing, embryonic development, and inheritance that he developed. Further, he provided mechanical, non-vitalist analogies for these theories, mentioning bellows, toy carts, the movement of water through porous pots, and even automatic puppets.[23] Aristotle was the first person to study biology systematically. He spent two years observing and describing the zoology of Lesbos and the surrounding seas, including in particular the Pyrrha lagoon in the centre of Lesbos.[1][33] His data are assembled from his own observations, statements given by people with specialised knowledge such as beekeepers and fishermen, and less accurate accounts provided by travellers from overseas.[34] His observations on catfish, electric fish (Torpedo) and angler fish are detailed, as is his writing on cephalopods including the octopus, cuttlefish and paper nautilus.[35] He reported that fishermen had asserted that the octopus’s hectocotyl arm was used in sexual reproduction.[36][37] He admitted its use in mating 'only for the sake of attachment', but rejected the idea that it was useful for generation, since \"it is outside the passage and indeed outside the body\".[38] In the 19th century, biologists found that the reported function was correct. He separated the aquatic mammals from fish, and knew that sharks and rays were part of the group he called Selachē (roughly, the modern zoologist's selachians[l]).[35] Among many other things, he gave accurate descriptions of the four-chambered stomachs of ruminants, and of the ovoviviparous embryological development of the dogfish.[40][41] His accounts of about 35 animals are sufficiently detailed to convince biologists that he dissected those species,[42] indeed vivisecting some;[43] he mentions the internal anatomy of roughly 110 animals in total.[42] Animals with blood included live-bearing tetrapods, Zōiotoka tetrapoda (roughly, the mammals), being warm, having four legs, and giving birth to their young. The cetaceans, Kētōdē, also had blood and gave birth to live young, but did not have legs, and therefore formed a separate group[n] (megista genē, defined by a set of functioning \"parts\"[49]).[50] The birds, Ornithes had blood and laid eggs, but had only 2 legs and were a distinct form (eidos) with feathers and beaks instead of teeth, so they too formed a distinct group, of over 50 kinds. The egg-bearing tetrapods, Ōiotoka tetrapoda (reptiles and amphibians) had blood and four legs, but were cold and laid eggs, so were a distinct group. The snakes, Opheis, similarly had blood, but no legs, and laid dry eggs, so were a separate group. The fishes, Ikhthyes, had blood but no legs, and laid wet eggs, forming a definite group. Among them, the selachians Selakhē (sharks and rays), had cartilages instead of bones[47] and were viviparous (Aristotle did not know that some selachians are oviparous).[51] Aristotle reported correctly that electric rays were able to stun their prey. Aristotle stated in the History of Animals that all beings were arranged in a fixed scale of perfection, reflected in their form (eidos).[o] They stretched from minerals to plants and animals, and on up to man, forming the scala naturae or great chain of being.[52][53] His system had eleven grades, arranged according to the potentiality of each being, expressed in their form at birth. The highest animals gave birth to warm and wet creatures alive, the lowest bore theirs cold, dry, and in thick eggs.[35] The system was based on Aristotle's interpretation of the four elements in his On Generation and Corruption: Fire (hot and dry); Air (hot and wet); Water (cold and wet); and Earth (cold and dry). These are arranged from the most energetic to the least, so the warm, wet young raised in a womb with a placenta were higher on the scale than the cold, dry, nearly mineral eggs of birds.[54][10] However, Aristotle is careful never to insist that a group fits perfectly in the scale; he knows animals have many combinations of attributes, and that placements are approximate.[55] Aristotle's pupil and successor at the Lyceum, Theophrastus, wrote the History of Plants, the first classical book of botany. It has an Aristotelian structure, but rather than focus on formal causes, as Aristotle did, Theophrastus described how plants functioned.[56][57] Where Aristotle expanded on grand theories, Theophrastus was quietly empirical.[58] Where Aristotle insisted that species have a fixed place on the scala naturae, Theophrastus suggests that one kind of plant can transform into another, as when a field sown to wheat turns to the weed darnel.[59] After Theophrastus, though interest in Aristotle's ideas survived, they were generally taken unquestioningly.[60] It is not until the age of Alexandria under the Ptolemies that advances in biology resumed. The first medical teacher at Alexandria, Herophilus of Chalcedon, corrected Aristotle, placing intelligence in the brain, and connected the nervous system to motion and sensation. Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not.[61] The book was mentioned by Al-Kindī (d. 850), and commented on by Avicenna (Ibn Sīnā) in his Kitāb al-Šifā (کتاب الشفاء, The Book of Healing). Avempace (Ibn Bājja) and Averroes (Ibn Rushd) commented on On the Parts of Animals and Generation of Animals, Averroes criticising Avempace's interpretations.[66] When the Christian Alfonso VI of Castileretook the Kingdom of Toledo from the Moors in 1085, an Arabic translation of Aristotle's works, with commentaries by Avicenna and Averroes emerged into European medieval scholarship. Michael Scot translated much of Aristotle's biology into Latin, c. 1225, along with many of Averroes's commentaries.[p]Albertus Magnus commented extensively on Aristotle, but added his own zoological observations and an encyclopedia of animals based on Thomas of Cantimpré. Later in the 13th century, Thomas Aquinas merged Aristotle's metaphysics with Christian theology. Whereas Albert had treated Aristotle's biology as science, writing that experiment was the only safe guide and joining in with the types of observation that Aristotle had made, Aquinas saw Aristotle purely as theory, and Aristotelian thought became associated with scholasticism.[66] The scholastic natural philosophy curriculum omitted most of Aristotle's biology, but included On the Soul.[68] Renaissance zoologists made use of Aristotle's zoology in two ways. Especially in Italy, scholars such as Pietro Pomponazzi and Agostino Nifo lectured and wrote commentaries on Aristotle. Elsewhere, authors used Aristotle as one of their sources, alongside their own and their colleagues' observations, to create new encyclopedias such as Konrad Gessner's 1551 Historia Animalium.[q] The title and the philosophical approach were Aristotelian, but the work was largely new. Edward Wotton similarly helped to found modern zoology by arranging the animals according to Aristotle's theories, separating out folklore from his 1552 De differentiis animalium.[68][69] Aristotle's system of classification had thus remained influential for many centuries.[70][51][71][72] Charles Darwinquoted a passage from Aristotle's Physics II 8 in The Origin of Species, which entertains the possibility of a selection process following the random combination of body parts. Darwin comments that \"We here see the principle of natural selection shadowed forth\".[83] However, two things mitigate against this interpretation. Firstly, Aristotle immediately rejected the possibility of such a process of assembling body parts. Secondly, according to Leroi, Aristotle was in any case discussing ontogeny, the Empedoclean coming into being of an individual from component parts, not phylogeny and natural selection.[84] Darwin considered Aristotle the most important early contributor to biological thought; in an 1882 letter he wrote that \"Linnaeus and Cuvier have been my two gods, though in very different ways, but they were mere schoolboys to old Aristotle.\"[85][86] Zoologists have frequently mocked Aristotle for errors and unverified secondhand reports. However, modern observation has confirmed one after another of his more surprising claims,[68] including the active camouflage of the octopus[87] and the ability of elephants to snorkel with their trunks while swimming.[88] Aristotle did not write anything that resembles a modern, unified textbook of biology. Instead, he wrote a large number of \"books\" which, taken together, give an idea of his approach to the science. Some of these interlock, referring to each other, while others, such as the drawings of The Anatomies are lost, but referred to in the History of Animals, where the reader is instructed to look at the diagrams to understand how the animal parts described are arranged,[96] and it has even been possible to reconstruct (admittedly with much associated uncertainty) what some of these illustrations may have looked like, from Aristotle's descriptions.[97] Aristotle's main biological works are the five books sometimes grouped as On Animals (De Animalibus), namely, with the conventional abbreviations shown in parentheses: ^The English and taxonomic Latin genus derive from this, and have related meanings. ^In modern terms, it has been argued that these roughly correspond to species, and some texts use that translation. Aristotle did not formulate a definition resembling that of a modern species, however, and some of his forms are other taxa such as genera or families. ^First Doctor: Most learned bachelor / Whom I esteem and honor, I would like to ask you the cause and reason why / Opium makes one sleep. Argan [the Aristotelian]: ... The reason is that in opium resides / A dormitive virtue, Of which it is the nature / To stupefy the senses.[31] ^It is not safe to assume that species or groups with Linnean names that resemble Aristotle's are the animals he was referring to, as zoologists including Linnaeus guessed rightly or wrongly what Aristotle meant in his short descriptions. Sometimes an ancient Greek name must mean exactly one species – hippos is definitely horse, when it's a land animal; but sometimes a name referred to several similar species, as English names often do today: for instance, kephalos means any of 4 species of grey mullet.[39] ^Aristotle did not know that complex invertebrates do make use of haemoglobin, but of a different kind from vertebrates. ^As a father to the science, he stands alone. The next figures significant enough to be named in MarineBio's history, for example, are Captain James Cook and Charles Darwin, some two millennia later.[90] ^Leroi has written several papers on the subject, cited in his book, and made a BBC film[92] about it."}
{"url": "https://www.biodiversitylibrary.org/bibliography/542", "text": "Join Our Mailing List Get Involved Harmful Content BHL acknowledges the existence of harmful content in many biodiversity science publications and original materials included in its collection. Please read BHL's Acknowledgment of Harmful Content for more information. Tools and Services BHL offers a wide range of free tools and services to support the use and re-use of our collections and data. Now Online BHL Consortium BHL operates as a worldwide consortium of natural history, botanical, research, and national libraries working together to digitize the natural history literature held in their collections and make it freely available for open access as part of a global \"biodiversity community.\""}
{"url": "https://en.m.wikipedia.org/wiki/Parakeet", "text": "Contents The name parakeet is derived from the French word perroquet, which is reflected in some older spellings that are still sometimes encountered, including paroquet or paraquet. However, in modern French, perruche is used to refer to parakeets and similar-sized parrots.[1] The Australian budgerigar, or shell parakeet, is a popular pet and the most common parakeet Parakeets comprise about 115 species of birds that are seed-eating parrots of small size, slender build, and long, tapering tails.[citation needed] The Australianbudgerigar, also known as \"budgie\", Melopsittacus undulatus, is probably the most common parakeet. It was first described by zoologists in 1891. It is the most popular species of parakeet kept as a pet in North America and Europe. The term \"grass parakeet\" (or grasskeet) refers to many small Australian parakeets native to grasslands such as the genus Neophema and the princess parrot. The Australian rosellas are also parakeets. Many of the smaller, long-tailed species of lories may be referred to as \"lorikeets\". The vernacular name ring-necked parakeet (not to be confused with the Australian ringneck) refers to a species of the genus Psittacula native to Africa and Asia that is popular as a pet and has become feral in many cities outside its natural range. In aviculture, the term \"conure\" is used for small to medium-sized parakeets of the genera Aratinga, Pyrrhura, and a few other genera of the tribeArini, which are mainly endemic to South America. As they are not all from one genus, taxonomists tend to avoid the term. Other South American species commonly called parakeets include the genus Brotogeris parakeets, the monk parakeet, and lineolated parakeets, although lineolateds have short tails. A larger species may be referred to as \"parrot\" or \"parakeet\" interchangeably. For example, \"Alexandrine parrot\" and \"Alexandrine parakeet\" are two common names for the same species, Psittacula eupatria, which is one of the largest species normally referred to as a parakeet. Many different species of parakeets are bred and sold commercially as pets, the budgerigar being the third most popular pet in the world,[3] after cats and dogs. Parakeets often breed more readily in groups; however, there can be conflicts between breeding pairs and individuals especially if space is limited. The presence of other parakeets encourages a pair to breed, which is why breeding in a group is better. Despite this, many breeders choose to breed in pairs to both avoid conflicts and know offspring's parentage with certainty. Parakeets lay an average of 4-8 eggs, while budgerigars (a species of parakeet) lay an average of 4-6 eggs.[citation needed]"}
{"url": "https://en.m.wikipedia.org/wiki/Bird_of_prey", "text": "The term raptor is derived from the Latin word rapio, meaning \"to seize or take by force\".[7] The common names for various birds of prey are based on structure, but many of the traditional names do not reflect the evolutionary relationships between the groups. Variations in shape and size Eagles tend to be large, powerful birds with long, broad wings and massive feet. Booted eagles have legs and feet feathered to the toes and build very large stick nests. Falcons and kestrels are medium-size birds of prey with long pointed wings, and many are particularly swift flyers. They belong to the family Falconidae, only distantly related to the Accipitriformes above. Caracaras are a distinct subgroup of the Falconidae unique to the New World, and most common in the Neotropics – their broad wings, naked faces and appetites of a generalist suggest some level of convergence with either Buteo or the vulturine birds, or both. True hawks are medium-sized birds of prey that usually belong to the genus Accipiter (see below). They are mainly woodland birds that hunt by sudden dashes from a concealed perch. They usually have long tails for tight steering. Buzzards are medium-large raptors with robust bodies and broad wings, or, alternatively, any bird of the genus Buteo (also commonly known as \"hawks\" in North America, while \"buzzard\" is colloquially used for vultures). Harriers are large, slender hawk-like birds with long tails and long thin legs. Most use a combination of keen eyesight and hearing to hunt small vertebrates, gliding on their long broad wings and circling low over grasslands and marshes. Kites have long wings and relatively weak legs. They spend much of their time soaring. They will take live vertebrate prey, but mostly feed on insects or even carrion. The osprey, a single species found worldwide that specializes in catching fish and builds large stick nests. Owls are variable-sized, typically night-specialized hunting birds. They fly almost silently due to their special feather structure that reduces turbulence. They have particularly acute hearing and nocturnal eyesight. The secretarybird is a single species with a large body and long, stilted legs endemic to the open grasslands of Sub-Saharan Africa. The taxonomy of Carl Linnaeus grouped birds (class Aves) into orders, genera, and species, with no formal ranks between genus and order. He placed all birds of prey into a single order, Accipitres, subdividing this into four genera: Vultur (vultures), Falco (eagles, hawks, falcons, etc.), Strix (owls), and Lanius (shrikes). This approach was followed by subsequent authors such as Gmelin, Latham and Turton. Louis Pierre Vieillot used additional ranks: order, tribe, family, genus, species. Birds of prey (order Accipitres) were divided into diurnal and nocturnal tribes; the owls remained monogeneric (family Ægolii, genus Strix), whilst the diurnal raptors were divided into three families: Vulturini, Gypaëti, and Accipitrini.[9] Thus Vieillot's families were similar to the Linnaean genera, with the difference that shrikes were no longer included amongst the birds of prey. In addition to the original Vultur and Falco (now reduced in scope), Vieillot adopted four genera from Savigny: Phene, Haliæetus, Pandion, and Elanus. He also introduced five new genera of vultures (Gypagus, Catharista, Daptrius, Ibycter, Polyborus)[note 1] and eleven new genera of accipitrines (Aquila, Circaëtus, Circus, Buteo, Milvus, Ictinia, Physeta, Harpia, Spizaëtus, Asturina, Sparvius). Falconimorphae is a deprecated superorder within Raptores, formerly composed of the orders Falconiformes and Strigiformes. The clade was invalidated after 2012. Falconiformes is now placed in Eufalconimorphae, while Strigiformes is placed in Afroaves.[10] The order Accipitriformes is believed to have originated 44 million years ago when it split from the common ancestor of the secretarybird (Sagittarius serpentarius) and the accipitrid species.[11] The phylogeny of Accipitriformes is complex and difficult to unravel. Widespread paraphylies were observed in many phylogenetic studies.[12][13][14][15][16] More recent and detailed studies show similar results.[17] However, according to the findings of a 2014 study, the sister relationship between larger clades of Accipitriformes was well supported (e.g. relationship of Harpagus kites to buzzards and sea eagles and these latter two with Accipiter hawks are sister taxa of the clade containing Aquilinae and Harpiinae).[11] The diurnal birds of prey are formally classified into six families of three different orders (Accipitriformes, Falconiformes and Cariamiformes). These families (with the exception of Cariamidae) were traditionally grouped together in a single order Falconiformes but are now split into two orders, the Falconiformes and Accipitriformes. The Cathartidae are sometimes placed separately in an enlarged stork family, Ciconiiformes, and may be raised to an order of their own, Cathartiiformes. The secretary bird and/or osprey are sometimes listed as subfamilies of Acciptridae: Sagittariinae and Pandioninae, respectively. Below is a simplified phylogeny of Telluraves which is the clade where the birds of prey belong to along with passerines and several near-passerine lineages.[18][10][19] The orders in bold text are birds of prey orders; this is to show the paraphyly of the group as well as their relationships to other birds. A recent phylogenomic study from Wu et al. (2024) has found an alternative phylogeny for the placement of the birds of prey. Their analysis has found support in a clade consisting of the Strigiformes and Accipitrimorphae in new clade Hieraves. Hieraves was also recovered to be the sister clade to Australaves (which it includes the Cariamiformes and Falconiformes along with Psittacopasserae). Below is their phylogeny from the study.[20] The earliest event occurred nearly 14 to 12 million years ago. This result seems to be one of the oldest dates published so far in the case of birds of prey.[11] For example, a previous reconstruction of migratory behaviour in one Buteo clade[16] with a result of the origin of migration around 5 million years ago was also supported by that study. Migratory species of raptors may have had a southern origin because it seems that all of the major lineages within Accipitridae had an origin in one of the biogeographic realms of the Southern Hemisphere. The appearance of migratory behaviour occurred in the tropics parallel with the range expansion of migratory species to temperate habitats.[11] Similar results of southern origin in other taxonomic groups can be found in the literature.[21][22][23] Distribution and biogeographic history highly determine the origin of migration in birds of prey. Based on some comparative analyses, diet breadth also has an effect on the evolution of migratory behaviour in this group,[11] but its relevance needs further investigation. The evolution of migration in animals seems to be a complex and difficult topic with many unanswered questions. A recent study discovered new connections between migration and the ecology, life history of raptors. A brief overview from abstract of the published paper shows that \"clutch size and hunting strategies have been proved to be the most important variables in shaping distribution areas, and also the geographic dissimilarities may mask important relationships between life history traits and migratory behaviours. The West Palearctic-Afrotropical and the North-South American migratory systems are fundamentally different from the East Palearctic-Indomalayan system, owing to the presence versus absence of ecological barriers.\"[24] Maximum entropy modelling can help in answering the question: why species winters at one location while the others are elsewhere. Temperature and precipitation related factors differ in the limitation of species distributions. \"This suggests that the migratory behaviours differ among the three main migratory routes for these species\"[24] which may have important conservational consequences in the protection of migratory raptors. Birds of prey (raptors) are known to display patterns of sexual dimorphism. It is commonly believed that the dimorphisms found in raptors occur due to sexual selection or environmental factors. In general, hypotheses in favor of ecological factors being the cause for sexual dimorphism in raptors are rejected. This is because the ecological model is less parsimonious, meaning that its explanation is more complex than that of the sexual selection model. Additionally, ecological models are much harder to test because a great deal of data is required.[25] Dimorphisms can also be the product of intrasexual selection between males and females. It appears that both sexes of the species play a role in the sexual dimorphism within raptors; females tend to compete with other females to find good places to nest and attract males, and males competing with other males for adequate hunting ground so they appear as the most healthy mate.[26] It has also been proposed that sexual dimorphism is merely the product of disruptive selection, and is merely a stepping stone in the process of speciation, especially if the traits that define gender are independent across a species. Sexual dimorphism can be viewed as something that can accelerate the rate of speciation.[27] In non-predatory birds, males are typically larger than females. However, in birds of prey, the opposite is the case. For instance, the kestrel is a type of falcon in which males are the primary providers, and the females are responsible for nurturing the young. In this species, the smaller the kestrels are, the less food is needed and thus, they can survive in environments that are harsher. This is particularly true in the male kestrels. It has become more energetically favorable for male kestrels to remain smaller than their female counterparts because smaller males have an agility advantage when it comes to defending the nest and hunting. Larger females are favored because they can incubate larger numbers of offspring, while also being able to brood a larger clutch size.[28] It is a long-standing belief that birds lack any sense of smell, but it has become clear that many birds do have functional olfactory systems. Despite this, most raptors are still considered to primarily rely on vision, with raptor vision being extensively studied. A 2020 review of the existing literature combining anatomical, genetic, and behavioural studies showed that, in general, raptors have functional olfactory systems that they are likely to use in a range of different contexts.[29] Birds of prey have been historically persecuted both directly and indirectly. In the Danish Faroe Islands, there were rewards Naebbetold (by royal decree from 1741) given in return for the bills of birds of prey shown by hunters. In Britain, kites and buzzards were seen as destroyers of game and killed, for instance in 1684-5 alone as many as 100 kites were killed. Rewards for their killing were also in force in the Netherlands from 1756. From 1705 to 1800, it has been estimated that 624087 birds of prey were killed in a part of Germany that included Hannover, Luneburg, Lauenburg and Bremen with 14125 claws deposited just in 1796–97.[30] Many species also develop lead poisoning after accidental consumption of lead shot when feeding on animals that had been shot by hunters.[31] Lead pellets from direct shooting that the birds have escaped from also cause reduced fitness and premature deaths.[32] Some evidence supports the contention that the African crowned eagle occasionally views human children as prey, with a witness account of one attack (in which the victim, a seven-year-old boy, survived and the eagle was killed),[33] and the discovery of part of a human child skull in a nest. This would make it the only living bird known to prey on humans, although other birds such as ostriches and cassowaries have killed humans in self-defense and a lammergeier might have killed Aeschylus by accident.[34] Many stories of Brazilian indigenous peoples speak about children mauled by Uiruuetê, the Harpy Eagle in Tupi language.[citation needed] Various large raptors like golden eagles are reported attacking human beings,[35] but its unclear if they intend to eat them or if they have ever been successful in killing one. Birds of prey have incredible vision and rely heavily on it for a number of tasks.[38] They utilize their high visual acuity to obtain food, navigate their surroundings, distinguish and flee from predators, mating, nest construction, and much more. They accomplish these tasks with a large eye in relation to their skull, which allows for a larger image to be projected onto the retina.[38] The visual acuity of some large raptors such as eagles and Old World vultures are the highest known among vertebrates; the wedge-tailed eagle has twice the visual acuity of a typical human and six times that of the common ostrich, the vertebrate with the largest eyes.[39] There are two regions in the retina, called the deep and shallow fovea, that are specialized for acute vision.[40] These regions contain the highest density of photoreceptors, and provide the highest points of visual acuity. The deep fovea points forward at an approximate 45° angle, while the shallow fovea points approximately 15° to the right or left of the head axis.[40] Several raptor species repeatedly cock their heads into three distinct positions while observing an object. First, is straight ahead with their head pointed towards the object. Second and third are sideways to the right or left of the object, with their head axis positioned approximately 40° adjacent to the object. This movement is believed to be associated with lining up the incoming image to fall on the deep fovea. Raptors will choose which head position to use depending on the distance to the object. At distances as close as 8m, they used primarily binocular vision. At distances greater than 21m, they spent more time using monocular vision. At distances greater than 40m, they spent 80% or more time using their monocular vision. This suggests that raptors tilt their head to rely on the highly acute deep fovea.[40] Like all birds, raptors possess tetrachromacy, however, due to their emphasis on visual acuity, many diurnal birds of prey have little ability to see ultraviolet light as this produces chromatic aberration which decreases the clarity of vision.[41]"}
{"url": "https://speakingofresearch.com/2013/12/12/eu-statistics-show-decline-in-animal-research-numbers/", "text": "EU statistics show Decline in Animal Research Numbers The EU has just put up its Seventh statistical report (and additional detailed stats), which provides the 2011 statistics of the numbers of animals used in research (though France provided 2010 stats). It shows that since 2008 (the sixth report), the number of animals has dropped over 4%, from 12 million down to 11.5 million animals (11,481,521 to be exact). The exact reason for this drop in the number of animals used is hard to pinpoint, however it is likely that the recession and subsequent economic stagnation across much of the EU may have reduced the amount of biomedical research (animal and non-animal) being conducted – in countries where the biomedical funding has risen, such as the UK, the numbers of animals in research has risen. Increasing work and funding into the 3Rs is also helping scientists find ways to reduce and replace (and refine) some animal research. The stats do provide a breakdown according to species (see pages 7-10). Mice, rats, fish, amphibians and reptiles together account for over 85% of research animals. Dogs and cats accounted for 0.03% and 0.16% respectively (and are included within the 0.25% of Carnivores used in research). Primates account for 0.05% of animals – primate use was also down by one third from the 2011 stats. Great apes are not used in the EU. Breakdown of the percentages of different species of animals used in research across the EU in 2011 There was a drop in the number of most animal species used, notably: 22-24% falls in the numbers of rats, guinea pigs and hamsters, a 9% drop in the number of cats, and 16% drop in the number of dogs used. Even the numbers of mice fell by 120,000. The main rise came from an increase in the number of fish used from 1.09 million to 1.40 million (a 29% rise). Click to Enlarge We can see how this breaks down among EU member states (Note: these statistics did not regard the breeding of a genetically altered animal (GAA) as a procedure to be counted, this will account for discrepancies between these numbers and those provided some member states e.g. the UK reports 3.7 million animals were used in 2011 including breeding. Future statistical reports should include breeding animals). France, Germany and the UK have the highest number of animals in research – they are also the three largest EU economies. Click to Enlarge Clear reporting of the numbers of animals used in research is an important step in openness."}
{"url": "https://en.m.wikipedia.org/wiki/Special:BookSources/978-0-691-12085-0", "text": "This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). If you arrived at this page by clicking an ISBN link in a Wikipedia page, you will find the full range of relevant search links for that specific book by scrolling to the find links below. To search for a different book, type that book's individual ISBN into this ISBN search box. Spaces and hyphens in the ISBN do not matter. Also, the number starts after the colon for \"ISBN-10:\" and \"ISBN-13:\" numbers. An ISBN identifies a specific edition of a book. Any given title may therefore have a number of different ISBNs. See #Find other editions below for finding other editions. An ISBN registration, even one corresponding to a book page on a major book distributor database, is not definite proof that such a book actually exists. A title may have been cancelled or postponed after the ISBN was assigned. Check to see if the book exists or not. Google Books and Amazon.com may be helpful if you want to verify citations in Wikipedia articles, because they often let you search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available). At the Open Library (part of the Internet Archive) you can borrow and read entire books online. Luxembourg Montenegro Netherlands Find this book in the Dutch-Union Catalogue that searches simultaneously in more than 400 Dutch electronic library systems (including regional libraries, university libraries, research libraries and the Royal Dutch library) Book-swapping websites Non-English book sources If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language."}
{"url": "https://www.the-scientist.com/mixing-it-up-in-the-web-of-life-65431", "text": "Many types of marine plankton are either animal-like or plant-like. But a huge number are both, and they are upending ideas about ocean ecology. Their color gave them away. Ecologist Diane Stoecker was looking at plankton in samples of ocean water from the dock in Woods Hole Harbor in Massachusetts some 40 years ago when she spotted something strange. Under the microscope, she recognized Laboea strobila, shaped like an ice-cream cone—“yellowish green and very beautiful,” she recalls—and the smaller, more spherical Strombidium species—also oddly greenish. Stoecker knew that these single-celled critters, named ciliates for the hairlike cilia that they bear, got their energy by feeding on other, smaller organisms. So why were the ones she saw so green—a color that generally signifies photosynthesis? Was the pigment leftover food, ingested algae or just the algae’s chloroplasts? After some groundbreaking experiments, Stoecker was one of the first scientists to describe how these types of plankton not only hunted their prey, but also sequestered the chloroplasts of their food sources and used..."}
{"url": "https://en.m.wikipedia.org/wiki/Urmetazoan", "text": "Molecular studies place animals in a supergroup called the opisthokonts, which also includes the choanoflagellates, fungi, and a few small parasitic protists. The name comes from the posterior location of the flagellum in motile cells, such as most animal spermatozoa, whereas other eukaryotes tend to have anterior flagella as well. Several different hypotheses for the animals' last common ancestor have been suggested. The placula hypothesis, proposed by Otto Bütschli, holds that the last common ancestor of animals was an amorphous blob with no symmetry or axis. The center of this blob rose slightly above the silt, forming a hollow that aided feeding on the sea floor underneath. As the cavity grew deeper and deeper, the organisms resembled a thimble, with an inside and an outside.[2] This body shape is found in sponges and cnidaria. This explanation leads to the formation of the bilaterian body plan; the urbilaterian would develop its symmetry when one end of the placula became adapted for forward movement, resulting in left-right symmetry.[2] The planula hypothesis, proposed by Bütschli, suggests that metazoa are derived from planula; that is, the larva of certain cnidaria, or the adult form of the placozoans. Under this hypothesis, the larva became sexually mature through paedomorphosis, and could reproduce without passing through a sessile phase. The gastraea hypothesis was proposed by Ernst Haeckel in 1874,[3] shortly after his work on the calcareous sponges. He proposed that this group of sponges is monophyletic with all eumetazoans, including the bilaterians. This suggests that the gastrulation and the gastrula stage are universal for eumetazoans. It has been perceived as problematic that gastrulation by invagination is by no means universal among eumetazoans. Only recently has an invagination been confirmed in a Calcarea sponge, albeit too early to form a remaining inner space (archenteron).[4] The bilaterogastraea hypothesis was developed by Gösta Jägersten as an adaptation of Ernst Haeckel's Gastraea hypothesis. He proposed that the Bilaterogastraea have a two-stage life cycle, with a pelagic juvenile and a benthic adult stage. The invagination of the original gastrula stage he saw as bilaterally symmetric rather than radially symmetric."}
{"url": "https://en.m.wikipedia.org/wiki/Special:BookSources/978-0-85112-235-9", "text": "This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). If you arrived at this page by clicking an ISBN link in a Wikipedia page, you will find the full range of relevant search links for that specific book by scrolling to the find links below. To search for a different book, type that book's individual ISBN into this ISBN search box. Spaces and hyphens in the ISBN do not matter. Also, the number starts after the colon for \"ISBN-10:\" and \"ISBN-13:\" numbers. An ISBN identifies a specific edition of a book. Any given title may therefore have a number of different ISBNs. See #Find other editions below for finding other editions. An ISBN registration, even one corresponding to a book page on a major book distributor database, is not definite proof that such a book actually exists. A title may have been cancelled or postponed after the ISBN was assigned. Check to see if the book exists or not. Google Books and Amazon.com may be helpful if you want to verify citations in Wikipedia articles, because they often let you search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available). At the Open Library (part of the Internet Archive) you can borrow and read entire books online. Luxembourg Montenegro Netherlands Find this book in the Dutch-Union Catalogue that searches simultaneously in more than 400 Dutch electronic library systems (including regional libraries, university libraries, research libraries and the Royal Dutch library) Book-swapping websites Non-English book sources If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language."}
{"url": "https://web.archive.org/web/20110212005358/https://animaldiversity.ummz.umich.edu/site/accounts/information/Dendrobatidae.html", "text": "Several representatives of this family of small, diurnal frogs are famous for their bright skin coloration and associated toxins. There are from four to seven genera in this family, and less than 200 species. Distribution is restricted to humid tropical America, from Nicaragua to Brazil, with the greatest diversity in northwestern South America. Dendrobatids are the most brightly colored of any anuran group. Although the large genus Colostethus is drab-colored and non-toxic, the derived \"aposematically colored dendrobatids\" (members of the genera Dendrobates, Phyllobates, and their kin) have skin toxins comprised of lipophilic alkaloids, some of which can easily kill a human if ingested. The only other anuran group exhibiting both bright colors and lipophilic alkaloids are the unrelated Mantellas (Rhacophoridae). Dendrobatids are small (20-40 mm snout-vent length, though the putative basal species, the nocturnal Aromobates, attains lengths of 62 mm). Synapomorphies of this family include the presence of a retroarticular process of the mandible, and the arrangement of the superficial slip of the m. depressor mandibulae. Additional possible synapomorphies include the tendon of the semitendinosus piercing the m. gracilis major and m. gracilis minor, divided dermal scutes on the dorsal surfaces of fingers, cephalic amplexus (amplexus is entirely lacking in some species), and the transport of aquatic larvae on the dorsum of the parent. Almost all dendrobatids are diurnal. Most are terrestrial; some are arboreal. The common name, dart-poison frogs, is derived from a practice of the Indians of the Ember· Choc&#243 in Colombia, in which they rub their blowgun darts onto the backs of Phyllobates terribilis to load the darts with poison (Myers et al., 1978). Dendrobatids are ant and mite specialists, and some researchers have argued that their skin toxins are derived from precursor molecules in the ants they eat (e.g. Caldwell 1996). The combination of aposematic coloration and diurnal habit has allowed members of most dendrobatid species to clump in space, largely freed from the risks of predation. Complex forms of sociality, territoriality, courtship, and parental care have evolved in many of these species. In all species for which there is data, tadpoles are carried on the back of the adult (sometimes the male, sometimes the female), usually from a terrestrial oviposition site to water. In several species of Dendrobates, the female carries tadpoles individually to the water-filled axils of bromeliads or treeholes, depositing a single tadpole into each crevice visited. The mother later returns, repeatedly, to each of her tadpoles, which have reduced beaks and denticles, and deposits unfertilized eggs for them to eat. Several species of dendrobatids are popular in the pet trade. Dendrobatids are neobatrachians, but relationships among the families of these \"advanced\" frogs are almost wholly unresolved. Within the Neobatrachia, dendrobatids have sometimes been placed in the Ranoidea, and sometimes in the Bufonoidea. Ambiguity at this level of history points to the utter confusion that has supplanted all attempts to make sense of dendrobatid evolution. Current hypotheses of relationship include dendrobatids and petropedetine ranids as sister taxa, dendrobatids and arthroleptids, and dendrobatids and hylodine leptodactylids. Other authors have simply placed Dendrobatidae within the Ranoidea, in part due to several characters they have that are plesiomorphic to Ranoidea (e.g. cartilaginous sternum, horizontal pupil, unnotched tongue). The dendrobatids are probably monophyletic. Within Dendrobatidae, Aromobates is usually considered basal. Finally, some genera currently in use have not undergone rigorous phylogenetic analysis (Minyobates and Epipedobates, members of which previously belonged to Dendrobates), and it is questionable if they will persist. There are no known dendrobatid fossils. Caldwell, J. P. 1996. The evolution of myrmecophagy and its correlates in poison frogs (Family Dendrobatidae). Journal of Zoology 240:75-101. Ford, L.S. 1990. The Phylogenetic Position of Poison-dart Frogs (Dendrobatidae): Reassesment of the Neobatrachian Phylogeny with Commentary on Complex Character Systems. Ph.D. Dissertation, The University of Kansas, Lawrence, Kansas. Disclaimer: The Animal Diversity Web is an educational resource written largely by and for college students. ADW doesn't cover all species in the world, nor does it include all the latest scientific information about organisms we describe. Though we edit our accounts for accuracy, we cannot guarantee all information in those accounts. While ADW staff and contributors provide references to books and websites that we believe are reputable, we cannot necessarily endorse the contents of references beyond our control."}
{"url": "https://en.m.wikipedia.org/wiki/Myocyte", "text": "Cardiac muscle cells form the cardiac muscle in the walls of the heart chambers, and have a single central nucleus.[7] Cardiac muscle cells are joined to neighboring cells by intercalated discs, and when joined in a visible unit they are described as a cardiac muscle fiber.[8] Smooth muscle cells control involuntary movements such as the peristalsis contractions in the esophagus and stomach. The Smooth muscle has no myofibrils or sarcomeres and is therefore non-striated. Smooth muscle cells have a single nucleus. Skeletal muscle cells are the individual contractile cells within a muscle and are more usually known as muscle fibers because of their longer threadlike appearance.[10] A single muscle such as the biceps brachii in a young adult human male contains around 253,000 muscle fibers.[11] Skeletal muscle fibers are the only muscle cells that are multinucleated with the nuclei usually referred to as myonuclei. This occurs during myogenesis with the fusion of myoblasts each contributing a nucleus to the newly formed muscle cell or myotube.[12] Fusion depends on muscle-specific proteins known as fusogens called myomaker and myomerger.[13] A striated muscle fiber contains myofibrils consisting of long protein chains of myofilaments. There are three types of myofilaments: thin, thick, and elastic that work together to produce a muscle contraction.[14] The thin myofilaments are filaments of mostly actin and the thick filaments are of mostly myosin and they slide over each other to shorten the fiber length in a muscle contraction. The third type of myofilament is an elastic filament composed of titin, a very large protein. In striations of muscle bands, myosin forms the dark filaments that make up the A band. Thin filaments of actin are the light filaments that make up the I band. The smallest contractile unit in the fiber is called the sarcomere which is a repeating unit within two Z bands. The sarcoplasm also contains glycogen which provides energy to the cell during heightened exercise, and myoglobin, the red pigment that stores oxygen until needed for muscular activity.[14] The sarcoplasmic reticulum, a specialized type of smooth endoplasmic reticulum, forms a network around each myofibril of the muscle fiber. This network is composed of groupings of two dilated end-sacs called terminal cisternae, and a single T-tubule (transverse tubule), which bores through the cell and emerge on the other side; together these three components form the triads that exist within the network of the sarcoplasmic reticulum, in which each T-tubule has two terminal cisternae on each side of it. The sarcoplasmic reticulum serves as a reservoir for calcium ions, so when an action potential spreads over the T-tubule, it signals the sarcoplasmic reticulum to release calcium ions from the gated membrane channels to stimulate muscle contraction.[14][15] In skeletal muscle, at the end of each muscle fiber, the outer layer of the sarcolemma combines with tendon fibers at the myotendinous junction.[16][17] Within the muscle fiber pressed against the sarcolemma are multiply flattened nuclei; embryologically, this multinucleate condition results from multiple myoblasts fusing to produce each muscle fiber, where each myoblast contributes one nucleus.[14] Cardiac muscle like the skeletal muscle is also striated and the cells contain myofibrils, myofilaments, and sarcomeres as the skeletal muscle cell. The cell membrane is anchored to the cell's cytoskeleton by anchor fibers that are approximately 10 nm wide. These are generally located at the Z lines so that they form grooves and transverse tubules emanate. In cardiac myocytes, this forms a scalloped surface.[18] The cytoskeleton is what the rest of the cell builds off of and has two primary purposes; the first is to stabilize the topography of the intracellular components and the second is to help control the size and shape of the cell. While the first function is important for biochemical processes, the latter is crucial in defining the surface-to-volume ratio of the cell. This heavily influences the potential electrical properties of excitable cells. Additionally, deviation from the standard shape and size of the cell can have a negative prognostic impact.[18] Smooth muscle cells are spindle-shaped with wide middles, and tapering ends. They have a single nucleus and range from 30 to 200 micrometers in length. This is thousands of times shorter than skeletal muscle fibers. The diameter of their cells is also much smaller which removes the need for T-tubules found in striated muscle cells. Although smooth muscle cells lack sarcomeres and myofibrils they do contain large amounts of the contractile proteins actin and myosin. Actin filaments are anchored by dense bodies (similar to the Z discs in sarcomeres) to the sarcolemma.[19] Myoblasts in skeletal muscle that do not form muscle fibers dedifferentiate back into myosatellite cells. These satellite cells remain adjacent to a skeletal muscle fiber, situated between the sarcolemma and the basement membrane[23] of the endomysium (the connective tissue investment that divides the muscle fascicles into individual fibers). To re-activate myogenesis, the satellite cells must be stimulated to differentiate into new fibers. When contracting, thin and thick filaments slide concerning each other by using adenosine triphosphate. This pulls the Z discs closer together in a process called the sliding filament mechanism. The contraction of all the sarcomeres results in the contraction of the whole muscle fiber. This contraction of the myocyte is triggered by the action potential over the cell membrane of the myocyte. The action potential uses transverse tubules to get from the surface to the interior of the myocyte, which is continuous within the cell membrane. Sarcoplasmic reticula are membranous bags that transverse tubules touch but remain separate from. These wrap themselves around each sarcomere and are filled with Ca2+.[26] Excitation of a myocyte causes depolarization at its synapses, the neuromuscular junctions, which triggers an action potential. With a singular neuromuscular junction, each muscle fiber receives input from just one somatic efferent neuron. Action potential in a somatic efferent neuron causes the release of the neurotransmitter acetylcholine.[27] When the acetylcholine is released it diffuses across the synapse and binds to a receptor on the sarcolemma, a term unique to muscle cells that refers to the cell membrane. This initiates an impulse that travels across the sarcolemma.[28] When the action potential reaches the sarcoplasmic reticulum it triggers the release of Ca2+ from the Ca2+ channels. The Ca2+ flows from the sarcoplasmic reticulum into the sarcomere with both of its filaments. This causes the filaments to start sliding and the sarcomeres to become shorter. This requires a large amount of ATP, as it is used in both the attachment and release of every myosin head. Very quickly Ca2+ is actively transported back into the sarcoplasmic reticulum, which blocks the interaction between the thin and thick filament. This in turn causes the muscle cell to relax.[28] There are four main types of muscle contraction: twitch, treppe, tetanus, and isometric/isotonic. Twitch contraction is the process in which a single stimulus signals a single contraction. In twitch contraction, the length of the contraction may vary depending on the size of the muscle cell. During treppe (or summation) contraction muscles do not start at maximum efficiency; instead, they achieve increased strength of contraction due to repeated stimuli. Tetanus involves a sustained contraction of muscles due to a series of rapid stimuli, which can continue until the muscles fatigue. Isometric contractions are skeletal muscle contractions that do not cause movement of the muscle. However, isotonic contractions are skeletal muscle contractions that do cause movement.[28] The evolutionary origin of muscle cells in animals is highly debated: One view is that muscle cells evolved once, and thus all muscle cells have a single common ancestor. Another view is that muscles cells evolved more than once, and any morphological or structural similarities are due to convergent evolution, and the development of shared genes that predate the evolution of muscle – even the mesoderm (the mesoderm is the germ layer that gives rise to muscle cells in vertebrates). Schmid & Seipel (2005)[29] argue that the origin of muscle cells is a monophyletic trait that occurred concurrently with the development of the digestive and nervous systems of all animals, and that this origin can be traced to a single metazoan ancestor in which muscle cells are present. They argue that molecular and morphological similarities between the muscles cells in Cnidaria and Ctenophora are similar enough to those of bilaterians that there would be one ancestor in metazoans from which muscle cells derive. In this case, Schmid & Seipel argue that the last common ancestor of Bilateria, Ctenophora and Cnidaria, was a triploblast (an organism having three germ layers), and that diploblasty, meaning an organism with two germ layers, evolved secondarily, because of their observation of the lack of mesoderm or muscle found in most cnidarians and ctenophores. By comparing the morphology of cnidarians and ctenophores to bilaterians, Schmid & Seipel were able to conclude that there were myoblast-like structures in the tentacles and gut of some species of cnidarians and the tentacles of ctenophores. Since this is a structure unique to muscle cells, these scientists determined based on the data collected by their peers that this is a marker for striated muscles similar to that observed in bilaterians. The authors also remark that the muscle cells found in cnidarians and ctenophores are often contested due to the origin of these muscle cells being the ectoderm rather than the mesoderm or mesendoderm. The origin of true muscle cells is argued by other authors to be the endoderm portion of the mesoderm and the endoderm. However, Schmid & Seipel (2005)[29] counter skepticism – about whether the muscle cells found in ctenophores and cnidarians are \"true\" muscle cells – by considering that cnidarians develop through a medusa stage and polyp stage. They note that in the hydrozoans' medusa stage, there is a layer of cells that separate from the distal side of the ectoderm, which forms the striated muscle cells in a way similar to that of the mesoderm; they call this third separated layer of cells the ectocodon. Schmid & Seipel argue that even in bilaterians, not all muscle cells are derived from the mesendoderm: Their key examples are that in both the eye muscles of vertebrates, and the muscles of spiralians, these cells derive from the ectodermal mesoderm, rather than the endodermal mesoderm. Furthermore, they argue that since myogenesis does occur in cnidarians with the help of the same molecular regulatory elements found in the specification of muscle cells in bilaterians, that there is evidence for a single origin for striated muscle.[29] In contrast to this argument for a single origin of muscle cells, Steinmetz, Kraus, et al. (2012)[30] argue that molecular markers such as the myosin II protein used to determine this single origin of striated muscle predate the formation of muscle cells. They use an example of the contractile elements present in the Porifera, or sponges, that do truly lack this striated muscle containing this protein. Furthermore, Steinmetz, Kraus, et al. present evidence for a polyphyletic origin of striated muscle cell development through their analysis of morphological and molecular markers that are present in bilaterians and absent in cnidarians, ctenophores, and bilaterians. Steinmetz, Kraus, et al. showed that the traditional morphological and regulatory markers such as actin, the ability to couple myosin side chains phosphorylation to higher concentrations of the positive concentrations of calcium, and other MyHC elements are present in all metazoans not just the organisms that have been shown to have muscle cells. Thus, the usage of any of these structural or regulatory elements in determining whether or not the muscle cells of the cnidarians and ctenophores are similar enough to the muscle cells of the bilaterians to confirm a single lineage is questionable according to Steinmetz, Kraus, et al. Furthermore, they explain that the orthologues of the Myc genes that have been used to hypothesize the origin of striated muscle occurred through a gene duplication event that predates the first true muscle cells (meaning striated muscle), and they show that the Myc genes are present in the sponges that have contractile elements but no true muscle cells. Steinmetz, Kraus, et al. also showed that the localization of this duplicated set of genes that serve both the function of facilitating the formation of striated muscle genes, and cell regulation and movement genes, were already separated into striated much and non-muscle MHC. This separation of the duplicated set of genes is shown through the localization of the striated much to the contractile vacuole in sponges, while the non-muscle much was more diffusely expressed during developmental cell shape and change. Steinmetz, Kraus, et al. found a similar pattern of localization in cnidarians except with the cnidarian N. vectensis having this striated muscle marker present in the smooth muscle of the digestive tract. Thus, they argue that the pleisiomorphic trait of the separated orthologues of much cannot be used to determine the monophylogeny of muscle, and additionally argue that the presence of a striated muscle marker in the smooth muscle of this cnidarian shows a fundamental different mechanism of muscle cell development and structure in cnidarians.[30] Steinmetz, Kraus, et al. (2012)[30] further argue for multiple origins of striated muscle in the metazoans by explaining that a key set of genes used to form the troponin complex for muscle regulation and formation in bilaterians is missing from the cnidarians and ctenophores, and 47 structural and regulatory proteins observed, Steinmetz, Kraus, et al. were not able to find even on unique striated muscle cell protein that was expressed in both cnidarians and bilaterians. Furthermore, the Z-disc seemed to have evolved differently even within bilaterians and there is a great deal of diversity of proteins developed even between this clade, showing a large degree of radiation for muscle cells. Through this divergence of the Z-disc, Steinmetz, Kraus, et al. argue that there are only four common protein components that were present in all bilaterians muscle ancestors and that of these for necessary Z-disc components only an actin protein that they have already argued is an uninformative marker through its pleisiomorphic state is present in cnidarians. Through further molecular marker testing, Steinmetz et al. observe that non-bilaterians lack many regulatory and structural components necessary for bilaterians muscle formation and do not find any unique set of proteins to both bilaterians and cnidarians and ctenophores that are not present in earlier, more primitive animals such as the sponges and amoebozoans. Through this analysis, the authors conclude that due to the lack of elements that bilaterian muscles are dependent on for structure and usage, nonbilaterian muscles must be of a different origin with a different set of regulatory and structural proteins.[30] In another take on the argument, Andrikou & Arnone (2015)[31] use the newly available data on gene regulatory networks to look at how the hierarchy of genes and morphogens and another mechanism of tissue specification diverge and are similar among early deuterostomes and protostomes. By understanding not only what genes are present in all bilaterians but also the time and place of deployment of these genes, Andrikou & Arnone discuss a deeper understanding of the evolution of myogenesis.[31] In their paper, Andrikou & Arnone (2015)[31] argue that to truly understand the evolution of muscle cells the function of transcriptional regulators must be understood in the context of other external and internal interactions. Through their analysis, Andrikou & Arnone found that there were conserved orthologues of the gene regulatory network in both invertebrate bilaterians and cnidarians. They argue that having this common, general regulatory circuit allowed for a high degree of divergence from a single well-functioning network. Andrikou & Arnone found that the orthologues of genes found in vertebrates had been changed through different types of structural mutations in the invertebrate deuterostomes and protostomes, and they argue that these structural changes in the genes allowed for a large divergence of muscle function and muscle formation in these species. Andrikou & Arnone were able to recognize not only any difference due to mutation in the genes found in vertebrates and invertebrates but also the integration of species-specific genes that could also cause divergence from the original gene regulatory network function. Thus, although a common muscle patterning system has been determined, they argue that this could be due to a more ancestral gene regulatory network being coopted several times across lineages with additional genes and mutations causing very divergent development of muscles. Thus it seems that the myogenic patterning framework may be an ancestral trait. However, Andrikou & Arnone explain that the basic muscle patterning structure must also be considered in combination with the cis regulatory elements present at different times during development. In contrast with the high level of gene family apparatuses structure, Andrikou and Arnone found that the cis-regulatory elements were not well conserved both in time and place in the network which could show a large degree of divergence in the formation of muscle cells. Through this analysis, it seems that the myogenic GRN is an ancestral GRN with actual changes in myogenic function and structure possibly being linked to later coopts of genes at different times and places.[31] Evolutionarily, specialized forms of skeletal and cardiac muscles predated the divergence of the vertebrate / arthropod evolutionary line.[32] This indicates that these types of muscle developed in a common ancestor sometime before 700 million years ago (mya). Vertebrate smooth muscle was found to have evolved independently from the skeletal and cardiac muscle types. The properties used for distinguishing fast, intermediate, and slow muscle fibers can be different for invertebrate flight and jump muscle.[33] To further complicate this classification scheme, the mitochondrial content, and other morphological properties within a muscle fiber, can change in a tsetse fly with exercise and age.[34]"}
{"url": "https://books.google.com/books?id=EXNFwB-O-WUC&pg=PA362", "text": "Biology: Concepts and Applications without Physiology Should there be warning labels on fast foods? Should SUV drivers pay extra taxes? Should employers be allowed to require drug testing of prospective employees? This introductory biology text helps you master biology while encouraging you to think about critical issues. The issues-oriented approach enlivens the subject matter and helps you grasp concepts by making connections between biology and real-life concerns. Here's how it works: An Impacts, Issues case study opens each chapter, focusing on a biology-related societal issue. You also view a short film that expands on the issue. Then each chapter's How Would You Vote? question, inspired by the case study, asks you to consider biology-related news, explore the facts behind the issue, apply your knowledge of biology, and cast a vote on the Web. You can also see state and nationwide voting tallies. This book has been widely praised for clear and engaging writing, art with step-by-step callouts, and terrific support from student media that all work together to help you get biology. Technology highlights include CengageNOW, an interactive online resource featuring personalized study plans and hundreds of animations, and an MP3 download of audio study tools that let you review concepts from the text anywhere! Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version. About the author (2007) For decades, Cecie Starr has been known as one of the best-selling biology textbook authors. Her texts, appreciated for their clarity in both the written word and the visual representation of biological concepts, include multiple editions of BIOLOGY: THE UNITY AND DIVERSITY OF LIFE, BIOLOGY: CONCEPTS AND APPLICATIONS, and BIOLOGY TODAY AND TOMORROW. Her original dream was to become an architect. Instead of building houses, she now builds, with care and attention to detail, texts based on this philosophy: \"I invite students into a chapter through an intriguing story. Once inside, they get the great windows that biologists construct on the world of life. Biology is not just another house. It is a conceptual mansion. I hope to do it justice.\""}
{"url": "https://en.m.wikipedia.org/wiki/McFarland_%26_Company", "text": "McFarland & Company McFarland & Company, Inc., is an American independent book publisher based in Jefferson, North Carolina, that specializes in academic and reference works, as well as general-interest adult nonfiction. Its president is Rhonda Herman. Its former president and current editor-in-chief is Robert Franklin, who founded the company in 1979.[2][3] McFarland employs a staff of about 50, and as of 2019[update] had published 7,800 titles.[3][4] McFarland's initial print runs average 600 copies per book.[5] Contents McFarland & Company focuses mainly on selling to libraries. It also utilizes direct mailing to connect with enthusiasts in niche categories.[6] The company is known for its sports literature, especially baseball history, as well as books about chess, military history, and film.[7][8] In 2007, the Mountain Times wrote that McFarland publishes about 275 scholarly monographs and reference book titles a year;[4][9] Robert Lee Brewer reported in 2015 that the number is about 350.[10] ^Slide, Anthony (2010). \"A Publishing Phenomenon that Begins and Ends with Scarecrow Press\". Film History. 22 (3): 304. doi:10.2979/fil.2010.22.3.298. JSTOR10.2979/fil.2010.22.3.298. S 2 CID192112592. McFarland [...] books were primarily aimed at the library market. It was a mail order publisher with no interest in bookstore sales, but unlike its major competitor, virtually from the start all of its books were typeset. ^Slide, Anthony (2010). \"A Publishing Phenomenon that Begins and Ends with Scarecrow Press\". Film History. 22 (3): 305. doi:10.2979/fil.2010.22.3.298. JSTOR10.2979/fil.2010.22.3.298. S 2 CID192112592. Most film scholars, students and buffs would assume that McFarland's main thrust has been towards film book Publishing [but] it is the largest publisher of military memoirs and baseball-oriented titles. It is also rich in books on women's, African-American, and gender studies, on U.S. history, and is proud of its automotive line. It also boasts of being the most prestigious publisher of historical and reference books on chess."}
{"url": "https://en.m.wikipedia.org/wiki/Hemichordata", "text": "The body plan of hemichordates is characterized by a muscular organization. The anteroposterior axis is divided into three parts: the anterior prosome, the intermediate mesosome, and the posterior metasome. The body of acorn worms is worm-shaped and divided into an anterior proboscis, an intermediate collar, and a posterior trunk. The proboscis is a muscular and ciliated organ used in locomotion and in the collection and transport of food particles. The mouth is located between the proboscis and the collar. The trunk is the longest part of the animal. It contains the pharynx, which is perforated with gill slits (or pharyngeal slits), the oesophagus, a long intestine, and a terminal anus. It also contains the gonads. A post-anal tail is present in juvenile member of the acorn worm family Harrimaniidae.[5] The prosome of pterobranchs is specialized into a muscular and ciliated cephalic shield used in locomotion and in secreting the coenecium. The mesosome extends into one pair (in the genus Rhabdopleura) or several pairs (in the genus Cephalodiscus) of tentaculated arms used in filter feeding. The metasome, or trunk, contains a looped digestive tract, gonads, and extends into a contractile stalk that connects individuals to the other members of the colony, produced by asexual budding. In the genus Cephalodiscus, asexually produced individuals stay attached to the contractile stalk of the parent individual until completing their development. In the genus Rhabdopleura, zooids are permanently connected to the rest of the colony via a common stolon system. This section needs expansion. You can help by adding to it. (November 2023) Hemochordates have an open circulatory system. The heart vesicle is located dorsally within the proboscis complex, and does not contain any blood. Instead it moves the blood indirectly by pulsating against the dorsal blood vessel.[11] Together with the Echinoderms, the hemichordates form the Ambulacraria, which are the closest extant phylogenetic relatives of chordates. Thus these marine worms are of great interest for the study of the origins of chordate development. There are several species of hemichordates, with a moderate diversity of embryological development among these species. Hemichordates are classically known to develop in two ways, both directly and indirectly.[12] Hemichordates are a phylum composed of two classes, the enteropneusts and the pterobranchs, both being forms of marine worm. The enteropneusts have two developmental strategies: direct and indirect development. The indirect developmental strategy includes an extended pelagic plankotrophic tornaria larval stage, which means that this hemichordate exists in a larval stage that feeds on plankton before turning into an adult worm.[13] The Pterobranch genus most extensively studied is Rhabdopleura from Plymouth, England and from Bermuda.[14][15][16][17] The following details the development of two popularly studied species of the hemichordata phylum Saccoglossus kowalevskii and Ptychodera flava. Saccoglossus kowalevskii is a direct developer and Ptychodera flava is an indirect developer. Most of what has been detailed in Hemichordate development has come from hemichordates that develop directly. Schematic of embryonic cleavage and development in P. flava and S. kowalevskii P. flava’s early cleavage pattern is similar to that of S. kowalevskii. The first and second cleavages from the single cell zygote of P. flava are equal cleavages, are orthogonal to each other and both include the animal and vegetal poles of the embryo. The third cleavage is equal and equatorial so that the embryo has four blastomeres both in the vegetal and the animal pole. The fourth division occurs mainly in blastomeres in the animal pole, which divide transversally as well as equally to make eight blastomeres. The four vegetal blastomeres divide equatorially but unequally and they give rise to four big macromeres and four smaller micromeres. Once this fourth division has occurred, the embryo has reached a 16 cell stage. P. flava has a 16 cell embryo with four vegetal micromeres, eight animal mesomeres and 4 larger macromeres. Further divisions occur until P. flava finishes the blastula stage and goes on to gastrulation. The animal mesomeres of P. flava go on to give rise to the larva’s ectoderm, animal blastomeres also appear to give rise to these structures though the exact contribution varies from embryo to embryo. The macromeres give rise to the posterior larval ectoderm and the vegetal micromeres give rise to the internal endomesodermal tissues.[18] Studies done on the potential of the embryo at different stages have shown that at both the two and four cell stage of development P. flava blastomeres can go on to give rise to a tornaria larvae, so fates of these embryonic cells don’t seem to be established till after this stage.[19] Eggs of S. kowalevskii are oval in shape and become spherical in shape after fertilization. The first cleavage occurs from the animal to the vegetal pole and usually is equal though very often can also be unequal. The second cleavage to reach the embryos four cell stage also occurs from the animal to the vegetal pole in an approximately equal fashion though like the first cleavage it’s possible to have an unequal division. The eight cell stage cleavage is latitudinal; so that each cell from the four cell stage goes on to make two cells. The fourth division occurs first in the cells of the animal pole, which end up making eight blastomeres (mesomeres) that are not radially symmetric, then the four vegetal pole blastomeres divide to make a level of four large blastomeres (macromeres) and four very small blastomeres (micromeres). The fifth cleavage occurs first in the animal cells and then in the vegetal cells to give a 32 cell blastomere. The sixth cleavage occurs in a similar order and completes a 64 cell stage, finally the seventh cleavage marks the end of the cleavage stage with a blastula with 128 blastomeres. This structure goes on to go through gastrulation movements which will determine the body plan of the resulting gill slit larva, this larva will ultimately give rise to the marine acorn worm.[20][21] Much of the genetic work done on hemichordates has been done to make comparison with chordates, so many of the genetic markers identified in this group are also found in chordates or are homologous to chordates in some way. Studies of this nature have been done particularly on S. kowalevskii, and like chordates S. kowalevskii has dorsalizing bmp-like factors such as bmp 2/4, which is homologous to Drosophila’s decapentaplegic dpp. The expression of bmp2/4 begins at the onset of gastrulation on the ectodermal side of the embryo, and as gastrulation progresses its expression is narrowed down to the dorsal midline but is not expressed in the post anal tail. The bmp antagonist chordin is also expressed in the endoderm of gastrulating S. kowalevskii. Besides these well known dorsalizing factors, further molecules known to be involved in dorsal ventral patterning are also present in S. kowalevskii, such as a netrin that groups with netrin gene class 1 and 2.[6] Netrin is important in patterning of the neural system in chordates, as well as is the molecule Shh, but S. kowalevskii was only found to have one hh gene and it appears to be expressed in a region that is uncommon to where it is usually expressed in developing chordates along the ventral midline. Hemichordata are divided into two classes: the Enteropneusta,[22] commonly called acorn worms, and the Pterobranchia, which includes the graptolites.[23] A third class, Planctosphaeroidea, is proposed based on a single species known only from larvae. The phylum contains about 120 living species.[24] Hemichordata appears to be sister to the Echinodermata as Ambulacraria; Xenoturbellida may be basal to that grouping. Pterobranchia may be derived from within Enteropneusta, making Enteropneusta paraphyletic. It is possible that the extinct organism Etacystis is a member of the Hemichordata, either within or with close affinity to the Pterobranchia.[25] There are 130 described species of Hemichordata and many new species are being discovered, especially in the deep sea.[26]"}
{"url": "https://en.m.wikipedia.org/wiki/Predator", "text": "Predators may actively search for or pursue prey or wait for it, often concealed. When prey is detected, the predator assesses whether to attack it. This may involve ambush or pursuit predation, sometimes after stalking the prey. If the attack is successful, the predator kills the prey, removes any inedible parts like the shell or spines, and eats it. At the most basic level, predators kill and eat other organisms. However, the concept of predation is broad, defined differently in different contexts, and includes a wide variety of feeding methods; and some relationships that result in the prey's death are not generally called predation. A parasitoid, such as an ichneumon wasp, lays its eggs in or on its host; the eggs hatch into larvae, which eat the host, and it inevitably dies. Zoologists generally call this a form of parasitism, though conventionally parasites are thought not to kill their hosts. A predator can be defined to differ from a parasitoid in that it has many prey, captured over its lifetime, where a parasitoid's larva has just one, or at least has its food supply provisioned for it on just one occasion.[1][2] Relation of predation to other feeding strategies There are other difficult and borderline cases. Micropredators are small animals that, like predators, feed entirely on other organisms; they include fleas and mosquitoes that consume blood from living animals, and aphids that consume sap from living plants. However, since they typically do not kill their hosts, they are now often thought of as parasites.[3][4] Animals that graze on phytoplankton or mats of microbes are predators, as they consume and kill their food organisms; but herbivores that browse leaves are not, as their food plants usually survive the assault.[5] When animals eat seeds (seed predation or granivory) or eggs (egg predation), they are consuming entire living organisms, which by definition makes them predators.[6][7][8] Scavengers, organisms that only eat organisms found already dead, are not predators, but many predators such as the jackal and the hyena scavenge when the opportunity arises.[9][10][5] Among invertebrates, social wasps (yellowjackets) are both hunters and scavengers of other insects.[11] Seed predation is restricted to mammals, birds, and insects but is found in almost all terrestrial ecosystems.[8][6] Egg predation includes both specialist egg predators such as some colubridsnakes and generalists such as foxes and badgers that opportunistically take eggs when they find them.[17][18][19] A basic foraging cycle for a predator, with some variations indicated[25] To feed, a predator must search for, pursue and kill its prey. These actions form a foraging cycle.[26][27] The predator must decide where to look for prey based on its geographical distribution; and once it has located prey, it must assess whether to pursue it or to wait for a better choice. If it chooses pursuit, its physical capabilities determine the mode of pursuit (e.g., ambush or chase).[28][29] Having captured the prey, it may also need to expend energy handling it (e.g., killing it, removing any shell or spines, and ingesting it).[25][26] Predators have a choice of search modes ranging from sit-and-wait to active or widely foraging.[30][25][31][32] The sit-and-wait method is most suitable if the prey are dense and mobile, and the predator has low energy requirements.[30] Wide foraging expends more energy, and is used when prey is sedentary or sparsely distributed.[28][30] There is a continuum of search modes with intervals between periods of movement ranging from seconds to months. Sharks, sunfish, Insectivorous birds and shrews are almost always moving while web-building spiders, aquatic invertebrates, praying mantises and kestrels rarely move. In between, plovers and other shorebirds, freshwater fish including crappies, and the larvae of coccinellid beetles (ladybirds), alternate between actively searching and scanning the environment.[30] The black-browed albatross regularly flies hundreds of kilometres across the nearly empty ocean to find patches of food. Prey distributions are often clumped, and predators respond by looking for patches where prey is dense and then searching within patches.[25] Where food is found in patches, such as rare shoals of fish in a nearly empty ocean, the search stage requires the predator to travel for a substantial time, and to expend a significant amount of energy, to locate each food patch.[33] For example, the black-browed albatross regularly makes foraging flights to a range of around 700 kilometres (430 miles), up to a maximum foraging range of 3,000 kilometres (1,860 miles) for breeding birds gathering food for their young.[a][34] With static prey, some predators can learn suitable patch locations and return to them at intervals to feed.[33] The optimal foraging strategy for search has been modelled using the marginal value theorem.[35] Search patterns often appear random. One such is the Lévy walk, that tends to involve clusters of short steps with occasional long steps. It is a good fit to the behaviour of a wide variety of organisms including bacteria, honeybees, sharks and human hunter-gatherers.[36][37] Having found prey, a predator must decide whether to pursue it or keep searching. The decision depends on the costs and benefits involved. A bird foraging for insects spends a lot of time searching but capturing and eating them is quick and easy, so the efficient strategy for the bird is to eat every palatable insect it finds. By contrast, a predator such as a lion or falcon finds its prey easily but capturing it requires a lot of effort. In that case, the predator is more selective.[28] One of the factors to consider is size. Prey that is too small may not be worth the trouble for the amount of energy it provides. Too large, and it may be too difficult to capture. For example, a mantid captures prey with its forelegs and they are optimized for grabbing prey of a certain size. Mantids are reluctant to attack prey that is far from that size. There is a positive correlation between the size of a predator and its prey.[28] A predator may also assess a patch and decide whether to spend time searching for prey in it.[25] This may involve some knowledge of the preferences of the prey; for example, ladybirds can choose a patch of vegetation suitable for their aphid prey.[38] To capture prey, predators have a spectrum of pursuit modes that range from overt chase (pursuit predation) to a sudden strike on nearby prey (ambush predation).[25][39][12] Another strategy in between ambush and pursuit is ballistic interception, where a predator observes and predicts a prey's motion and then launches its attack accordingly.[40] Ambush or sit-and-wait predators are carnivorous animals that capture prey by stealth or surprise. In animals, ambush predation is characterized by the predator's scanning the environment from a concealed position until a prey is spotted, and then rapidly executing a fixed surprise attack.[41][40] Vertebrate ambush predators include frogs, fish such as the angel shark, the northern pike and the eastern frogfish.[40][42][43][44] Among the many invertebrate ambush predators are trapdoor spiders and Australian Crab spiders on land and mantis shrimps in the sea.[41][45][46] Ambush predators often construct a burrow in which to hide, improving concealment at the cost of reducing their field of vision. Some ambush predators also use lures to attract prey within striking range.[40] The capturing movement has to be rapid to trap the prey, given that the attack is not modifiable once launched.[40] Ballistic interception is the strategy where a predator observes the movement of a prey, predicts its motion, works out an interception path, and then attacks the prey on that path. This differs from ambush predation in that the predator adjusts its attack according to how the prey is moving.[40] Ballistic interception involves a brief period for planning, giving the prey an opportunity to escape. Some frogs wait until snakes have begun their strike before jumping, reducing the time available to the snake to recalibrate its attack, and maximising the angular adjustment that the snake would need to make to intercept the frog in real time.[40] Ballistic predators include insects such as dragonflies, and vertebrates such as archerfish (attacking with a jet of water), chameleons (attacking with their tongues), and some colubrid snakes.[40] In pursuit predation, predators chase fleeing prey. If the prey flees in a straight line, capture depends only on the predator's being faster than the prey.[40] If the prey manoeuvres by turning as it flees, the predator must react in real time to calculate and follow a new intercept path, such as by parallel navigation, as it closes on the prey.[40] Many pursuit predators use camouflage to approach the prey as close as possible unobserved (stalking) before starting the pursuit.[40] Pursuit predators include terrestrial mammals such as humans, African wild dogs, spotted hyenas and wolves; marine predators such as dolphins, orcas and many predatory fishes, such as tuna;[47][48] predatory birds (raptors) such as falcons; and insects such as dragonflies.[49] An extreme form of pursuit is endurance or persistence hunting, in which the predator tires out the prey by following it over a long distance, sometimes for hours at a time. The method is used by human hunter-gatherers and by canids such as African wild dogs and domestic hounds. The African wild dog is an extreme persistence predator, tiring out individual prey by following them for many miles at relatively low speed.[50] A specialised form of pursuit predation is the lunge feeding of baleen whales. These very large marine predators feed on plankton, especially krill, diving and actively swimming into concentrations of plankton, and then taking a huge gulp of water and filtering it through their feathery baleen plates.[51][52] Pursuit predators may be social, like the lion and wolf that hunt in groups, or solitary.[2] Once the predator has captured the prey, it has to handle it: very carefully if the prey is dangerous to eat, such as if it possesses sharp or poisonous spines, as in many prey fish. Some catfish such as the Ictaluridae have spines on the back (dorsal) and belly (pectoral) which lock in the erect position; as the catfish thrashes about when captured, these could pierce the predator's mouth, possibly fatally. Some fish-eating birds like the osprey avoid the danger of spines by tearing up their prey before eating it.[53] In social predation, a group of predators cooperates to kill prey. This makes it possible to kill creatures larger than those they could overpower singly; for example, hyenas, and wolves collaborate to catch and kill herbivores as large as buffalo, and lions even hunt elephants.[54][55][56] It can also make prey more readily available through strategies like flushing of prey and herding it into a smaller area. For example, when mixed flocks of birds forage, the birds in front flush out insects that are caught by the birds behind. Spinner dolphins form a circle around a school of fish and move inwards, concentrating the fish by a factor of 200.[57] By hunting socially chimpanzees can catch colobus monkeys that would readily escape an individual hunter, while cooperating Harris hawks can trap rabbits.[54][58] Social hunting allows predators to tackle a wider range of prey, but at the risk of competition for the captured food. Solitary predators have more chance of eating what they catch, at the price of increased expenditure of energy to catch it, and increased risk that the prey will escape.[62][63] Ambush predators are often solitary to reduce the risk of becoming prey themselves.[64] Of 245 terrestrial members of the Carnivora (the group that includes the cats, dogs, and bears), 177 are solitary; and 35 of the 37 wild cats are solitary,[65] including the cougar and cheetah.[62][2] However, the solitary cougar does allow other cougars to share in a kill,[66] and the coyote can be either solitary or social.[67] Other solitary predators include the northern pike,[68]wolf spiders and all the thousands of species of solitary wasps among arthropods,[69][70] and many microorganisms and zooplankton.[22][71] Under the pressure of natural selection, predators have evolved a variety of physical adaptations for detecting, catching, killing, and digesting prey. These include speed, agility, stealth, sharp senses, claws, teeth, filters, and suitable digestive systems.[72] For detecting prey, predators have well-developed vision, smell, or hearing.[12] Predators as diverse as owls and jumping spiders have forward-facing eyes, providing accurate binocular vision over a relatively narrow field of view, whereas prey animals often have less acute all-round vision. Animals such as foxes can smell their prey even when it is concealed under 2 feet (60 cm) of snow or earth. Many predators have acute hearing, and some such as echolocatingbats hunt exclusively by active or passive use of sound.[73] Predators including big cats, birds of prey, and ants share powerful jaws, sharp teeth, or claws which they use to seize and kill their prey. Some predators such as snakes and fish-eating birds like herons and cormorants swallow their prey whole; some snakes can unhinge their jaws to allow them to swallow large prey, while fish-eating birds have long spear-like beaks that they use to stab and grip fast-moving and slippery prey.[73] Fish and other predators have developed the ability to crush or open the armoured shells of molluscs.[74] Many predators are powerfully built and can catch and kill animals larger than themselves; this applies as much to small predators such as ants and shrews as to big and visibly muscular carnivores like the cougar and lion.[73][2][75] Skull of brown bear has large pointed canines for killing prey, and self-sharpening carnassial teeth at rear for cutting flesh with a scissor-like action Size-selective predation: a lioness attacking a Cape buffalo, over twice her weight. Lions can attack much larger prey, including elephants, but do so much less often. Predators are often highly specialized in their diet and hunting behaviour; for example, the Eurasian lynx only hunts small ungulates.[76] Others such as leopards are more opportunistic generalists, preying on at least 100 species.[77][78] The specialists may be highly adapted to capturing their preferred prey, whereas generalists may be better able to switch to other prey when a preferred target is scarce. When prey have a clumped (uneven) distribution, the optimal strategy for the predator is predicted to be more specialized as the prey are more conspicuous and can be found more quickly;[79] this appears to be correct for predators of immobile prey, but is doubtful with mobile prey.[80] In size-selective predation, predators select prey of a certain size.[81] Large prey may prove troublesome for a predator, while small prey might prove hard to find and in any case provide less of a reward. This has led to a correlation between the size of predators and their prey. Size may also act as a refuge for large prey. For example, adult elephants are relatively safe from predation by lions, but juveniles are vulnerable.[82] In aggressive mimicry, certain predators, including insects and fishes, make use of coloration and behaviour to attract prey. Female Photurisfireflies, for example, copy the light signals of other species, thereby attracting male fireflies, which they capture and eat.[84]Flower mantises are ambush predators; camouflaged as flowers, such as orchids, they attract prey and seize it when it is close enough.[85]Frogfishes are extremely well camouflaged, and actively lure their prey to approach using an esca, a bait on the end of a rod-like appendage on the head, which they wave gently to mimic a small animal, gulping the prey in an extremely rapid movement when it is within range.[86] Many smaller predators such as the box jellyfish use venom to subdue their prey,[87] and venom can also aid in digestion (as is the case for rattlesnakes and some spiders).[88][89] The marbled sea snake that has adapted to egg predation has atrophied venom glands, and the gene for its three finger toxin contains a mutation (the deletion of two nucleotides) that inactives it. These changes are explained by the fact that its prey does not need to be subdued.[90] Physiological adaptations to predation include the ability of predatory bacteria to digest the complex peptidoglycan polymer from the cell walls of the bacteria that they prey upon.[23] Carnivorous vertebrates of all five major classes (fishes, amphibians, reptiles, birds, and mammals) have lower relative rates of sugar to amino acid transport than either herbivores or omnivores, presumably because they acquire plenty of amino acids from the animal proteins in their diet.[95] Predators and prey are natural enemies, and many of their adaptations seem designed to counter each other. For example, bats have sophisticated echolocation systems to detect insects and other prey, and insects have developed a variety of defences including the ability to hear the echolocation calls.[117][118] Many pursuit predators that run on land, such as wolves, have evolved long limbs in response to the increased speed of their prey.[119] Their adaptations have been characterized as an evolutionary arms race, an example of the coevolution of two species.[120] In a gene centered view of evolution, the genes of predator and prey can be thought of as competing for the prey's body.[120] However, the \"life-dinner\" principle of Dawkins and Krebs predicts that this arms race is asymmetric: if a predator fails to catch its prey, it loses its dinner, while if it succeeds, the prey loses its life.[120] Eastern coral snake, itself a predator, is venomous enough to kill predators that attack it, so when they avoid it, this behaviour must be inherited, not learnt. The metaphor of an arms race implies ever-escalating advances in attack and defence. However, these adaptations come with a cost; for instance, longer legs have an increased risk of breaking,[121] while the specialized tongue of the chameleon, with its ability to act like a projectile, is useless for lapping water, so the chameleon must drink dew off vegetation.[122] The \"life-dinner\" principle has been criticized on multiple grounds. The extent of the asymmetry in natural selection depends in part on the heritability of the adaptive traits.[122] Also, if a predator loses enough dinners, it too will lose its life.[121][122] On the other hand, the fitness cost of a given lost dinner is unpredictable, as the predator may quickly find better prey. In addition, most predators are generalists, which reduces the impact of a given prey adaption on a predator. Since specialization is caused by predator-prey coevolution, the rarity of specialists may imply that predator-prey arms races are rare.[122] It is difficult to determine whether given adaptations are truly the result of coevolution, where a prey adaptation gives rise to a predator adaptation that is countered by further adaptation in the prey. An alternative explanation is escalation, where predators are adapting to competitors, their own predators or dangerous prey.[123] Apparent adaptations to predation may also have arisen for other reasons and then been co-opted for attack or defence. In some of the insects preyed on by bats, hearing evolved before bats appeared and was used to hear signals used for territorial defence and mating.[124] Their hearing evolved in response to bat predation, but the only clear example of reciprocal adaptation in bats is stealth echolocation.[125] A more symmetric arms race may occur when the prey are dangerous, having spines, quills, toxins or venom that can harm the predator. The predator can respond with avoidance, which in turn drives the evolution of mimicry. Avoidance is not necessarily an evolutionary response as it is generally learned from bad experiences with prey. However, when the prey is capable of killing the predator (as can a coral snake with its venom), there is no opportunity for learning and avoidance must be inherited. Predators can also respond to dangerous prey with counter-adaptations. In western North America, the common garter snake has developed a resistance to the toxin in the skin of the rough-skinned newt.[122] Predators affect their ecosystems not only directly by eating their own prey, but by indirect means such as reducing predation by other species, or altering the foraging behaviour of a herbivore, as with the biodiversity effect of wolves on riverside vegetation or sea otters on kelp forests. This may explain population dynamics effects such as the cycles observed in lynx and snowshoe hares.[126][127][128] Trophic transfer within an ecosystem refers to the transport of energy and nutrients as a result of predation. Energy passes from one trophic level to the next as predators consume organic matter from another organism's body. Within each transfer, while there are uses of energy, there are also losses of energy. Marine trophic levels vary depending on locality and the size of the primary producers. There are generally up to six trophic levels in the open ocean, four over continental shelves, and around three in upwelling zones.[133] For example, a marine habitat with five trophic levels could be represented as follows: Herbivores (feed primarily on phytoplankton); Carnivores (feed primarily on other zooplankton/animals); Detritivores (feed primarily on dead organic matter/detritus; Omnivores (feed on a mixed diet of phyto- and zooplankton and detritus); and Mixotrophs which combine autotrophy (using light energy to grow without intake of any additional organic compounds or nutrients) with heterotrophy (feeding on other plants and animals for energy and nutrients—herbivores, omnivores and carnivores, and detritivores). Trophic transfer efficiency measures how effectively energy is transferred or passed up through higher trophic levels of the marine food web. As energy moves up the trophic levels, it decreases due to heat, waste, and the natural metabolic processes that occur as predators consume their prey. The result is that only about 10% of the energy at any trophic level is transferred to the next level. This is often referred to as \"the 10% rule\" which limits the number of trophic levels that an individual ecosystem is capable of supporting.[134] Predators may increase the biodiversity of communities by preventing a single species from becoming dominant. Such predators are known as keystone species and may have a profound influence on the balance of organisms in a particular ecosystem.[135] Introduction or removal of this predator, or changes in its population density, can have drastic cascading effects on the equilibrium of many other populations in the ecosystem. For example, grazers of a grassland may prevent a single dominant species from taking over.[136] The elimination of wolves from Yellowstone National Park had profound impacts on the trophic pyramid. In that area, wolves are both keystone species and apex predators. Without predation, herbivores began to over-graze many woody browse species, affecting the area's plant populations. In addition, wolves often kept animals from grazing near streams, protecting the beavers' food sources. The removal of wolves had a direct effect on the beaver population, as their habitat became territory for grazing. Increased browsing on willows and conifers along Blacktail Creek due to a lack of predation caused channel incision because the reduced beaver population was no longer able to slow the water down and keep the soil in place. The predators were thus demonstrated to be of vital importance in the ecosystem.[137] In the absence of predators, the population of a species can grow exponentially until it approaches the carrying capacity of the environment.[138] Predators limit the growth of prey both by consuming them and by changing their behavior.[139] Increases or decreases in the prey population can also lead to increases or decreases in the number of predators, for example, through an increase in the number of young they bear. Cyclical fluctuations have been seen in populations of predator and prey, often with offsets between the predator and prey cycles. A well-known example is that of the snowshoe hare and lynx. Over a broad span of boreal forests in Alaska and Canada, the hare populations fluctuate in near synchrony with a 10-year period, and the lynx populations fluctuate in response. This was first seen in historical records of animals caught by fur hunters for the Hudson's Bay Company over more than a century.[140][128][141][142] A simple model of a system with one species each of predator and prey, the Lotka–Volterra equations, predicts population cycles.[143] However, attempts to reproduce the predictions of this model in the laboratory have often failed; for example, when the protozoan Didinium nasutum is added to a culture containing its prey, Paramecium caudatum, the latter is often driven to extinction.[144] The Lotka–Volterra equations rely on several simplifying assumptions, and they are structurally unstable, meaning that any change in the equations can stabilize or destabilize the dynamics.[145][146] For example, one assumption is that predators have a linear functional response to prey: the rate of kills increases in proportion to the rate of encounters. If this rate is limited by time spent handling each catch, then prey populations can reach densities above which predators cannot control them.[144] Another assumption is that all prey individuals are identical. In reality, predators tend to select young, weak, and ill individuals, leaving prey populations able to regrow.[147] Many factors can stabilize predator and prey populations.[148] One example is the presence of multiple predators, particularly generalists that are attracted to a given prey species if it is abundant and look elsewhere if it is not.[149] As a result, population cycles tend to be found in northern temperate and subarctic ecosystems because the food webs are simpler.[150] The snowshoe hare-lynx system is subarctic, but even this involves other predators, including coyotes, goshawks and great horned owls, and the cycle is reinforced by variations in the food available to the hares.[151] A range of mathematical models have been developed by relaxing the assumptions made in the Lotka–Volterra model; these variously allow animals to have geographic distributions, or to migrate; to have differences between individuals, such as sexes and an age structure, so that only some individuals reproduce; to live in a varying environment, such as with changing seasons;[152][153] and analysing the interactions of more than just two species at once. Such models predict widely differing and often chaotic predator-prey population dynamics.[152][154] The presence of refuge areas, where prey are safe from predators, may enable prey to maintain larger populations but may also destabilize the dynamics.[155][156][157][158] Predation dates from before the rise of commonly recognized carnivores by hundreds of millions (perhaps billions) of years. Predation has evolved repeatedly in different groups of organisms.[5][159] The rise of eukaryotic cells at around 2.7 Gya, the rise of multicellular organisms at about 2 Gya, and the rise of mobile predators (around 600 Mya - 2 Gya, probably around 1 Gya) have all been attributed to early predatory behavior, and many very early remains show evidence of boreholes or other markings attributed to small predator species.[5] It likely triggered major evolutionary transitions including the arrival of cells, eukaryotes, sexual reproduction, multicellularity, increased size, mobility (including insect flight[160]) and armoured shells and exoskeletons.[5] The earliest predators were microbial organisms, which engulfed or grazed on others. Because the fossil record is poor, these first predators could date back anywhere between 1 and over 2.7 Gya (billion years ago).[5] Predation visibly became important shortly before the Cambrian period—around 550 million years ago—as evidenced by the almost simultaneous development of calcification in animals and algae,[161] and predation-avoiding burrowing. However, predators had been grazing on micro-organisms since at least 1,000 million years ago,[5][162][163] with evidence of selective (rather than random) predation from a similar time.[164] Meganeura monyi, a predatory Carboniferousinsect related to dragonflies, could fly to escape terrestrial predators. Its large size, with a wingspan of 65 cm (30 in), may reflect the lack of vertebrate aerial predators at that time. Humans, as omnivores, are to some extent predatory,[171] using weapons and tools to fish,[172]hunt and trap animals.[173] They also use other predatory species such as dogs, cormorants,[174] and falcons to catch prey for food or for sport.[175] Two mid-sized predators, dogs and cats, are the animals most often kept as pets in western societies.[176][177] Human hunters, including the San of southern Africa, use persistence hunting, a form of pursuit predation where the pursuer may be slower than prey such as a kudu antelope over short distances, but follows it in the midday heat until it is exhausted, a pursuit that can take up to five hours.[178][179] In biological pest control, predators (and parasitoids) from a pest's natural range are introduced to control populations, at the risk of causing unforeseen problems. Natural predators, provided they do no harm to non-pest species, are an environmentally friendly and sustainable way of reducing damage to crops and an alternative to the use of chemical agents such as pesticides.[180] Among poetry on the theme of predation, a predator's consciousness might be explored, such as in Ted Hughes's Pike.[184] The phrase \"Nature, red in tooth and claw\" from Alfred, Lord Tennyson's 1849 poem \"In Memoriam A.H.H.\" has been interpreted as referring to the struggle between predators and prey.[185] In mythology and folk fable, predators such as the fox and wolf have mixed reputations.[186] The fox was a symbol of fertility in ancient Greece, but a weather demon in northern Europe, and a creature of the devil in early Christianity; the fox is presented as sly, greedy, and cunning in fables from Aesop onwards.[186] The big bad wolf is known to children in tales such as Little Red Riding Hood, but is a demonic figure in the Icelandic Edda sagas, where the wolf Fenrir appears in the apocalyptic ending of the world.[186] In the Middle Ages, belief spread in werewolves, men transformed into wolves.[186] In ancient Rome, and in ancient Egypt, the wolf was worshipped, the she-wolf appearing in the founding myth of Rome, suckling Romulus and Remus.[186] More recently, in Rudyard Kipling's 1894 The Jungle Book, Mowgli is raised by the wolf pack.[186] Attitudes to large predators in North America, such as wolf, grizzly bear and cougar, have shifted from hostility or ambivalence, accompanied by active persecution, towards positive and protective in the second half of the 20th century.[187] ^Davison, Peter (1 December 2002). \"Predators and Prey | Selected Poems, 1957–1994 by Ted Hughes\". The New York Times. Retrieved 5 October 2018. Hughes's earliest books contained a bewildering profusion of poems between their covers: ... fish and fowl, beasts of the field and forest, vigorous embodiments of predators and prey. Hughes as a student had taken up anthropology, not literature, and he chose to meditate his way into trancelike states of preconsciousness before committing poems to paper. His poems, early or late, enter into the relations of living creatures; they move in close to animal consciousness: The Thought-Fox,Esther's Tomcat,Pike."}
{"url": "https://en.m.wikipedia.org/wiki/Biological_Reviews", "text": "The Cambridge Philosophical Society (CPS) is a scientific society at the University of Cambridge. It was founded in 1819. The name derives from the medieval use of the word philosophy to denote any research undertaken outside the fields of law, theology and medicine. The society was granted a royal charter by King William IV in 1832. The society is governed by an elected council of senior academics, which is chaired by the Society's President, according to a set of statutes. Its prime purpose is to \"keep alive the spirit of inquiry\". For over 200 years, this spirit has been kept alive by its members and its activities. The society is independent of the University of Cambridge, although its offices are located within the University of Cambridge estate in central Cambridge. The Society has provided an open forum and played a key role in raising the profile of the sciences to the public. The society is a registered charity: 213811 [3] and has 11 Trustees. It is assisted by a number of full-time and part-time paid staff. Membership of the Society is currently over 2,000. Members of the Society are called Fellows and are entitled to use the ‘FCPS’ post-nominals. Fellows are usually academics or graduate students involved in mathematical or scientific research within the University. A Fellow must be recommended in writing by both a Fellow of the Society who has been a member for at least three years and a person of appropriate standing, who knows the candidate in a professional capacity. Approved candidates are elected at open meetings of the Society following proposal at Council Meetings.[4] The society publishes one of the oldest mathematical journals in history; \"Mathematical Proceedings\" first published in 1843 and now published for Cambridge Philosophical Society by Cambridge University Press[5][6] It has also published \"Biological Reviews\" [7] since 1926. To commemorate the Society’s bicentenary, a blue plaque to the Society was erected in March 2019 on 2 All Saints Passage (the house built by the Society in 1833 to house its meeting room, library and collections). The plaque was unveiled by Sir Martin Rees at opening of the Society’s exhibition at Cambridge University Library.[11] Coloured plate from R.T. Lowe 'On the Fishes of Madeira' from Transactions of the Cambridge Philosophical Society, Volume 6, 1838. The society has built up an exceptional historical scientific record dating back to 1819. While not on public display, the archives can be viewed by prior arrangement with the Cambridge Philosophical Society.[12] The Society archives include the following: Minutes of Council and of General Meetings Membership and subscription records Archives relating to the various premises occupied by the Society Archives relating to the Society’s publications Archives of the Library and Reading Room predating 1976 (the date at which the Library, by then known as the Scientific Periodicals Library and later as the Central Science Library, became a dependent library of Cambridge University Library) Archives relating to events and activities Some archives of individual members, such as Sir Joseph Larmor (1857–1942, physicist and mathematician) During the voyage, Darwin corresponded by letter with Henslow after reaching South America, and collected specimens with him in mind, particularly plants. Extracts from ten of Darwin's letters from South America to Henslow were first read out at a meeting of the Cambridge Philosophical Society on 16 November 1835, around the time that the ship reached Tahiti. Two days later, Adam Sedgwick read geological notes based on the letters to the Geological Society of London. The Council of the Cambridge Philosophical Society had the extracts printed in a pamphlet dated 1 December 1835, for private distribution among the Members of the Society,. The pamphlet has been described as the first writing of Charles Darwin ever to be published, but earlier, while still at Cambridge University, he had his notes on insects published in a book by James Francis Stephens.[13][14] The readings were held and the pamphlet Extracts from Letters to Henslow was printed without Darwin's knowledge. Upon learning of this pamphlet, Darwin was \"a good deal horrified\" at Henslow making public \"what had been written without care or accuracy\". The publicity helped Darwin's career, and at the end of his life acknowledged their friendship as the most important \"circumstance\" of his life. The original pamphlet is now rare. According to American Book Prices Current only four copies have appeared at auction since 1975. On 19 June 2014 Christies auctioned an original copy in New York (Sale 2861) and realised $221,000.[15] In 1960 it was reprinted privately to mark the 100th anniversary of the publication of the \"Origin of Species\" on 26 November 1859 and issued to Members and Associates of the Society."}
{"url": "https://en.m.wikipedia.org/wiki/Protostomes", "text": "In animals at least as complex as earthworms, the first phase in gut development involves the embryo forming a dent on one side (the blastopore) which deepens to become its digestive tube (the archenteron). In the sister-clade, the deuterostomes (lit.'second-mouth'), the original dent becomes the anus while the gut eventually tunnels through to make another opening, which forms the mouth. The protostomes (from Greek πρωτο-prōto- 'first' + στόμαstóma 'mouth') were so named because it was once believed that in all cases the embryological dent formed the mouth while the anus was formed later, at the opening made by the other end of the gut.[4][1] It is now known that the fate of the blastopore among protostomes is extremely variable; while the evolutionary distinction between deuterostomes and protostomes remains valid, the descriptive accuracy of the name protostome is disputable.[1] Protostome and deuterostome embryos differ in several other ways. Many protostomes (the Spiralia clade) undergo spiral cleavage during cell division instead of radial cleavage.[5] Spiral cleavage happens because the cells' division planes are angled to the polar major axis, instead of being parallel or perpendicular to it. Another difference is that secondary body cavities (coeloms) generally form by schizocoely, where the coelom forms out of a solid mass of embryonic tissue splitting away from the rest, instead of by enterocoelic pouching, where the coelom would otherwise form out of in-folded gut walls.[6] The common ancestor of protostomes and deuterostomes was evidently a worm-like aquatic animal of the Ediacaran. The two clades diverged about 600 million years ago. Protostomes evolved into over a million species alive today, compared to ca. 73,000 deuterostome species.[7]"}
{"url": "https://en.m.wikipedia.org/wiki/E._O._Wilson", "text": "Wilson's work received both praise and criticism during his lifetime. His book Sociobiology was a particular flashpoint for controversy, and drew criticism from the Sociobiology Study Group.[7][8] Wilson's interpretation of the theory of evolution resulted in a widely reported dispute with Richard Dawkins.[9] Examinations of his letters after his death revealed that he had supported the psychologist J. Philippe Rushton, whose work on race and intelligence is widely regarded by the scientific community as deeply flawed and racist. In the same year that his parents divorced, Wilson blinded himself in his right eye in a fishing accident.[13] Despite the prolonged pain, he did not stop fishing. He did not complain because he was anxious to stay outdoors, and never sought medical treatment. Several months later, his right pupil clouded over with a cataract. He was admitted to Pensacola Hospital to have the lens removed. Wilson writes, in his autobiography, that the \"surgery was a terrifying [19th] century ordeal\". Wilson retained full sight in his left eye, with a vision of 20/10. The 20/10 vision prompted him to focus on \"little things\": \"I noticed butterflies and ants more than other kids did, and took an interest in them automatically.\" Although he had lost his stereoscopic vision, he could still see fine print and the hairs on the bodies of small insects. His reduced ability to observe mammals and birds led him to concentrate on insects.[14] At the age of nine, Wilson undertook his first expeditions at Rock Creek Park in Washington, D.C. He began to collect insects and he gained a passion for butterflies. He would capture them using nets made with brooms, coat hangers, and cheesecloth bags.[14] Going on these expeditions led to Wilson's fascination with ants. He describes in his autobiography how one day he pulled the bark of a rotting tree away and discovered citronella ants underneath.[14] The worker ants he found were \"short, fat, brilliant yellow, and emitted a strong lemony odor\".[14] Wilson said the event left a \"vivid and lasting impression\".[14] He also earned the Eagle Scout award and served as Nature Director of his Boy Scouts summer camp. At age 18, intent on becoming an entomologist, he began by collecting flies, but the shortage of insect pins during World War II caused him to switch to ants, which could be stored in vials. With the encouragement of Marion R. Smith, a myrmecologist from the National Museum of Natural History in Washington, Wilson began a survey of all the ants of Alabama. This study led him to report the first colony of fire ants in the U.S., near the port of Mobile.[15] Wilson said he went to 15 or 16 schools during 11 years of schooling.[12] He was concerned that he might not be able to afford to go to a university, and he tried to enlist in the United States Army, intending to earn U.S. government financial support for his education. He failed the Army medical examination due to his impaired eyesight,[14] but was able to afford to enroll in the University of Alabama, where he earned his Bachelor of Science in 1949 and Master of Science in biology in 1950. The next year, Wilson transferred to Harvard University.[14] Appointed to the Harvard Society of Fellows, he could travel on overseas expeditions, collecting ant species of Cuba and Mexico and travel the South Pacific, including Australia, New Guinea, Fiji, and New Caledonia, as well as to Sri Lanka. In 1955, he received his Ph.D. and married Irene Kelley.[16][17] From 1956 until 1996, Wilson was part of the faculty of Harvard. He began as an ant taxonomist and worked on understanding their microevolution, how they developed into new species by escaping environmental disadvantages and moving into new habitats. He developed a theory of the \"taxon cycle\".[16] In 1971, he published The Insect Societies, which argued that insect behavior and the behavior of other animals are influenced by similar evolutionary pressures.[20] In 1973, Wilson was appointed the curator of entomology at the Harvard Museum of Comparative Zoology.[21] In 1975, he published the book Sociobiology: The New Synthesis applying his theories of insect behavior to vertebrates, and in the last chapter, to humans. He speculated that evolved and inherited tendencies were responsible for hierarchical social organization among humans. In 1978 he published On Human Nature, which dealt with the role of biology in the evolution of human culture and won a Pulitzer Prize for General Nonfiction.[16] Wilson was named the Frank B. Baird, Jr., Professor of Science in 1976 and, after his retirement from Harvard in 1996, he became the Pellegrino University Professor Emeritus.[21] Wilson was characterized by several titles during his career, including the \"father of biodiversity,\"[23][24] \"ant man,\"[25] and \"Darwin's heir.\"[26][27][28] In a PBS interview, David Attenborough described Wilson as \"a magic name to many of us working in the natural world, for two reasons. First, he is a towering example of a specialist, a world authority. Nobody in the world has ever known as much as Ed Wilson about ants. But, in addition to that intense knowledge and understanding, he has the widest of pictures. He sees the planet and the natural world that it contains in amazing detail but extraordinary coherence\".[29] Although Dawkins defended Wilson during the so-called \"sociobiology debate\",[30] a disagreement between them arose over the theory of evolution.[9][31] The disagreement began in 2012 when Dawkins wrote a critical review of Wilson's book The Social Conquest of Earth in Prospect Magazine.[9] In the review, Dawkins criticized Wilson for rejecting kin selection and for supporting group selection, labeling it \"bland\" and \"unfocused,\" and he wrote that the book's theoretical errors were \"important, pervasive, and integral to its thesis in a way that renders it impossible to recommend\".[32][33] Wilson responded in the same magazine and wrote that Dawkins made \"little connection to the part he criticizes\" and accused him of engaging in rhetoric.[31] In 2014, Wilson said in an interview, \"There is no dispute between me and Richard Dawkins and there never has been, because he's a journalist, and journalists are people that report what the scientists have found and the arguments I’ve had have actually been with scientists doing research\".[31] Dawkins responded in a tweet: \"I greatly admire EO Wilson & his huge contributions to entomology, ecology, biogeography, conservation, etc. He's just wrong on kin selection\" and later added, \"Anybody who thinks I'm a journalist who reports what other scientists think is invited to read The Extended Phenotype\".[31] Biologist Jerry Coyne wrote that Wilson's remarks were \"unfair, inaccurate, and uncharitable\".[34] In 2021, in an obituary to Wilson, Dawkins stated that their dispute was \"purely scientific\".[35] Dawkins wrote that he stands by his critical review and doesn't regret \"its outspoken tone\", but noted that he also stood by his \"profound admiration for Professor Wilson and his life work\".[35] From the late 1980s to the early 1990s, Wilson wrote several emails to Rushton's colleagues defending Rushton's work in the face of widespread criticism for scholarly misconduct, misrepresentation of data, and confirmation bias, all of which were allegedly used by Rushton to support his personal ideas on race.[36] Wilson also sponsored an article written by Rushton in PNAS,[39] and during the review process, Wilson intentionally sought out reviewers for the article who he believed would likely already agree with its premise.[36] Wilson kept his support of Rushton's racist ideologies behind-the-scenes so as to not draw too much attention to himself or tarnish his own reputation.[40] Wilson responded to another request from Rushton to sponsor a second PNAS article with the following: \"You have my support in many ways, but for me to sponsor an article on racial differences in the PNAS would be counterproductive for both of us.\" Wilson also remarked that the reason Rushton's ideologies were not more widely supported is because of the \"... fear of being called racist, which is virtually a death sentence in American academia if taken seriously. I admit that I myself have tended to avoid the subject of Rushton's work, out of fear.\"[36] In 2022, the E.O. Wilson Biodiversity Foundation issued a statement rejecting Wilson's support of Rushton and racism, on behalf of the board of directors and staff.[41] Wilson used sociobiology and evolutionary principles to explain the behavior of social insects and then to understand the social behavior of other animals, including humans, thus establishing sociobiology as a new scientific field.[42] He argued that all animal behavior, including that of humans, is the product of heredity, environmental stimuli, and past experiences, and that free will is an illusion. He referred to the biological basis of behavior as the \"genetic leash\".[43]: 127–128 The sociobiological view is that all animal social behavior is governed by epigenetic rules worked out by the laws of evolution. This theory and research proved to be seminal, controversial, and influential.[44] Wilson argued that the unit of selection is a gene, the basic element of heredity. The target of selection is normally the individual who carries an ensemble of genes of certain kinds. With regard to the use of kin selection in explaining the behavior of eusocial insects, the \"new view that I'm proposing is that it was group selection all along, an idea first roughly formulated by Darwin.\"[45] Sociobiological research was at the time particularly controversial with regard to its application to humans.[46] The theory established a scientific argument for rejecting the common doctrine of tabula rasa, which holds that human beings are born without any innatemental content and that culture functions to increase human knowledge and aid in survival and success.[47] Sociobiology: The New Synthesis was initially met with praise by most biologists.[7][8] After substantial criticism of the book was launched by the Sociobiology Study Group, associated with the organization Science for the People, a major controversy known as the \"sociobiology debate\" ensued,[7][8] and Wilson was accused of racism, misogyny, and support for eugenics.[48] Several of Wilson's colleagues at Harvard,[49] such as Richard Lewontin and Stephen Jay Gould, both members of the Group, were strongly opposed. Both focused their criticism mostly on Wilson's sociobiological writings.[50] Gould, Lewontin, and other members, wrote \"Against 'Sociobiology'\" in an open letter criticizing Wilson's \"deterministic view of human society and human action\".[51] Other public lectures, reading groups, and press releases were organized criticizing Wilson's work. In response, Wilson produced a discussion article entitled \"Academic Vigilantism and the Political Significance of Sociobiology\" in BioScience.[52][53] Philosopher Mary Midgley encountered Sociobiology in the process of writing Beast and Man (1979)[59] and significantly rewrote the book to offer a critique of Wilson's views. Midgley praised the book for the study of animal behavior, clarity, scholarship, and encyclopedic scope, but extensively critiqued Wilson for conceptual confusion, scientism, and anthropomorphism of genetics.[60] Wilson, along with Bert Hölldobler, carried out a systematic study of ants and ant behavior,[63] culminating in the 1990 encyclopedic work The Ants. Because much self-sacrificing behavior on the part of individual ants can be explained on the basis of their genetic interests in the survival of the sisters, with whom they share 75% of their genes (though the actual case is some species' queens mate with multiple males and therefore some workers in a colony would only be 25% related), Wilson argued for a sociobiological explanation for all social behavior on the model of the behavior of the social insects. Wilson said in reference to ants that \"Karl Marx was right, socialism works, it is just that he had the wrong species\".[64] He asserted that individual ants and other eusocial species were able to reach higher Darwinian fitness putting the needs of the colony above their own needs as individuals because they lack reproductive independence: individual ants cannot reproduce without a queen, so they can only increase their fitness by working to enhance the fitness of the colony as a whole. Humans, however, do possess reproductive independence, and so individual humans enjoy their maximum level of Darwinian fitness by looking after their own survival and having their own offspring.[65] In his 1998 book Consilience: The Unity of Knowledge, Wilson discussed methods that have been used to unite the sciences and might be able to unite the sciences with the humanities. He argued that knowledge is a single, unified thing, not divided between science and humanistic inquiry.[66] Wilson used the term \"consilience\" to describe the synthesis of knowledge from different specialized fields of human endeavor. He defined human nature as a collection of epigenetic rules, the genetic patterns of mental development. He argued that culture and rituals are products, not parts, of human nature. He said art is not part of human nature, but our appreciation of art is. He suggested that concepts such as art appreciation, fear of snakes, or the incesttaboo (Westermarck effect) could be studied by scientific methods of the natural sciences and be part of interdisciplinary research.[67] Wilson coined the phrase scientific humanism as \"the only worldview compatible with science's growing knowledge of the real world and the laws of nature\".[68] Wilson argued that it is best suited to improve the human condition. In 2003, he was one of the signers of the Humanist Manifesto.[69] On the question of God, Wilson described his position as \"provisional deism\"[70] and explicitly denied the label of \"atheist\", preferring \"agnostic\".[71] He explained his faith as a trajectory away from traditional beliefs: \"I drifted away from the church, not definitively agnostic or atheistic, just Baptist & Christian no more.\"[43] Wilson argued that belief in God and the rituals of religion are products of evolution.[72] He argued that they should not be rejected or dismissed, but further investigated by science to better understand their significance to human nature. In his book The Creation, Wilson wrote that scientists ought to \"offer the hand of friendship\" to religious leaders and build an alliance with them, stating that \"Science and religion are two of the most potent forces on Earth and they should come together to save the creation.\"[73] Wilson made an appeal to the religious community on the lecture circuit at Midland College, Texas, for example, and that \"the appeal received a 'massive reply'\", that a covenant had been written and that a \"partnership will work to a substantial degree as time goes on\".[74] In a New Scientist interview published on January 21, 2015, however, Wilson said that religious faith is \"dragging us down\", and: I would say that for the sake of human progress, the best thing we could possibly do would be to diminish, to the point of eliminating, religious faiths. But certainly not eliminating the natural yearnings of our species or the asking of these great questions.[75] Wilson said that, if he could start his life over he would work in microbial ecology, when discussing the reinvigoration of his original fields of study since the 1960s.[76] He studied the mass extinctions of the 20th century and their relationship to modern society, and identifying mass extinction as the greatest threat to Earth's future.[77] In 1998 argued for an ecological approach at the Capitol: Now when you cut a forest, an ancient forest in particular, you are not just removing a lot of big trees and a few birds fluttering around in the canopy. You are drastically imperiling a vast array of species within a few square miles of you. The number of these species may go to tens of thousands. ... Many of them are still unknown to science, and science has not yet discovered the key role undoubtedly played in the maintenance of that ecosystem, as in the case of fungi, microorganisms, and many of the insects.[78] Understanding the scale of the extinction crisis led him to advocate for forest protection,[78] including the \"Act to Save America's Forests\", first introduced in 1998 and reintroduced in 2008, but never passed.[80] The Forests Now Declaration called for new markets-based mechanisms to protect tropical forests.[81] Wilson once said destroying a rainforest for economic gain was like burning a Renaissance painting to cook a meal.[82] In 2014, Wilson called for setting aside 50% of Earth's surface for other species to thrive in as the only possible strategy to solve the extinction crisis. The idea became the basis for his book Half-Earth (2016) and for the Half-Earth Project of the E.O. Wilson Biodiversity Foundation.[83][84] Wilson's influence regarding ecology through popular science was discussed by Alan G. Gross in The Scientific Sublime (2018).[85] Wilson was instrumental in launching the Encyclopedia of Life (EOL)[86] initiative with the goal of creating a global database to include information on the 1.9 million species recognized by science. Currently, it includes information on practically all known species. This open and searchable digital repository for organism traits, measurements, interactions and other data has more than 300 international partners and countless scientists providing global users' access to knowledge of life on Earth. For his part, Wilson discovered and described more than 400 species of ants.[87][88] In 1996, Wilson officially retired from Harvard University, where he continued to hold the positions of Professor Emeritus and Honorary Curator in Entomology.[89] He fully retired from Harvard in 2002 at age 73. After stepping down, he published more than a dozen books, including a digital biology textbook for the iPad.[10][90] Wilson at a \"fireside chat\" during which he received the Addison Emery Verrill Medal in 2007Wilson addresses the audience at the dedication of the Biophilia Center named for him at Nokuse Plantation in Walton County, Florida. ^While primary and eyewitness accounts agree that the phrase \"Racist Wilson you can't hide, we charge you with genocide!\" was chanted, and that water was poured on Wilson's head, they disagree on whether a cup[54][55] or a pitcher/jug[56][57] was used. ^Scientist says there is hope to save planet\"Archived copy\". Archived from the original on January 29, 2013. Retrieved September 21, 2009.{{cite web}}: CS1 maint: archived copy as title (link) mywesttexas.com, September 18, 2009"}
{"url": "https://en.m.wikipedia.org/wiki/Princeton_University_Press", "text": "The press was founded by Whitney Darrow, with the financial support of Charles Scribner, as a printing press to serve the Princeton community in 1905.[2] Its distinctive building was constructed in 1911 on William Street in Princeton.[3] Its first book was a new 1912 edition of John Witherspoon's Lectures on Moral Philosophy.[4] Contents Princeton University Press was founded in 1905 by a recent Princeton graduate, Whitney Darrow, with financial support from another Princetonian, Charles Scribner II. Darrow and Scribner purchased the equipment and assumed the operations of two already existing local publishers, that of the Princeton Alumni Weekly and the Princeton Press. The new press printed both local newspapers, university documents, The Daily Princetonian, and later added book publishing to its activities.[5] Beginning as a small, for-profit printer, Princeton University Press was reincorporated as a nonprofit in 1910.[6] Since 1911, the press has been headquartered in a purpose-built gothic-style building designed by Ernest Flagg. The design of press's building, which was named the Scribner Building in 1965, was inspired by the Plantin-Moretus Museum, a printing museum in Antwerp, Belgium. Princeton University Press established a European office, in Woodstock, England, north of Oxford, in 1999, and opened an additional office, in Beijing, in early 2017. Princeton University Press's Bollingen Series had its beginnings in the Bollingen Foundation, a 1943 project of Paul Mellon's Old Dominion Foundation. From 1945, the foundation had independent status, publishing and providing fellowships and grants in several areas of study, including archaeology, poetry, and psychology. The Bollingen Series was given to the university in 1969."}
{"url": "https://www.sciencedaily.com/releases/2011/08/110823180459.htm", "text": "How many species on Earth? About 8.7 million, new estimate says About 8.7 million (give or take 1.3 million) is the new, estimated total number of species on Earth -- the most precise calculation ever offered -- with 6.5 million species on land and 2.2 million in oceans. Announced by the Census of Marine Life, the figure is based on a new analytical technique. The number of species on Earth had been estimated previously at 3 million to 100 million. That is a new, estimated total number of species on Earth -- the most precise calculation ever offered -- with 6.5 million species found on land and 2.2 million (about 25 percent of the total) dwelling in the ocean depths. Announced today by Census of Marine Life scientists, the figure is based on an innovative, validated analytical technique that dramatically narrows the range of previous estimates. Until now, the number of species on Earth was said to fall somewhere between 3 million and 100 million. Furthermore, the study, published by PLoS Biology, says a staggering 86% of all species on land and 91% of those in the seas have yet to be discovered, described and catalogued. Says lead author Camilo Mora of the University of Hawaii and Dalhousie University in Halifax, Canada: \"The question of how many species exist has intrigued scientists for centuries and the answer, coupled with research by others into species' distribution and abundance, is particularly important now because a host of human activities and influences are accelerating the rate of extinctions. Many species may vanish before we even know of their existence, of their unique niche and function in ecosystems, and of their potential contribution to improved human well-being.\" \"This work deduces the most basic number needed to describe our living biosphere,\" says co-author Boris Worm of Dalhousie University. \"If we did not know -- even by an order of magnitude (1 million? 10 million? 100 million?) -- the number of people in a nation, how would we plan for the future?\" \"It is the same with biodiversity. Humanity has committed itself to saving species from extinction, but until now we have had little real idea of even how many there are.\" Dr. Worm notes that the recently-updated Red List issued by the International Union for the Conservation of Nature assessed 59,508 species, of which 19,625 are classified as threatened. This means the IUCN Red List, the most sophisticated ongoing study of its kind, monitors less than 1% of world species. The research is published alongside a commentary by Lord Robert May of Oxford, past-president of the UK's Royal Society, who praises the researchers' \"imaginative new approach.\" \"It is a remarkable testament to humanity's narcissism that we know the number of books in the US Library of Congress on 1 February 2011 was 22,194,656, but cannot tell you -- to within an order-of-magnitude -- how many distinct species of plants and animals we share our world with,\" Lord May writes. \"(W)e increasingly recognize that such knowledge is important for full understanding of the ecological and evolutionary processes which created, and which are struggling to maintain, the diverse biological riches we are heir to. Such biodiversity is much more than beauty and wonder, important though that is. It also underpins ecosystem services that -- although not counted in conventional GDP -- humanity is dependent upon.\" Drawing conclusions from 253 years of taxonomy since Linnaeus Swedish scientist Carl Linnaeus created and published in 1758 the system still used to formally name and describe species. In the 253 years since, about 1.25 million species -- roughly 1 million on land and 250,000 in the oceans -- have been described and entered into central databases (roughly 700,000 more are thought to have been described but have yet to reach the central databases). To now, the best approximation of Earth's species total was based on the educated guesses and opinions of experts, who variously pegged the figure in a range from 3 to 100 million -- wildly differing numbers questioned because there is no way to validate them. Drs. Mora and Worm, together with Dalhousie colleagues Derek P. Tittensor, Sina Adl and Alastair G.B. Simpson, refined the estimated species total to 8.7 million by identifying numerical patterns within the taxonomic classification system (which groups forms of life in a pyramid-like hierarchy, ranked upwards from species to genus, family, order, class, phylum, kingdom and domain). Analyzing the taxonomic clustering of the 1.2 million species today in the Catalogue of Life and the World Register of Marine Species, the researchers discovered reliable numerical relationships between the more complete higher taxonomic levels and the species level. Says Dr. Adl: \"We discovered that, using numbers from the higher taxonomic groups, we can predict the number of species. The approach accurately predicted the number of species in several well-studied groups such as mammals, fishes and birds, providing confidence in the method.\" When applied to all five known eukaryote* kingdoms of life on Earth, the approach predicted: ~7.77 million species of animals (of which 953,434 have been described and cataloged) ~298,000 species of plants (of which 215,644 have been described and cataloged) ~611,000 species of fungi (moulds, mushrooms) (of which 43,271 have been described and cataloged) ~36,400 species of protozoa (single-cell organisms with animal-like behavior, eg. movement, of which 8,118 have been described and cataloged) ~27,500 species of chromista (including, eg. brown algae, diatoms, water moulds, of which 13,033 have been described and cataloged) Total: 8.74 million eukaryote species on Earth. (* Notes: Organisms in the eukaryote domain have cells containing complex structures enclosed within membranes. The study looked only at forms of life accorded, or potentially accorded, the status of \"species\" by scientists. Not included: certain micro-organisms and virus \"types,\" for example, which could be highly numerous.) Within the 8.74 million total is an estimated 2.2 million (plus or minus 180,000) marine species of all kinds, about 250,000 (11%) of which have been described and catalogued. When it formally concluded in October 2010, the Census of Marine Life offered a conservative estimate of 1 million+ species in the seas. \"Like astronomers, marine scientists are using sophisticated new tools and techniques to peer into places never seen before,\" says Australian Ian Poiner, Chair of the Census' Scientific Steering Committee. \"During the 10-year Census, hundreds of marine explorers had the unique human experience and privilege of encountering and naming animals new to science. We may clearly enjoy the Age of Discovery for many years to come.\" \"The immense effort entering all known species in taxonomic databases such as the Catalogue of Life and the World Register of Marine Species makes our analysis possible,\" says co-author Derek Tittensor, who also works with Microsoft Research and the UN Environment Programme's World Conservation Monitoring Centre. \"As these databases grow and improve, our method can be refined and updated to provide an even more precise estimate.\" \"We have only begun to uncover the tremendous variety of life around us,\" says co-author Alastair Simpson. \"The richest environments for prospecting new species are thought to be coral reefs, seafloor mud and moist tropical soils. But smaller life forms are not well known anywhere. Some unknown species are living in our own backyards -- literally.\" \"Awaiting our discovery are a half million fungi and moulds whose relatives gave humanity bread and cheese,\" says Jesse Ausubel, Vice-President of the Alfred P. Sloan Foundation and co-founder of the Census of Marine Life. \"For species discovery, the 21st century may be a fungal century!\" Mr. Ausubel notes the enigma of why so much diversity exists, saying the answer may lie in the notions that nature fills every niche, and that rare species are poised to benefit from a change of conditions. In his analysis, Lord May says the practical benefits of taxonomic discovery are many, citing the development in the 1970s of a new strain of rice based on a cross between conventional species and one discovered in the wild. The result: 30% more grain yield, followed by efforts ever since to protect all wild varieties of rice, \"which obviously can only be done if we have the appropriate taxonomic knowledge.\" \"Given the looming problems of feeding a still-growing world population, the potential benefits of ramping up such exploration are clear.\" Based on current costs and requirements, the study suggests that describing all remaining species using traditional approaches could require up to 1,200 years of work by more than 300,000 taxonomists at an approximate cost of $US 364 billion. Fortunately, new techniques such as DNA barcoding are radically reducing the cost and time involved in new species identification. Concludes Dr. Mora: \"With the clock of extinction now ticking faster for many species, I believe speeding the inventory of Earth's species merits high scientific and societal priority. Renewed interest in further exploration and taxonomy could allow us to fully answer this most basic question: What lives on Earth?\" Census of Marine Life. \"How many species on Earth? About 8.7 million, new estimate says.\" ScienceDaily. ScienceDaily, 24 August 2011. <www.sciencedaily.com/releases/2011/08/110823180459.htm>. Census of Marine Life. (2011, August 24). How many species on Earth? About 8.7 million, new estimate says. ScienceDaily. Retrieved April 22, 2024 from www.sciencedaily.com/releases/2011/08/110823180459.htm Census of Marine Life. \"How many species on Earth? About 8.7 million, new estimate says.\" ScienceDaily. www.sciencedaily.com/releases/2011/08/110823180459.htm (accessed April 22, 2024). Explore More Feb. 27, 2023  Previous research estimated that it took hundreds of million years for the ancient Earth's magma ocean to solidify, but new research narrows these large uncertainties down to less than just a couple ... Feb. 9, 2023  About 250 million years ago, the Permian-Triassic mass extinction killed over 80 per cent of the planet's species. In the aftermath, scientists believe that life on earth was dominated by simple ... Aug. 18, 2020  Scientists measured 12-21 million tons of three of the most common types of plastic in the top 200 meters of the Atlantic. By assuming the concentration of plastic in the whole Atlantic is the same ... July 1, 2020  It is estimated that 15 million different species live on our planet, but only 2 million of them are currently known to science. Discovering new species is important as it helps to protect them. ... Copyright 1995-2024 ScienceDaily or by other parties, where indicated. All rights controlled by their respective owners. Content on this website is for information only. It is not intended to provide medical or other professional advice. Views expressed here do not necessarily reflect those of ScienceDaily, contributors or partners. Financial support for ScienceDaily comes from advertisements and referral programs."}
{"url": "https://en.m.wikipedia.org/wiki/File:CelegansGoldsteinLabUNC.jpg", "text": "Licensing This work is free and may be used by anyone for any purpose. If you wish to use this content, you do not need to request permission as long as you follow any licensing requirements mentioned on this page. attribution – You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. share alike – If you remix, transform, or build upon the material, you must distribute your contributions under the same or compatible license as the original."}
{"url": "https://en.m.wikipedia.org/wiki/Stephen_Jay_Gould", "text": "Gould's most significant contribution to evolutionary biology was the theory of punctuated equilibrium[2] developed with Niles Eldredge in 1972.[3] The theory proposes that most evolution is characterized by long periods of evolutionary stability, infrequently punctuated by swift periods of branching speciation. The theory was contrasted against phyletic gradualism, the popular idea that evolutionary change is marked by a pattern of smooth and continuous change in the fossil record.[4] When Gould was five years old his father took him to the Hall of Dinosaurs in the American Museum of Natural History, where he first encountered Tyrannosaurus rex. \"I had no idea there were such things—I was awestruck,\" Gould once recalled.[13] It was in that moment that he decided to become a paleontologist.[14] Raised in a secular Jewish home, Gould did not formally practice religion and preferred to be called an agnostic.[15] When asked directly if he was an agnostic in Skeptic magazine, he responded: If you absolutely forced me to bet on the existence of a conventional anthropomorphic deity, of course I'd bet no. But, basically, Huxley was right when he said that agnosticism is the only honorable position because we really cannot know. And that's right. I'd be real surprised if there turned out to be a conventional God. Though he \"had been brought up by a Marxist father\"[16] he stated that his father's politics were \"very different\" from his own.[17] In describing his own political views, he has said they \"tend to the left of center.\"[18] According to Gould the most influential political books he read were C. Wright Mills' The Power Elite and the political writings of Noam Chomsky.[18] Gould married artist Deborah Lee on October 3, 1965.[12] Gould met Lee while they were students together at Antioch College.[13] They had two sons, Jesse and Ethan, and were married for 30 years.[26] His second marriage in 1995 was to artist and sculptor Rhonda Roland Shearer.[12] In July 1982 Gould was diagnosed with peritoneal mesothelioma, a deadly form of cancer affecting the abdominal lining (the peritoneum). This cancer is frequently found in people who have ingested or inhaled asbestos fibers, a mineral which was used in the construction of Harvard's Museum of Comparative Zoology.[27][28] After a difficult two-year recovery, Gould published a column for Discover magazine in 1985 titled \"The Median Isn't the Message\", which discusses his reaction to reading that \"mesothelioma is incurable, with a median mortality of only eight months after discovery.\"[29] In his essay, he describes the actual significance behind this fact, and his relief upon recognizing that statistical averages are useful abstractions, and by themselves do not encompass \"our actual world of variation, shadings, and continua.\"[29] Gould was also an advocate of medical cannabis. When undergoing his cancer treatments he smoked marijuana to help alleviate the long periods of intense and uncontrollable nausea. According to Gould, the drug had a \"most important effect\" on his eventual recovery. He later complained that he could not understand how \"any humane person would withhold such a beneficial substance from people in such great need simply because others use it for different purposes.\"[30] On August 5, 1998, Gould's testimony assisted in the successful lawsuit of HIV activist Jim Wakeford, who sued the Government of Canada for the right to cultivate, possess, and use marijuana for medical purposes.[31] In February 2002, a 3-centimeter (1.2 in) lesion was found on Gould's chest radiograph, and oncologists diagnosed him with stage IV cancer. Gould died 10 weeks later on May 20, 2002, from a metastaticadenocarcinoma of the lung, an aggressive form of cancer which had already spread to his brain, liver, and spleen.[32] This cancer was unrelated to his previous bout of abdominal cancer in 1982.[33] He died in his home \"in a bed set up in the library of his SoHo loft, surrounded by his wife Rhonda, his mother Eleanor, and the many books he loved.\"[34] The punctuated equilibrium model (above) consists of morphological stability followed by episodic bursts of evolutionary change via rapid cladogenesis. It is contrasted (below) to phyletic gradualism, a more gradual, continuous model of evolution. Early in his career, Gould and his colleague Niles Eldredge developed the theory of punctuated equilibrium, which describes the rate of speciation in the fossil record as occurring relatively rapidly, which then alternates to a longer period of evolutionary stability.[3] It was Gould who coined the term \"punctuated equilibria\" though the theory was originally presented by Eldredge in his doctoral dissertation on Devoniantrilobites and his article published the previous year on allopatric speciation.[41] According to Gould, punctuated equilibrium revised a key pillar \"in the central logic of Darwinian theory.\"[17] Some evolutionary biologists have argued that while punctuated equilibrium was \"of great interest to biology generally,\"[42] it merely modified neo-Darwinism in a manner that was fully compatible with what had been known before.[43] Other biologists emphasize the theoretical novelty of punctuated equilibrium, and argued that evolutionary stasis had been \"unexpected by most evolutionary biologists\" and \"had a major impact on paleontology and evolutionary biology.\"[44] Comparisons were made to George Gaylord Simpson's work in Tempo and Mode in Evolution (1941), in which he also illustrated relatively sudden changes along evolutionary lines. Simpson describes the paleontological record as being characterized by predominantly gradual change (which he termed horotely), although he also documented examples of slow (bradytely), and rapid (tachytely) rates of evolution. Punctuated equilibrium and phyletic gradualism are not mutually exclusive (as Simpson's work demonstrates), and examples of each have been documented in different lineages. The debate between these two models is often misunderstood by non-scientists, and according to Richard Dawkins has been oversold by the media.[45] Some critics jokingly referred to the theory of punctuated equilibrium as \"evolution by jerks\",[46] which prompted Gould to describe phyletic gradualism as \"evolution by creeps.\"[47] Gould made significant contributions to evolutionary developmental biology,[48] especially in his work Ontogeny and Phylogeny.[35] In this book he emphasized the process of heterochrony, which encompasses two distinct processes: neoteny and terminal additions. Neoteny is the process where ontogeny is slowed down and the organism does not reach the end of its development. Terminal addition is the process by which an organism adds to its development by speeding and shortening earlier stages in the developmental process. Gould's influence in the field of evolutionary developmental biology continues to be seen today in areas such as the evolution of feathers.[49] In 1975, Gould's Harvard colleague E. O. Wilson introduced his analysis of animal behavior (including human behavior) based on a sociobiological framework that suggested that many social behaviors have a strong evolutionary basis.[52] In response, Gould, Richard Lewontin, and others from the Boston area wrote the subsequently well-referenced letter to The New York Review of Books entitled, \"Against 'Sociobiology'\". This open letter criticized Wilson's notion of a \"deterministic view of human society and human action.\"[53] But Gould did not rule out sociobiological explanations for many aspects of animal behavior, and later wrote: \"Sociobiologists have broadened their range of selective stories by invoking concepts of inclusive fitness and kin selection to solve (successfully I think) the vexatious problem of altruism—previously the greatest stumbling block to a Darwinian theory of social behavior... Here sociobiology has had and will continue to have success. And here I wish it well. For it represents an extension of basic Darwinism to a realm where it should apply.\"[54] When visiting Venice in 1978, Gould noted that the spandrels of the San Marco cathedral, while quite beautiful, were not spaces planned by the architect. Rather the spaces arise as \"necessary architectural byproducts of mounting a dome on rounded arches.\" Gould and Lewontin thus defined \"spandrels\" in the evolutionary biology context to mean any biological feature of an organism that arises as a necessary side consequence of other features, which is not directly selected for by natural selection. Proposed examples include the \"masculinized genitalia in female hyenas, exaptive use of an umbilicus as a brooding chamber by snails, the shoulder hump of the giant Irish deer, and several key features of human mentality.\"[57] In Voltaire's Candide, Dr. Pangloss is portrayed as a clueless scholar who, despite the evidence, insists that \"all is for the best in this best of all possible worlds\". Gould and Lewontin asserted that it is Panglossian for evolutionary biologists to view all traits as atomized things that had been naturally selected for, and criticised biologists for not granting theoretical space to other causes, such as phyletic and developmental constraints. The relative frequency of spandrels, so defined, versus adaptive features in nature, remains a controversial topic in evolutionary biology.[58][59][60] An illustrative example of Gould's approach can be found in Elisabeth Lloyd's case study suggesting that the female orgasm is a by-product of shared developmental pathways.[61] Gould also wrote on this topic in his essay \"Male Nipples and Clitoral Ripples,\" prompted by Lloyd's earlier work.[62] Gould was criticized by philosopher Daniel Dennett for using the term spandrel instead of pendentive,[63] a spandrel that curves across a right angle to support a dome. Robert Mark, a professor of civil engineering at Princeton, offered his expertise in the pages of American Scientist, noting that these definitions are often misunderstood in architectural theory. Mark concluded, \"Gould and Lewontin's misapplication of the term spandrel for pendentive perhaps implies a wider latitude of design choice than they intended for their analogy. But Dennett's critique of the architectural basis of the analogy goes even further astray because he slights the technical rationale of the architectural elements in question.\"[56] Gould favored the argument that evolution has no inherent drive towards long-term \"progress\". Uncritical commentaries often portray evolution as a ladder of progress, leading towards bigger, faster, and smarter organisms, the assumption being that evolution is somehow driving organisms to get more complex and ultimately more like humankind. Gould argued that evolution's drive was not towards complexity, but towards diversification. Because life is constrained to begin with a simple starting point (like bacteria), any diversity resulting from this start, by random walk, will have a skewed distribution and therefore be perceived to move in the direction of higher complexity. But life, Gould argued, can also easily adapt towards simplification, as is often the case with parasites.[64] In a review of Full House, Richard Dawkins approved of Gould's general argument, but suggested that he saw evidence of a \"tendency for lineages to improve cumulatively their adaptive fit to their particular way of life, by increasing the numbers of features which combine together in adaptive complexes. ... By this definition, adaptive evolution is not just incidentally progressive, it is deeply, dyed-in-the-wool, indispensably progressive.\"[65] Gould's arguments against progress in evolutionary biology did not extend towards a notion of progress in general or notions of cultural evolution. In Full House, Gould compares two notions of progress against one another. While the first concept of progress, evolutionary progress, is argued to be invalid for a number of biological considerations, Gould permits that evolution may operate in human cultural evolution through a Lamarckian mechanism. Gould goes on to argue that the disappearance of the 0.400 batting average in baseball is paradoxically due to the inclusion of better players in the league, rather than players becoming worse over time. In his view such a process is likely reflective in a number of cultural phenomena including sports, the visual arts, and music where, unlike in biological systems, the realm of aesthetic possibilities is constrained by a \"right wall\" of human limits and aesthetic preferences.[66] Gould later goes on to state that his arguments for biological evolution should not be applied to cultural change lest they be employed by, \"so-called 'political correctness' as a doctrine that celebrates all indigenous practice, and therefore permits no distinctions, judgements, or analyses.\"[64] Gould never embraced cladistics as a method of investigating evolutionary lineages and process, possibly because he was concerned that such investigations would lead to neglect of the details in historical biology, which he considered all-important. In the early 1990s this led him into a debate with Derek Briggs, who had begun to apply quantitative cladistic techniques to the Burgess Shale fossils, about the methods to be used in interpreting these fossils.[67] Around this time cladistics rapidly became the dominant method of classification in evolutionary biology. Inexpensive but increasingly powerful personal computers made it possible to process large quantities of data about organisms and their characteristics. Around the same time the development of effective polymerase chain reaction techniques made it possible to apply cladistic methods of analysis to biochemical and genetic features as well.[68] Most of Gould's empirical research pertained to land snails. He focused his early work on the Bermudian genus Poecilozonites, while his later work concentrated on the West Indian genus Cerion. According to Gould \"Cerion is the land snail of maximal diversity in form throughout the entire world. There are 600 described species of this single genus. In fact, they're not really species, they all interbreed, but the names exist to express a real phenomenon which is this incredible morphological diversity. Some are shaped like golf balls, some are shaped like pencils. ... Now my main subject is the evolution of form, and the problem of how it is that you can get this diversity amid so little genetic difference, so far as we can tell, is a very interesting one. And if we could solve this we'd learn something general about the evolution of form.\"[69] Given Cerion's extensive geographic diversity, Gould later lamented that if Christopher Columbus had only catalogued a single Cerion it would have ended the scholarly debate about which island Columbus had first set foot on in America.[70] Gould is one of the most frequently cited scientists in the field of evolutionary theory. His 1979 \"spandrels\" paper has been cited more than 5,000 times.[71] In Paleobiology—the flagship journal of his own speciality—only Charles Darwin and George Gaylord Simpson have been cited more often.[72] Gould was also a considerably respected historian of science. Historian Ronald Numbers has been quoted as saying: \"I can't say much about Gould's strengths as a scientist, but for a long time I've regarded him as the second most influential historian of science (next to Thomas Kuhn).\"[73] Gould's undergraduate course, Science B-16: History of the Earth and Life, was taught in a Harvard Science Center lecture hall with a 250-seat capacity. Science B-16 was so oversubscribed that an annual lottery was held to see which students would be allowed to enroll in the course. If a student was denied course enrollment three times, then their fourth entry into the lottery provided them with a guaranteed seat in the class.[74] Shortly before his death, Gould published The Structure of Evolutionary Theory (2002), a long treatise recapitulating his version of modern evolutionary theory. In an interview for the Dutch TV series Of Beauty and Consolation Gould remarked, \"In a couple of years I will be able to gather in one volume my view of how evolution works. It is to me a great consolation because it represents the putting together of a lifetime of thinking into one source. That book will never be particularly widely read. It's going to be far too long, and it's only for a few thousand professionals—very different from my popular science writings—but it is of greater consolation to me because it is a chance to put into one place a whole way of thinking about evolution that I've struggled with all my life.\"[75] A passionate advocate of evolutionary theory, Gould wrote prolifically on the subject, trying to communicate his understanding of contemporary evolutionary biology to a wide audience. A recurring theme in his writings is the history and development of pre-evolutionary and evolutionary thought. He was also an enthusiastic baseball fan and sabermetrician (analyst of baseball statistics), and made frequent reference to the sport in his essays. Many of his baseball essays were anthologized in his posthumously published book Triumph and Tragedy in Mudville (2003).[22] Although a self-described Darwinist, Gould's emphasis was less gradualist and reductionist than most neo-Darwinists. He fiercely opposed many aspects of sociobiology and its intellectual descendant evolutionary psychology. He devoted considerable time to fighting against creationism, creation science, and intelligent design. Most notably, Gould provided expert testimony against the equal-time creationism law in McLean v. Arkansas. Gould later developed the term \"non-overlapping magisteria\" (NOMA) to describe how, in his view, science and religion should not comment on each other's realm. Gould went on to develop this idea in some detail, particularly in the books Rocks of Ages (1999) and The Hedgehog, the Fox, and the Magister's Pox (2003). In a 1982 essay for Natural History Gould wrote: Our failure to discern a universal good does not record any lack of insight or ingenuity, but merely demonstrates that nature contains no moral messages framed in human terms. Morality is a subject for philosophers, theologians, students of the humanities, indeed for all thinking people. The answers will not be read passively from nature; they do not, and cannot, arise from the data of science. The factual state of the world does not teach us how we, with our powers for good and evil, should alter or preserve it in the most ethical manner.[76] Gould also spoke out against creationist misuse of his work and theory, especially with respect to how his theory of punctuated equilibrium relates to the presence of transitional fossils or forms: It is infuriating to be quoted again and again by creationists—whether through design or stupidity, I do not know—as admitting that the fossil record includes no transitional forms. Transitional forms are generally lacking at the species level but are abundant between larger groups. The evolution from reptiles to mammals . . . is well documented.[77] In 1997, he voiced a cartoon version of himself on the television series The Simpsons. In the episode \"Lisa the Skeptic\", Lisa finds a skeleton that many people believe is an apocalyptic angel. Lisa contacts Gould and asks him to test the skeleton's DNA. The fossil is discovered to be a marketing gimmick for a new mall.[85] During production, the only phrase Gould objected to was a line in the script that introduced him as the \"world's most brilliant paleontologist\".[86] In 2002, the show paid tribute to Gould after his death, dedicating the season 13 finale to his memory. Gould had died two days before the episode aired. Gould received many accolades for his scholarly work and popular expositions of natural history,[87] but a number of biologists felt his public presentations were out of step with mainstream evolutionary thinking.[88] The public debates between Gould's supporters and detractors have been so quarrelsome that they have been dubbed \"The Darwin Wars\" by several commentators.[89][90] John Maynard Smith, the eminent British evolutionary biologist, was among Gould's strongest critics. Maynard Smith thought that Gould misjudged the vital role of adaptation in biology, and was critical of Gould's acceptance of species selection as a major component of biological evolution.[91] In a review of Daniel Dennett's book Darwin's Dangerous Idea, Maynard Smith wrote that Gould \"is giving non-biologists a largely false picture of the state of evolutionary theory.\"[92] But Maynard Smith was not consistently negative, writing in a review of The Panda's Thumb that \"Stephen Gould is the best writer of popular science now active... Often he infuriates me, but I hope he will go right on writing essays like these.\"[93] Maynard Smith was also among those who welcomed Gould's reinvigoration of evolutionary paleontology.[43] One reason for criticism was that Gould appeared to be presenting his ideas as a revolutionary way of understanding evolution, and argued for the importance of mechanisms other than natural selection, mechanisms which he believed had been ignored by many professional evolutionists. As a result, many non-specialists sometimes inferred from his early writings that Darwinian explanations had been proven to be unscientific (which Gould never tried to imply). Along with many other researchers in the field, Gould's works were sometimes deliberately taken out of context by creationists as \"proof\" that scientists no longer understood how organisms evolved.[94] Gould himself corrected some of these misinterpretations and distortions of his writings in later works.[77] In his book Wonderful Life (1989) Gould famously described the Cambrian fauna of the Burgess Shale, emphasizing their bizarre anatomical designs, their sudden appearance, and the role chance played in determining which members survived. He used the Cambrian fauna as an example of the role contingency has in shaping the broader pattern of evolution. His view of contingency was criticized by Simon Conway Morris in his 1998 book The Crucible of Creation.[97] Conway Morris stressed members of the Cambrian fauna that resemble modern taxa. He also argued that convergent evolution has a tendency to produce \"similarities of organization\" and that the forms of life are restricted and channelled. In his book Life's Solution (2003) Conway Morris argued that the appearance of human-like animals is also likely.[98] Paleontologist Richard Fortey noted that prior to the release of Wonderful Life, Conway Morris shared a similar thesis to Gould's, but after Wonderful Life Conway Morris revised his interpretation and adopted a more deterministic position on the history of life.[99] Paleontologists Derek Briggs and Richard Fortey have also argued that much of the Cambrian fauna may be regarded as stem groups of living taxa,[100] though this is still a subject of intense research and debate, and the relationship of many Cambrian taxa to modern phyla has not been established in the eyes of many palaeontologists.[101] Richard Dawkins disagrees with the view that new phyla suddenly appeared in the Cambrian, arguing that for a new phylum \"to spring into existence, what actually has to happen on the ground is that a child is born which suddenly, out of the blue, is as different from its parents as a snail is from an earthworm. No zoologist who thinks through the implications, not even the most ardent saltationist, has ever supported any such notion.\"[102] In the Structure of Evolutionary Theory Gould stresses the difference between phyletic splitting and large anatomical transitions, noting that the two events may be separated by millions of years. Gould argues that no paleontologist regards the Cambrian explosion \"as a genealogical event—that is as the actual time of initial splitting\", but rather it \"marks an anatomical transition in the overt phenotypes of bilaterian organisms.\"[103] Gould also had a long-running public feud with E. O. Wilson and other evolutionary biologists concerning the disciplines of human sociobiology and evolutionary psychology, both of which Gould and Lewontin opposed, but which Richard Dawkins, Daniel Dennett, and Steven Pinker advocated.[104] These debates reached their climax in the 1970s, and included strong opposition from groups such as the Sociobiology Study Group and Science for the People.[105] Pinker accuses Gould, Lewontin, and other opponents of evolutionary psychology of being \"radical scientists\", whose stance on human nature is influenced by politics rather than science.[106] Gould stated that he made \"no attribution of motive in Wilson's or anyone else's case\" but cautioned that all human beings are influenced, especially unconsciously, by our personal expectations and biases. He wrote: I grew up in a family with a tradition of participation in campaigns for social justice, and I was active, as a student, in the civil rights movement at a time of great excitement and success in the early 1960s. Scholars are often wary of citing such commitments. … [but] it is dangerous for a scholar even to imagine that he might attain complete neutrality, for then one stops being vigilant about personal preferences and their influences—and then one truly falls victim to the dictates of prejudice. Objectivity must be operationally defined as fair treatment of data, not absence of preference.[107] Gould's primary criticism held that human sociobiological explanations lacked evidential support, and argued that adaptive behaviors are frequently assumed to be genetic for no other reason than their supposed universality, or their adaptive nature. Gould emphasized that adaptive behaviors can be passed on through culture as well, and either hypothesis is equally plausible.[108] Gould did not deny the relevance of biology to human nature, but reframed the debate as \"biological potentiality vs. biological determinism.\" Gould stated that the human brain allows for a wide range of behaviors. Its flexibility \"permits us to be aggressive or peaceful, dominant or submissive, spiteful or generous… Violence, sexism, and general nastiness are biological since they represent one subset of a possible range of behaviors. But peacefulness, equality, and kindness are just as biological—and we may see their influence increase if we can create social structures that permit them to flourish.\"[108] Gould was the author of The Mismeasure of Man (1981), a history and inquiry of psychometrics and intelligence testing, generating perhaps the greatest controversy of all his books and receiving both widespread praise[109] and extensive criticism.[110][111][112] Gould investigated the methods of nineteenth century craniometry, as well as the history of psychological testing. Gould wrote that both theories developed from an unfounded belief in biological determinism, the view that \"social and economic differences between human groups—primarily races, classes, and sexes—arise from inherited, inborn distinctions and that society, in this sense, is an accurate reflection of biology.\"[113] The book was reprinted in 1996 with the addition of a new foreword and a critical review of The Bell Curve. In 2011, a study conducted by six anthropologists criticized Gould's claim that Samuel Morton unconsciously manipulated his skull measurements, arguing that his analysis of Morton was influenced by his opposition to racism.[114][115][116] The group's paper was reviewed in an editorial in the journal Nature, which pointed out that the paper's authors might have been influenced by their own motivations, recommending a degree of caution, stating \"the critique leaves the majority of Gould's work unscathed,\" and noted that \"because they couldn't measure all the skulls, they do not know whether the average cranial capacities that Morton reported represent his sample accurately.[117] The journal stated that Gould's opposition to racism may have biased his interpretation of Morton's data, but also noted that \"Lewis and his colleagues have their own motivations. Several in the group have an association with the University of Pennsylvania, to whom Morton donated his collection of skulls, and have an interest in seeing the valuable but understudied skull collection freed from the stigma of bias and did not accept Gould's theory \"that the scientific method is inevitably tainted by bias.\"[117] In 2014, the group's paper was critically reviewed in the journal Evolution & Development by University of Pennsylvania philosopher professor Michael Weisberg, who tended to support Gould's original accusations, concluding that \"there is prima facie evidence of a racial bias in Morton's measurements\". Weisberg concludes that although Gould did make several errors and overstated his case in a number of places, Morton's work \"remains a cautionary example of racial bias in the science of human differences\".[118] In 2015, biologists and philosophers Jonathan Kaplan, Massimo Pigliucci, and Joshua Banta published an article arguing that no meaningful conclusions could be drawn from Morton's data. They agreed with Gould, and disagreed with the 2011 study, insofar as Morton's study was seriously flawed, but they agreed with the 2011 study insofar as Gould's analysis was in many ways not better than Morton's.[119] University of Pennsylvania anthropology doctoral student Paul Wolff Mitchell published an analysis of Morton's original, unpublished data, which neither Gould nor subsequent commentators had directly addressed. Mitchell concluded that Gould's specific argument about Morton's unconscious bias in measurement is not supported, but that it was true, as Gould had claimed, that Morton's racial biases influenced how he reported and interpreted his measurements, arguing that Morton's interpretation of his data was arbitrary and tendentious: Morton investigated averages and ignored variations in skull size so large that there was significant overlap.[120] A contemporary of Morton, Friedrich Tiedemann, had collected almost identical skull data and drawn conclusions opposite to Morton's on the basis of this overlap, arguing strongly against any conception of a racial hierarchy.[121][122] In his book Rocks of Ages (1999), Gould put forward what he described as \"a blessedly simple and entirely conventional resolution to ... the supposed conflict between science and religion.\"[123] He defines the term magisterium as \"a domain where one form of teaching holds the appropriate tools for meaningful discourse and resolution.\"[123] The non-overlapping magisteria (NOMA) principle therefore divides the magisterium of science to cover \"the empirical realm: what the Universe is made of (fact) and why does it work in this way (theory). The magisterium of religion extends over questions of ultimate meaning and moral value. These two magisteria do not overlap, nor do they encompass all inquiry.\"[123] He suggests that \"NOMA enjoys strong and fully explicit support, even from the primary cultural stereotypes of hard-line traditionalism\" and that NOMA is \"a sound position of general consensus, established by long struggle among people of goodwill in both magisteria.\"[123] This view has not been without criticism, however. In his book The God Delusion, Richard Dawkins argues that the division between religion and science is not so simple as Gould claims, as few religions exist without claiming the existence of miracles, which \"by definition violate the principles of science.\"[124] Dawkins also opposes the idea that religion has anything meaningful to say about ethics and values, and therefore has no authority to claim a magisterium of its own.[124] He goes on to say that he believes Gould is \"bending over backwards to be nice to an unworthy but powerful opponent\".[125] Similarly, humanist philosopher Paul Kurtz argues that Gould was wrong to posit that science has nothing to say about questions of ethics. In fact, Kurtz claims that science is a much better method than religion for determining moral principles.[126] The following is a list of books either written or edited by Stephen Jay Gould, including those published after his death in 2002. While some books have been republished at later dates, by multiple publishers, the list below comprises the original publisher and publishing date. ^ abGould, S. J. (1981). \"Official Transcript for Gould’s deposition in McLean v. Arkansas.\" (Nov. 27). Under oath Gould stated: \"My political views tend to the left of center. Q. Could you be more specific about your political views? A. I don't know how to be. I am not a joiner, so I am not a member of any organization. So I have always resisted labeling. But if you read my other book, The Mismeasure of Man, which is not included because it is not about evolution, you will get a sense of my political views.\" p. 153. ^Maynard Smith, John (November 30, 1995). \"Genes, Memes, & Minds\". The New York Review of Books. pp. 46–48. By and large, I think their [Spandrels] paper had a healthy effect. ... Their critique forced us to clean up our act and to provide evidence for our stories. But adaptationism remains the core of biological thinking. \"Awards include a National Book Award for The Panda's Thumb, a National Book Critics Circle Award for The Mismeasure of Man, the Phi Beta Kappa Book Award for Hen's Teeth and Horse's Toes, and a Pulitzer Prize Finalist for Wonderful Life, on which Gould commented \"close but, as they say, no cigar.\" Forty-four honorary degrees and 66 major fellowships, medals, and awards bear witness to the depth and scope of his accomplishments in both the sciences and humanities: Member of the National Academy of Sciences, President and Fellow of AAAS, MacArthur Foundation 'genius' Fellowship (in the first group of awardees), Humanist Laureate from the Academy of Humanism, Fellow of the Linnean Society of London, Fellow of the Royal Society of Edinburgh, Fellow of the American Academy of Arts and Sciences, Fellow of the European Union of Geosciences, Associate of the Muséum National D'Histoire Naturelle Paris, the Schuchert Award for excellence in paleontological research, Scientist of the Year from Discover magazine, the Silver Medal from the Zoological Society of London, the Gold Medal for Service to Zoology from the Linnean Society of London, the Edinburgh Medal from the City of Edinburgh, the Britannica Award and Gold Medal for dissemination of public knowledge, Public Service Award from the Geological Society of America, Anthropology in Media Award from the American Anthropological Association, Distinguished Service Award from the National Association of Biology Teachers, Distinguished Scientist Award from UCLA, the Randi Award for Skeptic of the Year from the Skeptics Society, and a Festschrift in his honour at Caltech.\" John Maynard Smith, one of the world's leading evolutionary biologists, recently summarized in the NYRB the sharply conflicting assessments of Stephen Jay Gould: \"Because of the excellence of his essays, he has come to be seen by non-biologists as the pre-eminent evolutionary theorist. In contrast, the evolutionary biologists with whom I have discussed his work tend to see him as a man whose ideas are so confused as to be hardly worth bothering with, but as one who should not be publicly criticized because he is at least on our side against the creationists.\" (NYRB, November 30, 1995, p. 46). No one can take any pleasure in the evident pain Gould is experiencing now that his actual standing within the community of professional evolutionary biologists is finally becoming more widely known. If what was a stake was solely one man's self-regard, common decency would preclude comment. But as Maynard Smith points out, more is at stake. Gould \"is giving non-biologists a largely false picture of the state of evolutionary theory\"—or as Ernst Mayr says of Gould and his small group of allies—they \"quite conspicuously misrepresent the views of [biology's] leading spokesmen.\"[1] Indeed, although Gould characterizes his critics as \"anonymous\" and \"a tiny coterie,\" nearly every major evolutionary biologist of our era has weighed in a vain attempt to correct the tangle of confusions that the higher profile Gould has inundated the intellectual world with.[2] The point is not that Gould is the object of some criticism—so properly are we all—it is that his reputation as a credible and balanced authority about evolutionary biology is non-existent among those who are in a professional position to know. Note: Where Tooby and Cosmides quote Ernst Mayr, Mayr does not mention Gould by name, but is speaking generally of the critics of the Neo-Darwinian Synthesis. Also, the list of major biologists provided by Tooby and Cosmides may not be fairly represented. E.g., Mayr, Williams, Dawkins, and Coyne have expressed public admiration for Gould as a scientist. In the first of his two articles that provoked Tooby and Cosmides, Gould had commented on the November 1995 review of his work by Maynard Smith: Gould, \"Darwinian Fundamentalism\", New York Review of Books 44 (June 12, 1997): 34–37. A false fact can be refuted, a false argument exposed; but how can one respond to a purely ad hominem attack? This harder, and altogether more discouraging, task may best be achieved by exposing internal inconsistency and unfairness of rhetoric. [quotation of Smith's criticism of Gould, November 1995 NYRB] It seems futile to reply to an attack so empty of content, and based only on comments by anonymous critics; [...] Instead of responding to Maynard Smith's attack against my integrity and scholarship, citing people unknown and with arguments unmentioned, let me, instead, merely remind him of the blatant inconsistency between his admirable past and lamentable present. Some sixteen years ago he wrote a highly critical but wonderfully supportive review of my early book of essays, The Panda's Thumb, stating: \"I hope it will be obvious that my wish to argue with Gould is a compliment, not a criticism.\" He then attended my series of Tanner Lectures at Cambridge in 1984 and wrote in a report for Nature, and under the remarkable title \"Paleontology at the High Table,\" the kindest and most supportive critical commentary I have ever received. He argued that the work of a small group of American paleobiologists had brought the entire subject back to theoretical centrality within the evolutionary sciences. [...] So we face the enigma of a man who has written numerous articles, amounting to tens of thousands of words, about my work—always strongly and incisively critical, always richly informed (and always, I might add, enormously appreciated by me). But now Maynard Smith needs to canvass unnamed colleagues to find out that my ideas are \"hardly worth bothering with\". He really ought to be asking himself why he has been bothering about my work so intensely, and for so many years. ^Dawkins, Richard (1998). Unweaving the Rainbow. Boston: Houghton Mifflin, pp. 196–197. \"It is when we ask what happens during the sudden bursts of species formation that the confusion... arises... Gould is aware of the difference between rapid gradualism and macromutation, but he treats the matter as though it were a minor detail, to be cleared up after we have taken on board the overarching question of whether evolution is episodic rather than gradual.\" ^In 1981 The Mismeasure of Man won the National Book Critics Circle Award for non-fiction. It was voted as the 17th-greatest science book of all time by Discover magazine vol. 27 (December 8, 2006); 9th-best skeptic book by The Skeptics Society (Frank Diller, \"Scientists' Nightstand\" American Scientist); and ranked 24th place for the best non-fiction book by the Modern Library."}
{"url": "https://ucmp.berkeley.edu/phyla/placozoa/placozoa.html", "text": "Placozoans are tiny amazing animals. Very little is known about them because they have never been observed in their natural habitat. No one knows what substrate they live on or what they eat in nature. It is even unknown whether or not they reproduce sexually like most animals. They were discovered in the late 1880's living on the glass walls of an aquarium in a European laboratory. Since then, most of what has been learned about their biology has come from studying cultures of them kept alive in various laboratories around the world. Not surprisingly, given their small size and squishy nature, fossil placozoans have yet to be discovered. Placozoan Morphology Placozoans are extremely simple animals. Perhaps not coincidentally, they also have the smallest amount of DNA ever measured for any type of animal. Their bodies are made up of a few thousand cells of just four types. You can compare this to sponges, which have anywhere from 10 to 20 different kinds of cells, to flies, which have roughly 90 different cell types, and to you and other mammals, which have over 200 different types of cells. Placozoans are transparent, flat, round (up to 3 millimeters across), and have two distinct sides. A tissue layer composed of two types of cells, column-shaped cylinder cells with cilia and gland cells without cilia, make up the ventral (or bottom) surface. The upper dorsal surface consists of a layer of just cover cells, which are ciliated and flattened toward the outside of the animal. The image above shows the dorsal surface of a small specimen (just over 4/10ths of a millimeter in diameter) seen from above through a microscope. The dorsal and ventral tissues appear to correspond to ectoderm and endoderm, the outer and inner tissue layers of most animals, but it is not yet known which is endoderm and which is ectoderm. The fourth type of placozoan cells are called fiber cells. These cells are star-shaped and reside in the space between the two tissue layers. The star shape results from thin extensions of the cells which connect to each other in a network. Cellular material such as microtubules and microfilaments traverse the extensions from fiber cell to fiber cell. It is hypothesized that this system of connected cells in important in coordinating the movement of placozoans. Placozoans can move in two ways, by gliding on their cilia and by changing their shape like an amoeba. Placozoan Feeding and Reproduction In the laboratory, placozoans have been kept alive by feeding them the flagellated chromistCryptomonas or the chlorophyteChlorella. It is unknown what placozoans feed on in nature; they may feed on a number of different organisms. A placozoan feeds with its ventral surface, which produces digestive enzymes. Often, individuals contract part of the ventral surface into a sac where digestion may take place more efficiently. Placozoans can reproduce asexually by either binary fission or, less often, by budding. Some laboratory observations suggest that sexual reproduction may occur. When the population density becomes high, placozoans start to degenerate. Usually a single egg or oocyte develops in the interspace of a degenerating placozoan. Small cells (without flagella) that also form when placozoans degenerate are inferred to be sperm cells. After fertilization, which does not appear to have been documented, cleavage begins. Development has only been observed to the 64 cell stage, at which point the cells cease to separate while the nuclear DNA continues to multiply until the nucleus bursts. Placozoan reproduction and development will probably not be fully understood until they are observed in their natural habitat. The Phylogenetic Position of Placozoa Some scientists have inferred that placozoans might be the earliest branch of animals (as shown in box A below) because they are so simple. However, the discovery that placozoan epithelial cells are connected by junctions of extracellular proteins (belt desmosomes), a condition present in all animals other than sponges, suggested that placozoans may have diverged later in the history of animals (box B below). More recently, data from molecular sequences (18S) have indicated that placozoans might have diverged even later in the history of animals (box C above). If this latter view were true, it would imply that placozoans are secondarily simplified from more complex ancestors that had a nervous system. Interestingly enough, placozoans contain cells, which are dispersed around their outer edge, that react with antibodies against a neuropeptide that is present in the nervous system of cnidarians. In any event, the alternative hypotheses for the phylogenetic position of Placozoa within the animals need further testing with additional data. Placozoan Diversity Just two species of placozoans have ever been described, Trichoplax adhaerens and Treptoplax reptans. The latter of these has never been seen since its description in 1896, causing some to doubt its existence. The former, however, has been reported from many tropical and subtropical locations around the world, including: the Bermudas, the Caribbean Sea, Eastern Australia, the Great Barrier Reef, Guam, Hawaii, Japan, the Mediterranean Sea, Palau, Papua New Guinea, the Red Sea, Vietnam, and Western Samoa. This prompts the question of whether Tricoplax adhaerens is really a single species. Furthermore, placozoans are so cryptic that their diversity might be much greater than we realize."}
{"url": "https://en.m.wikipedia.org/wiki/Leather", "text": "Leather Leather is a strong, flexible and durable material obtained from the tanning, or chemical treatment, of animal skins and hides to prevent decay. The most common leathers come from cattle, sheep, goats, equine animals, buffalo, pigs and hogs, and aquatic animals such as seals and alligators.[1][2] A variety of leather products and leather-working tools Leather can be used to make a variety of items, including clothing, footwear, handbags, furniture, tools and sports equipment, and lasts for decades. Leather making has been practiced for more than 7,000 years and the leading producers of leather today are China and India.[1][2][3] Critics of tanneries claim that they engage in unsustainable practices that pose health hazards to the people and the environment near them.[4] Production processes The leather manufacturing process is divided into three fundamental subprocesses: preparatory stages, tanning, and crusting. A further subprocess, finishing, can be added into the leather process sequence, but not all leathers receive finishing. Tanning is a process that stabilizes the proteins, particularly collagen, of the raw hide to increase the thermal, chemical and microbiological stability of the hides and skins, making it suitable for a wide variety of end applications. The principal difference between raw and tanned hides is that raw hides dry out to form a hard, inflexible material that, when rewetted, will putrefy, while tanned material dries to a flexible form that does not become putrid when rewetted. Many tanning methods and materials exist. The typical process sees tanners load the hides into a drum and immerse them in a tank that contains the tanning \"liquor\". The hides soak while the drum slowly rotates about its axis, and the tanning liquor slowly penetrates through the full thickness of the hide. Once the process achieves even penetration, workers slowly raise the liquor's pH in a process called basification, which fixes the tanning material to the leather. The more tanning material fixed, the higher the leather's hydrothermal stability and shrinkage temperature resistance. Crusting is a process that thins and lubricates leather. It often includes a coloring operation. Chemicals added during crusting must be fixed in place. Crusting culminates with a drying and softening operation, and may include splitting, shaving, dyeing, whitening or other methods. For some leathers, tanners apply a surface coating, called \"finishing\". Finishing operations can include oiling, brushing, buffing, coating, polishing, embossing, glazing, or tumbling, among others. Leather can be oiled to improve its water resistance. This currying process after tanning supplements the natural oils remaining in the leather itself, which can be washed out through repeated exposure to water. Frequent oiling of leather, with mink oil, neatsfoot oil, or a similar material keeps it supple and improves its lifespan dramatically.[5] Tanning methods Tanning processes largely differ in which chemicals are used in the tanning liquor. Some common types include: Vegetable-tanned leather is tanned using tannins extracted from vegetable matter, such as tree bark prepared in bark mills. It is the oldest known method. It is supple and light brown in color, with the exact shade depending on the mix of materials and the color of the skin. The color tan derives its name from the appearance of undyed vegetable-tanned leather. Vegetable-tanned leather is not stable in water; it tends to discolor, and if left to soak and then dry, it shrinks and becomes harder, a feature of vegetable-tanned leather that is exploited in traditional shoemaking. In hot water, it shrinks drastically and partly congeals, becoming rigid and eventually brittle. Boiled leather is an example of this, where the leather has been hardened by being immersed in boiling water, or in wax or similar substances. Historically, it was occasionally used as armor after hardening, and it has also been used for book binding.[6][7] Chrome-tanned leather is tanned using chromium sulfate and other chromiumsalts. It is also known as \"wet blue\" for the pale blue color of the undyed leather. The chrome tanning method usually takes approximately one day to complete, making it best suited for large-scale industrial use. This is the most common method in modern use. It is more supple and pliable than vegetable-tanned leather and does not discolor or lose shape as drastically in water as vegetable-tanned. However, there are environmental concerns with this tanning method, as chromium is a heavy metal; while the trivalent chromium used for tanning is harmless, other byproducts can contain toxic variants. The method was developed in the latter half of the 19th century as tanneries wanted to find ways to speed up the process and to make leather more waterproof.[8][7] Aldehyde-tanned leather is tanned using glutaraldehyde or oxazolidine compounds. It is referred to as \"wet white\" due to its pale cream color. It is the main type of \"chrome-free\" leather, often seen in shoes for infants and automobiles. Formaldehyde has been used for tanning in the past; it is being phased out due to danger to workers and sensitivity of many people to formaldehyde. Chamois leather is a form of aldehyde-tanned leather that is porous and highly water-absorbent. Chamois leather is made using oil (traditionally cod oil)[9] that oxidizes to produce the aldehydes that tan the leather. Brain tanned leathers are made by a labor-intensive process that uses emulsified oils, often those of animal brains such as deer, cattle, and buffalo. An example of this kind is buckskin. Leather products made in this manner are known for their exceptional softness and washability. Alum leather is transformed using aluminium salts mixed with a variety of binders and protein sources, such as flour and egg yolk. Alum leather is not actually tanned; rather the process is called \"tawing\", and the resulting material reverts to rawhide if soaked in water long enough to remove the alum salts. Grades In general, leather is produced in the following grades: Top-grain leather includes the outer layer of the hide, known as the grain, which features finer, more densely packed fibers, resulting in strength and durability. Depending on thickness, it may also contain some of the more fibrous under layer, known as the corium. Types of top-grain leather include: Full-grain leather contains the entire grain layer, without any removal of the surface. Rather than wearing out, it develops a patina during its useful lifetime. It is usually considered the highest quality leather. Furniture and footwear are often made from full-grain leather. Full-grain leather is typically finished with a soluble aniline dye. Russia leather is a form of full-grain leather. Corrected grain leather has the surface subjected to finishing treatments to create a more uniform appearance. This usually involves buffing or sanding away flaws in the grain, then dyeing and embossing the surface. Nubuck is top-grain leather that has been sanded or buffed on the grain side to give a slight nap of short protein fibers, producing a velvet-like surface. Split leather is created from the corium left once the top-grain has been separated from the hide, known as the drop split. In thicker hides, the drop split can be further split into a middle split and a flesh split. Bicast leather is split leather that is coated with a layer of polyurethane or vinyl with an embossed texture. This gives it the appearance of a grain. It is slightly stiffer than top-grain leather but has a more consistent texture.[10] Patent leather is leather that has been given a high-gloss finish by the addition of a coating. Dating to the late 1700s, it became widely popular after inventor Seth Boyden developed the first mass-production process, using a linseed-oil-based lacquer, in 1818. Modern versions are usually a form of bicast leather. Suede is made from the underside of a split to create a soft, napped finish. It is often made from younger or smaller animals, as the skins of adults often result in a coarse, shaggy nap. Bonded leather, also called reconstituted leather, is a material that uses leather scraps that are shredded and bonded together with polyurethane or latex onto a fiber mesh. The amount of leather fibers in the mix varies from 10% to 90%, affecting the properties of the product.[11] The term \"genuine leather\" does not describe a specific grade. The term often indicates split leather that has been extensively processed,[12] and some sources describe it as synonymous with bicast leather,[13] or made from multiple splits glued together and coated.[14][15] In some countries, when it is the description on a product label the term means nothing more than \"contains leather\";[16][17] depending on jurisdiction, regulations limit the term's use in product labelling.[18][17] Animals used A book bound in pigskin Today, most leather is made of cattle hides, which constitute about 65% of all leather produced. Other animals that are used include sheep (about 13%), goats (about 11%), and pigs (about 10%). Obtaining accurate figures from around the world is difficult, especially for areas where the skin may be eaten.[19][20] There are significant regional differences in leather production: i.e. goat leather was historically called \"Turkey\" or \"Morocco\" due to its association with the Middle East, while pig skin had historically been used the most in Germany.[21] Other animals mentioned below only constitute a fraction of a percent of total leather production. Horse hides are used to make particularly durable leathers. Shell cordovan is a horse leather made not from the outer skin but from an under layer, found only in equine species, called the shell. It is prized for its mirror-like finish and anti-creasing properties. Lamb and deerskin are used for soft leather in more expensive apparel. Deerskin is widely used in work gloves and indoor shoes. Reptilian skins, such as alligator, crocodile, and snake, are noted for their distinct patterns that reflect the scales of their species. This has led to hunting and farming of these species in part for their skins. The Argentine black and white tegu is one of the most exploited reptile species in the world in the leather trade. However, it is not endangered and while monitored, trade is legal in most South American countries.[22] Although originally raised for their feathers in the 19th century, ostriches are now more popular for both meat and leather.[27]Ostrich leather has a characteristic \"goose bump\" look because of the large follicles where the feathers grew. Different processes produce different finishes for many applications, including upholstery, footwear, automotive products, accessories, and clothing. In Thailand, stingray leather is used in wallets and belts. Stingray leather is tough and durable. The leather is often dyed black and covered with tiny round bumps in the natural pattern of the back ridge of an animal. These bumps are then usually dyed white to highlight the decoration. Stingray rawhide is also used as grips on Chinese swords, Scottish basket hilted swords, and Japanese katanas. Stingray leather is also used for high abrasion areas in motorcycle racing leathers (especially in gloves, where its high abrasion resistance helps prevent wear through in the event of an accident). For a given thickness, fish leather is typically much stronger due to its criss-crossed fibers.[28] Air pollution due to the transformation process (hydrogen sulfide is formed during mixing with acids and ammonia liberated during deliming, solvent vapors) Carbon footprint Estimates of the carbon footprint of bovine leather range from 65 to 150 kg of CO2 equivalent per square meter of production.[29] Water footprint One ton of hide or skin generally produces 20 to 80 m3 of waste water, including chromium levels of 100–400 mg/L, sulfide levels of 200–800 mg/L, high levels of fat and other solid wastes, and notable pathogen contamination. Producers often add pesticides to protect hides during transport. With solid wastes representing up to 70% of the wet weight of the original hides, the tanning process represents a considerable strain on water treatment installations.[30] Chemical waste disposal Tanning is especially polluting in countries where environmental regulations are lax, such as in India, the world's third-largest producer and exporter of leather. To give an example of an efficient pollution prevention system, chromium loads per produced tonne are generally abated from 8 kg to 1.5 kg. VOC emissions are typically reduced from 30 kg/t to 2 kg/t in a properly managed facility. A review of the total pollution load decrease achievable according to the United Nations Industrial Development Organization[33] posts precise data on the abatement achievable through industrially proven low-waste advanced methods, while noting, \"even though the chrome pollution load can be decreased by 94% on introducing advanced technologies, the minimum residual load 0.15 kg/t raw hide can still cause difficulties when using landfills and composting sludge from wastewater treatment on account of the regulations currently in force in some countries.\" In Kanpur, the self-proclaimed \"Leather City of World\"—with 10,000 tanneries as of 2011 and a city of three million on the banks of the Ganges—pollution levels were so high, that despite an industry crisis, the pollution control board decided to shut down 49 high-polluting tanneries out of 404 in July 2009.[34] In 2003 for instance, the main tanneries' effluent disposal unit was dumping 22 tonnes of chromium-laden solid waste per day in the open.[35] In the Hazaribagh neighborhood of Dhaka in Bangladesh, chemicals from tanneries end up in Dhaka's main river. Besides the environmental damage, the health of both local factory workers and the end consumer is also negatively affected.[36] After approximately 15 years of ignoring high court rulings, the government shut down more than 100 tanneries the weekend of 8 April 2017 in the neighborhood.[37] The higher cost associated with the treatment of effluents than to untreated effluent discharging leads to illegal dumping to save on costs. For instance, in Croatia in 2001, proper pollution abatement cost US$70–100 per ton of raw hides processed against $43/t for irresponsible behavior.[38] In November 2009, one of Uganda's main leather making companies was caught directly dumping waste water into a wetland adjacent to Lake Victoria.[39] Role of enzymes Enzymes like proteases, lipases, and amylases have an important role in the soaking, dehairing, degreasing, and bating operations of leather manufacturing. Proteases are the most commonly used enzymes in leather production. The enzyme must not damage or dissolve collagen or keratin, but should hydrolyze casein, elastin, albumin, globulin-like proteins, and nonstructural proteins that are not essential for leather making. This process is called bating.[40] Lipases are used in the degreasing operation to hydrolyze fat particles embedded in the skin.[41] Amylases are used to soften skin, to bring out the grain, and to impart strength and flexibility to the skin. These enzymes are rarely used. Preservation and conditioning The natural fibers of leather break down with the passage of time. Acidic leathers are particularly vulnerable to red rot, which causes powdering of the surface and a change in consistency. Damage from red rot is aggravated by high temperatures and relative humidities. Although it is chemically irreversible, treatments can add handling strength and prevent disintegration of red rotted leather. Exposure to long periods of low relative humidities (below 40%) can cause leather to become desiccated, irreversibly changing the fibrous structure of the leather. Chemical damage can also occur from exposure to environmental factors, including ultraviolet light, ozone, acid from sulfurous and nitrous pollutants in the air, or through a chemical action following any treatment with tallow or oil compounds. Both oxidation and chemical damage occur faster at higher temperatures. There are few methods to maintain and clean leather goods properly such as using damp cloth and avoid using a wet cloth or soaking the leather in water. Various treatments are available such as conditioners. Saddle soap is used for cleaning, conditioning, and softening leather. Leather shoes are widely conditioned with shoe polish.[42] In modern culture Due to its excellent resistance to abrasion and wind, leather found a use in rugged occupations. The enduring image of a cowboy in leather chaps gave way to the leather-jacketed and leather-helmeted aviator.[43] When motorcycles were invented, some riders took to wearing heavy leather jackets to protect from road rash and wind blast; some also wear chaps or full leather pants to protect the lower body. Leather's flexibility allows it to be formed and shaped into balls and protective gear. Subsequently, many sports use equipment made with leather, such as baseball gloves and the ball used in cricket and gridiron football. Leather fetishism is the name popularly used to describe a fetishistic attraction to people wearing leather, or in certain cases, to the garments themselves. Many cars and trucks come with optional or standard leather or \"leather faced\" seating. Religious sensitivities In countries with significant populations of individuals observing religions which place restrictions on material choices, vendors typically clarify the source of leather in their products. Such labeling helps facilitate religious observance, so, for example, a Muslim will not accidentally purchase pigskin or a Hindu can avoid cattleskin. Such taboos increase the demand for religiously neutral leathers such as ostrich and deer. Jainism prohibits the use of leather, since it is obtained by killing animals. Alternatives Many forms of artificial leather have been developed, usually involving polyurethane or vinyl coatings applied to a cloth backing. Many names and brands for such artificial leathers exist, including \"pleather\", a portmanteau of \"plastic leather\", and the brand name Naugahyde.[45]"}
{"url": "https://en.m.wikipedia.org/wiki/Basal_metabolism", "text": "Basal metabolic rate Basal metabolic rate (BMR) is the rate of energy expenditure per unit time by endothermic animals at rest.[1] It is reported in energy units per unit time ranging from watt (joule/second) to ml O2/min or joule per hour per kg body mass J/(h·kg). Proper measurement requires a strict set of criteria to be met. These criteria include being in a physically and psychologically undisturbed state and being in a thermally neutral environment while in the post-absorptive state (i.e., not actively digesting food).[1] In bradymetabolic animals, such as fish and reptiles, the equivalent term standard metabolic rate (SMR) applies. It follows the same criteria as BMR, but requires the documentation of the temperature at which the metabolic rate was measured. This makes BMR a variant of standard metabolic rate measurement that excludes the temperature data, a practice that has led to problems in defining \"standard\" rates of metabolism for many mammals.[1] Metabolism comprises the processes that the body needs to function.[2] Basal metabolic rate is the amount of energy per unit of time that a person needs to keep the body functioning at rest. Some of those processes are breathing, blood circulation, controlling body temperature, cell growth, brain and nerve function, and contraction of muscles. Basal metabolic rate affects the rate that a person burns calories and ultimately whether that individual maintains, gains, or loses weight. The basal metabolic rate accounts for about 60 to 75% of the daily calorie expenditure by individuals. It is influenced by several factors. In humans, BMR typically declines by 1–2% per decade after age 20, mostly due to loss of fat-free mass,[3] although the variability between individuals is high.[4] The body's generation of heat is known as thermogenesis and it can be measured to determine the amount of energy expended. BMR generally decreases with age, and with the decrease in lean body mass (as may happen with aging). Increasing muscle mass has the effect of increasing BMR. Aerobic (resistance) fitness level, a product of cardiovascular exercise, while previously thought to have effect on BMR, has been shown in the 1990s not to correlate with BMR when adjusted for fat-free body mass.[citation needed] But anaerobic exercise does increase resting energy consumption (see \"aerobic vs. anaerobic exercise\").[5] Illness, previously consumed food and beverages, environmental temperature, and stress levels can affect one's overall energy expenditure as well as one's BMR. Indirect calorimetry laboratory with canopy hood (dilution technique) BMR is measured under very restrictive circumstances when a person is awake. An accurate BMR measurement requires that the person's sympathetic nervous system not be stimulated, a condition which requires complete rest. A more common measurement, which uses less strict criteria, is resting metabolic rate (RMR).[6] BMR may be measured by gas analysis through either direct or indirect calorimetry, though a rough estimation can be acquired through an equation using age, sex, height, and weight. Studies of energy metabolism using both methods provide convincing evidence for the validity of the respiratory quotient (RQ), which measures the inherent composition and utilization of carbohydrates, fats and proteins as they are converted to energy substrate units that can be used by the body as energy. BMR is a flexible trait (it can be reversibly adjusted within individuals), with, for example, lower temperatures generally resulting in higher basal metabolic rates for both birds[7] and rodents.[8] There are two models to explain how BMR changes in response to temperature: the variable maximum model (VMM) and variable fraction model (VFM). The VMM states that the summit metabolism (or the maximum metabolic rate in response to the cold) increases during the winter, and that the sustained metabolism (or the metabolic rate that can be indefinitely sustained) remains a constant fraction of the former. The VFM says that the summit metabolism does not change, but that the sustained metabolism is a larger fraction of it. The VMM is supported in mammals, and, when using whole-body rates, passerine birds. The VFM is supported in studies of passerine birds using mass-specific metabolic rates (or metabolic rates per unit of mass). This latter measurement has been criticized by Eric Liknes, Sarah Scott, and David Swanson, who say that mass-specific metabolic rates are inconsistent seasonally.[9] In addition to adjusting to temperature, BMR also may adjust before annual migration cycles.[7] The red knot (ssp. islandica) increases its BMR by about 40% before migrating northward. This is because of the energetic demand of long-distance flights. The increase is likely primarily due to increased mass in organs related to flight.[10] The end destination of migrants affects their BMR: yellow-rumped warblers migrating northward were found to have a 31% higher BMR than those migrating southward.[7] In humans, BMR is directly proportional to a person's lean body mass.[11][12] In other words, the more lean body mass a person has, the higher their BMR; but BMR is also affected by acute illnesses and increases with conditions like burns, fractures, infections, fevers, etc.[12] In menstruating females, BMR varies to some extent with the phases of their menstrual cycle. Due to the increase in progesterone, BMR rises at the start of the luteal phase and stays at its highest until this phase ends. There are different findings in research how much of an increase usually occurs. Small sample, early studies, found various figures, such as; a 6% higher postovulatory sleep metabolism,[13] a 7% to 15% higher 24 hour expenditure following ovulation,[14] and an increase and a luteal phase BMR increase by up to 12%.[15][16] A study by the American Society of Clinical Nutrition found that an experimental group of female volunteers had an 11.5% average increase in 24 hour energy expenditure in the two weeks following ovulation, with a range of 8% to 16%. This group was measured via simultaneously direct and indirect calorimetry and had standardized daily meals and sedentary schedule in order to prevent the increase from being manipulated by change in food intake or activity level.[17] A 2011 study conducted by the Mandya Institute of Medical Sciences found that during a woman's follicular phase and menstrual cycle is no significant difference in BMR, however the calories burned per hour is significantly higher, up to 18%, during the luteal phase. Increased state anxiety (stress level) also temporarily increased BMR.[18] The early work of the scientists J. Arthur Harris and Francis G. Benedict showed that approximate values for BMR could be derived using body surface area (computed from height and weight), age, and sex, along with the oxygen and carbon dioxide measures taken from calorimetry. Studies also showed that by eliminating the sex differences that occur with the accumulation of adipose tissue by expressing metabolic rate per unit of \"fat-free\" or lean body mass, the values between sexes for basal metabolism are essentially the same. Exercise physiology textbooks have tables to show the conversion of height and body surface area as they relate to weight and basal metabolic values. The primary organ responsible for regulating metabolism is the hypothalamus. The hypothalamus is located on the diencephalon and forms the floor and part of the lateral walls of the third ventricle of the cerebrum. The chief functions of the hypothalamus are: The ANS regulates contraction of smooth muscle and cardiac muscle, along with secretions of many endocrine organs such as the thyroid gland (associated with many metabolic disorders). Through the ANS, the hypothalamus is the main regulator of visceral activities, such as heart rate, movement of food through the gastrointestinal tract, and contraction of the urinary bladder. production and regulation of feelings of rage and aggression regulation of body temperature regulation of food intake, through two centers: The feeding center or hunger center is responsible for the sensations that cause us to seek food. When sufficient food or substrates have been received and leptin is high, then the satiety center is stimulated and sends impulses that inhibit the feeding center. When insufficient food is present in the stomach and ghrelin levels are high, receptors in the hypothalamus initiate the sense of hunger. The thirst center operates similarly when certain cells in the hypothalamus are stimulated by the rising osmotic pressure of the extracellular fluid. If thirst is satisfied, osmotic pressure decreases. All of these functions taken together form a survival mechanism that causes us to sustain the body processes that BMR measures. The difference in BMR for men and women is mainly due to differences in body mass. For example, a 55-year-old woman weighing 130 pounds (59 kg) and 66 inches (170 cm) tall would have a BMR of 1,272 kilocalories (5,320 kJ) per day. The revised Harris–Benedict equation In 1984, the original Harris–Benedict equations were revised[20] using new data. In comparisons with actual expenditure, the revised equations were found to be more accurate:[21] According to this formula, the woman in the example above has a BMR of 1,204 kilocalories (5,040 kJ) per day. During the last 100 years, lifestyles have changed, and Frankenfield et al.[23] showed it to be about 5% more accurate. These formulas are based on body mass, which does not take into account the difference in metabolic activity between lean body mass and body fat. Other formulas exist which take into account lean body mass, two of which are the Katch–McArdle formula and Cunningham formula. The Katch–McArdle formula (resting daily energy expenditure) The Katch–McArdle formula is used to predict resting daily energy expenditure (RDEE).[24] The Cunningham formula is commonly cited to predict RMR instead of BMR; however, the formulas provided by Katch–McArdle and Cunningham are the same.[25] According to this formula, if the woman in the example has a body fat percentage of 30%, her resting daily energy expenditure (the authors use the term of basal and resting metabolism interchangeably) would be 1262 kcal per day. The basic metabolic rate varies between individuals. One study of 150 adults representative of the population in Scotland reported basal metabolic rates from as low as 1,027 kilocalories (4,300 kJ) per day to as high as 2,499 kilocalories (10,460 kJ); with a mean BMR of 1,500 kilocalories (6,300 kJ) per day. Statistically, the researchers calculated that 62% of this variation was explained by differences in fat free mass. Other factors explaining the variation included fat mass (7%), age (2%), and experimental error including within-subject difference (2%). The rest of the variation (27%) was unexplained. This remaining difference was not explained by sex nor by differing tissue size of highly energetic organs such as the brain.[26] A cross-sectional study of more than 1400 subjects in Europe and the US showed that once adjusted for differences in body composition (lean and fat mass) and age, BMR has fallen over the past 35 years.[27] The decline was also observed in a meta-analysis of more than 150 studies dating back to the early 1920s, translating into a decline in total energy expenditure of about 6%.[27] About 70% of a human's total energy expenditure is due to the basal life processes taking place in the organs of the body (see table). About 20% of one's energy expenditure comes from physical activity and another 10% from thermogenesis, or digestion of food (postprandial thermogenesis).[29] All of these processes require an intake of oxygen along with coenzymes to provide energy for survival (usually from macronutrients like carbohydrates, fats, and proteins) and expel carbon dioxide, due to processing by the Krebs cycle. For the BMR, most of the energy is consumed in maintaining fluid levels in tissues through osmoregulation, and only about one-tenth is consumed for mechanical work, such as digestion, heartbeat, and breathing.[30] What enables the Krebs cycle to perform metabolic changes to fats, carbohydrates, and proteins is energy, which can be defined as the ability or capacity to do work. The breakdown of large molecules into smaller molecules—associated with release of energy—is catabolism. The building up process is termed anabolism. The breakdown of proteins into amino acids is an example of catabolism, while the formation of proteins from amino acids is an anabolic process. Exergonic reactions are energy-releasing reactions and are generally catabolic. Endergonic reactions require energy and include anabolic reactions and the contraction of muscle. Metabolism is the total of all catabolic, exergonic, anabolic, endergonic reactions. Adenosine triphosphate (ATP) is the intermediate molecule that drives the exergonic transfer of energy to switch to endergonic anabolic reactions used in muscle contraction. This is what causes muscles to work which can require a breakdown, and also to build in the rest period, which occurs during the strengthening phase associated with muscular contraction. ATP is composed of adenine, a nitrogen containing base, ribose, a five carbon sugar (collectively called adenosine), and three phosphate groups. ATP is a high energy molecule because it stores large amounts of energy in the chemical bonds of the two terminal phosphate groups. The breaking of these chemical bonds in the Krebs Cycle provides the energy needed for muscular contraction. Because the ratio of hydrogen to oxygen atoms in all carbohydrates is always the same as that in water—that is, 2 to 1—all of the oxygen consumed by the cells is used to oxidize the carbon in the carbohydrate molecule to form carbon dioxide. Consequently, during the complete oxidation of a glucose molecule, six molecules of carbon dioxide and six molecules of water are produced and six molecules of oxygen are consumed. The chemical composition for fats differs from that of carbohydrates in that fats contain considerably fewer oxygen atoms in proportion to atoms of carbon and hydrogen. When listed on nutritional information tables, fats are generally divided into six categories: total fats, saturated fatty acid, polyunsaturated fatty acid, monounsaturated fatty acid, dietary cholesterol, and trans fatty acid. From a basal metabolic or resting metabolic perspective, more energy is needed to burn a saturated fatty acid than an unsaturated fatty acid. The fatty acid molecule is broken down and categorized based on the number of carbon atoms in its molecular structure. The chemical equation for metabolism of the twelve to sixteen carbon atoms in a saturated fatty acid molecule shows the difference between metabolism of carbohydrates and fatty acids. Palmitic acid is a commonly studied example of the saturated fatty acid molecule. The overall equation for the substrate utilization of palmitic acid is Proteins are composed of carbon, hydrogen, oxygen, and nitrogen arranged in a variety of ways to form a large combination of amino acids. Unlike fat the body has no storage deposits of protein. All of it is contained in the body as important parts of tissues, blood hormones, and enzymes. The structural components of the body that contain these amino acids are continually undergoing a process of breakdown and replacement. The respiratory quotient for protein metabolism can be demonstrated by the chemical equation for oxidation of albumin: The reason this is important in the process of understanding protein metabolism is that the body can blend the three macronutrients and based on the mitochondrial density, a preferred ratio can be established which determines how much fuel is utilized in which packets for work accomplished by the muscles. Protein catabolism (breakdown) has been estimated to supply 10% to 15% of the total energy requirement during a two-hour aerobic training session. This process could severely degrade the protein structures needed to maintain survival such as contractile properties of proteins in the heart, cellular mitochondria, myoglobin storage, and metabolic enzymes within muscles. The oxidative system (aerobic) is the primary source of ATP supplied to the body at rest and during low intensity activities and uses primarily carbohydrates and fats as substrates. Protein is not normally metabolized significantly, except during long term starvation and long bouts of exercise (greater than 90 minutes.) At rest approximately 70% of the ATP produced is derived from fats and 30% from carbohydrates. Following the onset of activity, as the intensity of the exercise increases, there is a shift in substrate preference from fats to carbohydrates. During high intensity aerobic exercise, almost 100% of the energy is derived from carbohydrates, if an adequate supply is available. Studies published in 1992[31] and 1997[32] indicate that the level of aerobic fitness of an individual does not have any correlation with the level of resting metabolism. Both studies find that aerobic fitness levels do not improve the predictive power of fat free mass for resting metabolic rate. However, recent research from the Journal of Applied Physiology, published in 2012,[33] compared resistance training and aerobic training on body mass and fat mass in overweight adults (STRRIDE AT/RT). When you consider time commitments against health benefits, aerobic training is the optimal mode of exercise for reducing fat mass and body mass as a primary consideration, resistance training is good as a secondary factor when aging and lean mass are a concern. Resistance training causes injuries at a much higher rate than aerobic training.[33] Compared to resistance training, it was found that aerobic training resulted in a significantly more pronounced reduction of body weight by enhancing the cardiovascular system which is what is the principal factor in metabolic utilization of fat substrates. Resistance training if time is available is also helpful in post-exercise metabolism, but it is an adjunctive factor because the body needs to heal sufficiently between resistance training episodes, whereas with aerobic training, the body can accept this every day. RMR and BMR are measurements of daily consumption of calories.[34][33] The majority of studies that are published on this topic look at aerobic exercise because of its efficacy for health and weight management. Anaerobic exercise, such as weight lifting, builds additional muscle mass. Muscle contributes to the fat-free mass of an individual and therefore effective results from anaerobic exercise will increase BMR.[35] However, the actual effect on BMR is controversial and difficult to enumerate. Various studies[36][37] suggest that the resting metabolic rate of trained muscle is around 55 kJ/kg per day. Even a substantial increase in muscle mass, say 5 kg, would make only a minor impact on BMR. In 1926, Raymond Pearl proposed that longevity varies inversely with basal metabolic rate (the \"rate of living hypothesis\"). Support for this hypothesis comes from the fact that mammals with larger body size have longer maximum life spans (large animals do have higher total metabolic rates, but the metabolic rate at the cellular level is much lower, and the breathing rate and heartbeat are slower in larger animals) and the fact that the longevity of fruit flies varies inversely with ambient temperature.[38] Additionally, the life span of houseflies can be extended by preventing physical activity.[39] This theory has been bolstered by several new studies linking lower basal metabolic rate to increased life expectancy, across the animal kingdom—including humans. Calorie restriction and reduced thyroid hormone levels, both of which decrease the metabolic rate, have been associated with higher longevity in animals.[40][41][42][43][unreliable medical source?] One problem with understanding the associations of lifespan and metabolism is that changes in metabolism are often confounded by other factors that may affect lifespan. For example under calorie restriction whole body metabolic rate goes down with increasing levels of restriction, but body temperature also follows the same pattern. By manipulating the ambient temperature and exposure to wind it was shown in mice and hamsters that body temperature is a more important modulator of lifespan than metabolic rate.[45] In allometric scaling, maximum potential life span (MPLS) is directly related to metabolic rate (MR), where MR is the recharge rate of a biomass made up of covalent bonds. That biomass (W) is subjected to deterioration over time from thermodynamic, entropic pressure. Metabolism is essentially understood as redox coupling, and has nothing to do with thermogenesis. Metabolic efficiency (ME) is then expressed as the efficiency of this coupling, a ratio of amperes[clarification needed] captured and used by biomass, to the amperes available for that purpose. MR is measured in watts, W is measured in grams. These factors are combined in a power law, an elaboration on Kleiber's law relating MR to W and MPLS, that appears as MR = W^ (4ME-1)/4ME.[clarification needed] When ME is 100%, MR = W^3/4; this is popularly known as quarter power scaling, a version of allometric scaling that is premised upon unrealistic estimates of biological efficiency. The equation reveals that as ME drops below 20%, for W < one gram, MR/MPLS increases so dramatically as to endow W with virtual immortality by 16%. The smaller W is to begin with, the more dramatic is the increase in MR as ME diminishes. All of the cells of an organism fit into this range, i.e., less than one gram, and so this MR will be referred to as BMR. But the equation reveals that as ME increases over 25%, BMR approaches zero. The equation also shows that for all W > one gram, where W is the organization of all of the BMRs of the organism's structure, but also includes the activity of the structure, as ME increases over 25%, MR/MPLS increases rather than decreases, as it does for BMR. An MR made up of an organization of BMRs will be referred to as an FMR. As ME decreases below 25%, FMR diminishes rather than increases as it does for BMR. The antagonism between FMR and BMR is what marks the process of aging of biomass W in energetic terms. The ME for the organism is the same as that for the cells, such that the success of the organism's ability to find food (and lower its ME), is key to maintaining the BMR of the cells driven, otherwise, by starvation, to approaching zero; while at the same time a lower ME diminishes the FMR/MPLS of the organism.[citation needed] A person's metabolism varies with their physical condition and activity. Weight training can have a longer impact on metabolism than aerobic training, but there are no known mathematical formulas that can exactly predict the length and duration of a raised metabolism from trophic changes with anabolic neuromuscular training. A decrease in food intake will typically lower the metabolic rate as the body tries to conserve energy.[46] Researcher Gary Foster estimates that a very low calorie diet of fewer than 800 calories a day would reduce the metabolic rate by more than 10 percent.[47] Heart rate is determined by the medulla oblongata and part of the pons, two organs located inferior to the hypothalamus on the brain stem. Heart rate is important for basal metabolic rate and resting metabolic rate because it drives the blood supply, stimulating the Krebs cycle.[citation needed] During exercise that achieves the anaerobic threshold, it is possible to deliver substrates that are desired for optimal energy utilization. The anaerobic threshold is defined as the energy utilization level of heart rate exertion that occurs without oxygen during a standardized test with a specific protocol for accuracy of measurement,[citation needed] such as the Bruce Treadmill protocol (see metabolic equivalent of task). With four to six weeks of targeted training the body systems can adapt to a higher perfusion of mitochondrial density for increased oxygen availability for the Krebs cycle, or tricarboxylic cycle, or the glycolytic cycle.[citation needed] This in turn leads to a lower resting heart rate, lower blood pressure, and increased resting or basal metabolic rate.[citation needed] By measuring heart rate we can then derive estimations of what level of substrate utilization is actually causing biochemical metabolism in our bodies at rest or in activity.[49] This in turn can help a person to maintain an appropriate level of consumption and utilization by studying a graphical representation of the anaerobic threshold. This can be confirmed by blood tests and gas analysis using either direct or indirect calorimetry to show the effect of substrate utilization.[citation needed] The measures of basal metabolic rate and resting metabolic rate are becoming essential tools for maintaining a healthy body weight.[citation needed]"}
{"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2614224/", "text": "Share RESOURCES As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more: PMC Disclaimer | PMC Copyright Notice Abstract Unravelling the timing of the metazoan radiation is crucial for elucidating the macroevolutionary processes associated with the Cambrian explosion. Because estimates of metazoan divergence times derived from molecular clocks range from quite shallow (Ediacaran) to very deep (Mesoproterozoic), it has been difficult to ascertain whether there is concordance or quite dramatic discordance between the genetic and geological fossil records. Here, we show using a range of molecular clock methods that the major pulse of metazoan divergence times was during the Ediacaran, which is consistent with a synoptic reading of the Ediacaran macrobiota. These estimates are robust to changes in priors, and are returned with or without the inclusion of a palaeontologically derived maximal calibration point. Therefore, the two historical records of life both suggest that although the cradle of Metazoa lies in the Cryogenian, and despite the explosion of ecology that occurs in the Cambrian, it is the emergence of bilaterian taxa in the Ediacaran that sets the tempo and mode of macroevolution for the remainder of geological time. 1. Introduction Central to unravelling the causality and biological significance of the Cambrian explosion is accurately and precisely elucidating the origination times of the metazoan phyla. Despite the fact that the Cambrian explosion is geologically obvious (Darwin 1859), it has long been argued that this same geological record, owing to its incompleteness, might be misleading when considering metazoan origins (Runnegar 1982b). As Runnegar (1986) recognized, a second ‘fossil record’, the genetic record written in the DNA of all living organisms, could be used to test hypotheses about the completeness of the geological record (Peterson et al. 2007), and initial attempts at using a molecular clock strongly suggested that metazoans had a deep and cryptic Precambrian history (Runnegar 1982a, 1986; Wray et al. 1996; reviewed recently by Conway Morris 2006). Nonetheless, several palaeontologists have cogently argued that the fossil record provides positive evidence for the absence of Early Neoproterozoic and Mesoproterozoic animals, casting doubt on the veracity of these molecular clock estimates (Budd & Jensen 2000, 2003; Jensen et al. 2005; Conway Morris 2006; Butterfield 2007). Hence, comparisons between the genetic and geological fossil records of early animal evolution, as currently understood, suggest that either the geological record is woefully incomplete or that there is something seriously awry with our reading of the genetic record (Bromham 2006). To explore the apparent incongruity between the known fossil record and the very deep estimates of metazoan diversification as revealed by molecular clocks, Peterson and colleagues (Peterson et al. 2004; Peterson & Butterfield 2005) assembled the largest novel dataset yet, showing that the two records were remarkably concordant: metazoans originated sometime during the Cryogenian, and bilaterians arose during the Ediacaran. Part of the reason for the prior discrepancy concerned the use of vertebrate divergence times. Peterson et al. (2004) discovered that there was an approximately twofold rate reduction across the vertebrate protein-coding genome as compared with the three invertebrate lineages examined (echinoderms, molluscs and insects), consistent with total genome comparisons between vertebrates and dipteran insects (Zdobnov et al. 2002). However, some studies using invertebrate calibrations have also inferred divergence times consistent with a cryptic Precambrian history of Metazoa (Pisani et al. 2004; Regier et al. 2005), suggesting that the twofold rate reduction across the vertebrate genome is only one of many factors influencing the estimation of divergence times (Linder et al. 2005; Peterson & Butterfield 2005). In addition, Peterson et al.'s (2004) estimates and explanations were called into question by several workers, notably Blair & Hedges (2005) who argued that Peterson et al. (2004) used palaeontologically derived calibration points as maxima as opposed to minima, which generated spuriously shallow estimates for metazoan divergences. Although false, as Peterson et al. (2004) stated explicitly (see also Peterson & Butterfield 2005), this criticism highlights an important issue surrounding the use of molecular clocks, namely the proper way to incorporate calibration points into molecular clock analyses (Benton & Donoghue 2007). Recent experimental analyses have shown the importance of numerous, well-constrained calibration points for returning accurate and precise estimates of divergence times, and thus highlighting the need to pay particular attention to this aspect of molecular dating (Roger & Hug 2006; Hug & Roger 2007). Nonetheless, difficulties arise when incorporating fossils into a molecular clock analysis: unlike the establishment of a minimal divergence time for any two taxa, which is simply the first appearance of either one of the taxa, estimating the maximum divergence time is much more difficult (Benton & Donoghue 2007). Two types of maxima have been proposed: a ‘hard’ maximum proposes an absolute value for the oldest possible date of divergence; whereas a ‘soft’ maximum treats a divergence as having some chance of being older than a particular date, depending on a probability distribution used to describe the calibration point (Hedges & Kumar 2004; Yang & Rannala 2006; Benton & Donoghue 2007). Most modern molecular clock methods (e.g. Sanderson 1997, 2002; Thorne et al. 1998; Drummond et al. 2006) allow constraining, as well as fixing, the age of a calibration point, so that every fossil divergence can be defined using a minimum and a maximum. This is a significant improvement over older molecular clock approaches (e.g. Kumar & Hedges 1998) because it allows the integration of palaeontological uncertainty in the estimation of divergence times. However, most existing molecular clock software including ‘r 8 s’ (Sanderson 2004) and ‘Multidivtime’ (Thorne & Kishino 2002), do not distinguish between hard and soft maxima, instead treating all maxima as hard. The difficulty here is that divergence times estimated with uncertain maxima treated as if they were hard can only give minimum estimates for the true divergence time, as the soft maxima might significantly underestimate the true age of the calibration points. Nonetheless, Drummond et al. (2006) have now implemented Bayesian relaxed molecular clock methods (in the software package Beast) where soft maxima can be properly modelled using a probability distribution, and can thus be older than their proposed fossil date. Here, we set out to explore the diversification of animal phyla in the Neoproterozoic using alternative relaxed molecular clock approaches while testing the stability of our results to the choice of different priors and to the deletion of palaeontologically derived maxima, and modelling soft maxima using the most appropriate probability distribution. We find that, although deleting or relaxing maxima tends to push divergence times towards the past (as expected), all estimates are largely congruent between algorithms. We conclude that a synoptic reading of both the geological and genetic fossil records demonstrates that the Ediacaran was the time of major diversification of most higher-level animal taxa and set the stage for Phanerozoic-like macroecology and macroevolution. (b) Molecular clock calibration Calibration points were taken from Peterson et al. (2004) except for the minimum estimate for crown-group Eleutherozoa, which was adjusted from 475 to 480 Myr ago in light of the discovery of a slightly older asterozoan (Blake & Guensberg 2005), and the minimum and the maximum for crown-group Diptera were taken from Benton & Donoghue (2007). Several new maxima and minima were incorporated into this analysis. First, the maximum for the origin of crown-group echinoderms was set at 520 Myr ago, the first appearance of stereom in the fossil record. Because stereom is a highly distinctive skeletal material, and its presence in numerous stem-group taxa (Smith 2005) demonstrates that stereom is a total-group echinoderm character, it must have evolved before the origin of the crown group. Indeed, if stereom had evolved much earlier (i.e. before the Tommotian, ˜525 Myr ago), then one would expect that stereom would be aragonitic as opposed to calcitic, given that mineral choice seems dictated by the sea water chemistry at the time the skeleton was first acquired (Porter 2007). Second, this same time point also sets the minimum for Ambulacraria (Echinodermata+Hemichordata), as echinoderms appear before hemichordates in the rock record (Budd & Jensen 2003). Third, because ambulacrarians are characterized by the possession of four to six coeloms in each animal (Peterson et al. 2000; Smith et al. 2004), and because coeloms cannot predate the first appearance of bilaterian traces (Budd & Jensen 2000, 2003), the first appearance of traces sets the maximum age for crown-group Ambulacraria, ca 555 Myr ago (Martin et al. 2000; Jensen et al. 2005). Fourth, the maximum for the origin of Gastropoda+Bivalvia, is the first appearance of skeletons in the fossil record, ca 542 Myr ago (Bengtson 1994; Amthor et al. 2003). Fifth, the maximum for the origin of crown-group demosponges, is the first appearance of demosponge-specific biomarkers (McCaffrey et al. 1994; Love et al. 2006; see Peterson et al. 2007 for discussion), sometime after the Sturtian, ca 657 Myr ago (Kendall et al. 2006). Finally, the maximum for the origin of crown-group Eumetazoa, which was only used in the Beast analyses, is argued to be 635 Myr ago based on palaeoecological observations (Peterson & Butterfield 2005). Newly incorporated minima include the first appearance of arthropod traces 525 Myr ago (Budd & Jensen 2003) as a minimum for the divergence between insects and the priapulid, the first appearance of medusozoans 500 Myr ago (Hagadorn et al. 2002) as a minimum for the origin of the crown-group Cnidaria, and the first appearance of vertebrates 520 Myr ago, as the minimum for the origin of crown-group chordates (Benton & Donoghue 2007). (c) Molecular estimates of divergence times Molecular estimates of divergence times were obtained using the Bayesian methods of Thorne et al. (1998) as implemented in Multidivtime (Thorne & Kishino 2002), and Drummond et al. (2006) as implemented in Beast v. 1.4.2 (Drummond & Rambaut 2007). All divergence times were calculated assuming the tree topology of figure 1, which was derived from MrBayes (see above and Sperling et al. 2007). For the Multidivtime analyses, branch lengths were estimated using the Estbranches program from the Multidivtime package, under the WAG model. For Beast analyses, starting branch lengths were assigned arbitrarily to match the constraints imposed by the calibrations. The timing of the metazoan radiation according to the molecular clock. The phylogenetic tree for 41 metazoan taxa rooted on the yeast Saccharomyces cerevisiae as determined by Bayesian phylogenetic analysis (see text) is shown. The deuterostomes are shown in red, spiralian protostomes in green, ecdysozoan protostomes in blue, cnidarians in orange, the homoscleromorph Oscarella in salmon pink, calcisponges in purple and demosponges in magenta. The nodes of the tree are positioned according to the optimum as determined from the Bayesian autocorrelated method of Thorne et al. (1998), as implemented in the software package ‘Multidivtime’ (Thorne & Kishino 2002) using a root prior of 1000 Myr ago (s.d.=500 Myr ago). The 95% HPD credibility intervals are shown in brackets. The red crosses are the estimates for clades with internal calibration points as determined by Bayesian algorithm Beast (Drummond et al. 2006) using uniform priors and an exponential rate distribution; black Xs are the estimates using exponential priors and the same rate distribution. Note that much of the metazoan diversification occurs during the Ediacaran (brown), which lies between the Cryogenian (ice blue) and the Cambrian (green). For the Multidivtime analyses, a prior age for the root node (in our case the Fungi–Metazoa split) must be specified. We assumed a 1000 Myr ago prior for this node (Knoll 1992; Douzery et al. 2004), and then tested whether this choice affected our results by performing analyses in which this age was changed to 100 Myr ago (s.d.=500 Myr ago), 1500 Myr ago (s.d.=500 Myr ago) and 2000 Myr ago (s.d.=750 Myr ago). Other priors used in Multidivtime analyses include the mean and standard deviation of the prior distribution at the root node, and ‘Minab’ (parameter for beta prior on proportional node depth). The mean and standard deviation of the prior distribution of the rate at the root node were set to 0.039, as estimated from the data following the procedure outlined in the Multidivtime manual, and the effect that 100-fold changes to this parameter had on the results were assessed. The Minab parameter affects the distribution of the nodes through time—Minab values greater than 1 will cause the nodes to repel each other, while values smaller than 1 will cause the nodes to attract each other. This parameter was set to 1 for our analyses, but we assessed how changing the Minab parameter from 0.6 to 1.4 affected our results. Beast implements uncorrelated relaxed clock methods, which assumes an overall distribution of rates across branches, but does not assume that the rates on adjacent branches are autocorrelated. We used both the exponential and lognormal rate distributions with two different calibration schemes: one with hard maxima in which most calibrations were treated as uniform priors on clade ages, and a second with only soft maxima, in which all calibrations were treated as exponential priors, with 95% of their density lying between the uniform maximum and minimum. In both schemes, the maximum at 635 Myr ago was treated as an exponential prior, with 90% of its density lying below 635 Myr ago, giving a 10% prior chance that this calibration point is incorrect. All other priors and operators were kept at default settings, except that all operators that alter the tree topology were disabled. Ninety-five per cent highest posterior density (HPD) credibility intervals are automatically calculated by Multidivtime, and were calculated using the program Tracer for the Beast analyses. To test whether our priors dominated the posterior distribution, all our Beast and Multidivtime analyses were also performed without data and the results obtained in these runs were compared with those obtained when the data were actually analysed. 3. Results Molecular divergence times were estimated using the topology shown in figure 1. Support for Cnidaria and Deuterostomia was low (67 and 33%, respectively), probably owing to long-branch artefacts (Pisani 2004) associated with Ciona and Obelia in particular (indeed the value for Deuterostomia increases to more than 90% with the removal of Ciona), but given the clear monophyly of the phyla Chordata and Cnidaria constraining these nodes should not generate spurious molecular divergence estimates. Most of the other nodes were strongly supported, including Calcispongia+Eumetazoa and Eumetazoa, supporting the results of Peterson & Butterfield (2005), and contra the conclusions of Rokas and colleagues (Rokas et al. 2005; see also Baurain et al. 2006). Indeed, within Protostomia, for example, all but one node (Stylochus+Nemertea) have posterior probability values above 80%, and both Lophotrochozoa and Ecdysozoa, as well as Annelida+Mollusca, have clade credibility values of 100%. In addition, we find strong support for the node Homoscleromorpha+Eumetazoa, which indicates that there are at least three independent extant sponge lineages (Sperling et al. 2007). Using this topology as a constraint tree, divergence times were estimated using the Bayesian autocorrelated method of Thorne et al. (1998), as implemented in the software package Multidivtime (Thorne & Kishino 2002). These Bayesian estimates are robust to changes in the age of the root prior as the estimates are essentially the same whether the age is 100 Myr ago (s.d.=500 Myr ago) or 2000 Myr ago (s.d.=750 Myr ago; table 1), suggesting that the age of the root prior is not biasing the analyses. Also, changing the value of Minab, or the mean rate of evolution of the root node, did not change our results (not shown). Running the analyses without data confirmed that our results were not dominated by our choice of priors (not shown). The suggestion that fungi diverged from animals ca 1000 Myr ago (Knoll 1992; Douzery et al. 2004) was confirmed by all our analyses that did not assume a particular age for the root node. Thus, we used the values derived from the 1000 Myr ago (s.d.=500 Myr ago) prior on figure 1. Table 1 Optima (maxima and minima) in millions of years derived from Multidivtime (M) and Beast (B) for five key metazoan divergences. eEstimates derived using an exponential rate distribution and uniform priors. fEstimates derived using an exponential rate distribution and exponential priors; gEstimates derived using a lognormal rate distribution and uniform priors; hEstimates derived using a lognormal rate distribution and exponential priors. The removal of the deeper calibration point, namely the maximum age of 657 Myr ago for the origin of crown-group demosponges, resulted in increasing the estimate for the age of crown-group Metazoa by approximately 18% (from 766 to 904 Myr ago; table 1). Nonetheless, the age for both crown-group Protostomia and crown-group Deuterostomia increased by only approximately 4–5%, suggesting that the results derived with the use of this maximum are generally robust. Given its position in the tree, the geological depth of the divergence, and the unique nature of the evidence (biomarkers), this maximum is most likely adding both accuracy and precision to the clock estimates. We next explored these same divergence times using the models implemented in Beast (Drummond et al. 2006). In general, the estimates derived from Beast using an exponential rate distribution and uniform priors (red crosses in figure 1) are similar to those derived from Multidivtime (table 1). The analyses that use exponential priors are somewhat deeper than those that use uniform priors (black Xs in figure 1), and those using a lognormal rate distribution are deeper than those derived from an exponential rate distribution (table 1), presumably because the exponential distribution on rates is leading to greater autocorrelation between rates. Analyses without data again confirmed that the priors were not dominating the data (results not shown). 4. Discussion (a) Concordance between the genetic and geological fossil records Here we have shown, using a variety of analyses and appropriately testing for biases that may have been introduced by the use of palaeontologically derived maxima, that the genetic fossil record strongly supports the notion that the diversification of metazoans in general, and bilaterian metazoans in particular, occurred during the Ediacaran Period, 635–542 Myr ago (Knoll et al.2004, 2006). How do these molecular estimates compare with the known geological record? Macroscopic fossils of the Ediacara biota span the upper half of the Ediacaran Period, from 575 to 542 Myr ago (Grotzinger et al. 1995; Martin et al. 2000; Bowring et al. 2003; Condon et al. 2005). Since most of these fossils occur as soft-bodied impressions in relatively coarse-grained siliciclastic sedimentary rocks, a comprehensive array of palaeobiological interpretations of the Ediacara biota has been put forth. Nonetheless, a few taxa stand out as potential candidates for affinities within Metazoa. One taxon in particular, Kimberella, has generated much discussion as a possible triploblastic metazoan. Not only does it compare well in external form to molluscs (Fedonkin & Waggoner 1997), in a few cases an everted proboscis is preserved (Gehling et al. 2005) that is inferred to contain a radula-like organ given the association between specimens of Kimberella (figure 2a, asterisk) and aligned sets of paired scratch marks (figure 2a, arrows; Gehling et al. 2005). These findings suggest that Kimberella was preserved in place while grazing on substrate microbial mats (Seilacher 1999; Gehling et al. 2005). Given that we estimated the divergence between annelids and molluscs to be ca 570 Myr ago (figure 1), it is possible, if not probable, that Kimberella is allied with modern molluscs. What about other higher-level clades? Our estimates suggest that arthropods diverged from priapulids ca 575 Myr ago, suggesting that stem-group panarthropods (Nielsen 2001) should be present in Upper Ediacaran rocks. Interestingly, several taxa compare favourably with a panarthropod interpretation. For example, large specimens of Parvancorina show lateral structures originating on either side of the medial ridge that might be characterized as appendages (figure 2b). In fact, in external form, Parvancorina bears a striking resemblance to the unmineralized kite-shaped Cambrian arthropod Skania (Lin et al. 2006). Spriggina (figure 2c) also preserves large numbers of appendage-like structures, and still others like Marywadea (figure 2d) show apparent cephalic branching structures that resemble digestive caecae in arthropods. Importantly (see below), all of these taxa were no larger than 10 cm in maximum dimension (Gehling 1999; Fedonkin 2003; see figure 2), and appear simultaneously with the first demonstrable trace fossils (Droser et al. 2005; Jensen et al. 2005). The absence of arthropod scratch marks (Seilacher 1999), though, is not too worrisome given that such traces would demand the presence of sclerotized appendages to cut through the ubiquitously present microbial mats, a character not necessitated by the presence of stem-group panarthropods, or even deeply nested stem-group arthropods, in Ediacaran-aged sediments. Indeed, the distinct possibility remains that this fauna preserves numerous stem-group forms ranging from basal triploblasts up through basal ecdysozoans, spiralians and possibly even deuterostomes. Given the enigmatic nature of some very prominent taxa like Dickinsonia (figure 2e), a taxon that appears capable of some form of limited motility (Gehling et al. 2005), a position for Dickinsonia within total-group Eumetazoa is not out of the question. In fact, mobile but saprophytic feeding without the use of a gut would be compelling evidence that some form of ectomesoderm predates the advent of endoderm. (b) Discordance between the genetic and geological fossil records Of course, many others have addressed these questions using a similar approach, and it is worth comparing our results against not only the fossil record but also with other molecular clock estimates as well. It compares well with some molecular analyses, notably Peterson et al. (2004) and Peterson & Butterfield (2005), all of whom argued that the last common ancestor of protostomes and deuterostomes evolved not more than 635 Myr ago. But recently, Blair & Hedges (2005), argued for much deeper divergences, based on a series of penalized likelihood (Sanderson 2002) analyses using r 8 s (Sanderson 2004) in which every calibration point was treated as a minimum. They suggested that the divergence between ambulacrarian and chordate deuterostomes was 896 Myr ago (with the 95% CI spanning from 832 to 1022 Myr ago). They further argued that the divergence between hemichordates and echinoderms was 876 Myr ago (725 and 1074 Myr ago), and the origin of crown-group echinoderms was 730 Myr ago. Finally, they estimated that the divergence between starfish and sea urchins was 580 Myr ago. Unfortunately, their results are most likely spurious because as Sanderson (2004) pointed out, r 8 s cannot converge on a unique solution if only minima are used to calibrate penalized likelihood analyses, which is supported by the fact that their estimate for the origin of a mineralized, coelomate taxon like crown-group Echinodermata precedes their appearance in the fossil record by some 200 Myr ago. Of course, neither the genetic nor the geological fossil record has a monopoly on historical accuracy, and as much as molecular evolutionists need to keep in mind the relevant palaeontological data, palaeontologists need to keep in mind estimates derived from molecular clocks (Donoghue & Benton 2007). For example, Budd & Jensen (2000, 2003) argued that bilaterians could not have had an extensive Precambrian history, as suggested by almost all molecular clocks, as the trace fossil record, and the inferred morphology of these animals, is not consistent with an origin much before 555 Myr ago. They observed that possession of coelom(s) and a blood vascular system (BVS) is inconsistent with a meiofaunal origin, as tiny organisms would have had no need for a transport system like the BVS, and are only consistent with a size large enough to be detected in the geological record. In general, we agree with their arguments, and use their insights to set a maximum age for crown-group Ambulacraria (see above). However, the same argument cannot be extended to many other parts of the bilaterian tree. Contra Budd & Jensen (2000), there is no evidence for homology of coeloms either between protostomes and deuterostomes or even within both protostomes and deuterostomes. Since the coelom is, by definition, just a mesodermally lined cavity (Ruppert 1991; Nielsen 2001), the possession of the space itself cannot be used as an argument of similarity. Instead, topological similarity must be used, and when it is, it strongly suggests homology, for example, within Ambulacraria (Peterson et al. 2000; Smith et al. 2004), but not homology between any other higher taxa (Ruppert 1991; Nielsen 2001). Thus, outside of Ambulacraria, the trace fossil record cannot be used to set a maximum for most bilaterian divergences. In fact, the small size of many putative Ediacaran bilaterians (figure 2), and the fact that acoel flatworms are now recognized as the sister group to the remaining bilaterians (Baguñà & Riutort 2004; Peterson et al. 2005; Sempere et al. 2007), is consistent with an argument that small size and absence of a coelom are primitive for Bilateria. This then removes the final obstacle to a pre-555 Myr ago origin for Bilateria, which is consistent with both the appearance of many different bilaterian lineages in the Ediacaran (figure 2) and the molecular clock (figure 1). Despite the presence of many different stem-group taxa, the Ediacaran is still a transitional ecology, with these organisms confined to a two-dimensional mat world. This stands in dramatic contrast to the Early Cambrian where the multi-tiered food webs that so typify the Phanerozoic were established with the eumetazoan invasion of both the pelagos and the infaunal benthos (Butterfield 1997, 2001; Vannier & Chen 2000, 2005; Dzik 2005; Peterson et al. 2005; Vannier et al. 2007). Hence, although the Ediacaran is an apparent quantum leap in ecological complexity as compared with the ‘boring billions’ that characterize Earth before the Ediacaran, it is still relatively simple when compared with the Cambrian, yet another quantum leap in organismal and ecological evolution. Thus, the Ediacaran stands as the transition interval between the ‘Precambrian’ and the Phanerozoic (Butterfield 2007). Whether the Ediacaran transition was triggered by the introduction of eumetazoans, as argued by Peterson & Butterfield (2005), or by the introduction of mobile, macrophagous triploblasts, as is suggested by our analyses reported here (figure 1), or some other factor or combination of factors, remains to be more fully studied through continued exploration of the relevant rock sections throughout the world, and continued improvements in molecular clock methods. 5. Conclusions Both the genetic and geological fossil records, each with their own inherent biases and artefacts, are largely congruent with one another, and for historical disciplines congruence of independent datasets is the strongest argument one can make for historical accuracy (Pisani et al. 2007). Thus, our analyses suggest that while the cradle of metazoan life is in the Cryogenian, and the explosion of metazoan ecology occurred in the Cambrian, it is the emergence of bilaterians in the Ediacaran that established the ecological and evolutionary rules that largely govern Earth's macrobiota for the remainder of geological time. Acknowledgments K.J.P. was supported by the National Science Foundation; J.A.C. was supported by an RCUK Academic Fellowship and J.G.G. was supported by the Australian Research Council Discovery Project (DG0453393), the A.R.C. Linkage Project LP0774959 including the South Australian Museum and Beach Petroleum Pty Ltd, and the SA Museum Waterhouse Club. We would like to thank P. Donoghue (U. Bristol) for his usual perspicacity, two anonymous reviewers for their helpful comments on an earlier version of this paper, and T. Littlewood (NHM) and M. Telford (UCL) for inviting us to contribute to this symposium volume. Finally, K.J.P. would like to thank all of the students who have come through the lab and contributed data to this project, and the South Australian Museum for a very enlightening visit. Footnotes One contribution of 17 to a Discussion Meeting Issue ‘Evolution of the animals: a Linnean tercentenary celebration’."}
{"url": "https://pubmed.ncbi.nlm.nih.gov/28377584/", "text": "Abstract The phylum Rotifera consists of minuscule, nonsegmented animals with a unique body plan and an unresolved phylogenetic position. The presence of pharyngeal articulated jaws supports an inclusion in Gnathifera nested in the Spiralia. Comparison of Hox genes, involved in animal body plan patterning, can be used to infer phylogenetic relationships. Here, we report the expression of five Hox genes during embryogenesis of the rotifer Brachionus manjavacas and show how these genes define different functional components of the nervous system and not the usual bilaterian staggered expression along the anteroposterior axis. Sequence analysis revealed that the lox5-parapeptide, a key signature in lophotrochozoan and platyhelminthean Hox6/lox5 genes, is absent and replaced by different signatures in Rotifera and Chaetognatha, and that the MedPost gene, until now unique to Chaetognatha, is also present in rotifers. Collectively, our results support an inclusion of chaetognaths in gnathiferans and Gnathifera as sister group to the remaining spiralians.Rotifers are microscopic animals with an unusual, nonsegmented body plan consisting of a head, trunk and foot. Here, Fröbius and Funch investigate the role of Hox genes-which are widely used in animal body plan patterning-in rotifer embryogenesis and find non-canonical expression in the nervous system. Conflict of interest statement Figures Hox gene data places rotifers and chaetognaths in Gnathifera within Spiralia. a Phylogenetic… Fig. 1 Hox gene data places rotifers and chaetognaths in Gnathifera within Spiralia. a Phylogenetic tree depicting the relationship of MedPost genes to PG8 and posterior class Hox genes. Tree topology is from Bayesian analysis. Bayesian posterior probabilities based on 400,000 trees from 40,000,000 generations and ML support values from 1000 iterations are shown above branches. Single values represent Bayesian posterior probabilities only. Asterisks denote ML support below 50%. b Alignment of ten amino acids of the carboxy flanking region to the homeodomain of PG6 genes. Sequences highlighted with yellow contain the new signature found in rotifers and chaetognaths. Blue highlighting marks the lox5-parapeptide of lophotrochozoan genes. Neither is found in Ecdysozoa, Ambulacraria, Chordata, or Xenacoelomorpha. c Summary of representative characteristics of the Hox cluster within different metazoan taxa. The tree to the left represents bilaterian phylogeny with Cnidaria as an outgroup. Boxes in the middle depict Hox gene contingents (color coded according to the assignment of the Hox genes to the different paralogous groups) isolated from representative species. The right hand column summarizes characteristic Hox gene evolution and duplication events along with presence of special Hox signatures resulting in Hox genes characterizing the respective groups Fig. 2 Expression of Hox genes during… Fig. 2 Expression of Hox genes during embryogenesis of Brachionus manjavacas . a Schematic of… Fig. 2 Expression of Hox genes during embryogenesis of Brachionus manjavacas. a Schematic of embryonic stages of Brachionus manjavacas with morphological characteristics used for staging. b Whole-mount in situ hybridization on amictic female embryos. Adults are only shown for genes with expression persisting into the adult stage. Anterior to the top. Mostly ventral views are shown. pv, posterior view, dorsal side up; lv, lateral view, ventral to the left. Scale bar, 10 µm Fig. 3 Body plans and nervous systems… Fig. 3 Body plans and nervous systems in Rotifera and Chaetognatha. a Diagram of Hox… Fig. 3 Body plans and nervous systems in Rotifera and Chaetognatha. a Diagram of Hox gene expression in the nervous system of Brachionus manjavacas. b Comparison of rotiferan and chaetognath body plans with respect to the structure of the nervous system. Both groups have a dorsal brain and additional nerve plexi: mastax nerves and ganglia in rotifers and vestibular, and esophageal ganglia in chaetognaths as well as a caudal ganglion in rotifers and a ventral nerve centre in chaetognaths with the latter possibly incorporating functional subsets, these are still separated from the caudal ganglion in rotifers, e.g., innervation of sensory lateral antennae"}
{"url": "https://en.m.wikipedia.org/wiki/Praying_mantis", "text": "Mantis Mantises are an order (Mantodea) of insects that contains over 2,400 species in about 430 genera in 33 families. The largest family is the Mantidae (\"mantids\"). Mantises are distributed worldwide in temperate and tropical habitats. They have triangular heads with bulging eyes supported on flexible necks. Their elongated bodies may or may not have wings, but all Mantodea have forelegs that are greatly enlarged and adapted for catching and gripping prey; their upright posture, while remaining stationary with forearms folded, has led to the common name praying mantis. The closest relatives of mantises are termites and cockroaches (Blattodea), which are all within the superorderDictyoptera. Mantises are sometimes confused with stick insects (Phasmatodea), other elongated insects such as grasshoppers (Orthoptera), or other more distantly related insects with raptorial forelegs such as mantisflies (Mantispidae). Mantises are mostly ambush predators, but a few ground-dwelling species are found actively pursuing their prey. They normally live for about a year. In cooler climates, the adults lay eggs in autumn, then die. The eggs are protected by their hard capsules and hatch in the spring. Females sometimes practice sexual cannibalism, eating their mates after copulation. Over 2,400 species of mantis in about 430 genera are recognized.[1] They are predominantly found in tropical regions, but some live in temperate areas.[2][3] The systematics of mantises have long been disputed. Mantises, along with stick insects (Phasmatodea), were once placed in the order Orthoptera with the cockroaches (now Blattodea) and ice crawlers (now Grylloblattodea). Kristensen (1991) combined the Mantodea with the cockroaches and termites into the order Dictyoptera, suborder Mantodea.[4][5] The name mantodea is formed from the Ancient Greek words μάντις (mantis) meaning \"prophet\", and εἶδος (eidos) meaning \"form\" or \"type\". It was coined in 1838 by the German entomologist Hermann Burmeister.[6][7] The order is occasionally called the mantes, using a Latinized plural of Greek mantis. The name mantid properly refers only to members of the family Mantidae, which was, historically, the only family in the order. The other common name, praying mantis, applied to any species in the order[8] (though in Europe mainly to Mantis religiosa), comes from the typical \"prayer-like\" posture with folded forelimbs.[9][10] The vernacular plural \"mantises\" (used in this article) was confined largely to the US, with \"mantids\" predominantly used as the plural in the UK and elsewhere, until the family Mantidae was further split in 2002.[11][12] One of the earliest classifications splitting an all-inclusive Mantidae into multiple families was that proposed by Beier in 1968, recognizing eight families,[14] though it was not until Ehrmann's reclassification into 15 families in 2002[12] that a multiple-family classification became universally adopted. Klass, in 1997, studied the external male genitalia and postulated that the families Chaeteessidae and Metallyticidae diverged from the other families at an early date.[15] However, as previously configured, the Mantidae and Thespidae especially were considered polyphyletic,[16] so the Mantodea have been revised substantially as of 2019 and now includes 29 families.[17] Life restoration of Santanmantis, a primitive fossil mantis known from the Early Cretaceous of Brazil, and one of the oldest members of the group Mantises are thought to have evolved from cockroach-like ancestors.[19] The earliest confidently identified mantis fossils date to the Early Cretaceous.[16] Fossils of the group are rare: by 2022, 37 fossil species are known.[16][20] Fossil mantises, including one from Japan with spines on the front legs as in modern mantises, have been found in Cretaceous amber.[21] Most fossils in amber are nymphs; compression fossils (in rock) include adults. Fossil mantises from the Crato Formation in Brazil include the 10 mm (0.39 in) long Santanmantis axelrodi, described in 2003; as in modern mantises, the front legs were adapted for catching prey. Well-preserved specimens yield details as small as 5 μm through X-ray computed tomography.[16] Extinct families and genera include: Because of the superficially similar raptorialforelegs, mantidflies may be confused with mantises, though they are unrelated. Their similarity is an example of convergent evolution; mantidflies do not have tegmina (leathery forewings) like mantises, their antennae are shorter and less thread-like, and the raptorial tibia is more muscular than that of a similar-sized mantis and bends back farther in preparation for shooting out to grasp prey.[22] The raptorial foreleg, showing the unusually long coxa, which, together with the trochanter, gives the impression of a femur. The femur itself is the proximal segment of the grasping part of the leg. Mantis moving on a wall. Mantises have large, triangular heads with a beak-like snout and mandibles. They have two bulbous compound eyes, three small simple eyes, and a pair of antennae. The articulation of the neck is also remarkably flexible; some species of mantis can rotate their heads nearly 180°.[10] The mantis thorax consists of a prothorax, a mesothorax, and a metathorax. In all species apart from the genus Mantoida, the prothorax, which bears the head and forelegs, is much longer than the other two thoracic segments. The prothorax is also flexibly articulated, allowing for a wide range of movements of the head and fore limbs while the remainder of the body remains more or less immobile.[23][24] Mantises also are unique to the Dictyoptera in that they have tympanate hearing, with two tympana in an auditory chamber in their metathorax. Most mantises can only hear ultrasound.[25] Mantises have two spiked, grasping forelegs (\"raptorial legs\") in which prey items are caught and held securely. In most insect legs, including the posterior four legs of a mantis, the coxa and trochanter combine as an inconspicuous base of the leg; in the raptorial legs, however, the coxa and trochanter combine to form a segment about as long as the femur, which is a spiky part of the grasping apparatus (see illustration). Located at the base of the femur is a set of discoidal spines, usually four in number, but ranging from none to as many as five depending on the species. These spines are preceded by a number of tooth-like tubercles, which, along with a similar series of tubercles along the tibia and the apical claw near its tip, give the foreleg of the mantis its grasp on its prey. The foreleg ends in a delicate tarsus used as a walking appendage, made of four or five segments and ending in a two-toed claw with no arolium.[23][26] Mantises can be loosely categorized as being macropterous (long-winged), brachypterous (short-winged), micropterous (vestigial-winged), or apterous (wingless). If not wingless, a mantis has two sets of wings: the outer wings, or tegmina, are usually narrow and leathery. They function as camouflage and as a shield for the hindwings, which are clearer and more delicate.[23][27] The abdomen of all mantises consists of 10 tergites, with a corresponding set of nine sternites visible in males and seven visible in females. The abdomen tends to be slimmer in males than females, but ends in a pair of cerci in both sexes.[23] Mantises have stereo vision.[28][29][30] They locate their prey by sight; their compound eyes contain up to 10,000 ommatidia. A small area at the front called the fovea has greater visual acuity than the rest of the eye, and can produce the high resolution necessary to examine potential prey. The peripheral ommatidia are concerned with perceiving motion; when a moving object is noticed, the head is rapidly rotated to bring the object into the visual field of the fovea. Further motions of the prey are then tracked by movements of the mantis's head so as to keep the image centered on the fovea.[26][31] The eyes are widely spaced and laterally situated, affording a wide binocular field of vision and precise stereoscopic vision at close range.[32] The dark spot on each eye that moves as it rotates its head is a pseudopupil. This occurs because the ommatidia that are viewed \"head-on\" absorb the incident light, while those to the side reflect it.[33] As their hunting relies heavily on vision, mantises are primarily diurnal. Many species, however, fly at night, and then may be attracted to artificial lights. They have good night vision.[34] Mantises in the family Liturgusidae collected at night have been shown to be predominately males;[35] this is probably true for most mantises. Nocturnal flight is especially important to males in locating less-mobile females by detecting their pheromones. Flying at night exposes mantises to fewer bird predators than diurnal flight would. Many mantises also have an auditory thoracic organ that helps them avoid bats by detecting their echolocation calls and responding evasively.[36][37] Mantises are generalist predators of arthropods.[2] The majority of mantises are ambush predators that only feed upon live prey within their reach. They either camouflage themselves and remain stationary, waiting for prey to approach, or stalk their prey with slow, stealthy movements.[38] Larger mantises sometimes eat smaller individuals of their own species,[39] as well as small vertebrates such as lizards, frogs, fish, and particularly small birds.[40][41][42] Most mantises stalk tempting prey if it strays close enough, and will go further when they are especially hungry.[43] Once within reach, mantises strike rapidly to grasp the prey with their spiked raptorial forelegs.[44] Some ground and bark species pursue their prey in a more active way. For example, members of a few genera such as the ground mantises Entella, Ligaria, and Ligariella run over dry ground seeking prey, much as tiger beetles do.[23] The fore gut of some species extends the whole length of the insect and can be used to store prey for digestion later. This may be advantageous in an insect that feeds intermittently.[45]Chinese mantises live longer, grow faster, and produce more young when they are able to eat pollen.[46] Mantises are preyed on by vertebrates such as frogs, lizards, and birds, and by invertebrates such as spiders, large species of hornets, and ants.[47] Some hunting wasps, such as some species of Tachytes also paralyze some species of mantis to feed their young.[48] Generally, mantises protect themselves by camouflage, most species being cryptically colored to resemble foliage or other backgrounds, both to avoid predators and to better snare their prey.[49] Those that live on uniformly colored surfaces such as bare earth or tree bark are dorsoventrally flattened so as to eliminate shadows that might reveal their presence.[50] The species from different families called flower mantises are aggressive mimics: they resemble flowers convincingly enough to attract prey that come to collect pollen and nectar.[51][52][53] Some species in Africa and Australia are able to turn black after a molt towards the end of the dry season; at this time of year, bush fires occur and this coloration enables them to blend in with the fire-ravaged landscape (fire melanism).[50] When directly threatened, many mantis species stand tall and spread their forelegs, with their wings fanning out wide. The fanning of the wings makes the mantis seem larger and more threatening, with some species enhancing this effect with bright colors and patterns on their hindwings and inner surfaces of their front legs. If harassment persists, a mantis may strike with its forelegs and attempt to pinch or bite. As part of the bluffing (deimatic) threat display, some species may also produce a hissing sound by expelling air from the abdominal spiracles. Mantises lack chemical protection, so their displays are largely bluff. When flying at night, at least some mantises are able to detect the echolocation sounds produced by bats; when the frequency begins to increase rapidly, indicating an approaching bat, they stop flying horizontally and begin a descending spiral toward the safety of the ground, often preceded by an aerial loop or spin. If caught, they may slash captors with their raptorial legs.[50][54][55] Mantises, like stick insects, show rocking behavior in which the insect makes rhythmic, repetitive side-to-side movements. Functions proposed for this behavior include the enhancement of crypsis by means of the resemblance to vegetation moving in the wind. However, the repetitive swaying movements may be most important in allowing the insects to discriminate objects from the background by their relative movement, a visual mechanism typical of animals with simpler sight systems. Rocking movements by these generally sedentary insects may replace flying or running as a source of relative motion of objects in the visual field.[56] As ants may be predators of mantises, genera such as Loxomantis, Orthodera, and Statilia, like many other arthropods, avoid attacking them. A variety of arthropods, including some early-instar mantises, exploit this behavior and mimic ants to evade their predators.[57] The mating season in temperate climates typically takes place in autumn,[58][59] while in tropical areas, mating can occur at any time of the year.[59] To mate following courtship, the male usually leaps onto the female's back, clasping her thorax and wing bases with his forelegs. He then arches his abdomen to deposit and store sperm in a special chamber near the tip of the female's abdomen. The female lays between 10 and 400 eggs, depending on the species. Eggs are typically deposited in a froth mass-produced by glands in the abdomen. This froth hardens, creating a protective capsule, which together with the egg mass is called an ootheca. Depending on the species, the ootheca can be attached to a flat surface, wrapped around a plant, or even deposited in the ground.[58] Despite the versatility and durability of the eggs, they are often preyed on, especially by several species of parasitoid wasps. In a few species, mostly ground and bark mantises in the family Tarachodidae, the mother guards the eggs.[58] The cryptic Tarachodes maurus positions herself on bark with her abdomen covering her egg capsule, ambushing passing prey and moving very little until the eggs hatch.[4] An unusual reproductive strategy is adopted by Brunner's stick mantis from the southern United States: no males have ever been found in this species, and the females breed parthenogenetically.[2] The ability to reproduce by parthenogenesis has been recorded in at least two other species, Sphodromantis viridis and Miomantis sp., although these species usually reproduce sexually.[60][61][62] In temperate climates, adults do not survive the winter and the eggs undergo a diapause, hatching in the spring.[5] As in closely related insect groups in the superorder Dictyoptera, mantises go through three life stages: egg, nymph, and adult (mantises are among the hemimetabolous insects). For smaller species, the eggs may hatch in 3–4 weeks as opposed to 4–6 weeks for larger species. The nymphs may be colored differently from the adult, and the early stages are often mimics of ants. A mantis nymph grows bigger as it molts its exoskeleton. Molting can happen five to 10 times before the adult stage is reached, depending on the species. After the final molt, most species have wings, though some species remain wingless or brachypterous (\"short-winged\"), particularly in the female sex. The lifespan of a mantis depends on the species; smaller ones may live 4–8 weeks, while larger species may live 4–6 months.[2][24] Sexual cannibalism is common among most predatory species of mantises in captivity. It has sometimes been observed in natural populations, where about a quarter of male–female encounters result in the male being eaten by the female.[63][64][65] Around 90% of the predatory species of mantises exhibit sexual cannibalism.[66] Adult males typically outnumber females at first, but their numbers may be fairly equivalent later in the adult stage,[5] possibly because females selectively eat the smaller males.[67] In Tenodera sinensis, 83% of males escape cannibalism after an encounter with a female, but since multiple matings occur, the probability of a male's being eaten increases cumulatively.[64] The female may begin feeding by biting off the male's head (as they do with regular prey), and if mating has begun, the male's movements may become even more vigorous in its delivery of sperm. Early researchers thought that because copulatory movement is controlled by a ganglion in the abdomen, not the head, removal of the male's head was a reproductive strategy by females to enhance fertilization while obtaining sustenance. Later, this behavior appeared to be an artifact of intrusive laboratory observation. Whether the behavior is natural in the field or also the result of distractions caused by the human observer remains controversial. Mantises are highly visual organisms and notice any disturbance in the laboratory or field, such as bright lights or moving scientists. Chinese mantises that had been fed ad libitum (so that they were not hungry) actually displayed elaborate courtship behavior when left undisturbed. The male engages the female in a courtship dance, to change her interest from feeding to mating.[68] Under such circumstances, the female has been known to respond with a defensive deimatic display by flashing the colored eyespots on the inside of her front legs.[69] The reason for sexual cannibalism has been debated; experiments show that females on poor diets are likelier to engage in sexual cannibalism than those on good diets.[70] Some hypothesize that submissive males gain a selective advantage by producing offspring; this is supported by a quantifiable increase in the duration of copulation among males which are cannibalized, in some cases doubling both the duration and the chance of fertilization. This is contrasted by a study where males were seen to approach hungry females with more caution, and were shown to remain mounted on hungry females for a longer time, indicating that males that actively avoid cannibalism may mate with multiple females. The same study also found that hungry females generally attracted fewer males than those that were well fed.[71] The act of dismounting after copulation is dangerous for males, for it is the time that females most frequently cannibalize their mates. An increase in mounting duration appears to indicate that males wait for an opportune time to dismount a hungry female, who would be likely to cannibalize her mate.[69] Experiments have revealed that the sex ratio in an environment determines male copulatory behavior of Mantis religiosa which in turn affects the cannibalistic tendencies of the female and support the sperm competition hypothesis because the polyandrous treatment recorded the highest copulation duration time and lowest cannibalism. This further suggests that dismounting the female can make males susceptible to cannibalism.[72] One of the earliest mantis references is in the ancient Chinese dictionary Erya, which gives its attributes in poetry, where it represents courage and fearlessness, and a brief description. A later text, the Jingshi Zhenglei Daguan Bencao [zh] (transl. \"Great History of Medical Material Annotated and Arranged by Types, Based upon the Classics and Historical Works\") from 1108, gives accurate details of the construction of the egg packages, the development cycle, anatomy, and the function of the antennae. Although mantises are rarely mentioned in Ancient Greek sources, a female mantis in threat posture is accurately illustrated on a series of fifth-century BC silver coins, including didrachms, from Metapontum in Lucania.[73] In the 10th century AD, Byzantine era Adages, Suidas describes an insect resembling a slow-moving green locust with long front legs.[74] He translates Zenobius 2.94 with the words seriphos (maybe a mantis) and graus, an old woman, implying a thin, dried-up stick of a body.[75] Mantises are a common motif in Luna Polychrome ceramics of pre-Columbian Nicaragua, and are believed to represent a deity or spirit called \"Madre Culebra\".[76] Western descriptions of the biology and morphology of the mantises became more accurate in the 18th century. Roesel von Rosenhof illustrated and described mantises and their cannibalistic behavior in the Insekten-Belustigungen (Insect Entertainments).[77] he [Geronimo the gecko] crashed into the mantis and made her reel, and grabbed the underside of her thorax in his jaws. Cicely [the mantis] retaliated by snapping both her front legs shut on Geronimo's hindlegs. They rustled and staggered across the ceiling and down the wall, each seeking to gain some advantage.[79] Two martial arts separately developed in China have movements and fighting strategies based on those of the mantis.[88][89] As one of these arts was developed in northern China, and the other in southern parts of the country, the arts are today referred to (both in English and Chinese) as 'Northern Praying Mantis'[90] and 'Southern Praying Mantis'.[89] Both are very popular in China, and have also been exported to the West in recent decades.[89][90][91][92] According to local beliefs in Africa, this insect brings good luck.[93] The mantis was revered by the southern African Khoi and San in whose cultures man and nature were intertwined; for its praying posture, the mantis was even named Hottentotsgot (\"god of the Hottentots\") in the Afrikaans language that had developed among the first European settlers.[94] However, at least for the San, the mantis was only one of the manifestations of a trickster-deity, ǀKaggen, who could assume many other forms, such as a snake, hare or vulture.[95] Several ancient civilizations did consider the insect to have supernatural powers; for the Greeks, it had the ability to show lost travelers the way home; in the Ancient EgyptianBook of the Dead, the \"bird-fly\" is a minor god that leads the souls of the dead to the underworld; in a list of 9th-century BC Nineveh grasshoppers (buru), the mantis is named necromancer (buru-enmeli) and soothsayer (buru-enmeli-ashaga).[77][96] Some pre-Columbian cultures in western Nicaragua have preserved oral traditions of the mantis as \"Madre Culebra\", a powerful predator and symbol of female symbolic authority.[76] Mantises are among the insects most widely kept as pets.[97][98] Because the lifespan of a mantis is only about a year, people who want to keep mantises often breed them. In 2013 at least 31 species were kept and bred in the United Kingdom, the Netherlands, and the United States.[99] In 1996 at least 50 species were known to be kept in captivity by members of the Mantis Study Group.[100] Naturally occurring mantis populations provide plant pest control.[101] Gardeners who prefer to avoid pesticides may encourage mantises in the hope of controlling insect pests.[102] However, mantises do not have key attributes of biological pest control agents; they do not specialize in a single pest insect, and do not multiply rapidly in response to an increase in such a prey species, but are general predators. They therefore have \"negligible value\" in biological control.[102] Two species, the Chinese mantis and the European mantis, were deliberately introduced to North America in the hope that they would serve as pest controls for agriculture; they have spread widely in both the United States and Canada.[103] In 2016, the Association for the Advancement of Artificial Intelligence had produced a prototype robot inspired by the forelegs of the praying mantis, with front legs that allow the robot to walk, climb steps, and grasp objects. The multi-jointed leg provides dexterity via a rotatable joint. Future models may include a more spiked foreleg to improve the grip and ability to support more weight.[104] ^Annandale, Nelson (1900). \"Observations on the habits and natural surroundings of insects made during the 'Skeat Expedition' to the Malay Peninsula, 1899–1900\". Proceedings of the Zoological Society of London. 69: 862–865."}
{"url": "https://en.m.wikipedia.org/wiki/Hadean", "text": "Contents The eon's name \"Hadean\" comes from Hades, the Greek god of the underworld (and can be used to describe the underworld itself), referring to the hellish conditions then prevailing on early Earth: the planet had just been formed from recent accretion, and its surface was still molten with superheated lava, the abundance of short-lived radioactive elements, and frequent impact events with other Solar System bodies. The term was coined by American geologist Preston Cloud, originally to label the period before the earliest-known rocks on Earth.[7][8]W.B. Harland later coined an almost synonymous term, the Priscoan period, from priscus, the Latin word for 'ancient'.[9] Other, older texts refer to the eon as the Pre-Archean.[10][11] In the last decades of the 20th century, geologists identified a few Hadean rocks from western Greenland, northwestern Canada, and Western Australia. In 2015, traces of carbon minerals interpreted as \"remains of biotic life\" were found in 4.1-billion-year-old rocks in Western Australia.[12][13] In many other areas, xenocryst (or relict) Hadean zircons enclosed in older rocks indicate that younger rocks have formed on older terranes and have incorporated some of the older material. One example occurs in the Guiana shield from the Iwokrama Formation of southern Guyana where zircon cores have been dated at 4.22 Ga.[15] Part of the ancient planet is theorized to have been disrupted by the impact that created the Moon, which should have caused the melting of one or two large regions of Earth. Earth's present composition suggests that there was not complete remelting as it is difficult to completely melt and mix huge rock masses.[17] However, a fair fraction of material should have been vaporized by this impact. The material would have condensed within 2,000 years.[18] The initial magma ocean solidified within 5 million years,[19] leaving behind hot volatiles which probably resulted in a heavy CO 2 atmosphere with hydrogen and water vapor. The initial heavy atmosphere had a surface temperature of 230 °C (446 °F) and an atmospheric pressure of above 27 standard atmospheres.[18] Studies of zircons have found that liquid water may have existed between 4.0 and 4.4 billion years ago, very soon after the formation of Earth.[14][20] Liquid water oceans existed despite the high surface temperature, because at an atmospheric pressure of 27 atmospheres, water remains liquid even at those high temperatures.[18] The most likely source of the water in the Hadean ocean was outgassing from the Earth's mantle.[21]Bombardment origin of a substantial amount of water is unlikely, due to the incompatibility of isotope fractions between the Earth and comets.[16] Asteroid impacts during the Hadean and into the Archean would have periodically disrupted the ocean. The geological record from 3.2 Gya contains evidence of multiple impacts of objects up to 100 kilometres (62 mi) in diameter.[22] Each such impact would have boiled off up to 100 metres (330 ft) of a global ocean, and temporarily raised the atmospheric temperature to 500 °C (932 °F).[22] However, the frequency of meteorite impacts is still under study: the Earth may have gone through long periods when liquid oceans and life were possible.[20] The liquid water would absorb the carbon dioxide in the early atmosphere, not enough by itself to substantially reduce the amount of CO 2.[18] A 2008 study of zircons found that Australian Hadean rock contains minerals pointing to the existence of plate tectonics as early as 4 billion years ago (approximately 600 million years after Earth's formation).[23] However, some geologists suggest that the zircons could have been formed by meteorite impacts.[24] The direct evidence of Hadean geology from zircons is limited, because the zircons are largely gathered in one locality in Australia.[6][25] Geophysical models are underconstrained, but can paint a general picture of the state of Earth in the Hadean.[6][26] Mantle convection in the Hadean was likely vigorous, due to lower viscosity.[6] The lower viscosity was due to the high levels of radiogenic heat and the fact that water in the mantle had not yet fully outgassed.[27] Whether the vigorous convection led to plate tectonics in the Hadean or was confined under a rigid lid is still a matter of debate.[6][25][28][29] The presence of Hadean oceans are thought to trigger plate tectonics.[30] Subduction due to plate tectonics would have removed carbonate from the early oceans, contributing to the removal of the CO 2-rich early atmosphere. Removal of this early atmosphere is evidence of Hadean plate tectonics.[31] If plate tectonics occurred in the Hadean, it would have formed continental crust.[32] Different models predict different amounts of continental crust during the Hadean.[33] The work of Dhiume et al. predicts that by the end of the Hadean, the continental crust had only 25% of today's area.[34] The models of Korenaga, et al. predict that the continental crust grew to present-day volume sometime between 4.2 and 4.0 Gya.[32][35] The amount of exposed land in the Hadean is only loosely dependent on the amount of continental crust: it also depends on the ocean level.[6] In models where plate tectonics started in the Archean, Earth has a global ocean in the Hadean.[36][37] The high heat of the mantle may have made it difficult to support high elevations in the Hadean.[38][39] If continents did form in the Hadean, their growth competed with outgassing of water from the mantle.[6] Continents may have appeared in the mid-Hadean, and then disappeared under a thick ocean by the end of the Hadean.[40] The limited amount of land has implications for the origin of life.[6] Abundant Hadean-like geothermalmicroenvironments were shown to have the potential to support the synthesis and replication of RNA and thus possibly the evolution of a primitive life form.[41] Porous rock systems comprising heated air-water interfaces were shown to allow ribozymecatalyzed RNA replication of sense and antisense strands followed by subsequent strand-dissociation, thus enabling combined synthesis, release and folding of active ribozymes.[41] Such a primitive RNA system also may have been able to undergo template strand switching during replication (genetic recombination) as occurs during the RNA replication of extant coronaviruses.[42]"}
{"url": "https://en.m.wikipedia.org/wiki/Beef", "text": "An uncooked rib roastWagyu cattle are an example of a breed raised primarily for beef In prehistoric times, humankind hunted aurochs and later domesticated them. Since that time, numerous breeds of cattle have been bred specifically for the quality or quantity of their meat. Today, beef is the third most widely consumed meat in the world, after pork and poultry. As of 2018, the United States, Brazil, and China were the largest producers of beef. Etymology The word beef is from the Latin word bōs,[1] in contrast to cow which is from Middle English cou (both words have the same Indo-European root *gʷou-).[2] After the Norman Conquest, the French-speaking nobles who ruled England naturally used French words to refer to the meats they were served. Thus, various Anglo-Saxon words were used for the animal (such as nēat, or cu for adult females) by the peasants, but the meat was called boef (ox) (Modern French bœuf) by the French nobles — who did not often deal with the live animal — when it was served to them. This is one example of the common English dichotomy between the words for animals (with largely Germanic origins) and their meat (with Romanic origins) that is also found in such English word-pairs as pig/pork, deer/venison, sheep/mutton and chicken/poultry (also the less common goat/chevon).[3]Beef is cognate with bovine through the Late Latinbovīnus.[4] The rarely used plural form of beef is beeves.[5] History People have eaten the flesh of bovines since prehistoric times; some of the earliest known cave paintings, such as those of Lascaux, show aurochs in hunting scenes.[6] People domesticated cattle to provide ready access to beef, milk, and leather.[7] Cattle have been domesticated at least twice over the course of evolutionary history. The first domestication event occurred around 10,500 years ago with the evolution of Bos taurus. The second was more recent, around 7,000 years ago, with the evolution of Bos indicus in the Indian subcontinent. There is a possible third domestication event 8,500 years ago, with a potential third species Bos africanus arising in Africa.[8] In the United States, the growth of the beef business was largely due to expansion in the Southwest. Upon the acquisition of grasslands through the Mexican–American War of 1848, and later the expulsion of the Plains Indians from this region and the Midwest, the American livestock industry began, starting primarily with the taming of wild longhorn cattle. Chicago and New York City were the first to benefit from these developments in their stockyards and in their meat markets.[9] Production Cattle is the third most commonly consumed meat worldwideBeef (and buffalo meat) production has grown substantially over the recent 60 years.Beef has the highest emissions intensity of any agricultural commodity. Beef cattle are raised and fed using a variety of methods, including feedlots, free range, ranching, backgrounding and intensive animal farming. Concentrated Animal Feeding Operations (CAFOs), commonly referred to as factory farms, are commonly used to meet the demand of beef production. CAFOs supply 70.4% of cows in the US market and 99% of all meat in the United States supply.[10] Cattle CAFOs can also be a source of E. coli contamination in the food supply[11] due to the prevalence of manure in CAFOs. These E. coli contaminations include one strain, E. coli O157:H7, which can be toxic to humans, because cattle typically hold this strain in their digestive system.[12] Another consequence of unsanitary conditions created by high-density confinement systems is increased use of antibiotics in order to prevent illness.[13] An analysis of FDA sales data by the Natural Resources Defense Council found 42% of medically important antibiotic use in the U.S. was on cattle, posing concerns about the development of antibiotic resistant bacteria.[14] In 2023 production was forecast to peak by 2035.[15] The consumption of beef poses numerous threats to the natural environment. Of all agricultural products, beef requires some of the most land and water, and its production results in the greatest amount of greenhouse gas emissions (GHG),[18] air pollution, and water pollution.[19] A 2021 study added up GHG emissions from the entire lifecycle, including production, transportation, and consumption, and estimated that beef contributed about 4 billion tonnes (9%) of anthropogenic greenhouse gases in 2010.[20]: 728 Cattle populations graze around 26% of all land on Earth, not including the large agricultural fields that are used to grow cattle feed.[21][22] According to FAO, \"Ranching-induced deforestation is one of the main causes of loss of some unique plant and animal species in the tropical rainforests of Central and South America as well as carbon release in the atmosphere.\"[23] Beef is also the primary driver of deforestation in the Amazon, with around 80% of all converted land being used to rear cattle.[24][25][26] 91% of Amazon land deforested since 1970 has been converted to cattle ranching.[21][27] 41% of global deforestation from 2005 to 2013 has been attributed to the expansion of beef production.[28] This is due to the higher ratio of net energy of gain to net energy of maintenance where metabolizable energy intake is higher.[29] The ratio of feed required to produce an equivalent amount of beef (live weight) has been estimated at 7:1 to 43:1, compared with about 2:1 for chicken.[30][31][32] However, assumptions about feed quality are implicit in such generalizations. For example, production of a kilogram of beef cattle live weight may require between 4 and 5 kilograms of feed high in protein and metabolizable energy content, or more than 20 kilograms of feed of much lower quality.[29] A simple exchange of beef to soy beans (a common feed source for cattle) in Americans' diets would, according to one estimate, result in meeting between 46 and 74 percent of the reductions needed to meet the 2020 greenhouse gas emission goals of the United States as pledged in 2009.[33][needs update] A 2021 CSIRO trial concluded that feeding cattle a 3% diet of the seaweed Asparagopsis taxiformis could reduce the methane component of their emissions by 80%.[34][35] While such feed options are still experimental, even when looking at the most widely used feeds around the globe, there is high variability in efficiency.[36] One study found that shifting compositions of current feeds, production areas, and informed land restoration could enable greenhouse gas emissions reductions of 34–85% annually (612–1,506 MtCO 2 e yr−1) without increasing costs to global beef production.[37] Global statistics In 2018, the United States, Brazil, and China produced the most beef with 12.22 million tons, 9.9 million tons, and 6.46 million tons respectively.[48] The top 3 beef exporting countries in 2019 were Australia (14.8% of total exports), the United States (13.4% of total exports), and Brazil (12.6% of total exports).[49] Beef production is also important to the economies of Japan, Argentina, Uruguay, Canada, Paraguay, Mexico, Belarus and Nicaragua. Top 5 cattle and beef exporting countries As per 2020, Brazil was the largest beef exporter in the world followed by Australia, United States, India (Includes Carabeef only) and Argentina.[50] Brazil, Australia, the United States and India accounted for roughly 61% of the world's beef exports.[51] Top 10 cattle and beef producing countries The world produced 60.57 million metric tons of beef in 2020, down 950K metric tons from the prior year. Major decline for production of beef was from India up to 510k and Australia down to 309K metric tons from the prior year.[54] By 2017, it was already reported that farmers in Nepal kept fewer cattle due to the losses imposed by a longer hot season.[56]: 747 Cow-calf ranches in Southeast Wyoming are expected to suffer greater losses in the future as the hydrological cycle becomes more variable and affects forage growth. Even though the annual mean precipitation is not expected to change much, there will be more unusually dry years as well as unusually wet years, and the negatives will outweigh the positives. Keeping smaller herds to be more flexible when dry years hit was suggested as an adaptation strategy.[57] Since more variable and therefore less predictable precipitation is one of the well-established effects of climate change on the water cycle,[58]: 85 similar patterns were later established across the rest of the United States,[59] and then globally.[60] As of 2022, it has been suggested that every additional millimeter of annual precipitation increases beef production by 2.1% in the tropical countries and reduces it by 1.9% in temperate ones, yet the effects of warming are much larger. Under SSP3-7.0, a scenario of significant warming and very low adaptation, every additional 1 °C (1.8 °F) would decrease global beef production by 9.7%, mainly because of its impact on tropical and poor countries. In the countries which can afford adaptation measures, production would fall by around 4%, but by 27% in those which cannot.[61] In 2024, another study suggested that the impacts would be milder - a 1% decrease per every additional 1 °C (1.8 °F) in low-income countries and 0.2% in high-income ones, and a 3.2% global decline in beef production by 2100 under SSP3-7.0.[62] The same paper suggests that out of the top 10 beef-producing countries (Argentina, Australia, Brazil, China, France, India, Mexico, Russia, Turkey and the U.S.), only China, Russia and the U.S. would see overall production gains with increased warming, with the rest experiencing declines.[62] Other research suggests that east and south of Argentina may become more suitable to cattle ranching due to climate-driven shifts in rainfall, but a shift to Zebu breeds would likely be needed to minimize the impact of warming.[63] Beef is first divided into primal cuts, large pieces of the animal initially separated by butchering. These are basic sections from which steaks and other subdivisions are cut. The term \"primal cut\" is quite different from \"prime cut\", used to characterize cuts considered to be of higher quality. Since the animal's legs and neck muscles do the most work, they are the toughest; the meat becomes more tender as distance from hoof and horn increases. Different countries and cuisines have different cuts and names, and sometimes use the same name for a different cut; for example, the cut described as \"brisket\" in the United States is from a significantly different part of the carcass than British brisket.[citation needed] To improve tenderness of beef, it is often aged (i.e., stored refrigerated) to allow endogenous proteolytic enzymes to weaken structural and myofibrillar proteins. Wet aging is accomplished using vacuum packaging to reduce spoilage and yield loss. Dry aging involves hanging primals (usually ribs or loins) in humidity-controlled coolers. Outer surfaces dry out and can support growth of molds (and spoilage bacteria, if too humid), resulting in trim and evaporative losses. Evaporation concentrates the remaining proteins and increases flavor intensity; the molds can contribute a nut-like flavor. After two to three days there are significant effects. The majority of the tenderizing effect occurs in the first 10 days. Boxed beef, stored and distributed in vacuum packaging, is, in effect, wet aged during distribution. Premium steakhouses dry age for 21 to 28 days or wet age up to 45 days for maximum effect on flavor and tenderness. Meat from less tender cuts or older cattle can be mechanically tenderized by forcing small, sharp blades through the cuts to disrupt the proteins. Also, solutions of exogenous proteolytic enzymes (papain, bromelin or ficin) can be injected to augment the endogenous enzymes. Similarly, solutions of salt and sodium phosphates can be injected to soften and swell the myofibrillar proteins. This improves juiciness and tenderness. Salt can improve the flavor, but phosphate can contribute a soapy flavor. Dry heat Cooking the beef over or under a high radiant heat source, generally in excess of 340 °C (650 °F). This leads to searing of the surface of the beef, which creates a flavorsome crust. In Australia, New Zealand, the United States, Canada, the UK, Germany and The Netherlands, grilling, particularly over charcoal, is sometimes known as barbecuing, often shortened to \"BBQ\". When cooked over charcoal, this method can also be called charbroiling. A way of cooking meat in a hot oven, producing roast beef. Liquid is not usually added; the beef may be basted by fat on the top, or by spooning hot fat from the oven pan over the top. A gravy may be made from the cooking juices, after skimming off excess fat. Roasting is suitable for thicker pieces of meat; the other methods listed are usually for steaks and similar cuts. Internal temperature Beef can be cooked to various degrees, from very rare to well done. The degree of cooking corresponds to the temperature in the approximate center of the meat, which can be measured with a meat thermometer. Beef can be cooked using the sous-vide method, which cooks the entire steak to the same temperature, but when cooked using a method such as broiling or roasting it is typically cooked such that it has a \"bulls eye\" of doneness, with the least done (coolest) at the center and the most done (warmest) at the outside. Frying Meat can be cooked in boiling oil, typically by shallow frying, although deep frying may be used, often for meat enrobed with breadcrumbs as in milanesas or finger steaks. Larger pieces such as steaks may be cooked this way, or meat may be cut smaller as in stir frying, typically an Asian way of cooking: cooking oil with flavorings such as garlic, ginger and onions is put in a very hot wok. Then small pieces of meat are added, followed by ingredients which cook more quickly, such as mixed vegetables. The dish is ready when the ingredients are 'just cooked'. Moist heat Moist heat cooking methods include braising, pot roasting, stewing and sous-vide. These techniques are often used for cuts of beef that are tougher, as these longer, lower-temperature cooking methods have time to dissolve connecting tissue which otherwise makes meat remain tough after cooking. cooking meats, in a covered container, with small amounts of liquids (usually seasoned or flavored). Unlike stewing, braised meat is not fully immersed in liquid, and usually is browned before the oven step. Sous-vide, French for \"under vacuum\", is a method of cooking food sealed in airtight plastic bags in a water bath for a long time—72 hours is not unknown—at an accurately determined temperature much lower than normally used for other types of cooking. The intention is to maintain the integrity of ingredients and achieve very precise control of cooking. Although water is used in the method, only moisture in or added to the food bags is in contact with the food. Beef roasted with vinegar and sliced with spiced paste, often called \"cold beef\" Meat has usually been cooked in water which is just simmering, such as in stewing; higher temperatures make meat tougher by causing the proteins to contract. Since thermostatic temperature control became available, cooking at temperatures well below boiling, 52 °C (126 °F) (sous-vide) to 90 °C (194 °F) (slow cooking), for prolonged periods has become possible; this is just hot enough to convert the tough collagen in connective tissue into gelatin through hydrolysis, with minimal toughening. With the adequate combination of temperature and cooking time, pathogens, such as bacteria will be killed, and pasteurization can be achieved. Because browning (Maillard reactions) can only occur at higher temperatures (above the boiling point of water), these moist techniques do not develop the flavors associated with browning. Meat will often undergo searing in a very hot pan, grilling or browning with a torch before moist cooking (though sometimes after). Thermostatically controlled methods, such as sous-vide, can also prevent overcooking by bringing the meat to the exact degree of doneness desired, and holding it at that temperature indefinitely. The combination of precise temperature control and long cooking duration makes it possible to be assured that pasteurization has been achieved, both on the surface and the interior of even very thick cuts of meat, which can not be assured with most other cooking techniques. (Although extremely long-duration cooking can break down the texture of the meat to an undesirable degree.) Beef can be cooked quickly at the table through several techniques. In hot pot cooking, such as shabu-shabu, very thinly sliced meat is cooked by the diners at the table by immersing it in a heated pot of water or stock with vegetables. In fondue bourguignonne, diners dip small pieces of beef into a pot of hot oil at the table. Both techniques typically feature accompanying flavorful sauces to complement the meat. Raw beef Raw sliced beef Steak tartare is a French dish made from finely chopped or ground (minced) raw meat (often beef). More accurately, it is scraped so as not to let even the slightest of the sinew fat get into the scraped meat. It is often served with onions, capers, seasonings such as fresh ground pepper and Worcestershire sauce, and sometimes raw egg yolk. The Belgian or Dutch dish filet américain is also made of finely chopped ground beef, though it is seasoned differently, and either eaten as a main dish or can be used as a dressing for a sandwich. Kibbeh nayyeh is a similar Lebanese and Syrian dish. And in Ethiopia, a ground raw meat dish called tire siga or kitfo is eaten (upon availability). Carpaccio of beef is a thin slice of raw beef dressed with olive oil, lemon juice and seasoning. Often, the beef is partially frozen before slicing to allow very thin slices to be cut. Yukhoe is a variety of hoe, raw dishes in Korean cuisine which is usually made from raw ground beef seasoned with various spices or sauces. The beef part used for yukhoe is tender rump steak. For the seasoning, soy sauce, sugar, salt, sesame oil, green onion, and ground garlic, sesame seed, black pepper and juice of bae (Korean pear) are used. The beef is mostly topped with the yolk of a raw egg. Cured, smoked, and dried beef Bresaola is an air-dried, salted beef that has been aged about two to three months until it becomes hard and a dark red, almost purple, colour. It is lean, has a sweet, musty smell and is tender. It originated in Valtellina, a valley in the Alps of northern Italy's Lombardy region. Bündnerfleisch is a similar product from neighbouring Switzerland. Chipped beef is an American industrially produced air-dried beef product, described by one of its manufacturers as being \"similar to bresaola, but not as tasty.\"[66] Beef jerky is dried, salted, smoked beef popular in the United States. Pastrami is often made from beef; raw beef is salted, then partly dried and seasoned with various herbs and spices, and smoked. Corned beef is a cut of beef cured or pickled in a seasoned brine. The corn in corned beef refers to the grains of coarse salts (known as corns) used to cure it. The term corned beef can denote different styles of brine-cured beef, depending on the region. Some, like American-style corned beef, are highly seasoned and often considered delicatessen fare. Spiced beef is a cured and salted joint of round, topside, or silverside, traditionally served at Christmas in Ireland. It is a form of salt beef, cured with spices and saltpetre, intended to be boiled or broiled in Guinness or a similar stout, and then optionally roasted for a period after.[67] There are various other recipes for pickled beef. Sauerbraten is a German variant. Consumption Beef is the third most widely consumed meat in the world, accounting for about 25% of meat production worldwide, after pork and poultry at 38% and 30% respectively.[68] Cancer Coronary heart disease A 2010 meta-analysis found that processed red meat (and all processed meat) was correlated with a higher risk of coronary heart disease, although based on studies that separated the two, this meta-analysis found that red meat intake was not associated with higher incidence of coronary heart disease.[77] As of 2020, there is substantial evidence for a link between high consumption of red meat and coronary heart disease.[78][79][80] Dioxins Some cattle raised in the United States feed on pastures fertilized with sewage sludge. Elevated dioxins may be present in meat from these cattle.[81] E. coli recalls Ground beef has been subject to recalls in the United States, due to Escherichia coli (E. coli) contamination: February 2011, American Food Service, a Pico Rivera, Calif. establishment, is recalling approximately 1,440 kg (3,170 lb) of fresh ground beef patties and other bulk packages of ground beef products that may be contaminated with E. coli O157:H7.[83] January 2012, Hannaford Supermarkets recalled all ground beef with sell by dates 17 December 2011 or earlier.[89] September 2012, XL Foods recalled more than 1800 products believed to be contaminated with E. coli 0157:H7. The recalled products were produced at the company's plant in Brooks, Alberta, Canada; this was the largest recall of its kind in Canadian History.[90][91] Deaths in the UK caused by vCJD from the start of the BSE outbreak up until 2009. MM and MV refer to the two genotypes of vCJD.[93] Since then, other countries have had outbreaks of BSE: In May 2003, after a cow with BSE was discovered in Alberta, Canada, the American border was closed to live Canadian cattle, but was reopened in early 2005.[94] In June 2005, Dr. John Clifford, chief veterinary officer for the United States Department of Agriculture animal health inspection service, confirmed a fully domestic case of BSE in Texas. Clifford would not identify the ranch, calling that \"privileged information.\"[95] The 12-year-old animal was alive at the time when Oprah Winfrey raised concerns about cannibalistic feeding practices on her show[96] which aired 16 April 1996. In 2010, the EU, through the European Food Safety Authority (EFSA), proposed a roadmap to gradually lift the restrictions on the feed ban.[97] In 2013, the ban on feeding mammal-based products to cattle,[98] was amended to allow for certain milk, fish, eggs, and plant-fed farm animal products to be used.[99] Restrictions Religious and cultural prohibitions Most Indic religions reject the killing and eating of cows. Hinduism prohibits cow beef known as Go-Maans in Hindi. Bovines have a sacred status in India especially the cow, due to their provision of sustenance for families. Bovines are generally considered to be integral to the landscape. However, they do not consider the cow to be a god.[100] Many of India's rural economies depend on cattle farming; hence they have been revered in society.[101][102] Since the Vedic period, cattle, especially cows, were venerated as a source of milk, and dairy products, and their relative importance in transport services and farming like ploughing, row planting, ridging. Veneration grew with the advent of Jainism and the Gupta period.[103] In medieval India, Maharaja Ranjit Singh issued a proclamation on stopping cow slaughter. Conflicts over cow slaughter often have sparked religious riots that have led to loss of human life and in one 1893 riot alone, more than 100 people were killed for the cause.[104] For religious reasons, the ancient Egyptian priests also refrained from consuming beef. Buddhists and Sikhs are also against wrongful slaughtering of animals, but they do not have a wrongful eating doctrine.[105] In ancient China, the killing of cattle and consumption of beef was prohibited, as they were valued for their role in agriculture. This custom is still followed by a few Chinese families across the world.[106] Legal prohibition India Most of the North Indian states[108] prohibit the killing of cow and consumption of beef for religious reasons.[109][110][111][112][113] Certain Hindu castes and sects continue to avoid beef from their diets.[114][115] Article 48 of the Constitution of India mandates the state may take steps for preserving and improving the bovine breeds, and prohibit the slaughter, of cows and calves and other milch and draught cattle. Article 47 of the Constitution of India provides states must raise the level of nutrition and the standard of living and to improve public health as among its primary duties, based on this a reasonableness in slaughter of common cattle was instituted, if the animals ceased to be capable of breeding, providing milk, or serving as draught animals. The overall mismanagement of India's common cattle is dubbed in academic fields as \"India's bovine burden.\"[116][117] In 2017, a rule against the slaughter of cattle and the eating of beef was signed into law by presidential assent as a modified version of Prevention of Cruelty to Animals Act, 1960. The original act, however, did permit the humane slaughter of animals for use as food.[118][119] Existing meat export policy in India prohibits the export of beef (meat of cow, oxen and calf). Bone-in meat, a carcass, or half carcass of buffalo is also prohibited from export. Only the boneless meat of buffalo, meat of goat and sheep and birds is permitted for export.[120][121] In 2017, India sought a total \"beef ban\" and Australian market analysts predicted that this would create market opportunities for leather traders and meat producers there and elsewhere. Their prediction estimated a twenty percent shortage of beef and a thirteen percent shortage of leather in the world market.[122] Nepal The cow is the national animal of Nepal, and slaughter of cattle is prohibited by law.[123][124] Cuba In 2003, Cuba banned cow slaughter due to severe shortage of milk and milk products.[125] On 14 April 2021, the ban was loosened, allowing ranchers to do as they wish as long as state quotas were met and the health of the herd could be ensured.[126]"}
{"url": "https://en.m.wikipedia.org/wiki/Slaughterhouse", "text": "Slaughterhouses that produce meat that is not intended for human consumption are sometimes referred to as knacker's yards or knackeries. This is where animals are slaughtered that are not fit for human consumption or that can no longer work on a farm, such as retired work horses. Slaughtering animals on a large scale poses significant issues in terms of logistics, animal welfare, and the environment, and the process must meet public health requirements. Due to public aversion in different cultures, determining where to build slaughterhouses is also a matter of some consideration. Frequently, animal rights groups raise concerns about the methods of transport to and from slaughterhouses, preparation prior to slaughter, animal herding, stunning methods, and the killing itself.[1] Until modern times, the slaughter of animals generally took place in a haphazard and unregulated manner in diverse places. Early maps of London show numerous stockyards in the periphery of the city, where slaughter occurred in the open air or under cover such as wet markets. A term for such open-air slaughterhouses was shambles, and there are streets named \"The Shambles\" in some English and Irish towns (e.g., Worcester, York, Bandon) which got their name from having been the site on which butchers killed and prepared animals for consumption. Fishamble Street, Dublin was formerly a fish-shambles. Sheffield had 183 slaughterhouses in 1910, and it was estimated that there were 20,000 in England and Wales.[2] The slaughterhouse emerged as a coherent institution in the 19th century.[3] A combination of health and social concerns, exacerbated by the rapid urbanisation experienced during the Industrial Revolution, led social reformers to call for the isolation, sequester and regulation of animal slaughter. As well as the concerns raised regarding hygiene and disease, there were also criticisms of the practice on the grounds that the effect that killing had, both on the butchers and the observers, \"educate[d] the men in the practice of violence and cruelty, so that they seem to have no restraint on the use of it.\"[4] An additional motivation for eliminating private slaughter was to impose a careful system of regulation for the \"morally dangerous\" task of putting animals to death.[citation needed] As a result of this tension, meat markets within the city were closed and abattoirs built outside city limits. An early framework for the establishment of public slaughterhouses was put in place in Paris in 1810, under the reign of the Emperor Napoleon. Five areas were set aside on the outskirts of the city and the feudal privileges of the guilds were curtailed.[5] As the meat requirements of the growing number of residents in London steadily expanded, the meat markets both within the city and beyond attracted increasing levels of public disapproval. Meat had been traded at Smithfield Market as early as the 10th century. By 1726, it was regarded as \"without question, the greatest in the world\", by Daniel Defoe.[6] By the middle of the 19th century, in the course of a single year 220,000 head of cattle and 1,500,000 sheep would be \"violently forced into an area of five acres, in the very heart of London, through its narrowest and most crowded thoroughfares\".[7] By the early 19th century, pamphlets were being circulated arguing in favor of the removal of the livestock market and its relocation outside of the city due to the extremely low hygienic conditions[8] as well as the brutal treatment of the cattle.[9] In 1843, the Farmer's Magazine published a petition signed by bankers, salesmen, aldermen, butchers and local residents against the expansion of the livestock market.[7] The Town Police Clauses Act 1847 created a licensing and registration system, though few slaughter houses were closed.[10] An Act of Parliament was eventually passed in 1852. Under its provisions, a new cattle-market was constructed in Copenhagen Fields, Islington. The new Metropolitan Cattle Market was also opened in 1855, and West Smithfield was left as waste ground for about a decade, until the construction of the new market began in the 1860s under the authority of the 1860 Metropolitan Meat and Poultry Market Act.[11] The market was designed by architect Sir Horace Jones and was completed in 1868. A cut and cover railway tunnel was constructed beneath the market to create a triangular junction with the railway between Blackfriars and King's Cross.[12] This allowed animals to be transported into the slaughterhouse by train and the subsequent transfer of animal carcasses to the Cold Store building, or direct to the meat market via lifts. At the same time, the first large and centralized slaughterhouse in Paris was constructed in 1867 under the orders of Napoleon III at the Parc de la Villette and heavily influenced the subsequent development of the institution throughout Europe. These slaughterhouses were regulated by law to ensure good standards of hygiene, the prevention of the spread of disease and the minimization of needless animal cruelty. The slaughterhouse had to be equipped with a specialized water supply system to effectively clean the operating area of blood and offal. Veterinary scientists, notably George Fleming and John Gamgee, campaigned for stringent levels of inspection to ensure that epizootics such as rinderpest (a devastating outbreak of the disease covered all of Britain in 1865) would not be able to spread. By 1874, three meat inspectors were appointed for the London area, and the Public Health Act 1875 required local authorities to provide central slaughterhouses (they were only given powers to close unsanitary slaughterhouses in 1890).[13] Yet the appointment of slaughterhouse inspectors and the establishment of centralised abattoirs took place much earlier in the British colonies, such as the colonies of New South Wales and Victoria, and in Scotland where 80% of cattle were slaughtered in public abattoirs by 1930.[14] In Victoria the Melbourne Abattoirs Act 1850 (NSW) \"confined the slaughtering of animals to prescribed public abattoirs, while at the same time prohibiting the killing of sheep, lamb, pigs or goats at any other place within the city limits\".[15] Animals were shipped alive to British ports from Ireland, from Europe and from the colonies and slaughtered in large abattoirs at the ports. Conditions were often very poor.[16] Attempts were also made throughout the British Empire to reform the practice of slaughter itself, as the methods used came under increasing criticism for causing undue pain to the animals. The eminent physician, Benjamin Ward Richardson, spent many years in developing more humane methods of slaughter. He brought into use no fewer than fourteen possible anesthetics for use in the slaughterhouse and even experimented with the use of electric current at the Royal Polytechnic Institution.[17] As early as 1853, he designed a lethal chamber that would gas animals to death relatively painlessly[citation needed], and he founded the Model Abattoir Society in 1882 to investigate and campaign for humane methods of slaughter. The invention of refrigeration and the expansion of transportation networks by sea and rail allowed for the safe exportation of meat around the world. Additionally, meat-packing millionaire Philip Danforth Armour's invention of the \"disassembly line\" greatly increased the productivity and profit margin of the meat packing industry: \"according to some, animal slaughtering became the first mass-production industry in the United States.\" This expansion has been accompanied by increased concern about the physical and mental conditions of the workers along with controversy over the ethical and environmental implications of slaughtering animals for meat.[3] The Edinburgh abattoir, which was built in 1910, had well lit laboratories, hot and cold water, gas, microscopes and equipment for cultivating organisms. The English 1924 Public Health (Meat) Regulations required notification of slaughter to enable inspection of carcasses and enabled inspected carcasses to be marked.[18] The development of slaughterhouses was linked with industrial expansion of by-products. By 1932 the British by-product industry was worth about £97 million a year, employing 310,000 people. The Aberdeen slaughterhouse sent hooves to Lancashire to make glue, intestines to Glasgow for sausages and hides to the Midland tanneries. In January 1940 the British government took over the 16,000 slaughterhouses and by 1942 there were only 779.[19] In the latter part of the 20th century, the layout and design of most U.S. slaughterhouses was influenced by the work of Temple Grandin.[20][non-primary source needed] She suggested that reducing the stress of animals being led to slaughter may help slaughterhouse operators improve efficiency and profit.[21] In particular she applied an understanding of animal psychology to design pens and corrals which funnel a herd of animals arriving at a slaughterhouse into a single file ready for slaughter. Her corrals employ long sweeping curves[22][23][24] so that each animal is prevented from seeing what lies ahead and just concentrates on the hind quarters of the animal in front of it. This design – along with the design elements of solid sides, solid crowd gate, and reduced noise at the end point – work together to encourage animals forward in the chute and to not reverse direction.[25][non-primary source needed] Beginning in 2008 the Local Infrastructure for Local Agriculture, a non-profit committed to revitalizing opportunities for \"small farmers and strengthening the connection between local supply and demand\",[26] constructed a mobile slaughterhouse facility in efforts for small farmers to process meat quickly and cost effectively. Named the Modular Harvest System, or M.H.S., it received USDA approval in 2010. The M.H.S. consists of three separate trailers: One for slaughtering, one for consumable body parts, and one for other body parts. Preparation of individual cuts is done at a butchery or other meat preparation facility.[26] The standards and regulations governing slaughterhouses vary considerably around the world. In many countries the slaughter of animals is regulated by custom and tradition rather than by law. In the non-Western world, including the Arab world, the Indian sub-continent, etc., both forms of meat are available: one which is produced in modern mechanized slaughterhouses, and the other from local butcher shops.[citation needed] In some communities animal slaughter and permitted species may be controlled by religious laws, most notably halal for Muslims and kashrut for Jewish communities. This can cause conflicts with national regulations when a slaughterhouse adhering to the rules of religious preparation is located in some Western countries. In Jewish law, captive bolts and other methods of pre-slaughter paralysis are generally not permissible, due to it being forbidden for an animal to be stunned prior to slaughter. Various halal food authorities have more recently permitted the use of a recently developed fail-safe system of head-only stunning where the shock is non-fatal, and where it is possible to reverse the procedure and revive the animal after the shock. The use of electronarcosis and other methods of dulling the sensing has been approved by the Egyptian Fatwa Committee. This allows these entities to continue their religious techniques while keeping accordance to the national regulations.[27] In some societies, traditional cultural and religious aversion to slaughter led to prejudice against the people involved. In Japan, where the ban on slaughter of livestock for food[specify] was lifted in the late 19th century, the newly found slaughter industry drew workers primarily from villages of burakumin, who traditionally worked in occupations relating to death (such as executioners and undertakers). In some parts of western Japan, prejudice faced by current and former residents of such areas (burakumin \"hamlet people\") is still a sensitive issue. Because of this, even the Japanese word for \"slaughter\" (屠殺 tosatsu) is deemed politically incorrect by some pressure groups as its inclusion of the kanji for \"kill\" (殺) supposedly portrays those who practise it in a negative manner. Some countries have laws that exclude specific animal species or grades of animal from being slaughtered for human consumption, especially those that are taboo food. The former Indian Prime MinisterAtal Bihari Vajpayee suggested in 2004 introducing legislation banning the slaughter of cows throughout India, as Hinduism holds cows as sacred and considers their slaughter unthinkable and offensive. This was often opposed on grounds of religious freedom. The slaughter of cows and the importation of beef into the nation of Nepal are strictly forbidden. Refrigeration technology allowed meat from the slaughterhouse to be preserved for longer periods. This led to the concept as the slaughterhouse as a freezing works. Prior to this, canning was an option.[28] Freezing works are common in New Zealand, Australia and South Africa. In countries where meat is exported for a substantial profit the freezing works were built near docks, or near transport infrastructure.[29] Mobile poultry processing units (MPPUs) follow the same principles, but typically require only one trailer and, in much of the United States, may legally operate under USDA exemptions not available to red meat processors.[30] Several MPPUs have been in operation since before 2010, under various models of operation and ownership.[31] Most countries have laws in regard to the treatment of animals in slaughterhouses. In the United States, there is the Humane Slaughter Act of 1958, a law requiring that all swine, sheep, cattle, and horses be stunned unconscious with application of a stunning device by a trained person before being hoisted up on the line. There is some debate over the enforcement of this act. This act, like those in many countries, exempts slaughter in accordance to religious law, such as koshershechita[citation needed] and dhabiha halal.[citation needed] Most strict interpretations of kashrut require that the animal be fully sensible when its carotid artery is cut.[citation needed] In 1997, Gail Eisnitz, chief investigator for the Humane Farming Association (HFA),[32] released the book Slaughterhouse. Within, she unveils the interviews of slaughterhouse workers in the U.S. who say that, because of the speed with which they are required to work, animals are routinely skinned while apparently alive and still blinking, kicking and shrieking. Eisnitz argues that this is not only cruel to the animals but also dangerous for the human workers, as cows weighing several thousands of pounds thrashing around in pain are likely to kick out and debilitate anyone working near them.[33] This would imply that certain slaughterhouses throughout the country are not following the guidelines and regulations spelled out by the Humane Slaughter Act, requiring all animals to be put down and thus insusceptible to pain by some form, typically electronarcosis, before undergoing any form of violent action. According to the HFA, Eiznitz interviewed slaughterhouse workers representing over two million hours of experience, who, without exception, told her that they have beaten, strangled, boiled and dismembered animals alive or have failed to report those who do. The workers described the effects the violence has had on their personal lives, with several admitting to being physically abusive or taking to alcohol and other drugs.[34] The HFA alleges that workers are required to kill up to 1,100 hogs an hour and end up taking their frustration out on the animals.[34] Eisnitz interviewed one worker, who had worked in ten slaughterhouses, about pig production. He told her: Hogs get stressed out pretty easy. If you prod them too much, they have heart attacks. If you get a hog in the chute that's had the shit prodded out of him and has a heart attack or refuses to move, you take a meat hook and hook it into his bunghole. You try to do this by clipping the hipbone. Then you drag him backwards. You're dragging these hogs alive, and a lot of times the meat hook rips out of the bunghole. I've seen hams – thighs – completely ripped open. I've also seen intestines come out. If the hog collapses near the front of the chute, you shove the meat hook into his cheek and drag him forward.[35] American slaughterhouse workers are three times more likely to suffer serious injury than the average American worker.[37]NPR reports that pig and cattle slaughterhouse workers are nearly seven times more likely to suffer repetitive strain injuries than average.[38]The Guardian reports that on average there are two amputations a week involving slaughterhouse workers in the United States.[39] On average, one employee of Tyson Foods, the largest meat producer in America, is injured and amputates a finger or limb per month.[40] The Bureau of Investigative Journalism reported that over a period of six years, in the UK 78 slaughter workers lost fingers, parts of fingers or limbs, more than 800 workers had serious injuries, and at least 4,500 had to take more than three days off after accidents.[41] In a 2018 study in the Italian Journal of Food Safety, slaughterhouse workers are instructed to wear ear protectors to protect their hearing from the loud noises in the facility.[42] A 2004 study in the Journal of Occupational and Environmental Medicine found that \"excess risks were observed for mortality from all causes, all cancers, and lung cancer\" in workers employed in the New Zealand meat processing industry.[43] The worst thing, worse than the physical danger, is the emotional toll. If you work in the stick pit [where hogs are killed] for any period of time – that lets you kill things but doesn't let you care. You may look a hog in the eye that's walking around in the blood pit with you and think, \"God, that really isn't a bad looking animal.\" You may want to pet it. Pigs down on the kill floor have come up to nuzzle me like a puppy. Two minutes later I had to kill them – beat them to death with a pipe. I can't care. Working at slaughterhouses often leads to a high amount of psychological trauma.[45][46] A 2016 study in Organization indicates, \"Regression analyses of data from 10,605 Danish workers across 44 occupations suggest that slaughterhouse workers consistently experience lower physical and psychological well-being along with increased incidences of negative coping behavior.\"[47] A 2009 study by criminologist Amy Fitzgerald indicates, \"slaughterhouse employment increases total arrest rates, arrests for violent crimes, arrests for rape, and arrests for other sex offenses in comparison with other industries.\"[48] As authors from the PTSD Journal explain, \"These employees are hired to kill animals, such as pigs and cows that are largely gentle creatures. Carrying out this action requires workers to disconnect from what they are doing and from the creature standing before them. This emotional dissonance can lead to consequences such as domestic violence, social withdrawal, anxiety, drug and alcohol abuse, and PTSD.\"[49] Starting in the 1980s, Cargill, Conagra Brands, Tyson Foods and other large food companies moved most slaughterhouse operations to rural areas of the Southern United States which were more hostile to unionization efforts.[50] Slaughterhouses in the United States commonly illegally employ and exploit underage workers and undocumented immigrants.[51][52] In 2010, Human Rights Watch described slaughterhouse line work in the United States as a human rights crime.[53] In a report by Oxfam America, slaughterhouse workers were observed not being allowed breaks, were often required to wear diapers, and were paid below minimum wage.[54] ^Grandin, Temple (September 2011). \"Directions for laying out curved cattle handling facilities for ranches, feedlots, and properties\". Dr. Temple Grandin's Web Page. Dr. Temple Grandin. Retrieved 10 December 2012. Round crowd pens and curved single file chutes work better than straight ones, but they must be laid out correctly. A curved chute works more efficiently than a straight one because it prevents cattle from seeing people and other activities at the end of the chute.\" \"A round crowd pen will work better than a straight crowd pen because, as cattle go around a 180° turn, they think they are going back to where they came from ^Grandin, Temple (July 2011). \"Sample Designs of Cattle Races and Corrals\". Dr. Temple Grandin's Web Page. Dr. Temple Grandin. Retrieved 10 December 2012. Why does a curved chute and round crowd pen work better than a straight one? As the animals go around the curve, they think they are going back to where they came from. The animals can not see people and other moving objects at the end of the chute. It takes advantage of the natural circling behaviour of cattle and sheep. ^Grandin, Temple (1993). \"Teaching Principles of Behavior and Equipment Design for Handling Livestock\". J. Anim. Sci. 71 (4): 1065–70. doi:10.2527/1993.7141065x. hdl:10217/4153. PMID8478279. Retrieved 10 December 2012. Some of the design principles that are taught are the use of solid sides on chutes and crowd pens to prevent animals from seeing out with their wide-angle vision and layout of curved chutes and round crowd pens. Some people believe the animals can smell or hear death, however, and these may be area that need improvement, such as the use of scent masking agents or acoustical barriers. As well, some animals in some situations may grow to learn that after their fellows are corralled in that area, their fellows never return. An improvement could be made by detouring off some of the animals so that they return to the pack (after the odors and sounds are masked so they will return untraumatized). A circular crowd pen and a curved chute reduced the time spent moving cattle by up to 50% (Vowles and Hollier, 1982 [Vowles, W. J., and T. J. Hollier. 1982. The influence of yard design on the movement of animals. Proc. Aust. Soc. Anim. Prod. 14:597]). ^Grandin, Temple (July 2010). \"Improving the Movement of Cattle, Pigs, and Sheep during handling on farms, ranches, and slaughter plants\". Dr Temple Grandin. Retrieved 10 December 2012. Cattle will move more easily through a curved race. Solid sides which prevent the cattle from seeing people and other distractions outside the fence should be installed on the chutes (races) and the crowd pen which leads up to the single file chute. The use of solid sides is especially important in slaughter plants, truck loading ramps, and other places where there is much activity outside the fence. Solid sides are essential in slaughter plants to block the animal's view of people and equipment. A curved chute (race) with solid sides at a ranch facility. It works better than a straight chute because cattle think they are going back to where they came from. The outer fence is solid to prevent the cattle from seeing distractions outside the fence... The facility must be located in a pasture that has no nearby equipment, moving vehicles or extra people, or put inside a building that has solid side walls. In many facilities, adding solid fences will improve animal movement... Solid sides in these areas help prevent cattle from becoming agitated when they see activity outside the fence – such as people. Cattle tend to be calmer in a chute with solid sides. Cattle move more easily through the curved race system because they can not see people and other distractions ahead."}
{"url": "https://web.archive.org/web/20180612140908/https://royalsociety.org/topics-policy/publications/2004/non-human-animals/", "text": "These crawls are part of an effort to archive pages as they are created and archive the pages that they refer to. That way, as the pages that are referenced are changed or taken from the web, a link to the version that was live when the page was written will be preserved. Then the Internet Archive hopes that references to these archived pages will be put in place of a link that would be otherwise be broken, or a companion link to allow people to see what was originally intended by a page's authors. This is a collection of web page captures from links added to, or changed on, Wikipedia pages. The idea is to bring a reliability to Wikipedia outlinks so that if the pages referenced by Wikipedia articles are changed, or go away, a reader can permanently find what was originally referred to. The use of non-human animals in research Downloads The Royal Society has published a guide for researchers about the use of non-human animals in research. The use of non-human animals in research: a guide for scientists (February 2004) has been produced by the Society's Animals in Research Committee. The guide addresses issues such as: examples of medical advances achieved by the use of animals; the theoretical framework behind the use of animals; the legislation that regulates their use; discussion of the philosophies that underpin the debate about the use of animals in research. The Royal Society hopes that this document will be of use to all animal researchers, particularly those scientists in the early stages of their research careers."}
{"url": "https://en.m.wikipedia.org/wiki/Amorphea", "text": "The International Society of Protistologists, the recognised body for taxonomy of protozoa, recommended in 2012 that the term Unikont be changed to Amorphea because the name \"Unikont\" is based on a hypothesized synapomorphy that the ISOP authors and other scientists later rejected.[1][5] Amoebozoa seems to be monophyletic with two major branches: Conosa and Lobosa. Conosa is divided into the aerobic infraphylum Semiconosia (Mycetozoa and Variosea) and secondarily anaerobic Archamoebae. Lobosa consists entirely of non-flagellated lobose amoebae and has been divided into two classes: Discosea, which have flattened cells, and Tubulinea, which has predominantly tube-shaped pseudopodia.[11] Cavalier-Smith[2] originally proposed that unikonts ancestrally had a single flagellum and single basal body. This is unlikely, however, as flagellated opisthokonts, as well as some flagellated Amoebozoa, including Breviata, actually have two basal bodies, as in typical 'bikonts' (even though only one is flagellated in most unikonts). This paired arrangement can also be seen in the organization of centrioles in typical animal cells. In spite of the name of the group, the common ancestor of all 'unikonts' was probably a cell with two basal bodies."}
{"url": "https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-8-32", "text": "Anaerobic animals from an ancient, anoxic ecological niche Abstract Tiny marine animals that complete their life cycle in the total absence of light and oxygen are reported by Roberto Danovaro and colleagues in this issue of BMC Biology. These fascinating animals are new members of the phylum Loricifera and possess mitochondria that in electron micrographs look very much like hydrogenosomes, the H2-producing mitochondria found among several unicellular eukaryotic lineages. The discovery of metazoan life in a permanently anoxic and sulphidic environment provides a glimpse of what a good part of Earth's past ecology might have been like in 'Canfield oceans', before the rise of deep marine oxygen levels and the appearance of the first large animals in the fossil record roughly 550-600 million years ago. The findings underscore the evolutionary significance of anaerobic deep sea environments and the anaerobic lifestyle among mitochondrion-bearing cells. They also testify that a fuller understanding of eukaryotic and metazoan evolution will come from the study of modern anoxic and hypoxic habitats. Commentary The newly reported tiny marine animals that complete their life cycle in the total absence of light and oxygen are members of the phylum Loricifera, a phylum discovered less than 30 years ago, and they are less than a millimetre in size [1]. They were collected from a deep basin at the bottom of the Mediterranean Sea, where they inhabit a nearly salt-saturated brine that, because of its density (>1.2 g/cm3), does not mix with the waters above. As a consequence, this environment is completely anoxic and, due to the activity of sulphate reducers, contains sulphide at a concentration of 2.9 mM. Despite such harsh conditions, this anoxic and sulphidic environment is teeming with microbial life, both chemosynthetic prokaryotes that are primary producers [2], and a broad diversity of eukaryotic heterotrophs at the next trophic level [3, 4]. That this ecological niche also supports animals is a surprise that poses all sorts of interesting questions. Despite being unexpected, however, the finding ties together recent developments from several independent fields (marine biology, cell biology, evolutionary theory and geochemistry) that all point to the evolutionary significance of eukaryotic life in anaerobic environments. The first question raised is 'how?'. How is it possible that animals can inhabit this anoxic and sulphidic environment? This might seem impossible to some, after all one often reads that 'animals have an absolute requirement for oxygen' [5] or 'sulphide is poisonous' [6]. However, not all animals are strictly dependent upon oxygen. Some use different terminal electron acceptors other than oxygen in their mitochondrial respiratory chains, most commonly fumarate, leading to the excretion of succinate and propionate [7], often accompanied by acetate excretion as well [8]. Since the mechanism of sulphide toxicity to animals entails the inhibition of cytochrome c oxidase [9], mitochondria that are not dependent upon that final mediator of an electron transfer to O2 also do not have such a problem with sulphide. Among animal lineages, facultative anaerobic mitochondria have been studied from various free living invertebrates, including the oyster Mytilus (Mollusca) [10], the peanut worm Sipunculus (Sipuncula) [11] or the polychaete worm Arenicola (Annelida) [12] and parasites like Fasciola (Platyhelminthes) [13] and Ascaris (Nematoda) [14]. However, such oxygen-independent energy metabolism in animals is often restricted to some stages -- albeit sometimes prolonged -- of the lifecycle. The Loriciferans that Danovaro et al. [1] describe spend their entire life cycle in the sediment: what was once seen as an 'absolute requirement' for O2 among animals should now be seen as a lineage-specific preference, albeit it one that is very pronounced, especially among those animals that, like ourselves, live on land, permanently above the soil line. Anaerobic mitochondria: more common all the time A look at the phylogenetic distribution of characterized anaerobic mitochondria among animal lineages shows that these are not clustered but spread across metazoan phylogeny (Figure 1). Are these recent adaptations to anaerobic habitats or are they holdovers from our more distant evolutionary past? The biochemistry and the enzyme equipment used in the facultatively anaerobic mitochondria of metazoans is nearly identical across lineages [7, 10–15], strongly indicating a common origin from the metazoan ancestor that might have lived some 600 million years (MY) ago [16]. The examples in Figure 1 cover both the Lophotrochozoa and the Ecdysozoa, where the newly-described Loricifera with their distinctive organelles belong. Although no biochemical data are yet available for these intriguing new Loriciferan mitochondria, from the electron micrographs presented by Danovaro et al. [1] the organelles look like hydrogenosomes - anaerobic forms of mitochondria that generate H2 and adenosine triphosphate (ATP) from pyruvate oxidation [17] and which were previously found only in unicellular eukaryotes. Danovaro et al. [1] also report that the animals harbour structures resembling prokaryotic endosymbionts, reminiscent of the methanogenic endosymbionts found in some hydrogenosome-bearing protists; fluorescence of F420, a typical methanogen cofactor [18], or lack thereof, will bring more insights as to what these structures are. Figure 1 Schematic phylogeny of animals based on the report by Dunnet al[48] which, however, did not include the phylum Loricifera [1] (highlighted in red). It was placed here (dotted line) as branching with Priapulida and Kinorhyncha as reported elsewhere [49, 50]. Higher taxon designations are those used in references [48–50]. The Cycloneuralia, where Loricifera belong, are currently grouped within the Ecdysozoa, which also includes nematodes and insects. To the right of the phylogeny, several species are listed whose facultatively anaerobic mitochondria have been studied [10–14]; in no way does this imply the absence of anaerobic mitochondira in other groups. No biochemical data are yet available for the Loriciferan mitochondria [1]. If we follow the anaerobic lifestyle further back into evolutionary history (Figure 2), beyond the origin of the metazoans, we see that the phylogenetic distribution of eukaryotes with facultative anaerobic mitochondria, eukaryotes with hydrogenosomes and eukaryotes that possess mitosomes (reduced forms of mitochondria with no direct role in ATP synthesis [19]) shows a similar picture to that seen for animals. In all six of the major lineages (or supergroups) of eukaryotes that are currently recognized [20], forms with anaerobic mitochondria have been found [21]. The newest additions to the growing collection of anaerobic mitochondrial metabolisms are the denitrifying foraminiferans [22, 23]. For this group, the underlying enzymes also have yet to be worked out. However, for the remainder of the eukaryotes summarized in Figure 2, a handful of about a dozen enzymes make the difference between a 'normal' (in the sense of 'familiar from older college textbooks') O2-respiring mitochondrion found in mammals, and the energy metabolism of eukaryotes with anaerobic mitochondria, hydrogenosomes or mitosomes [7, 19, 21, 24]. Notably, the full complement of those enzymes, once thought to be specific to eukaryotic anaerobes, surprisingly turned up in the green alga Chlamydomonas reinhardtii [24], which produces O2 in the light, has typical O2-respiring mitochondria but, within about 30 min of exposure to heterotrophic, anoxic and dark conditions, expresses its anaerobic biochemistry to make H2 [25, 26] in the same way as trichomonads, the group in which hydrogenosomes were discovered [17]. Thus, Chlamydomonas provides evidence which indicates that the ability to inhabit oxygen-harbouring, as well as anoxic environments, is an ancestral feature of eukaryotes and their mitochondria; in that sense it is a true missing link that unites mitochondria like our own and those from the anoxic world [19]. Figure 2 Protistan schematic phylogeny representing current view about eukaryotes phylogeny grouping all known eukaryotic life to six major clades or supergroups [20], together with information about the mitochondria from some anaerobic, facultatively anaerobic or parasitic representatives [19, 21]. With greater sampling of anoxic habitats [3, 4, 27], additional information about anaerobic eukaryotes can be expected. The latest additions are the new species of loriciferan animals (red) from the anoxic L'Atalante basin [1]. LECA: last eukaryotic common ancestor. Note the absence of primitively amitochondriate lineages among eukaryotes [19, 21]. It is not yet known what role, if any, the mitochondria of the newly discovered loriciferans play in the energy metabolism of those animals and, unfortunately, the same is true for the myriad of fascinating eukaryotic protists that inhabit the very same environment where the anoxic loriciferans were found. In a series of recent papers, Stoeck and colleagues have surveyed the protistan diversity from L'Atalante and similar habitats with environmental sequencing and metagenomic techniques [3, 4, 27]. Those reports lead to a wealth of representatives from many of the same groups shown in Figure 2 being uncovered: ciliates, fungi and chromalveolates, in addition to representatives of many other eukaryotic groups, including the choanoflagellates [3], which are regarded as the unicellular sisters to the metazoan clade. The investigation of anaerobic mitochondria remains an area of rapid progress and it will be a challenge to discover what those mitochondria are doing in real-life anoxic and hypoxic environments, which are very widespread among modern habitats [28] and where eukaryotic anaerobes abound [27]. Evolutionary significance? What is the evolutionary significance of the new findings? The L'Atalante basin has been anoxic for only about 50,000 years [1] but have all of its microbial and metazoan inhabitants only recently adapted to life in anoxic conditions during that time? Hardly. Nobody seriously considers that anaerobic prokaryotes dwelling in such anaerobic habitats, such as methanogens and sulphate reducers, have only recently adapted to anaerobic niches. The prokaryote inhabitants have existed for well over a billion years, and have reached this new habitat by dispersal, not by adaptive evolution de novo and in situ. Indeed, geochemical evidence has shown that methanogenesis and sulphate reduction, and the niches in which they occur, are truly ancient [29, 30]. (For marine environments, dispersal is not a fundamental problem because, despite vast distances, similar types of seafloor habitats often harbour similar communities, from microbes to large animals [31]). However, when it comes to eukaryotes, there is still a curious tendency to assume that eukaryotes only invaded anaerobic niches of late. Perhaps this stems from a tendency (latently anthropocentric, no doubt) to view mitochondria as obligately O2-dependent organelles with the anaerobic forms of mitochondria being rare exceptions, even though the data (for example in Figures 1 and 2 or elsewhere [7, 8, 17, 19, 21]) tell us otherwise. In environments such as the L'Atalante basin, 'normal', O2-dependent mitochondria are the rare exception, if they exist at all, but eukaryotes abound [1, 3, 4, 32, 33]. Hence, further study of mitochondria from such environments should be revealing. That, however, is easier said than done, since, as seen in the present study [1] as in other work on organisms from anoxic marine sediments [32], a considerable effort has to be invested in order just to demonstrate that the organisms are even alive and not just sunken carcasses. Work on organisms from these environments poses substantial technical challenges, making every new insight all the more exciting. While Danovaro et al. opt for the term 'enigmatic' in discussing the evolutionary significance of their findings, we have a decidedly different view. Given that anaerobic forms of mitochondria are widespread throughout the eukaryotic world, we see eukaryotes in anaerobic habitats as evidence for evolution in the Darwinian sense of descent with modification, with the traits that support survival in anaerobic environments having been conserved from earlier phases of Earth's history. This view is underpinned by what geologists and geochemists have been trying to tell biologists over the last 10 years about the prevalence of anoxic and sulphidic environments during the early phase of eukaryotic and metazoan evolution, but with the biologists perhaps not taking as much notice as they should. The bigger picture: add geological time What are the geologists trying to tell us about anoxic and sulphidic marine habitats? A readable summary of about 10 year's progress is given in three papers [34–36]. In a nutshell, the geologists are saying that the rise in atmospheric oxygen some 2.4 billion years ago is one thing, but that the oxygen levels in the ocean, where evolution was taking place, is quite another. Several lines of isotope evidence indicate that deep ocean waters were not fully oxygenated until about 580 MY ago, about the time when the first large animals made their fossil debut. The reason for this anoxia, they say, has to do with high levels of sulphide in marine environments, from the workings of sulphate reducers [36], themselves strict anaerobes. The message for biologists is that the Earth's oceans appear to have been largely anoxic and sulphidic (like the L'Atalante basin, but not hypersaline) below the photic zone (the upper about 200 m) [34, 36] and, possibly, also in the lower photic zone [35] for the time spanning roughly 1.8 billion years ago to about the beginning of the Cambrian period 560 MY ago. That was the time during which eukaryotes arose and diversified [21]. Hence, it should hardly be surprising that the anaerobic lifestyle is widespread among eukaryotic lineages, right up into the animals [37]. One view is that the oxygenation of deep environments allowed the animals to become larger [38], which is different from saying that oxygen might in any way be causal to the Cambrian appearance of diverse animal forms [39]. Geologists have not been trying to hide their perspective from us biologists [40–42], nor have we biologists been trying to hide our progress in understanding eukaryotic anaerobes [21, 43–46]. However, it seems that it will still take some time for a new default view of deep marine environments in the Late Precambrian -- a new synthesis of sorts -- to be accepted by biologists. The scenario is one of widespread anoxic and sulphidic habitats, almost certainly (in our view) teeming with little eukaryotic creatures, all with their mitochondria well suited to life with little or no oxygen [37] and all more or less like the ones we see in anoxic and sulphidic environments today. Such environments staged and witnessed the origin of evolution's greatest early inventions [47] and the new insights emerging from them deserve our close attention. Heiner I, Kristensen RM: Two new species of the genus Pliciloricus (Loricifera, Pliciloricidae) from the Faroe Bank, North Atlantic. Zoologischer Anzeiger. 2005, 243: 121-138. 10.1016/j.jcz.2004.05.002. Authors’ original submitted files for images Rights and permissions This article is published under license to BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited."}
{"url": "https://en.m.wikipedia.org/wiki/Special:BookSources/978-81-7133-896-2", "text": "This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). If you arrived at this page by clicking an ISBN link in a Wikipedia page, you will find the full range of relevant search links for that specific book by scrolling to the find links below. To search for a different book, type that book's individual ISBN into this ISBN search box. Spaces and hyphens in the ISBN do not matter. Also, the number starts after the colon for \"ISBN-10:\" and \"ISBN-13:\" numbers. An ISBN identifies a specific edition of a book. Any given title may therefore have a number of different ISBNs. See #Find other editions below for finding other editions. An ISBN registration, even one corresponding to a book page on a major book distributor database, is not definite proof that such a book actually exists. A title may have been cancelled or postponed after the ISBN was assigned. Check to see if the book exists or not. Google Books and Amazon.com may be helpful if you want to verify citations in Wikipedia articles, because they often let you search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available). At the Open Library (part of the Internet Archive) you can borrow and read entire books online. Luxembourg Montenegro Netherlands Find this book in the Dutch-Union Catalogue that searches simultaneously in more than 400 Dutch electronic library systems (including regional libraries, university libraries, research libraries and the Royal Dutch library) Book-swapping websites Non-English book sources If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language."}
{"url": "https://web.archive.org/web/20161223074013/https://www.ncbi.nlm.nih.gov/books/NBK26810/", "text": "The Internet Archive discovers and captures web pages through many different web crawls. At any given time several distinct crawls are running, some for months, and some every day or longer. View the web archive through the Wayback Machine. Content crawled via the Wayback Machine Live Proxy mostly by the Save Page Now feature on web.archive.org. Liveweb proxy is a component of Internet Archive’s wayback machine project. The liveweb proxy captures the content of a web page in real time, archives it into a ARC or WARC file and returns the ARC/WARC record back to the wayback machine to process. The recorded ARC/WARC file becomes part of the wayback machine in due course of time. Molecular Biology of the Cell. 4th edition. The Extracellular Matrix of Animals Tissues are not made up solely of cells. A substantial part of their volume is extracellular space, which is largely filled by an intricate network of macromolecules constituting the extracellular matrix (Figure 19-33). This matrix is composed of a variety of proteins and polysaccharides that are secreted locally and assembled into an organized meshwork in close association with the surface of the cell that produced them. Cells surrounded by spaces filled with extracellular matrix. The particular cells shown in this low-power electron micrograph are those in an embryonic chick limb bud. The cells have not yet acquired their specialized characteristics. (Courtesy of Cheryll (more...) Whereas we have discussed cell junctions chiefly in the context of epithelial tissues, our account of the extracellular matrix concentrates on connective tissues (Figure 19-34). The extracellular matrix in connective tissue is frequently more plentiful than the cells it surrounds, and it determines the tissue's physical properties. Connective tissues form the framework of the vertebrate body, but the amounts found in different organs vary greatly—from cartilage and bone, in which they are the major component, to brain and spinal cord, in which they are only minor constituents. The connective tissue underlying an epithelium. This tissue contains a variety of cells and extracellular matrix components. The predominant cell type is the fibroblast, which secretes abundant extracellular matrix. Variations in the relative amounts of the different types of matrix macromolecules and the way in which they are organized in the extracellular matrix give rise to an amazing diversity of forms, each adapted to the functional requirements of the particular tissue. The matrix can become calcified to form the rock-hard structures of bone or teeth, or it can form the transparent matrix of the cornea, or it can adopt the ropelike organization that gives tendons their enormous tensile strength. At the interface between an epithelium and connective tissue, the matrix forms a basal lamina (see Figure 19-34), which is important in controlling cell behavior. The vertebrate extracellular matrix was once thought to serve mainly as a relatively inert scaffold to stabilize the physical structure of tissues. But now it is clear that the matrix has a far more active and complex role in regulating the behavior of the cells that contact it, influencing their survival, development, migration, proliferation, shape, and function. The extracellular matrix has a correspondingly complex molecular composition. Although our understanding of its organization is still incomplete, there has been rapid progress in characterizing many of its major components. We focus on the extracellular matrix of vertebrates, but the origins of the extracellular matrix are very ancient and virtually all multicellular organisms, make it; examples include the cuticles of worms and insects, the shells of mollusks, and, as we discuss later, the cell walls of plants. The Extracellular Matrix Is Made and Oriented by the Cells Within It The macromolecules that constitute the extracellular matrix are mainly produced locally by cells in the matrix. As we discuss later, these cells also help to organize the matrix: the orientation of the cytoskeleton inside the cell can control the orientation of the matrix produced outside. In most connective tissues, the matrix macromolecules are secreted largely by cells called fibroblasts (Figure 19-35). In certain specialized types of connective tissues, such as cartilage and bone, however, they are secreted by cells of the fibroblast family that have more specific names: chondroblasts, for example, form cartilage, and osteoblasts form bone. Fibroblasts in connective tissue. This scanning electron micrograph shows tissue from the cornea of a rat. The extracellular matrix surrounding the fibroblasts is composed largely of collagen fibrils (there are no elastic fibers in the cornea). The glycoproteins, (more...) Two main classes of extracellular macromolecules make up the matrix: (1) polysaccharide chains of the class called glycosaminoglycans (GAGs), which are usually found covalently linked to protein in the form of proteoglycans, and (2) fibrous proteins, including collagen, elastin, fibronectin, and laminin, which have both structural and adhesive functions. We shall see that the members of both classes come in a great variety of shapes and sizes. The proteoglycan molecules in connective tissue form a highly hydrated, gel-like “ground substance” in which the fibrous proteins are embedded. The polysaccharide gel resists compressive forces on the matrix while permitting the rapid diffusion of nutrients, metabolites, and hormones between the blood and the tissue cells. The collagen fibers both strengthen and help organize the matrix, and rubberlike elastin fibers give it resilience. Finally, many matrix proteins help cells attach in the appropriate locations. Glycosaminoglycan (GAG) Chains Occupy Large Amounts of Space and Form Hydrated Gels Glycosaminoglycans (GAGs) are unbranched polysaccharide chains composed of repeating disaccharide units. They are called GAGs because one of the two sugars in the repeating disaccharide is always an amino sugar (N-acetylglucosamine or N-acetylgalactosamine), which in most cases is sulfated. The second sugar is usually a uronic acid (glucuronic or iduronic). Because there are sulfate or carboxyl groups on most of their sugars, GAGs are highly negatively charged (Figure 19-36). Indeed, they are the most anionic molecules produced by animal cells. Four main groups of GAGs are distinguished according to their sugars, the type of linkage between the sugars, and the number and location of sulfate groups: (1) hyaluronan, (2) chondroitin sulfate and dermatan sulfate, (3) heparan sulfate, and (4) keratan sulfate. The repeating disaccharide sequence of a dermatan sulfate glycosaminoglycan (GAG) chain. These chains are typically 70–200 sugars long. There is a high density of negative charges along the chain resulting from the presence of both carboxyl and (more...) Polysaccharide chains are too stiff to fold up into the compact globular structures that polypeptide chains typically form. Moreover, they are strongly hydrophilic. Thus, GAGs tend to adopt highly extended conformations that occupy a huge volume relative to their mass (Figure 19-37), and they form gels even at very low concentrations. Their high density of negative charges attracts a cloud of cations, most notably Na+, that are osmotically active, causing large amounts of water to be sucked into the matrix. This creates a swelling pressure, or turgor, that enables the matrix to withstand compressive forces (in contrast to collagen fibrils, which resist stretching forces). The cartilage matrix that lines the knee joint, for example, can support pressures of hundreds of atmospheres in this way. The relative dimensions and volumes occupied by various macromolecules. Several proteins, a glycogen granule, and a single hydrated molecule of hyaluronan are shown. The GAGs in connective tissue usually constitute less than 10% of the weight of the fibrous proteins. But, because they form porous hydrated gels, the GAG chains fill most of the extracellular space, providing mechanical support to the tissue. In one rare human genetic disease, there is a severe deficiency in the synthesis of the dermatan sulfate disaccharide shown in Figure 19-36. The affected individuals have a short stature, prematurely aged appearance, and generalized defects in their skin, joints, muscles, and bones. It should be emphasized, however, that, in invertebrates and plants, other types of polysaccharides often dominate the extracellular matrix. Thus, in higher plants, as we discuss later, cellulose (polyglucose) chains are packed tightly together in ribbonlike crystalline arrays to form the microfibrillar component of the cell wall. In insects, crustaceans, and other arthropods, chitin (poly-N-acetylglucosamine) similarly forms the main component of the exoskeleton. Together, cellulose and chitin are the most abundant biopolymers on Earth. Hyaluronan Is Thought to Facilitate Cell Migration During Tissue Morphogenesis and Repair Hyaluronan (also called hyaluronic acid or hyaluronate) is the simplest of the GAGs (Figure 19-38). It consists of a regular repeating sequence of up to 25,000 nonsulfated disaccharide units, is found in variable amounts in all tissues and fluids in adult animals, and is especially abundant in early embryos. Hyaluronan is not typical of the majority of GAGs. In contrast with all of the others, it contains no sulfated sugars, all its disaccharide units are identical, its chain length is enormous (thousands of sugar monomers), and it is not generally linked covalently to any core protein. Moreover, whereas other GAGs are synthesized inside the cell and released by exocytosis, hyaluronan is spun out directly from the cell surface by an enzymecomplex embedded in the plasma membrane. The repeating disaccharide sequence in hyaluronan, a relatively simple GAG. This ubiquitous molecule in vertebrates consists of a single long chain of up to 25,000 sugars. Note the absence of sulfate groups. Hyaluronan is thought to have a role in resisting compressive forces in tissues and joints. It is also important as a space filler during embryonic development, where it can be used to force a change in the shape of a structure, as a small quantity expands with water to occupy a large volume (see Figure 19-37). Hyaluronan synthesized from the basal side of an epithelium, for example, often serves to create a cell-free space into which cells subsequently migrate; this occurs in the formation of the heart, the cornea, and several other organs. When cell migration ends, the excess hyaluronan is generally degraded by the enzymehyaluronidase. Hyaluronan is also produced in large quantities during wound healing, and it is an important constituent of joint fluid, where it serves as a lubricant. Many of the functions of hyaluronan depend on specific interactions with other molecules, including both proteins and proteoglycans—molecules consisting of GAG chains covalently linked to a protein. Some of these molecules that bind to hyaluronan are constituents of the extracellular matrix, while others are integral components of the surface of cells. Proteoglycans Are Composed of GAG Chains Covalently Linked to a Core Protein Except for hyaluronan, all GAGs are found covalently attached to protein in the form of proteoglycans, which are made by most animal cells. The polypeptide chain, or core protein, of a proteoglycan is made on membrane-bound ribosomes and threaded into the lumen of the endoplasmic reticulum. The polysaccharide chains are mainly assembled on this core protein in the Golgi apparatus. First, a special link tetrasaccharide is attached to a serine side chain on the core protein to serve as a primer for polysaccharide growth; then, one sugar at a time is added by specific glycosyl transferases (Figure 19-39). While still in the Golgi apparatus, many of the polymerized sugars are covalently modified by a sequential and coordinated series of reactions. Epimerizations alter the configuration of the substituents around individual carbon atoms in the sugar molecule; sulfations increase the negative charge. The linkage between a GAG chain and its core protein in a proteoglycan molecule. A specific link tetrasaccharide is first assembled on a serine side chain. In most cases, it is unclear how the particular serine is selected, but it seems that a specific (more...) Proteoglycans are usually easily distinguished from other glycoproteins by the nature, quantity, and arrangement of their sugar side chains. By definition, at least one of the sugar side chains of a proteoglycan must be a GAG. Whereas glycoproteins contain 1–60% carbohydrate by weight in the form of numerous relatively short, branched oligosaccharide chains, proteoglycans can contain as much as 95% carbohydrate by weight, mostly in the form of long, unbranched GAG chains, each typically about 80 sugars long. Proteoglycans can be huge. The proteoglycan aggrecan, for example, which is a major component of cartilage, has a mass of about 3 × 106 daltons with over 100 GAG chains. Other proteoglycans are much smaller and have only 1–10 GAG chains; an example is decorin, which is secreted by fibroblasts and has a single GAG chain (Figure 19-40). Examples of a small (decorin) and a large (aggrecan) proteoglycan found in the extracellular matrix. These two proteoglycans are compared with a typical secreted glycoprotein molecule, pancreatic ribonuclease B. All three are drawn to scale. The core (more...) In principle, proteoglycans have the potential for almost limitless heterogeneity. Even a single type of core protein can vary greatly in the number and types of attached GAG chains. Moreover, the underlying repeating pattern of disaccharides in each GAG can be modified by a complex pattern of sulfate groups. The heterogeneity of these GAGs makes it difficult to identify and classify proteoglycans in terms of their sugars. The sequences of many core proteins have been determined with the aid of recombinant DNA techniques, and they, too, are extremely diverse. Although a few small families have been recognized, no common structural feature clearly distinguishes proteoglycan core proteins from other proteins, and many have one or more domains that are homologous to domains found in other proteins of the extracellular matrix or plasma membrane. Thus, it is probably best to regard proteoglycans as a diverse group of highly glycosylated glycoproteins whose functions are mediated by both their core proteins and their GAG chains. Proteoglycans Can Regulate the Activities of Secreted Proteins Given the great abundance and structural diversity of proteoglycan molecules, it would be surprising if their function were limited to providing hydrated space around and between cells. Their GAG chains, for example, can form gels of varying pore size and charge density; one possible function, therefore, is to serve as selective sieves to regulate the traffic of molecules and cells according to their size, charge, or both. Evidence suggests that a heparan sulfate proteoglycan called perlecan has this role in the basal lamina of the kidney glomerulus, which filters molecules passing into the urine from the bloodstream (discussed below). Proteoglycans are thought to have a major role in chemical signaling between cells. They bind various secreted signal molecules, such as certain protein growth factors, and can enhance or inhibit their signaling activity. For example, the heparan sulfate chains of proteoglycans bind to fibroblast growth factors (FGFs), which stimulate a variety of cell types to proliferate; this interaction oligomerizes the growth factor molecules, enabling them to cross-link and activate their cell-surface receptors, which are transmembrane tyrosine kinases (see Figure 15-50B). Whereas in most cases the signal molecules bind to the GAG chains of the proteoglycan, this is not always so. Some members of the transforming growth factor β (TGF-β) family bind to the core proteins of several matrix proteoglycans, including decorin; binding to decorin inhibits the activity of the growth factors. Proteoglycans also bind, and regulate the activities of, other types of secreted proteins, including proteolytic enzymes (proteases) and protease inhibitors. Binding to a proteoglycan could control the activity of a secreted protein in any of the following ways: (1) it could immobilize the protein close to the site where it is produced, thereby restricting its range of action; (2) it could sterically block the activity of the protein; (3) it could provide a reservoir of the protein for delayed release; (4) it could protect the protein from proteolytic degradation, thereby prolonging its action; (5) it could alter or concentrate the protein for more effective presentation to cell-surface receptors. Proteoglycans are thought to act in all these ways to help regulate the activities of secreted proteins. An example of the last function occurs in inflammatory responses, in which heparan sulfate proteoglycans immobilize secreted chemotactic attractants called chemokines (discussed in Chapter 24) on the endothelial surface of a blood vessel at an inflammatory site. In this way, the chemokines remain there for a prolonged period, stimulating white blood cells to leave the bloodstream and migrate into the inflamed tissue. An aggrecan aggregate from fetal bovine cartilage. (A) An electron micrograph of an aggrecan aggregate shadowed with platinum. Many free aggrecan molecules are also visible. (B) A drawing of the giant aggrecan aggregate shown in (A). It consists of about (more...) Moreover, besides associating with one another, GAGs and proteoglycans associate with fibrous matrix proteins such as collagen and with protein meshworks such as the basal lamina, creating extremely complex structures. Decorin, which binds to collagen fibrils, is essential for collagen fiber formation; mice that cannot make decorin have fragile skin that has reduced tensile strength. The arrangement of proteoglycan molecules in living tissues is generally hard to determine. As the molecules are highly water-soluble, they may be washed out of the extracellular matrix when tissue sections are exposed to aqueous solutions during fixation. In addition, changes in pH, ionic, or osmotic conditions can drastically alter their conformation. Thus, specialized methods must be used to visualize them in tissues (Figure 19-42). Proteoglycans in the extracellular matrix of rat cartilage. The tissue was rapidly frozen at -196°C, and fixed and stained while still frozen (a process called freeze substitution) to prevent the GAG chains from collapsing. In this electron micrograph, (more...) Cell-Surface Proteoglycans Act as Co-receptors Not all proteoglycans are secreted components of the extracellular matrix. Some are integral components of plasma membranes and have their core protein either inserted across the lipid bilayer or attached to the lipid bilayer by a glycosylphosphatidylinositol (GPI) anchor. Some of these plasma membrane proteoglycans act as co-receptors that collaborate with conventional cell-surface receptor proteins, in both binding cells to the extracellular matrix and initiating the response of cells to some extracellular signal proteins. In addition, some conventional receptors have one or more GAG chains and are therefore proteoglycans themselves. Among the best-characterized plasma membrane proteoglycans are the syndecans, which have a membrane-spanning core protein. The extracellular domains of these transmembrane proteoglycans carry up to three chondroitin sulfate and heparan sulfate GAG chains, while their intracellular domains are thought to interact with the actincytoskeleton in the cell cortex. Syndecans are located on the surface of many types of cells, including fibroblasts and epithelial cells, where they serve as receptors for matrix proteins. In fibroblasts, syndecans can be found in focal adhesions, where they modulate integrin function by interacting with fibronectin on the cell surface and with cytoskeletal and signaling proteins inside the cell. Syndecans also bind FGFs and present them to FGF receptor proteins on the same cell. Similarly, another plasma membraneproteoglycan, called betaglycan, binds TGF-β and may present it to TGF-β receptors. The importance of proteoglycans as co-receptors is illustrated by the severe developmental defects that can occur when specific proteoglycans are inactivated by mutation. In Drosophila, for example, signaling by the secreted signal proteinWingless depends on the protein's binding to a specific heparan sulfate proteoglycan co-receptor called Dally on the target cell. In mutant flies deficient in Dally, Wingless signaling fails, and the severe developmental defects that result are similar to those that result from mutations in the winglessgene itself. In some tissues, inactivation of Dally also inhibits signaling by a secreted protein of the TGF-β family called Decapentaplegic (DPP). Some of the proteoglycans discussed in this chapter are summarized in Table 19-4. Collagens Are the Major Proteins of the Extracellular Matrix The collagens are a family of fibrous proteins found in all multicellular animals. They are secreted by connective tissue cells, as well as by a variety of other cell types. As a major component of skin and bone, they are the most abundant proteins in mammals, constituting 25% of the total protein mass in these animals. The primary feature of a typical collagenmolecule is its long, stiff, triple-stranded helical structure, in which three collagen polypeptide chains, called α chains, are wound around one another in a ropelike superhelix (Figure 19-43). Collagens are extremely rich in proline and glycine, both of which are important in the formation of the triple-stranded helix. Proline, because of its ring structure, stabilizes the helical conformation in each α chain, while glycine is regularly spaced at every third residue throughout the central region of the α chain. Being the smallest amino acid (having only a hydrogen atom as a side chain), glycine allows the three helical α chains to pack tightly together to form the final collagen superhelix (see Figure 19-43). The structure of a typical collagen molecule. (A) A model of part of a single collagen α chain in which each amino acid is represented by a sphere. The chain is about 1000 amino acids long. It is arranged as a left-handed helix, with three amino (more...) So far, about 25 distinct collagen α chains have been identified, each encoded by a separate gene. Different combinations of these genes are expressed in different tissues. Although in principle more than 10,000 types of triple-stranded collagen molecules could be assembled from various combinations of the 25 or so α chains, only about 20 types of collagen molecules have been found. The main types of collagen found in connective tissues are types I, II, III, V, and XI, type I being the principal collagen of skin and bone and by far the most common. These are the fibrillar collagens, or fibril-forming collagens, with the ropelike structure illustrated in Figure 19-43. After being secreted into the extracellular space, these collagen molecules assemble into higher-order polymers called collagen fibrils, which are thin structures (10–300 nm in diameter) many hundreds of micrometers long in mature tissues and clearly visible in electron micrographs (Figure 19-44; see also Figure 19-42). Collagen fibrils often aggregate into larger, cablelike bundles, several micrometers in diameter, which can be seen in the light microscope as collagen fibers. Fibroblast surrounded by collagen fibrils in the connective tissue of embryonic chick skin. In this electron micrograph, the fibrils are organized into bundles that run approximately at right angles to one another. Therefore, some bundles are oriented (more...) Collagen types IX and XII are called fibril-associated collagens because they decorate the surface of collagen fibrils. They are thought to link these fibrils to one another and to other components in the extracellular matrix. Types IV and VII are network-forming collagens. Type IV molecules assemble into a feltlike sheet or meshwork that constitutes a major part of mature basal laminae, while type VII molecules form dimers that assemble into specialized structures called anchoring fibrils. Anchoring fibrils help attach the basal lamina of multilayered epithelia to the underlying connective tissue and therefore are especially abundant in the skin. There are also a number of “collagen-like” proteins, including type XVII, which has a transmembrane domain and is found in hemidesmosomes, and type XVIII, which is located in the basal laminae of blood vessels. Cleavage of the C-terminal domain of type XVIII collagen yields a peptide called endostatin, which inhibits new blood vessel formation and is therefore being investigated as an anticancer drug. Some of the collagen types discussed in this chapter are listed in Table 19-5. Many proteins that contain a repeated pattern of amino acids have evolved by duplications of DNA sequences. The fibrillar collagens apparently arose in this way. Thus, the genes that encode the α chains of most of these collagens are very large (up to 44 kilobases in length) and contain about 50 exons. Most of the exons are 54, or multiples of 54, nucleotides long, suggesting that these collagens arose by multiple duplications of a primordial gene containing 54 nucleotides and encoding exactly 6 Gly-X-Y repeats (see Figure 19-43). Collagens Are Secreted with a Nonhelical Extension at Each End Individual collagenpolypeptide chains are synthesized on membrane-bound ribosomes and injected into the lumen of the endoplasmic reticulum (ER) as larger precursors, called pro-α chains. These precursors not only have the short amino-terminal signal peptide required to direct the nascent polypeptide to the ER, they also have additional amino acids, called propeptides, at both their N- and C-terminal ends. In the lumen of the ER, selected prolines and lysines are hydroxylated to form hydroxyproline and hydroxylysine, respectively, and some of the hydroxylysines are glycosylated. Each pro-α chain then combines with two others to form a hydrogen-bonded, triple-stranded, helical molecule known as procollagen. Hydroxylysines and hydroxyprolines (Figure 19-45) are infrequently found in other animal proteins, although hydroxyproline is abundant in some proteins in the plant cell wall. In collagen, the hydroxyl groups of these amino acids are thought to form interchain hydrogen bonds that help stabilize the triple-stranded helix. Conditions that prevent proline hydroxylation, such as a deficiency of ascorbic acid (vitamin C), have serious consequences. In scurvy, the disease caused by a dietary deficiency of vitamin C that was common in sailors until the nineteenth century, the defective pro-α chains that are synthesized fail to form a stable triple helix and are immediately degraded within the cell. Consequently, with the gradual loss of the preexisting normal collagen in the matrix, blood vessels become extremely fragile and teeth become loose in their sockets, implying that in these particular tissues the degradation and replacement of collagen occur relatively rapidly. In many other adult tissues, however, the turnover of collagen (and other extracellular matrix macromolecules) is thought to be very slow. In bone, to take an extreme example, collagen molecules persist for about 10 years before they are degraded and replaced. By contrast, most cell proteins have half-lives of hours or days. Hydroxylysine and hydroxyproline. These modified amino acids are common in collagen. They are formed by enzymes that act after the lysine and proline have been incorporated into procollagen molecules. After Secretion, Fibrillar Procollagen Molecules Are Cleaved to Collagen Molecules, Which Assemble into Fibrils After secretion, the propeptides of the fibrillar procollagen molecules are removed by specific proteolytic enzymes outside the cell. This converts the procollagen molecules to collagen molecules, which assemble in the extracellular space to form much larger collagen fibrils. The propeptides have at least two functions. First, they guide the intracellular formation of the triple-stranded collagen molecules. Second, because they are removed only after secretion, they prevent the intracellular formation of large collagen fibrils, which could be catastrophic for the cell. The process of fibril formation is driven, in part, by the tendency of the collagen molecules, which are more than a thousandfold less soluble than procollagen molecules, to self-assemble. The fibrils begin to form close to the cell surface, often in deep infoldings of the plasma membrane formed by the fusion of secretory vesicles with the cell surface. The underlying cortical cytoskeleton can therefore influence the sites, rates, and orientation of fibril assembly. When viewed in an electron microscope, collagen fibrils have characteristic cross-striations every 67 nm, reflecting the regularly staggered packing of the individual collagen molecules in the fibril. After the fibrils have formed in the extracellular space, they are greatly strengthened by the formation of covalent cross-links between lysine residues of the constituent collagen molecules (Figure 19-46). The types of covalent bonds involved are found only in collagen and elastin. If cross-linking is inhibited, the tensile strength of the fibrils is drastically reduced; collagenous tissues become fragile, and structures such as skin, tendons, and blood vessels tend to tear. The extent and type of cross-linking vary from tissue to tissue. Collagen is especially highly cross-linked in the Achilles tendon, for example, where tensile strength is crucial. Cross-links formed between modified lysine side chains within a collagen fibril. Covalent intramolecular and intermolecular cross-links are formed in several steps. First, certain lysines and hydroxylysines are deaminated by the extracellular enzyme lysyl (more...) Figure 19-47 summarizes the various steps in the synthesis and assembly of collagen fibrils. Given the large number of enzymatic steps involved, it is not surprising that there are many human genetic diseases that affect fibril formation. Mutations affecting type I collagen cause osteogenesis imperfecta, characterized by weak bones that fracture easily. Mutations affecting type II collagen cause chondrodysplasias, characterized by abnormal cartilage, which leads to bone and joint deformities. Mutations affecting type III collagen cause Ehlers-Danlos syndrome, characterized by fragile skin and blood vessels and hypermobile joints. The intracellular and extracellular events in the formation of a collagen fibril. (A) Note that collagen fibrils are shown assembling in the extracellular space contained within a large infolding in the plasma membrane. As one example of how collagen (more...) Fibril-associated Collagens Help Organize the Fibrils In contrast to GAGs, which resist compressive forces, collagen fibrils form structures that resist tensile forces. The fibrils have various diameters and are organized in different ways in different tissues. In mammalian skin, for example, they are woven in a wickerwork pattern so that they resist tensile stress in multiple directions. In tendons, they are organized in parallel bundles aligned along the major axis of tension. In mature bone and in the cornea, they are arranged in orderly plywoodlike layers, with the fibrils in each layer lying parallel to one another but nearly at right angles to the fibrils in the layers on either side. The same arrangement occurs in tadpole skin (Figure 19-48). Collagen fibrils in the tadpole skin. This electron micrograph shows the plywoodlike arrangement of the fibrils. Successive layers of fibrils are laid down nearly at right angles to each other. This organization is also found in mature bone and in the (more...) The connective tissue cells themselves must determine the size and arrangement of the collagen fibrils. The cells can express one or more genes for the different types of fibrillar procollagen molecules. But even fibrils composed of the same mixture of fibrillar collagen molecules have different arrangements in different tissues. How is this achieved? Part of the answer is that cells can regulate the disposition of the collagen molecules after secretion by guiding collagen fibril formation in close association with the plasma membrane (see Figure 19-46). In addition, as the spatial organization of collagen fibrils at least partly reflects their interactions with other molecules in the matrix, cells can influence this organization by secreting, along with their fibrillar collagens, different kinds and amounts of other matrix macromolecules. Fibril-associated collagens, such as types IX and XII collagens, are thought to be especially important in this regard. They differ from fibrillar collagens in several ways. 1. Their triple-stranded helical structure is interrupted by one or two short nonhelical domains, which makes the molecules more flexible than fibrillar collagen molecules. 2. They are not cleaved after secretion and therefore retain their propeptides. 3. They do not aggregate with one another to form fibrils in the extracellular space. Instead, they bind in a periodic manner to the surface of fibrils formed by the fibrillar collagens. Type IX molecules bind to type-II-collagen-containing fibrils in cartilage, the cornea, and the vitreous of the eye (Figure 19-49), whereas type XII molecules bind to type-I-collagen-containing fibrils in tendons and various other tissues. Fibril-associated collagens are thought to mediate the interactions of collagen fibrils with one another and with other matrix macromolecules. In this way, they have a role in determining the organization of the fibrils in the matrix. Cells Help Organize the Collagen Fibrils They Secrete by Exerting Tension on the Matrix Cells interact with the extracellular matrix mechanically as well as chemically, with dramatic effects on the architecture of the tissue. Thus, for example, fibroblasts work on the collagen they have secreted, crawling over it and tugging on it—helping to compact it into sheets and draw it out into cables. When fibroblasts are mixed with a meshwork of randomly oriented collagen fibrils that form a gel in a culture dish, the fibroblasts tug on the meshwork, drawing in collagen from their surroundings and thereby causing the gel to contract to a small fraction of its initial volume. By similar activities, a cluster of fibroblasts surrounds itself with a capsule of densely packed and circumferentially oriented collagen fibers. If two small pieces of embryonic tissue containing fibroblasts are placed far apart on a collagen gel, the intervening collagen becomes organized into a compact band of aligned fibers that connect the two explants (Figure 19-50). The fibroblasts subsequently migrate out from the explants along the aligned collagen fibers. Thus, the fibroblasts influence the alignment of the collagen fibers, and the collagen fibers in turn affect the distribution of the fibroblasts. Fibroblasts presumably have a similar role in generating long-range order in the extracellular matrix inside the body—in helping to create tendons and ligaments, for example, and the tough, dense layers of connective tissue that ensheathe and bind together most organs. The shaping of the extracellular matrix by cells. This micrograph shows a region between two pieces of embryonic chick heart (rich in fibroblasts as well as heart muscle cells) that were cultured on a collagen gel for 4 days. A dense tract of aligned (more...) Elastin Gives Tissues Their Elasticity Many vertebrate tissues, such as skin, blood vessels, and lungs, need to be both strong and elastic in order to function. A network of elastic fibers in the extracellular matrix of these tissues gives them the required resilience so that they can recoil after transient stretch (Figure 19-51). Elastic fibers are at least five times more extensible than a rubber band of the same cross-sectional area. Long, inelastic collagen fibrils are interwoven with the elastic fibers to limit the extent of stretching and prevent the tissue from tearing. Elastic fibers. These scanning electron micrographs show (A) a low-power view of a segment of a dog's aorta and (B) a high-power view of the dense network of longitudinally oriented elastic fibers in the outer layer of the same blood vessel. All the other (more...) The main component of elastic fibers is elastin, a highly hydrophobic protein (about 750 amino acids long), which, like collagen, is unusually rich in proline and glycine but, unlike collagen, is not glycosylated and contains some hydroxy-proline but no hydroxylysine. Soluble tropoelastin (the biosynthetic precursor of elastin) is secreted into the extracellular space and assembled into elastic fibers close to the plasma membrane, generally in cell-surface infoldings. After secretion, the tropoelastin molecules become highly cross-linked to one another, generating an extensive network of elastin fibers and sheets. The cross-links are formed between lysines by a mechanism similar to the one discussed earlier that operates in cross-linking collagen molecules. The elastinprotein is composed largely of two types of short segments that alternate along the polypeptide chain: hydrophobic segments, which are responsible for the elastic properties of the molecule; and alanine- and lysine-rich α-helical segments, which form cross-links between adjacent molecules. Each segment is encoded by a separate exon. There is still controversy, however, concerning the conformation of elastin molecules in elastic fibers and how the structure of these fibers accounts for their rubberlike properties. In one view, the elastin polypeptide chain, like the polymer chains in ordinary rubber, adopts a loose “random coil” conformation, and it is the random coil structure of the component molecules cross-linked into the elastic fiber network that allows the network to stretch and recoil like a rubber band (Figure 19-52). Stretching a network of elastin molecules. The molecules are joined together by covalent bonds (red) to generate a cross-linked network. In this model, each elastin molecule in the network can expand and contract as a random coil, so that the entire assembly (more...) Elastin is the dominantextracellular matrixprotein in arteries, comprising 50% of the dry weight of the largest artery—the aorta. Mutations in the elastingene causing a deficiency of the protein in mice or humans result in narrowing of the aorta or other arteries as a result of excessive proliferation of smooth muscle cells in the arterial wall. Apparently, the normal elasticity of an artery is required to restrain the proliferation of these cells. Elastic fibers are not composed solely of elastin. The elastin core is covered with a sheath of microfibrils, each of which has a diameter of about 10 nm. Microfibrils are composed of a number of distinct glycoproteins, including the large glycoproteinfibrillin, which binds to elastin and is essential for the integrity of elastic fibers. Mutations in the fibrillin gene result in Marfan's syndrome, a relatively common human genetic disease affecting connective tissues that are rich in elastic fibers; in the most severely affected individuals, the aorta is prone to rupture. Microfibrils are thought to be important in the assembly of elastic fibers. They appear before elastin in developing tissues and seem to form a scaffold on which the secreted elastin molecules are deposited. As the elastin is deposited, the microfibrils become displaced to the periphery of the growing fiber. Fibronectin Is an Extracellular Protein That Helps Cells Attach to the Matrix The extracellular matrix contains a number of noncollagen proteins that typically have multiple domains, each with specific binding sites for other matrix macromolecules and for receptors on the surface of cells. These proteins therefore contribute to both organizing the matrix and helping cells attach to it. The first of them to be well characterized was fibronectin, a large glycoprotein found in all vertebrates. Fibronectin is a dimer composed of two very large subunits joined by disulfide bonds at one end. Each subunit is folded into a series of functionally distinct domains separated by regions of flexible polypeptide chain (Figure 19-53A and B). The domains in turn consist of smaller modules, each of which is serially repeated and usually encoded by a separate exon, suggesting that the fibronectingene, like the collagen genes, evolved by multiple exon duplications. All forms of fibronectin are encoded by a single large gene that contains about 50 exons of similar size. Transcription produces a single large RNAmolecule that can be alternatively spliced to produce the various isoforms of fibronectin. The main type of module, called the type III fibronectin repeat, binds to integrins. It is about 90 amino acids long and occurs at least 15 times in each subunit. The type III fibronectin repeat is among the most common of all protein domains in vertebrates. The structure of a fibronectin dimer. (A) Electron micrographs of individual fibronectin dimer molecules shadowed with platinum; red arrows mark the C-termini. (B) The two polypeptide chains are similar but generally not identical (being made from the (more...) One way to analyze a complex multifunctional proteinmolecule like fibronectin is to chop it into pieces and determine the function of its individual domains. When fibronectin is treated with a low concentration of a proteolytic enzyme, the polypeptide chain is cut in the connecting regions between the domains, leaving the domains themselves intact. One can then show that one of its domains binds to collagen, another to heparin, another to specific receptors on the surface of various types of cells, and so on (see Figure 19-53B). Synthetic peptides corresponding to different segments of the cell-binding domain have been used to identify a specific tripeptide sequence (Arg-Gly-Asp, or RGD), which is found in one of the type III repeats (see Figure 19-53C), as a central feature of the binding site. Even very short peptides containing this RGD sequence can compete with fibronectin for the binding site on cells, thereby inhibiting the attachment of the cells to a fibronectin matrix. If these peptides are coupled to a solid surface, they cause cells to adhere to it. The RGD sequence is not confined to fibronectin. It is found in a number of extracellular proteins, including, for example, the blood-clotting factor fibrinogen. Fibrinogen peptides containing this RGD sequence have been useful in the development of anti-clotting drugs that mimic these peptides. Snakes use a similar strategy to cause their victims to bleed: they secrete RGD-containing anti-clotting proteins called disintegrins into their venom. RGD sequences are recognized by several members of the integrin family of cell-surface matrix receptors. Each integrin, however, specifically recognizes its own small set of matrix molecules, indicating that tight binding requires more than just the RGD sequence. Fibronectin Exists in Both Soluble and Fibrillar Forms There are multiple isoforms of fibronectin. One, called plasma fibronectin, is soluble and circulates in the blood and other body fluids, where it is thought to enhance blood clotting, wound healing, and phagocytosis. All of the other forms assemble on the surface of cells and are deposited in the extracellular matrix as highly insoluble fibronectin fibrils. In these cell-surface and matrix forms, fibronectin dimers are cross-linked to one another by additional disulfide bonds. Unlike fibrillar collagen molecules, which can be made to self-assemble into fibrils in a test tube, fibronectin molecules assemble into fibrils only on the surface of certain cells. This is because additional proteins are needed for fibril formation, especially fibronectin-binding integrins. In the case of fibroblasts, fibronectin fibrils are associated with integrins at sites called fibrillar adhesions. These are distinct from focal adhesions, in that they are more elongated and contain different intracellular anchor proteins. The fibronectin fibrils on the cell surface are highly stretched and under tension. The tension is exerted by the cell and is essential for fibril formation, as we discuss below. Some secreted proteins function to prevent fibronectin assembly in inappropriate places. Uteroglobin, for example, binds to fibronectin and prevents it from forming fibrils in the kidney. Mice that have a mutation in the uteroglobin gene accumulate insoluble fibronectin fibrils in their kidneys. The importance of fibronectin in animal development is dramatically demonstrated by gene inactivation experiments. Mutant mice that are unable to make fibronectin die early in embryogenesis because their endothelial cells fail to form proper blood vessels. This defect is thought to result from abnormalities in the interactions of these cells with the surrounding extracellular matrix, which normally contains fibronectin. The fibronectin fibrils that form on or near the surface of fibroblasts are usually aligned with adjacent intracellular actin stress fibers (Figure 19-54). In fact, intracellular actin filaments promote the assembly of secreted fibronectin molecules into fibrils and influence fibril orientation. If cells are treated with the drug cytochalasin, which disrupts actin filaments, the fibronectin fibrils dissociate from the cell surface (just as they do during mitosis when a cell rounds up). Coalignment of extracellular fibronectin fibrils and intracellular actin filament bundles. (A) The fibronectin is revealed in two rat fibroblasts in culture by the binding of rhodamine-coupled anti-fibronectin antibodies. (B) The actin is revealed by (more...) The interactions between extracellular fibronectin fibrils and intracellular actin filaments across the fibroblastplasma membrane are mediated mainly by integrin transmembrane adhesion proteins. The contractile actin and myosin cytoskeleton thereby pulls on the fibronectin matrix to generate tension. As a result, the fibronectin fibrils are stretched, exposing a cryptic (hidden) binding site in the fibronectin molecules that allows them to bind directly to one another. In addition, the stretching exposes more binding sites for integrins. In this way, the actin cytoskeleton promotes fibronectin polymerization and matrix assembly. Extracellular signals can regulate the assembly process by altering the actincytoskeleton and thereby the tension on the fibrils. Many other extracellular matrix proteins have multiple repeats similar to the type III fibronectin repeat, and it is possible that tension exerted on these proteins also uncovers cryptic binding sites and thereby influences their polymerization. Glycoproteins in the Matrix Help Guide Cell Migration Fibronectin is important not only for cell adhesion to the matrix but also for guiding cell migrations in vertebrate embryos. Large amounts of fibronectin, for example, are found along the pathway followed by migrating prospective mesodermal cells during amphibian gastrulation (discussed in Chapter 21). Although all cells of the early embryo can attach to fibronectin, only these migrating cells can spread and migrate on fibronectin. The migration is inhibited by an injection into the developing amphibian embryo of various ligands that disrupt the ability of the cells to bind to fibronectin. Many matrix proteins are believed to have a role in guiding cell movements during development. The tenascins and thrombospondins, for example, are composed of several types of short amino acid sequences that are repeated many times and form functionally distinct domains. They can either promote or inhibit cell adhesion, depending on the cell type. Indeed, anti-adhesive interactions are as important as adhesive ones in guiding cell migration, as we discuss in Chapter 21. As mentioned earlier, basal laminae are flexible, thin (40–120 nm thick) mats of specialized extracellular matrix that underlie all epithelial cell sheets and tubes. They also surround individual muscle cells, fat cells, and Schwann cells (which wrap around peripheral nerve cell axons to form myelin). The basal lamina thus separates these cells and epithelia from the underlying or surrounding connective tissue. In other locations, such as the kidney glomerulus, a basal lamina lies between two cell sheets and functions as a highly selective filter (Figure 19-55). Basal laminae have more than simple structural and filtering roles, however. They are able to determine cell polarity, influence cell metabolism, organize the proteins in adjacent plasma membranes, promote cell survival, proliferation, or differentiation, and serve as specific highways for cell migration. Three ways in which basal laminae are organized. Basal laminae (yellow) surround certain cells (such as skeletal muscle cells), underlie epithelia, and are interposed between two cell sheets (as in the kidney glomerulus). Note that, in the kidney glomerulus, (more...) The basal lamina is synthesized largely by the cells that rest on it (Figure 19-56). In some multilayered epithelia, such as the stratified squamous epithelium that forms the epidermis of the skin, the basal lamina is tethered to the underlying connective tissue by specialized anchoring fibrils made of type VII collagen molecules. The term basement membrane is often used to describe the composite of the basal lamina and this layer of collagen fibrils. In one type of skin disease, these connections are either absent or destroyed, and the epidermis and its basal lamina become detached from the underlying connective tissue, causing blistering. The basal lamina in the cornea of a chick embryo. In this scanning electron micrograph, some of the epithelial cells (E) have been removed to expose the upper surface of the matlike basal lamina (BL). A network of collagen fibrils (C) in the underlying (more...) Although its precise composition varies from tissue to tissue and even from region to region in the same lamina, most mature basal laminae contain type IV collagen, the large heparan sulfate proteoglycan perlecan, and the glycoproteins laminin and nidogen (also called entactin). Type IV collagens exist in several isoforms. They all have a more flexible structure than the fibrillar collagens; their triple-stranded helix is interrupted in 26 regions, allowing multiple bends. They are not cleaved after secretion, but interact via their uncleaved terminal domains to assemble extracellularly into a flexible, sheetlike, multilayered network. Early in development, basal laminae contain little or no type IV collagen and consist mainly of laminin molecules. Laminin-1 (classical laminin) is a large, flexible protein composed of three very long polypeptide chains (α, β, and γ) arranged in the shape of an asymmetric cross and held together by disulfide bonds (Figure 19-57). Several isoforms of each type of chain can associate in different combinations to form a large family of laminins. The laminin γ-1 chain is a component of most laminin heterotrimers, and mice lacking it die during embryogenesis because they are unable to make a basal lamina. Like many other proteins in the extracellular matrix, the laminin in basement membranes consists of several functional domains: one binds to perlecan, one to nidogen, and two or more to laminin receptor proteins on the surface of cells. The structure of laminin. (A) The subunits of a laminin-1 molecule. This multidomain glycoprotein is composed of three polypeptides (α, β, and γ) that are disulfide-bonded into an asymmetric crosslike structure. Each of the polypeptide (more...) Like type IV collagen, laminins can self-assemble in vitro into a feltlike sheet, largely through interactions between the ends of the laminin arms. As nidogen and perlecan can bind to both laminin and type IV collagen, it is thought that they connect the type IV collagen and laminin networks (Figure 19-58). In tissues, laminins and type IV collagen preferentially polymerize while bound to receptors on the surface of the cells producing the proteins. Many of the cell-surface receptors for type IV collagen and laminin are members of the integrin family. Another important type of laminin receptor is the transmembrane proteindystroglycan, which, together with integrins, may organize the assembly of the basal lamina. A model of the molecular structure of a basal lamina. (A) The basal lamina is formed by specific interactions (B) between the proteins type IV collagen, laminin, and nidogen, and the proteoglycan perlecan. Arrows in (B) connect molecules that can bind (more...) The comparative shapes and sizes of some of the major extracellular matrix macromolecules. Protein is shown in green, and glycosaminoglycan in red. Basal Laminae Perform Diverse Functions As we have mentioned, in the kidney glomerulus, an unusually thick basal lamina acts as a molecular filter, preventing the passage of macromolecules from the blood into the urine as urine is formed (see Figure 19-55). The heparan sulfate proteoglycan in the basal lamina seems to be important for this function: when its GAG chains are removed by specific enzymes, the filtering properties of the lamina are destroyed. Type IV collagen also has a role, as a human hereditary kidney disorder (Alport syndrome) results from mutations in type IV collagen α-chain genes. The basal lamina can also act as a selective barrier to the movement of cells. The lamina beneath an epithelium, for example, usually prevents fibroblasts in the underlying connective tissue from making contact with the epithelial cells. It does not, however, stop macrophages, lymphocytes, or nerve processes from passing through it. The basal lamina is also important in tissue regeneration after injury. When tissues such as muscles, nerves, and epithelia are damaged, the basal lamina survives and provides a scaffold along which regenerating cells can migrate. In this way, the original tissue architecture is readily reconstructed. In some cases, as in the skin or cornea, the basal lamina becomes chemically altered after injury—for example, by the addition of fibronectin, which promotes the cell migration required for wound healing. A particularly striking example of the instructive role of the basal lamina in regeneration comes from studies on the neuromuscular junction, the site where the nerve terminals of a motor neuron form a chemical synapse with a skeletal muscle cell (discussed in Chapter 11). The basal lamina that surrounds the muscle cell separates the nerve and muscle cell plasma membranes at the synapse, and the synaptic region of the lamina has a distinctive chemical character, with special isoforms of type IV collagen and laminin and a heparan sulfate proteoglycan called agrin. This basal lamina at the synapse has a central role in reconstructing the synapse after nerve or muscle injury. If a frog muscle and its motor nerve are destroyed, the basal lamina around each muscle cell remains intact and the sites of the old neuromuscular junctions are still recognizable. If the motor nerve, but not the muscle, is allowed to regenerate, the nerve axons seek out the original synaptic sites on the empty basal lamina and differentiate there to form normal-looking nerve terminals. Thus, the junctional basal lamina by itself can guide the regeneration of motor nerve terminals. Similar experiments show that the basal lamina also controls the localization of the acetylcholine receptors that cluster in the muscle cell plasma membrane at a neuromuscular junction. If the muscle and nerve are both destroyed, but now the muscle is allowed to regenerate while the nerve is prevented from doing so, the acetylcholine receptors synthesized by the regenerated muscle localize predominantly in the region of the old junctions, even though the nerve is absent (Figure 19-60). Thus, the junctional basal lamina apparently coordinates the local spatial organization of the components in each of the two cells that form a neuromuscular junction. Some of the matrix proteins have been identified. Motor neuron axons, for example, deposit agrin in the junctional basal lamina, where it triggers the assembly of acetylcholine receptors and other proteins in the junctional plasma membrane of the muscle cell. Conversely, muscle cells deposit a particular isoform of laminin in the junctional basal lamina. Both agrin and this isoform of laminin are essential for the formation of normal neuromuscular junctions. Regeneration experiments demonstrating the special character of the junctional basal lamina at a neuromuscular junction. When the nerve, but not the muscle, is allowed to regenerate after both the nerve and muscle have been damaged (upper part of figure), (more...) The extracellular matrix can influence the organization of a cell's cytoskeleton. This can be vividly demonstrated by using transformed (cancerlike) fibroblasts in culture (discussed in Chapter 23). Transformed cells often make less fibronectin than normal cultured cells and behave differently. They adhere poorly to the culture substratum, for example, and fail to flatten out or develop the organized intracellular bundles of actin filaments known as stress fibers. The decrease in fibronectin production and adhesion may contribute to the tendency of cancer cells to break away from the primary tumor and spread to other parts of the body. In some cases, fibronectin deficiency seems also to be at least partly responsible for this abnormal morphology of cancer cells: if the cells are grown on a matrix of organized fibronectin fibrils, they flatten out and assemble intracellular stress fibers that are aligned with the extracellular fibronectin fibrils. This interaction between the extracellular matrix and the cytoskeleton is reciprocal in that intracellular actin filaments can promote the assembly and influence the orientation of fibronectin fibrils, as described earlier. Since the cytoskeleton can exert forces that orient the matrix macromolecules the cell secretes and the matrix macromolecules can in turn organize the cytoskeleton of the cells they contact, the extracellular matrix can in principle propagate order from cell to cell (Figure 19-61), creating large-scale oriented structures, as described earlier (see Figure 19-50). The integrins serve as the main adaptors in this ordering process, mediating the interactions between cells and the matrix around them. How the extracellular matrix could, in principle, propagate order from cell to cell within a tissue. For simplicity, the figure represents a hypothetical scheme in which one cell influences the orientation of its neighboring cells. It is more likely, (more...) Most cells need to attach to the extracellular matrix to grow and proliferate—and, in many cases, even to survive. This dependence of cell growth, proliferation, and survival on attachment to a substratum is known as anchorage dependence, and it is mediated mainly by integrins and the intracellular signals they generate. The physical spreading of a cell on the matrix also has a strong influence on intracellular events. Cells that are forced to spread over a large surface area survive better and proliferate faster than cells that are not so spread out, even if in both cases the cells have the same area making contact with the matrix directly (Figure 19-62). This stimulatory effect of cell spreading presumably helps tissues to regenerate after injury. If cells are lost from an epithelium, for example, the spreading of the remaining cells into the vacated space will stimulate them to proliferate until they fill the gap. It is still uncertain, however, how a cell senses its extent of spreading so as to adjust its behavior accordingly. Anchorage dependence and the importance of cell spreading. For many cells, contact with the extracellular matrix is essential for survival, growth, and proliferation. In this experiment, the extent of cell spreading on a substratum, rather than the number (more...) The Controlled Degradation of Matrix Components Helps Cells Migrate The regulated turnover of extracellular matrix macromolecules is crucial to a variety of important biological processes. Rapid degradation occurs, for example, when the uterus involutes after childbirth, or when the tadpole tail is resorbed during metamorphosis (see Figure 17-36). A more localized degradation of matrix components is required when cells migrate through a basal lamina. This occurs when white blood cells migrate across the basal lamina of a blood vessel into tissues in response to infection or injury, and when cancer cells migrate from their site of origin to distant organs via the bloodstream or lymphatic vessels—the process known as metastasis. Even in the seemingly static extracellular matrix of adult animals, there is a slow, continuous turnover, with matrix macromolecules being degraded and resynthesized. In each of these cases, matrix components are degraded by extracellular proteolytic enzymes (proteases) that are secreted locally by cells. Thus, antibodies that recognize the products of proteolytic cleavage stain matrix only around cells. Many of these proteases belong to one of two general classes. Most are matrix metalloproteases, which depend on bound Ca2+ or Zn2+ for activity; the others are serine proteases, which have a highly reactive serine in their active site. Together, metalloproteases and serine proteases cooperate to degrade matrix proteins such as collagen, laminin, and fibronectin. Some metalloproteases, such as the collagenases, are highly specific, cleaving particular proteins at a small number of sites. In this way, the structural integrity of the matrix is largely retained, but cell migration can be greatly facilitated by the small amount of proteolysis. Other metalloproteases may be less specific, but, because they are anchored to the plasma membrane, they can act just where they are needed. The importance of proteolysis in cell migration can be shown by using protease inhibitors, which often block migration. Moreover, cells that migrate readily on type I collagen in culture can no longer do so if the collagen is made resistant to proteolysis by mutating the collagenase-sensitive cleavage sites. The proteolysis of matrix proteins can contribute to cell migration in several ways: (1) it can simply clear a path through the matrix; (2) it can expose cryptic sites on the cleaved proteins that promote cell binding, cell migration, or both; (3) it can promote cell detachment so that a cell can move onward, or (4) it can release extracellular signal proteins that stimulate cell migration. Three basic mechanisms operate to ensure that the proteases that degrade the matrix components are tightly controlled. Local activation: Many proteases are secreted as inactive precursors that can be activated locally when needed. An example is plasminogen, an inactive protease precursor that is abundant in the blood. It is cleaved locally by other proteases called plasminogen activators to yield the active serine proteaseplasmin, which helps break up blood clots. Tissue-type plasminogen activator (tPA) is often given to patients who have just had a heart attack or thrombotic stroke; it helps dissolve the arterial clot that caused the attack, thereby restoring bloodflow to the tissue. Confinement by cell-surface receptors: Many cells have receptors on their surface that bind proteases, thereby confining the enzyme to the sites where it is needed. A second type of plasminogen activator called urokinase-type plasminogen activator (uPA) is an example. It is found bound to receptors on the growing tips of axons and at the leading edge of some migrating cells, where it may serve to clear a pathway for their migration. Receptor-bound uPA may also help some cancer cells metastasize (Figure 19-63). The importance of proteases bound to cell-surface receptors. (A) Human prostate cancer cells make and secrete the serine protease uPA, which binds to cell-surface uPA receptor proteins. (B) The same cells have been transfected with DNA that encodes an (more...) Secretion of inhibitors: The action of proteases is confined to specific areas by various secreted protease inhibitors, including the tissue inhibitors of metalloproteases (TIMPs) and the serine protease inhibitors known as serpins. These inhibitors are protease-specific and bind tightly to the activated enzyme, blocking its activity. An attractive idea is that the inhibitors are secreted by cells at the margins of areas of active protein degradation in order to protect uninvolved matrix; they may also protect cell-surface proteins required for cell adhesion and migration. The overexpression of TIMPs inhibits the migration of some cell types, indicating the importance of metalloproteases for the migration. Summary Cells in connective tissues are embedded in an intricate extracellular matrix that not only binds the cells together but also influences their survival, development, shape, polarity, and behavior. The matrix contains various protein fibers interwoven in a hydrated gel composed of a network of glycosaminoglycan (GAG) chains. GAGs are a heterogeneous group of negatively charged polysaccharide chains that (except for hyaluronan) are covalently linked to protein to form proteoglycan molecules. They occupy a large volume and form hydrated gels in the extracellular space. Proteoglycans are also found on the surface of cells, where they function as co-receptors to help cells respond to secreted signal proteins. Fiber-forming proteins strengthen the matrix and give it form. They also provide surfaces for cells to adhere to. Elastin molecules form an extensive cross-linked network of fibers and sheets that can stretch and recoil, imparting elasticity to the matrix. The fibrillar collagens (types I, II, III, V, and XI) are ropelike, triple-stranded helical molecules that aggregate into long fibrils in the extracellular space. The fibrils in turn can assemble into a variety of highly ordered arrays. Fibril-associated collagen molecules, such as types IX and XII, decorate the surface of collagen fibrils and influence the interactions of the fibrils with one another and with other matrix components. In contrast, type IV collagen molecules assemble into a sheetlike meshwork that is a crucial component of all mature basal laminae. All basal laminae are based on a mesh of laminin molecules. The collagen and laminin networks in mature basal laminae are bridged by the protein nidogen and the large heparan sulfate proteoglycan perlecan. Fibronectin and laminin are examples of large, multidomain matrix glycoproteins. By means of their multiple binding domains, such proteins help organize the matrix and help cells adhere to it. Matrix proteins such as collagens, laminins, and fibronectin are assembled into fibrils or networks on the surface of the cells that produce them by a process that depends on the underlying actin cortex. The organization of the matrix can reciprocally influence the organization of the cell's cytoskeleton and can mechanically influence cell spreading. The matrix also influences cell behavior by binding to cell-surface receptors that activate intracellular signaling pathways. Matrix components are degraded by extracellular proteolytic enzymes. Most of these are matrix metalloproteases, which depend on bound Ca2+ or Zn2+ for activity, while others are serine proteases, which have a reactive serine in their active site. Various mechanisms operate to ensure that the degradation of matrix components is tightly controlled. Cells can, for example, cause a localized degradation of matrix components to clear a path through the matrix. By agreement with the publisher, this book is accessible by the search feature, but cannot be browsed."}
{"url": "https://en.m.wikipedia.org/wiki/Human_evolution", "text": "The evolutionary history of primates can be traced back 65 million years.[11][12][13][14][15] One of the oldest known primate-like mammal species, the Plesiadapis, came from North America;[16][17][18][19][20][21] another, Archicebus, came from China.[22] Other similar basal primates were widespread in Eurasia and Africa during the tropical conditions of the Paleocene and Eocene. David R. Begun[23] concluded that early primates flourished in Eurasia and that a lineage leading to the African apes and humans, including to Dryopithecus, migrated south from Europe or Western Asia into Africa. The surviving tropical population of primates—which is seen most completely in the Upper Eocene and lowermost Oligocene fossil beds of the Faiyum depression southwest of Cairo—gave rise to all extant primate species, including the lemurs of Madagascar, lorises of Southeast Asia, galagos or \"bush babies\" of Africa, and to the anthropoids, which are the Platyrrhines or New World monkeys, the Catarrhines or Old World monkeys, and the great apes, including humans and other hominids. The earliest known catarrhine is Kamoyapithecus from the uppermost Oligocene at Eragaleit in the northern Great Rift Valley in Kenya, dated to 24 million years ago.[24] Its ancestry is thought to be species related to Aegyptopithecus, Propliopithecus, and Parapithecus from the Faiyum, at around 35 mya.[25] In 2010, Saadanius was described as a close relative of the last common ancestor of the crown catarrhines, and tentatively dated to 29–28 mya, helping to fill an 11-million-year gap in the fossil record.[26] The presence of other generalized non-cercopithecids of Middle Miocene from sites far distant, such as Otavipithecus from cave deposits in Namibia, and Pierolapithecus and Dryopithecus from France, Spain and Austria, is evidence of a wide diversity of forms across Africa and the Mediterranean basin during the relatively warm and equable climatic regimes of the Early and Middle Miocene. The youngest of the Miocene hominoids, Oreopithecus, is from coal beds in Italy that have been dated to 9 million years ago. In 2023, an analysis of Anadoluvius turkae from 8.7 million years ago led the researchers of the study to conclude that hominids originated in Europe,[27] with the Eastern Mediterranean hominids originating from earlier central and western European hominids.[28] The authors state that the \"oldest known hominines are European\", and also stating that the \"more likely and more parsimonious interpretation is that hominines evolved over a lengthy period in Europe and dispersed into Africa before 7 Ma\".[29] Molecular evidence indicates that the lineage of gibbons diverged from the line of great apes some 18–12 mya, and that of orangutans (subfamily Ponginae)[b] diverged from the other great apes at about 12 million years; there are no fossils that clearly document the ancestry of gibbons, which may have originated in a so-far-unknown Southeast Asian hominoid population, but fossil proto-orangutans may be represented by Sivapithecus from India and Griphopithecus from Turkey, dated to around 10 mya.[30] Hominidae subfamily Homininae (African hominids) diverged from Ponginae (orangutans) about 14 mya. Hominins (including humans and the Australopithecine and Panina subtribes) parted from the Gorillini tribe (gorillas) between 8 and 9 mya; Australopithecine (including the extinct biped ancestors of humans) separated from the Pan genus (containing chimpanzees and bonobos) 4–7 mya.[10] The Homo genus is evidenced by the appearance of H. habilis over 2 mya,[a] while anatomically modern humans emerged in Africa approximately 300,000 years ago. Species close to the last common ancestor of gorillas, chimpanzees and humans may be represented by Nakalipithecus fossils found in Kenya and Ouranopithecus found in Greece. Molecular evidence suggests that between 8 and 4 million years ago, first the gorillas, and then the chimpanzees (genus Pan) split off from the line leading to the humans. Human DNA is approximately 98.4% identical to that of chimpanzees when comparing single nucleotide polymorphisms (see human evolutionary genetics). The fossil record, however, of gorillas and chimpanzees is limited; both poor preservation– rain forest soils tend to be acidic and dissolve bone– and sampling bias probably contribute to this problem. Other hominins probably adapted to the drier environments outside the equatorial belt; and there they encountered antelope, hyenas, dogs, pigs, elephants, horses, and others. The equatorial belt contracted after about 8 million years ago, and there is very little fossil evidence for the split—thought to have occurred around that time—of the hominin lineage from the lineages of gorillas and chimpanzees. The earliest fossils argued by some to belong to the human lineage are Sahelanthropus tchadensis (7 Ma) and Orrorin tugenensis (6 Ma), followed by Ardipithecus (5.5–4.4 Ma), with species Ar. kadabba and Ar. ramidus. It has been argued in a study of the life history of Ar. ramidus that the species provides evidence for a suite of anatomical and behavioral adaptations in very early hominins unlike any species of extant great ape.[31] This study demonstrated affinities between the skull morphology of Ar. ramidus and that of infant and juvenile chimpanzees, suggesting the species evolved a juvenalised or paedomorphic craniofacial morphology via heterochronic dissociation of growth trajectories. It was also argued that the species provides support for the notion that very early hominins, akin to bonobos (Pan paniscus) the less aggressive species of the genus Pan, may have evolved via the process of self-domestication. Consequently, arguing against the so-called \"chimpanzee referential model\"[32] the authors suggest it is no longer tenable to use chimpanzee (Pan troglodytes) social and mating behaviors in models of early hominin social evolution. When commenting on the absence of aggressive canine morphology in Ar. ramidus and the implications this has for the evolution of hominin social psychology, they wrote: Of course Ar. ramidus differs significantly from bonobos, bonobos having retained a functional canine honing complex. However, the fact that Ar. ramidus shares with bonobos reduced sexual dimorphism, and a more paedomorphic form relative to chimpanzees, suggests that the developmental and social adaptations evident in bonobos may be of assistance in future reconstructions of early hominin social and sexual psychology. In fact the trend towards increased maternal care, female mate selection and self-domestication may have been stronger and more refined in Ar. ramidus than what we see in bonobos.[31]: 128 The authors argue that many of the basic human adaptations evolved in the ancient forest and woodland ecosystems of late Miocene and early Pliocene Africa. Consequently, they argue that humans may not represent evolution from a chimpanzee-like ancestor as has traditionally been supposed. This suggests many modern human adaptations represent phylogenetically deep traits and that the behavior and morphology of chimpanzees may have evolved subsequent to the split with the common ancestor they share with humans. The genus Australopithecus evolved in eastern Africa around 4 million years ago before spreading throughout the continent and eventually becoming extinct 2 million years ago. During this time period various forms of australopiths existed, including Australopithecus anamensis, Au. afarensis, Au. sediba, and Au. africanus. There is still some debate among academics whether certain African hominid species of this time, such as Au. robustus and Au. boisei, constitute members of the same genus; if so, they would be considered to be Au. robust australopiths whilst the others would be considered Au. gracile australopiths. However, if these species do indeed constitute their own genus, then they may be given their own name, Paranthropus. A new proposed species Australopithecus deyiremeda is claimed to have been discovered living at the same time period of Au. afarensis. There is debate if Au.deyiremeda is a new species or is Au. afarensis.[33]Australopithecus prometheus, otherwise known as Little Foot has recently been dated at 3.67 million years old through a new dating technique, making the genus Australopithecus as old as afarensis.[34] Given the opposable big toe found on Little Foot, it seems that the specimen was a good climber. It is thought given the night predators of the region that he built a nesting platform at night in the trees in a similar fashion to chimpanzees and gorillas. The earliest documented representative of the genus Homo is Homo habilis, which evolved around 2.8 million years ago,[35] and is arguably the earliest species for which there is positive evidence of the use of stone tools. The brains of these early hominins were about the same size as that of a chimpanzee, although it has been suggested that this was the time in which the human SRGAP 2 gene doubled, producing a more rapid wiring of the frontal cortex. During the next million years a process of rapid encephalization occurred, and with the arrival of Homo erectus and Homo ergaster in the fossil record, cranial capacity had doubled to 850 cm3.[36] (Such an increase in human brain size is equivalent to each generation having 125,000 more neurons than their parents.) It is believed that H. erectus and H. ergaster were the first to use fire and complex tools, and were the first of the hominin line to leave Africa, spreading throughout Africa, Asia, and Europe between 1.3 to 1.8 million years ago. A model of the phylogeny of H. sapiens during the Middle Paleolithic. The horizontal axis represents geographic location; the vertical axis represents time in millions of years ago.[50]Homo heidelbergensis is shown as diverging into Neanderthals, Denisovans and H. sapiens. With the expansion of H. sapiens after 200 kya, Neanderthals, Denisovans and unspecified archaic African hominins are shown as again subsumed into the H. sapiens lineage. In addition, admixture events in modern African populations are indicated. Homo sapiens is the only extant species of its genus, Homo. While some (extinct) Homo species might have been ancestors of Homo sapiens, many, perhaps most, were likely \"cousins\", having speciated away from the ancestral hominin line.[51][52] There is yet no consensus as to which of these groups should be considered a separate species and which should be a subspecies; this may be due to the dearth of fossils or to the slight differences used to classify species in the genus Homo.[52] The Sahara pump theory (describing an occasionally passable \"wet\" Sahara desert) provides one possible explanation of the early variation in the genus Homo. Based on archaeological and paleontological evidence, it has been possible to infer, to some extent, the ancient dietary practices[53] of various Homo species and to study the role of diet in physical and behavioral evolution within Homo.[54][55][56][57][58] Some anthropologists and archaeologists subscribe to the Toba catastrophe theory, which posits that the supereruption of Lake Toba on Sumatran island in Indonesia some 70,000 years ago caused global consequences,[59] killing the majority of humans and creating a population bottleneck that affected the genetic inheritance of all humans today.[60] The genetic and archaeological evidence for this remains in question however.[61] Nonetheless, on 31 August 2023, researchers reported, based on genetic studies, that a human ancestor population bottleneck (from a possible 100,000 to 1000 individuals) occurred \"around 930,000 and 813,000 years ago ... lasted for about 117,000 years and brought human ancestors close to extinction.\"[62][63] Homo habilis lived from about 2.8[35] to 1.4 Ma. The species evolved in South and East Africa in the Late Pliocene or Early Pleistocene, 2.5–2 Ma, when it diverged from the australopithecines with the development of smaller molars and larger brains. One of the first known hominins, it made tools from stone and perhaps animal bones, leading to its name homohabilis (Latin 'handy man') bestowed by discoverer Louis Leakey. Some scientists have proposed moving this species from Homo into Australopithecus due to the morphology of its skeleton being more adapted to living in trees rather than walking on two legs like later hominins.[64] The first fossils of Homo erectus were discovered by Dutch physician Eugene Dubois in 1891 on the Indonesian island of Java. He originally named the material Anthropopithecus erectus (1892–1893, considered at this point as a chimpanzee-like fossil primate) and Pithecanthropus erectus (1893–1894, changing his mind as of based on its morphology, which he considered to be intermediate between that of humans and apes).[69] Years later, in the 20th century, the German physician and paleoanthropologistFranz Weidenreich (1873–1948) compared in detail the characters of Dubois' Java Man, then named Pithecanthropus erectus, with the characters of the Peking Man, then named Sinanthropus pekinensis. Weidenreich concluded in 1940 that because of their anatomical similarity with modern humans it was necessary to gather all these specimens of Java and China in a single species of the genus Homo, the species H. erectus.[70][71] Homo erectus lived from about 1.8 Ma to about 70,000 years ago – which would indicate that they were probably wiped out by the Toba catastrophe; however, nearby H. floresiensis survived it. The early phase of H. erectus, from 1.8 to 1.25 Ma, is considered by some to be a separate species, H. ergaster, or as H. erectus ergaster, a subspecies of H. erectus. Many paleoanthropologists now use the term Homo ergaster for the non-Asian forms of this group, and reserve H. erectus only for those fossils that are found in Asia and meet certain skeletal and dental requirements which differ slightly from H. ergaster. In Africa in the Early Pleistocene, 1.5–1 Ma, some populations of Homo habilis are thought to have evolved larger brains and to have made more elaborate stone tools; these differences and others are sufficient for anthropologists to classify them as a new species, Homo erectus—in Africa.[72] The evolution of locking knees and the movement of the foramen magnum are thought to be likely drivers of the larger population changes. This species also may have used fire to cook meat. Richard Wrangham notes that Homo seems to have been ground dwelling, with reduced intestinal length, smaller dentition, and \"brains [swollen] to their current, horrendously fuel-inefficient size\",[73] and hypothesizes that control of fire and cooking, which released increased nutritional value, was the key adaptation that separated Homo from tree-sleeping Australopithecines.[74] H. rhodesiensis, estimated to be 300,000–125,000 years old. Most current researchers place Rhodesian Man within the group of Homo heidelbergensis, though other designations such as archaic Homo sapiens and Homo sapiens rhodesiensis have been proposed. In February 2006 a fossil, the Gawis cranium, was found which might possibly be a species intermediate between H. erectus and H. sapiens or one of many evolutionary dead ends. The skull from Gawis, Ethiopia, is believed to be 500,000–250,000 years old. Only summary details are known, and the finders have not yet released a peer-reviewed study. Gawis man's facial features suggest that it is either an intermediate species or an example of a \"Bodo man\" female.[79] Homo neanderthalensis, alternatively designated as Homo sapiens neanderthalensis,[80] lived in Europe and Asia from 400,000[81] to about 28,000 years ago.[82] There are a number of clear anatomical differences between anatomically modern humans (AMH) and Neanderthal specimens, many relating to the superior Neanderthal adaptation to cold environments. Neanderthal surface to volume ratio was even lower than that among modern Inuit populations, indicating superior retention of body heat. Neanderthals also had significantly larger brains, as shown from brain endocasts, casting doubt on their intellectual inferiority to modern humans. However, the higher body mass of Neanderthals may have required larger brain mass for body control.[83] Also, recent research by Pearce, Stringer, and Dunbar has shown important differences in brain architecture. The larger size of the Neanderthal orbital chamber and occipital lobe suggests that they had a better visual acuity than modern humans, useful in the dimmer light of glacial Europe. Neanderthals may have had less brain capacity available for social functions. Inferring social group size from endocranial volume (minus occipital lobe size) suggests that Neanderthal groups may have been limited to 120 individuals, compared to 144[citation needed] possible relationships for modern humans. Larger social groups could imply that modern humans had less risk of inbreeding within their clan, trade over larger areas (confirmed in the distribution of stone tools), and faster spread of social and technological innovations. All these may have all contributed to modern Homo sapiens replacing Neanderthal populations by 28,000 BP.[83] Earlier evidence from sequencing mitochondrial DNA suggested that no significant gene flow occurred between H. neanderthalensis and H. sapiens, and that the two were separate species that shared a common ancestor about 660,000 years ago.[84][85][86] However, a sequencing of the Neanderthal genome in 2010 indicated that Neanderthals did indeed interbreed with anatomically modern humans c. 45,000-80,000 years ago, around the time modern humans migrated out from Africa, but before they dispersed throughout Europe, Asia and elsewhere.[87] The genetic sequencing of a 40,000-year-old human skeleton from Romania showed that 11% of its genome was Neanderthal, implying the individual had a Neanderthal ancestor 4–6 generations previously,[88] in addition to a contribution from earlier interbreeding in the Middle East. Though this interbred Romanian population seems not to have been ancestral to modern humans, the finding indicates that interbreeding happened repeatedly.[89] All modern non-African humans have about 1% to 4% (or 1.5% to 2.6% by more recent data) of their DNA derived from Neanderthals.[90][87][91] This finding is consistent with recent studies indicating that the divergence of some human alleles dates to one Ma, although this interpretation has been questioned.[92][93] Neanderthals and AMH Homo sapiens could have co-existed in Europe for as long as 10,000 years, during which AMH populations exploded, vastly outnumbering Neanderthals, possibly outcompeting them by sheer numbers.[94] In 2008, archaeologists working at the site of Denisova Cave in the Altai Mountains of Siberia uncovered a small bone fragment from the fifth finger of a juvenile member of another human species, the Denisovans.[95] Artifacts, including a bracelet, excavated in the cave at the same level were carbon dated to around 40,000 BP. As DNA had survived in the fossil fragment due to the cool climate of the Denisova Cave, both mtDNA and nuclear DNA were sequenced.[45][96] While the divergence point of the mtDNA was unexpectedly deep in time,[97] the full genomic sequence suggested the Denisovans belonged to the same lineage as Neanderthals, with the two diverging shortly after their line split from the lineage that gave rise to modern humans.[45] Modern humans are known to have overlapped with Neanderthals in Europe and the Near East for possibly more than 40,000 years,[98] and the discovery raises the possibility that Neanderthals, Denisovans, and modern humans may have co-existed and interbred. The existence of this distant branch creates a much more complex picture of humankind during the Late Pleistocene than previously thought.[96][99] Evidence has also been found that as much as 6% of the DNA of some modern Melanesians derive from Denisovans, indicating limited interbreeding in Southeast Asia.[100][101] Alleles thought to have originated in Neanderthals and Denisovans have been identified at several genetic loci in the genomes of modern humans outside Africa. HLA haplotypes from Denisovans and Neanderthal represent more than half the HLA alleles of modern Eurasians,[47] indicating strong positive selection for these introgressed alleles. Corinne Simoneti at Vanderbilt University, in Nashville and her team have found from medical records of 28,000 people of European descent that the presence of Neanderthal DNA segments may be associated with a higher rate of depression.[102] The flow of genes from Neanderthal populations to modern humans was not all one way. Sergi Castellano of the Max Planck Institute for Evolutionary Anthropology reported in 2016 that while Denisovan and Neanderthal genomes are more related to each other than they are to us, Siberian Neanderthal genomes show more similarity to modern human genes than do European Neanderthal populations. This suggests Neanderthal populations interbred with modern humans around 100,000 years ago, probably somewhere in the Near East.[103] Studies of a Neanderthal child at Gibraltar show from brain development and tooth eruption that Neanderthal children may have matured more rapidly than Homo sapiens.[104] H. floresiensis, which lived from approximately 190,000 to 50,000 years before present (BP), has been nicknamed the hobbit for its small size, possibly a result of insular dwarfism.[105]H. floresiensis is intriguing both for its size and its age, being an example of a recent species of the genus Homo that exhibits derived traits not shared with modern humans. In other words, H. floresiensis shares a common ancestor with modern humans, but split from the modern human lineage and followed a distinct evolutionary path. The main find was a skeleton believed to be a woman of about 30 years of age. Found in 2003, it has been dated to approximately 18,000 years old. The living woman was estimated to be one meter in height, with a brain volume of just 380 cm3 (considered small for a chimpanzee and less than a third of the H. sapiens average of 1400 cm3).[105] However, there is an ongoing debate over whether H. floresiensis is indeed a separate species.[106] Some scientists hold that H. floresiensis was a modern H. sapiens with pathological dwarfism.[107] This hypothesis is supported in part, because some modern humans who live on Flores, the Indonesian island where the skeleton was found, are pygmies. This, coupled with pathological dwarfism, could have resulted in a significantly diminutive human. The other major attack on H. floresiensis as a separate species is that it was found with tools only associated with H. sapiens.[107] The hypothesis of pathological dwarfism, however, fails to explain additional anatomical features that are unlike those of modern humans (diseased or not) but much like those of ancient members of our genus. Aside from cranial features, these features include the form of bones in the wrist, forearm, shoulder, knees, and feet. Additionally, this hypothesis fails to explain the find of multiple examples of individuals with these same characteristics, indicating they were common to a large population, and not limited to one individual.[106] In 2016, fossil teeth and a partial jaw from hominins assumed to be ancestral to H. floresiensis were discovered[108] at Mata Menge, about 74 km (46 mi) from Liang Bua. They date to about 700,000 years ago[109] and are noted by Australian archaeologist Gerrit van den Bergh for being even smaller than the later fossils.[110] A small number of specimens from the island of Luzon, dated 50,000 to 67,000 years ago, have recently been assigned by their discoverers, based on dental characteristics, to a novel human species, H. luzonensis.[111] H. sapiens (the adjective sapiens is Latin for \"wise\" or \"intelligent\") emerged in Africa around 300,000 years ago, likely derived from H. heidelbergensis or a related lineage.[112][113] In September 2019, scientists reported the computerized determination, based on 260 CT scans, of a virtual skull shape of the last common human ancestor to modern humans/H. sapiens, representative of the earliest modern humans, and suggested that modern humans arose between 260,000 and 350,000 years ago through a merging of populations in East and South Africa.[114][115] Between 400,000 years ago and the second interglacial period in the Middle Pleistocene, around 250,000 years ago, the trend in intra-cranial volume expansion and the elaboration of stone tool technologies developed, providing evidence for a transition from H. erectus to H. sapiens. The direct evidence suggests there was a migration of H. erectusout of Africa, then a further speciation of H. sapiens from H. erectus in Africa. A subsequent migration (both within and out of Africa) eventually replaced the earlier dispersed H. erectus. This migration and origin theory is usually referred to as the \"recent single-origin hypothesis\" or \"out of Africa\" theory. H. sapiensinterbred with archaic humans both in Africa and in Eurasia, in Eurasia notably with Neanderthals and Denisovans.[45][100] Bipedalism is the basic adaptation of the hominid and is considered the main cause behind a suite of skeletal changes shared by all bipedal hominids. The earliest hominin, of presumably primitive bipedalism, is considered to be either Sahelanthropus[121] or Orrorin, both of which arose some 6 to 7 million years ago. The non-bipedal knuckle-walkers, the gorillas and chimpanzees, diverged from the hominin line over a period covering the same time, so either Sahelanthropus or Orrorin may be our last shared ancestor. Ardipithecus, a full biped, arose approximately 5.6 million years ago.[122] The early bipeds eventually evolved into the australopithecines and still later into the genus Homo. There are several theories of the adaptation value of bipedalism. It is possible that bipedalism was favored because it freed the hands for reaching and carrying food, saved energy during locomotion,[123] enabled long-distance running and hunting, provided an enhanced field of vision, and helped avoid hyperthermia by reducing the surface area exposed to direct sun; features all advantageous for thriving in the new savanna and woodland environment created as a result of the East African Rift Valley uplift versus the previous closed forest habitat.[123][124][125] A 2007 study provides support for the hypothesis that walking on two legs, or bipedalism, evolved because it used less energy than quadrupedal knuckle-walking.[126][127] However, recent studies suggest that bipedality without the ability to use fire would not have allowed global dispersal.[128] This change in gait saw a lengthening of the legs proportionately when compared to the length of the arms, which were shortened through the removal of the need for brachiation. Another change is the shape of the big toe. Recent studies suggest that australopithecines still lived part of the time in trees as a result of maintaining a grasping big toe. This was progressively lost in habilines. Anatomically, the evolution of bipedalism has been accompanied by a large number of skeletal changes, not just to the legs and pelvis, but also to the vertebral column, feet and ankles, and skull.[129] The femur evolved into a slightly more angular position to move the center of gravity toward the geometric center of the body. The knee and ankle joints became increasingly robust to better support increased weight. To support the increased weight on each vertebra in the upright position, the human vertebral column became S-shaped and the lumbar vertebrae became shorter and wider. In the feet the big toe moved into alignment with the other toes to help in forward locomotion. The arms and forearms shortened relative to the legs making it easier to run. The foramen magnum migrated under the skull and more anterior.[130] The most significant changes occurred in the pelvic region, where the long downward facing iliac blade was shortened and widened as a requirement for keeping the center of gravity stable while walking;[30] bipedal hominids have a shorter but broader, bowl-like pelvis due to this. A drawback is that the birth canal of bipedal apes is smaller than in knuckle-walking apes, though there has been a widening of it in comparison to that of australopithecine and modern humans, thus permitting the passage of newborns due to the increase in cranial size. This is limited to the upper portion, since further increase can hinder normal bipedal movement.[131] The shortening of the pelvis and smaller birth canal evolved as a requirement for bipedalism and had significant effects on the process of human birth, which is much more difficult in modern humans than in other primates. During human birth, because of the variation in size of the pelvic region, the fetal head must be in a transverse position (compared to the mother) during entry into the birth canal and rotate about 90 degrees upon exit.[132] The smaller birth canal became a limiting factor to brain size increases in early humans and prompted a shorter gestation period leading to the relative immaturity of human offspring, who are unable to walk much before 12 months and have greater neoteny, compared to other primates, who are mobile at a much earlier age.[125] The increased brain growth after birth and the increased dependency of children on mothers had a major effect upon the female reproductive cycle,[133] and the more frequent appearance of alloparenting in humans when compared with other hominids.[134] Delayed human sexual maturity also led to the evolution of menopause with one explanation, the grandmother hypothesis, providing that elderly women could better pass on their genes by taking care of their daughter's offspring, as compared to having more children of their own.[135][136] The human species eventually developed a much larger brain than that of other primates—typically 1,330 cm3 (81 cu in) in modern humans, nearly three times the size of a chimpanzee or gorilla brain.[139] After a period of stasis with Australopithecus anamensis and Ardipithecus, species which had smaller brains as a result of their bipedal locomotion,[140] the pattern of encephalization started with Homo habilis, whose 600 cm3 (37 cu in) brain was slightly larger than that of chimpanzees. This evolution continued in Homo erectus with 800–1,100 cm3 (49–67 cu in), and reached a maximum in Neanderthals with 1,200–1,900 cm3 (73–116 cu in), larger even than modern Homo sapiens. This brain increase manifested during postnatal brain growth, far exceeding that of other apes (heterochrony). It also allowed for extended periods of social learning and language acquisition in juvenile humans, beginning as much as 2 million years ago. Encephalization may be due to a dependency on calorie-dense, difficult-to-acquire food.[141] Furthermore, the changes in the structure of human brains may be even more significant than the increase in size.[142][143][144][54] Fossilized skulls shows the brain size in early humans fell within the range of modern humans 300,000 years ago, but only got its present-day brain shape between 100,000 and 35,000 years ago.[145] The size and shape of the skull changed over time. The leftmost, and largest, is a replica of a modern human skull. The temporal lobes, which contain centers for language processing, have increased disproportionately, as has the prefrontal cortex, which has been related to complex decision-making and moderating social behavior.[139] Encephalization has been tied to increased starches[53] and meat[146][147] in the diet, however a 2022 meta study called into question the role of meat.[148] Other factors are the development of cooking,[149] and it has been proposed that intelligence increased as a response to an increased necessity for solving social problems as human society became more complex.[150] Changes in skull morphology, such as smaller mandibles and mandible muscle attachments, allowed more room for the brain to grow.[151] The increase in volume of the neocortex also included a rapid increase in size of the cerebellum. Its function has traditionally been associated with balance and fine motor control, but more recently with speech and cognition. The great apes, including hominids, had a more pronounced cerebellum relative to the neocortex than other primates. It has been suggested that because of its function of sensory-motor control and learning complex muscular actions, the cerebellum may have underpinned human technological adaptations, including the preconditions of speech.[152][153][154][155] The immediate survival advantage of encephalization is difficult to discern, as the major brain changes from Homo erectus to Homo heidelbergensis were not accompanied by major changes in technology. It has been suggested that the changes were mainly social and behavioural, including increased empathic abilities,[156][157] increases in size of social groups,[150][158][159] and increased behavioral plasticity.[160] Humans are unique in the ability to acquire information through social transmission and adapt that information.[161] The emerging field of cultural evolution studies human sociocultural change from an evolutionary perspective.[162] The reduced degree of sexual dimorphism in humans is visible primarily in the reduction of the male canine tooth relative to other ape species (except gibbons) and reduced brow ridges and general robustness of males. Another important physiological change related to sexuality in humans was the evolution of hidden estrus. Humans are the only hominoids in which the female is fertile year round and in which no special signals of fertility are produced by the body (such as genital swelling or overt changes in proceptivity during estrus).[175] Nonetheless, humans retain a degree of sexual dimorphism in the distribution of body hair and subcutaneous fat, and in the overall size, males being around 15% larger than females.[176] These changes taken together have been interpreted as a result of an increased emphasis on pair bonding as a possible solution to the requirement for increased parental investment due to the prolonged infancy of offspring.[177] A number of other changes have also characterized the evolution of humans, among them an increased reliance on vision rather than smell (highly reduced olfactory bulb); a longer juvenile developmental period and higher infant dependency;[181] a smaller gut and small, misaligned teeth; faster basal metabolism;[182] loss of body hair;[183] an increase in eccrine sweat gland density that is ten times higher than any other catarrhinian primates,[184] yet humans use 30% to 50% less water per day compared to chimps and gorillas;[185] more REM sleep but less sleep in total;[186] a change in the shape of the dental arcade from u-shaped to parabolic; development of a chin (found in Homo sapiens alone); styloid processes; and a descended larynx. As the human hand and arms adapted to the making of tools and were used less for climbing, the shoulder blades changed too. As a side effect, it allowed human ancestors to throw objects with greater force, speed and accuracy.[187] \"A sharp rock\", an Oldowan pebble tool, the most basic of human stone toolsThe harnessing of fire was a pivotal milestone in human history.Acheulean hand-axes from Kent. H. erectusflint work. The types shown are (clockwise from top) cordate, ficron and ovate.Venus of Willendorf, an example of Paleolithic art, dated 24–26,000 years ago The use of tools has been interpreted as a sign of intelligence, and it has been theorized that tool use may have stimulated certain aspects of human evolution, especially the continued expansion of the human brain.[188] Paleontology has yet to explain the expansion of this organ over millions of years despite being extremely demanding in terms of energy consumption. The brain of a modern human consumes, on average, about 13 watts (260 kilocalories per day), a fifth of the body's resting power consumption.[189] Increased tool use would allow hunting for energy-rich meat products, and would enable processing more energy-rich plant products. Researchers have suggested that early hominins were thus under evolutionary pressure to increase their capacity to create and use tools.[190] Precisely when early humans started to use tools is difficult to determine, because the more primitive these tools are (for example, sharp-edged stones) the more difficult it is to decide whether they are natural objects or human artifacts.[188] There is some evidence that the australopithecines (4 Ma) may have used broken bones as tools, but this is debated.[191] Many species make and use tools, but it is the human genus that dominates the areas of making and using more complex tools. The oldest known tools are flakes from West Turkana, Kenya, which date to 3.3 million years ago.[192] The next oldest stone tools are from Gona, Ethiopia, and are considered the beginning of the Oldowan technology. These tools date to about 2.6 million years ago.[193] A Homo fossil was found near some Oldowan tools, and its age was noted at 2.3 million years old, suggesting that maybe the Homo species did indeed create and use these tools. It is a possibility but does not yet represent solid evidence.[194] The third metacarpal styloid process enables the hand bone to lock into the wrist bones, allowing for greater amounts of pressure to be applied to the wrist and hand from a grasping thumb and fingers. It allows humans the dexterity and strength to make and use complex tools. This unique anatomical feature separates humans from apes and other nonhuman primates, and is not seen in human fossils older than 1.8 million years.[195] Bernard Wood noted that Paranthropus co-existed with the early Homo species in the area of the \"Oldowan Industrial Complex\" over roughly the same span of time. Although there is no direct evidence which identifies Paranthropus as the tool makers, their anatomy lends to indirect evidence of their capabilities in this area. Most paleoanthropologists agree that the early Homo species were indeed responsible for most of the Oldowan tools found. They argue that when most of the Oldowan tools were found in association with human fossils, Homo was always present, but Paranthropus was not.[194] In 1994, Randall Susman used the anatomy of opposable thumbs as the basis for his argument that both the Homo and Paranthropus species were toolmakers. He compared bones and muscles of human and chimpanzee thumbs, finding that humans have 3 muscles which are lacking in chimpanzees. Humans also have thicker metacarpals with broader heads, allowing more precise grasping than the chimpanzee hand can perform. Susman posited that modern anatomy of the human opposable thumb is an evolutionary response to the requirements associated with making and handling tools and that both species were indeed toolmakers.[194] Anthropologists describe modern human behavior to include cultural and behavioral traits such as specialization of tools, use of jewellery and images (such as cave drawings), organization of living space, rituals (such as grave gifts), specialized hunting techniques, exploration of less hospitable geographical areas, and barter trade networks, as well as more general traits such as language and complex symbolic thinking. Debate continues as to whether a \"revolution\" led to modern humans (\"big bang of human consciousness\"), or whether the evolution was more gradual.[49] Until about 50,000–40,000 years ago, the use of stone tools seems to have progressed stepwise. Each phase (H. habilis, H. ergaster, H. neanderthalensis) marked a new technology, followed by very slow development until the next phase. Currently paleoanthropologists are debating whether these Homo species possessed some or many modern human behaviors. They seem to have been culturally conservative, maintaining the same technologies and foraging patterns over very long periods. Around 50,000 BP, human culture started to evolve more rapidly. The transition to behavioral modernity has been characterized by some as a \"Great Leap Forward\",[196] or as the \"Upper Palaeolithic Revolution\",[197] due to the sudden appearance in the archaeological record of distinctive signs of modern behavior and big game hunting.[198] Evidence of behavioral modernity significantly earlier also exists from Africa, with older evidence of abstract imagery, widened subsistence strategies, more sophisticated tools and weapons, and other \"modern\" behaviors, and many scholars have recently argued that the transition to modernity occurred sooner than previously believed.[49][199][200][201] Other scholars consider the transition to have been more gradual, noting that some features had already appeared among archaic African Homo sapiens 300,000–200,000 years ago.[202][203][204][205][206] Recent evidence suggests that the Australian Aboriginal population separated from the African population 75,000 years ago, and that they made a 160 km (99 mi) sea journey 60,000 years ago, which may diminish the significance of the Upper Paleolithic Revolution.[207] Modern humans started burying their dead, making clothing from animal hides, hunting with more sophisticated techniques (such as using pit traps or driving animals off cliffs), and cave painting.[208] As human culture advanced, different populations innovated existing technologies: artifacts such as fish hooks, buttons, and bone needles show signs of cultural variation, which had not been seen prior to 50,000 BP. Typically, the older H. neanderthalensis populations did not vary in their technologies, although the Chatelperronian assemblages have been found to be Neanderthal imitations of H. sapiensAurignacian technologies.[209] Anatomically modern human populations continue to evolve, as they are affected by both natural selection and genetic drift. Although selection pressure on some traits, such as resistance to smallpox, has decreased in the modern age, humans are still undergoing natural selection for many other traits. Some of these are due to specific environmental pressures, while others are related to lifestyle changes since the development of agriculture (10,000 years ago), urbanization (5,000), and industrialization (250 years ago). It has been argued that human evolution has accelerated since the development of agriculture 10,000 years ago and civilization some 5,000 years ago, resulting, it is claimed, in substantial genetic differences between different current human populations,[210] and more recent research indicates that for some traits, the developments and innovations of human culture have driven a new form of selection that coexists with, and in some cases has largely replaced, natural selection.[211] Particularly conspicuous is variation in superficial characteristics, such as Afro-textured hair, or the recent evolution of light skin and blond hair in some populations, which are attributed to differences in climate. Particularly strong selective pressures have resulted in high-altitude adaptation in humans, with different ones in different isolated populations. Studies of the genetic basis show that some developed very recently, with Tibetans evolving over 3,000 years to have high proportions of an allele of EPAS1 that is adaptive to high altitudes. Other evolution is related to endemic diseases: the presence of malaria selects for sickle cell trait (the heterozygous form of sickle cell gene), while in the absence of malaria, the health effects of sickle-cell anemia select against this trait. For another example, the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G 127 V versus non-immune alleles. The frequency of this genetic variant is due to the survival of immune persons.[213][214] Some reported trends remain unexplained and the subject of ongoing research in the novel field of evolutionary medicine: polycystic ovary syndrome (PCOS) reduces fertility and thus is expected to be subject to extremely strong negative selection, but its relative commonality in human populations suggests a counteracting selection pressure. The identity of that pressure remains the subject of some debate.[215] Recent human evolution related to agriculture includes genetic resistance to infectious disease that has appeared in human populations by crossing the species barrier from domesticated animals,[216] as well as changes in metabolism due to changes in diet, such as lactase persistence. Culturally-driven evolution can defy the expectations of natural selection: while human populations experience some pressure that drives a selection for producing children at younger ages, the advent of effective contraception, higher education, and changing social norms have driven the observed selection in the opposite direction.[217] However, culturally-driven selection need not necessarily work counter or in opposition to natural selection: some proposals to explain the high rate of recent human brain expansion indicate a kind of feedback whereupon the brain's increased social learning efficiency encourages cultural developments that in turn encourage more efficiency, which drive more complex cultural developments that demand still-greater efficiency, and so forth.[218] Culturally-driven evolution has an advantage in that in addition to the genetic effects, it can be observed also in the archaeological record: the development of stone tools across the Palaeolithic period connects to culturally-driven cognitive development in the form of skill acquisition supported by the culture and the development of increasingly complex technologies and the cognitive ability to elaborate them.[219] In contemporary times, since industrialization, some trends have been observed: for instance, menopause is evolving to occur later.[220] Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations.[220] The word homo, the name of the biological genus to which humans belong, is Latin for \"human\".[e] It was chosen originally by Carl Linnaeus in his classification system.[f] The word \"human\" is from the Latin humanus, the adjectival form of homo. The Latin \"homo\" derives from the Indo-European root *dhghem, or \"earth\".[221] Linnaeus and other scientists of his time also considered the great apes to be the closest relatives of humans based on morphological and anatomical similarities.[222] The possibility of linking humans with earlier apes by descent became clear only after 1859 with the publication of Charles Darwin's On the Origin of Species, in which he argued for the idea of the evolution of new species from earlier ones. Darwin's book did not address the question of human evolution, saying only that \"Light will be thrown on the origin of man and his history.\"[223] A major problem in the 19th century was the lack of fossil intermediaries. Neanderthal remains were discovered in a limestone quarry in 1856, three years before the publication of On the Origin of Species, and Neanderthal fossils had been discovered in Gibraltar even earlier, but it was originally claimed that these were the remains of a modern human who had suffered some kind of illness.[225] Despite the 1891 discovery by Eugène Dubois of what is now called Homo erectus at Trinil, Java, it was only in the 1920s when such fossils were discovered in Africa, that intermediate species began to accumulate.[226] In 1925, Raymond Dart described Australopithecus africanus.[227] The type specimen was the Taung Child, an australopithecine infant which was discovered in a cave. The child's remains were a remarkably well-preserved tiny skull and an endocast of the brain. Although the brain was small (410 cm3), its shape was rounded, unlike that of chimpanzees and gorillas, and more like a modern human brain. Also, the specimen showed short canine teeth, and the position of the foramen magnum (the hole in the skull where the spine enters) was evidence of bipedal locomotion. All of these traits convinced Dart that the Taung Child was a bipedal human ancestor, a transitional form between apes and humans. During the 1960s and 1970s, hundreds of fossils were found in East Africa in the regions of the Olduvai Gorge and Lake Turkana. These searches were carried out by the Leakey family, with Louis Leakey and his wife Mary Leakey, and later their son Richard and daughter-in-law Meave, fossil hunters and paleoanthropologists. From the fossil beds of Olduvai and Lake Turkana they amassed specimens of the early hominins: the australopithecines and Homo species, and even H. erectus. These finds cemented Africa as the cradle of humankind. In the late 1970s and the 1980s, Ethiopia emerged as the new hot spot of paleoanthropology after \"Lucy\", the most complete fossil member of the species Australopithecus afarensis, was found in 1974 by Donald Johanson near Hadar in the desertic Afar Triangle region of northern Ethiopia. Although the specimen had a small brain, the pelvis and leg bones were almost identical in function to those of modern humans, showing with certainty that these hominins had walked erect.[228] Lucy was classified as a new species, Australopithecus afarensis, which is thought to be more closely related to the genus Homo as a direct ancestor, or as a close relative of an unknown ancestor, than any other known hominid or hominin from this early time range.[229] (The specimen was nicknamed \"Lucy\" after the Beatles' song \"Lucy in the Sky with Diamonds\", which was played loudly and repeatedly in the camp during the excavations.)[230] The Afar Triangle area would later yield discovery of many more hominin fossils, particularly those uncovered or described by teams headed by Tim D. White in the 1990s, including Ardipithecus ramidus and A. kadabba.[231] In 2013, fossil skeletons of Homo naledi, an extinct species of hominin assigned (provisionally) to the genus Homo, were found in the Rising Star Cave system, a site in South Africa's Cradle of Humankind region in Gauteng province near Johannesburg.[232][233] As of September 2015[update], fossils of at least fifteen individuals, amounting to 1,550 specimens, have been excavated from the cave.[233] The species is characterized by a body mass and stature similar to small-bodied human populations, a smaller endocranial volume similar to Australopithecus, and a cranialmorphology (skull shape) similar to early Homo species. The skeletal anatomy combines primitive features known from australopithecines with features known from early hominins. The individuals show signs of having been deliberately disposed of within the cave near the time of death. The fossils were dated close to 250,000 years ago,[234] and thus are not a direct ancestor but a contemporary with the first appearance of larger-brained anatomically modern humans.[235] The genetic revolution in studies of human evolution started when Vincent Sarich and Allan Wilson measured the strength of immunological cross-reactions of blood serumalbumin between pairs of creatures, including humans and African apes (chimpanzees and gorillas).[236] The strength of the reaction could be expressed numerically as an immunological distance, which was in turn proportional to the number of amino acid differences between homologous proteins in different species. By constructing a calibration curve of the ID of species' pairs with known divergence times in the fossil record, the data could be used as a molecular clock to estimate the times of divergence of pairs with poorer or unknown fossil records. In their seminal 1967 paper in Science, Sarich and Wilson estimated the divergence time of humans and apes as four to five million years ago,[236] at a time when standard interpretations of the fossil record gave this divergence as at least 10 to as much as 30 million years. Subsequent fossil discoveries, notably \"Lucy\", and reinterpretation of older fossil materials, notably Ramapithecus, showed the younger estimates to be correct and validated the albumin method. On the basis of a separation from the orangutan between 10 and 20 million years ago, earlier studies of the molecular clock suggested that there were about 76 mutations per generation that were not inherited by human children from their parents; this evidence supported the divergence time between hominins and chimpanzees noted above. However, a 2012 study in Iceland of 78 children and their parents suggests a mutation rate of only 36 mutations per generation; this datum extends the separation between humans and chimpanzees to an earlier period greater than 7 million years ago (Ma). Additional research with 226 offspring of wild chimpanzee populations in eight locations suggests that chimpanzees reproduce at age 26.5 years on average; which suggests the human divergence from chimpanzees occurred between 7 and 13 mya. And these data suggest that Ardipithecus (4.5 Ma), Orrorin (6 Ma) and Sahelanthropus (7 Ma) all may be on the hominid lineage, and even that the separation may have occurred outside the East African Rift region. Furthermore, analysis of the two species' genes in 2006 provides evidence that after human ancestors had started to diverge from chimpanzees, interspecies mating between \"proto-human\" and \"proto-chimpanzees\" nonetheless occurred regularly enough to change certain genes in the new gene pool: A new comparison of the human and chimpanzee genomes suggests that after the two lineages separated, they may have begun interbreeding... A principal finding is that the X chromosomes of humans and chimpanzees appear to have diverged about 1.2 million years more recently than the other chromosomes. The research suggests: There were in fact two splits between the human and chimpanzee lineages, with the first being followed by interbreeding between the two populations and then a second split. The suggestion of a hybridization has startled paleoanthropologists, who nonetheless are treating the new genetic data seriously.[239] In the 1990s, several teams of paleoanthropologists were working throughout Africa looking for evidence of the earliest divergence of the hominin lineage from the great apes. In 1994, Meave Leakey discovered Australopithecus anamensis. The find was overshadowed by Tim D. White's 1995 discovery of Ardipithecus ramidus, which pushed back the fossil record to 4.2 million years ago. Anthropologists in the 1980s were divided regarding some details of reproductive barriers and migratory dispersals of the genus Homo. Subsequently, genetics has been used to investigate and resolve these issues. According to the Sahara pump theory evidence suggests that the genus Homo have migrated out of Africa at least three and possibly four times (e.g. Homo erectus, Homo heidelbergensis and two or three times for Homo sapiens). Recent evidence suggests these dispersals are closely related to fluctuating periods of climate change.[244] Recent evidence suggests that humans may have left Africa half a million years earlier than previously thought. A joint Franco-Indian team has found human artifacts in the Siwalk Hills north of New Delhi dating back at least 2.6 million years. This is earlier than the previous earliest finding of genus Homo at Dmanisi, in Georgia, dating to 1.85 million years. Although controversial, tools found at a Chinese cave strengthen the case that humans used tools as far back as 2.48 million years ago.[245] This suggests that the Asian \"Chopper\" tool tradition, found in Java and northern China may have left Africa before the appearance of the Acheulian hand axe. Up until the genetic evidence became available, there were two dominant models for the dispersal of modern humans. The multiregional hypothesis proposed that the genus Homo contained only a single interconnected population as it does today (not separate species), and that its evolution took place worldwide continuously over the last couple of million years. This model was proposed in 1988 by Milford H. Wolpoff.[246][247] In contrast, the \"out of Africa\" model proposed that modern H. sapiens speciated in Africa recently (that is, approximately 200,000 years ago) and the subsequent migration through Eurasia resulted in the nearly complete replacement of other Homo species. This model has been developed by Chris Stringer and Peter Andrews.[248][249] Known H. sapiens migration routes in the Pleistocene Sequencing mtDNA and Y-DNA sampled from a wide range of indigenous populations revealed ancestral information relating to both male and female genetic heritage, and strengthened the \"out of Africa\" theory and weakened the views of multiregional evolutionism.[250] Aligned in genetic tree differences were interpreted as supportive of a recent single origin.[251] \"Out of Africa\" has thus gained much support from research using female mitochondrial DNA and the male Y chromosome. After analysing genealogy trees constructed using 133 types of mtDNA, researchers concluded that all were descended from a female African progenitor, dubbed Mitochondrial Eve. \"Out of Africa\" is also supported by the fact that mitochondrial genetic diversity is highest among African populations.[252] A broad study of African genetic diversity, headed by Sarah Tishkoff, found the San people had the greatest genetic diversity among the 113 distinct populations sampled, making them one of 14 \"ancestral population clusters\". The research also located a possible origin of modern human migration in southwestern Africa, near the coastal border of Namibia and Angola.[253] The fossil evidence was insufficient for archaeologist Richard Leakey to resolve the debate about exactly where in Africa modern humans first appeared.[254] Studies of haplogroups in Y-chromosomal DNA and mitochondrial DNA have largely supported a recent African origin.[255] All the evidence from autosomal DNA also predominantly supports a Recent African origin. However, evidence for archaic admixture in modern humans, both in Africa and later, throughout Eurasia has recently been suggested by a number of studies.[256] Recent sequencing of Neanderthal[90] and Denisovan[45] genomes shows that some admixture with these populations has occurred. All modern human groups outside Africa have 1–4% or (according to more recent research) about 1.5–2.6% Neanderthal alleles in their genome,[91] and some Melanesians have an additional 4–6% of Denisovan alleles. These new results do not contradict the \"out of Africa\" model, except in its strictest interpretation, although they make the situation more complex. After recovery from a genetic bottleneck that some researchers speculate might be linked to the Toba supervolcano catastrophe, a fairly small group left Africa and interbred with Neanderthals, probably in the Middle East, on the Eurasian steppe or even in North Africa before their departure. Their still predominantly African descendants spread to populate the world. A fraction in turn interbred with Denisovans, probably in southeastern Asia, before populating Melanesia.[100]HLA haplotypes of Neanderthal and Denisova origin have been identified in modern Eurasian and Oceanian populations.[47] The Denisovan EPAS1 gene has also been found in Tibetan populations.[257] Studies of the human genome using machine learning have identified additional genetic contributions in Eurasians from an \"unknown\" ancestral population potentially related to the Neanderthal-Denisovan lineage.[258] There are still differing theories on whether there was a single exodus from Africa or several. A multiple dispersal model involves the Southern Dispersal theory,[259][260][261] which has gained support in recent years from genetic, linguistic and archaeological evidence. In this theory, there was a coastal dispersal of modern humans from the Horn of Africa crossing the Bab el Mandib to Yemen at a lower sea level around 70,000 years ago. This group helped to populate Southeast Asia and Oceania, explaining the discovery of early human sites in these areas much earlier than those in the Levant.[259] This group seems to have been dependent upon marine resources for their survival. Stephen Oppenheimer has proposed a second wave of humans may have later dispersed through the Persian Gulf oases, and the Zagros mountains into the Middle East. Alternatively it may have come across the Sinai Peninsula into Asia, from shortly after 50,000 yrs BP, resulting in the bulk of the human populations of Eurasia. It has been suggested that this second group possibly possessed a more sophisticated \"big game hunting\" tool technology and was less dependent on coastal food sources than the original group. Much of the evidence for the first group's expansion would have been destroyed by the rising sea levels at the end of each glacial maximum.[259] The multiple dispersal model is contradicted by studies indicating that the populations of Eurasia and the populations of Southeast Asia and Oceania are all descended from the same mitochondrial DNA L3 lineages, which support a single migration out of Africa that gave rise to all non-African populations.[262] On the basis of the early date of Badoshan Iranian Aurignacian, Oppenheimer suggests that this second dispersal may have occurred with a pluvial period about 50,000 years before the present, with modern human big-game hunting cultures spreading up the Zagros Mountains, carrying modern human genomes from Oman, throughout the Persian Gulf, northward into Armenia and Anatolia, with a variant travelling south into Israel and to Cyrenicia.[198] Recent genetic evidence suggests that all modern non-African populations, including those of Eurasia and Oceania, are descended from a single wave that left Africa between 65,000 and 50,000 years ago.[263][264][265] The evidence on which scientific accounts of human evolution are based comes from many fields of natural science. The main source of knowledge about the evolutionary process has traditionally been the fossil record, but since the development of genetics beginning in the 1970s, DNA analysis has come to occupy a place of comparable importance. The studies of ontogeny, phylogeny and especially evolutionary developmental biology of both vertebrates and invertebrates offer considerable insight into the evolution of all life, including how humans evolved. The specific study of the origin and life of humans is anthropology, particularly paleoanthropology which focuses on the study of human prehistory.[266] The closest living relatives of humans are bonobos and chimpanzees (both genus Pan) and gorillas (genus Gorilla).[267] With the sequencing of both the human and chimpanzee genome, as of 2012[update] estimates of the similarity between their DNA sequences range between 95% and 99%.[267][268][32] By using the technique called the molecular clock which estimates the time required for the number of divergent mutations to accumulate between two lineages, the approximate date for the split between lineages can be calculated. The gibbons (family Hylobatidae) and then the orangutans (genus Pongo) were the first groups to split from the line leading to the hominins, including humans—followed by gorillas (genus Gorilla), and, ultimately, by the chimpanzees (genus Pan). The splitting date between hominin and chimpanzee lineages is placed by some between 4 to 8 million years ago, that is, during the Late Miocene.[269][270][271][272] Speciation, however, appears to have been unusually drawn out. Initial divergence occurred sometime between 7 to 13 million years ago, but ongoing hybridization blurred the separation and delayed complete separation during several millions of years. Patterson (2006) dated the final divergence at 5 to 6 million years ago.[273] Genetic evidence has also been employed to compare species within the genus Homo, investigating gene flow between early modern humans and Neanderthals, and to enhance the understanding of the early human migration patterns and splitting dates. By comparing the parts of the genome that are not under natural selection and which therefore accumulate mutations at a fairly steady rate, it is possible to reconstruct a genetic tree incorporating the entire human species since the last shared ancestor. Each time a certain mutation (single-nucleotide polymorphism) appears in an individual and is passed on to his or her descendants, a haplogroup is formed including all of the descendants of the individual who will also carry that mutation. By comparing mitochondrial DNA which is inherited only from the mother, geneticists have concluded that the last female common ancestor whose genetic marker is found in all modern humans, the so-called mitochondrial Eve, must have lived around 200,000 years ago. Human evolutionary genetics studies how human genomes differ among individuals, the evolutionary past that gave rise to them, and their current effects. Differences between genomes have anthropological, medical and forensic implications and applications. Genetic data can provide important insight into human evolution. In May 2023, scientists reported a more complicated pathway of human evolution than previously understood. According to the studies, humans evolved from different places and times in Africa, instead of from a single location and period of time.[274][275] There is little fossil evidence for the divergence of the gorilla, chimpanzee and hominin lineages.[276] The earliest fossils that have been proposed as members of the hominin lineage are Sahelanthropus tchadensis dating from 7 million years ago, Orrorin tugenensis dating from 5.7 million years ago, and Ardipithecus kadabba dating to 5.6 million years ago. Each of these have been argued to be a bipedal ancestor of later hominins but, in each case, the claims have been contested. It is also possible that one or more of these species are ancestors of another branch of African apes, or that they represent a shared ancestor between hominins and other apes. The question then of the relationship between these early fossil species and the hominin lineage is still to be resolved. From these early species, the australopithecines arose around 4 million years ago and diverged into robust (also called Paranthropus) and gracile branches, one of which (possibly A. garhi) probably went on to become ancestors of the genus Homo. The australopithecine species that is best represented in the fossil record is Australopithecus afarensis with more than 100 fossil individuals represented, found from Northern Ethiopia (such as the famous \"Lucy\"), to Kenya, and South Africa. Fossils of robust australopithecines such as Au. robustus (or alternatively Paranthropus robustus) and Au./P. boisei are particularly abundant in South Africa at sites such as Kromdraai and Swartkrans, and around Lake Turkana in Kenya. The earliest member of the genus Homo is Homo habilis which evolved around 2.8 million years ago.[35]H. habilis is the first species for which we have positive evidence of the use of stone tools. They developed the Oldowan lithic technology, named after the Olduvai Gorge in which the first specimens were found. Some scientists consider Homo rudolfensis, a larger bodied group of fossils with similar morphology to the original H. habilis fossils, to be a separate species, while others consider them to be part of H. habilis—simply representing intraspecies variation, or perhaps even sexual dimorphism. The brains of these early hominins were about the same size as that of a chimpanzee, and their main adaptation was bipedalism as an adaptation to terrestrial living. During the next million years, a process of encephalization began and, by the arrival (about 1.9 million years ago) of H. erectus in the fossil record, cranial capacity had doubled. H. erectus were the first of the hominins to emigrate from Africa, and, from 1.8 to 1.3 million years ago, this species spread through Africa, Asia, and Europe. One population of H. erectus, also sometimes classified as separate species H. ergaster, remained in Africa and evolved into H. sapiens. It is believed that H. erectus and H. ergaster were the first to use fire and complex tools. In Eurasia, H. erectus evolved into species such as H. antecessor, H. heidelbergensis and H. neanderthalensis. The earliest fossils of anatomically modern humans are from the Middle Paleolithic, about 300–200,000 years ago such as the Herto and Omo remains of Ethiopia, Jebel Irhoud remains of Morocco, and Florisbad remains of South Africa; later fossils from the Skhul Cave in Israel and Southern Europe begin around 90,000 years ago (0.09 million years ago). As modern humans spread out from Africa, they encountered other hominins such as H. neanderthalensis and the Denisovans, who may have evolved from populations of H. erectus that had left Africa around 2 million years ago. The nature of interaction between early humans and these sister species has been a long-standing source of controversy, the question being whether humans replaced these earlier species or whether they were in fact similar enough to interbreed, in which case these earlier populations may have contributed genetic material to modern humans.[277][278] This migration out of Africa is estimated to have begun about 70–50,000 years BP and modern humans subsequently spread globally, replacing earlier hominins either through competition or hybridization. They inhabited Eurasia and Oceania by 40,000 years BP, and the Americas by at least 14,500 years BP.[279] A model of the evolution of the genus Homo over the last 2 million years (vertical axis). The rapid \"Out of Africa\" expansion of H. sapiens is indicated at the top of the diagram, with admixture indicated with Neanderthals, Denisovans, and unspecified archaic African hominins. The hypothesis of interbreeding, also known as hybridization, admixture or hybrid-origin theory, has been discussed ever since the discovery of Neanderthal remains in the 19th century.[280] The linear view of human evolution began to be abandoned in the 1970s as different species of humans were discovered that made the linear concept increasingly unlikely. In the 21st century with the advent of molecular biology techniques and computerization, whole-genome sequencing of Neanderthal and human genome were performed, confirming recent admixture between different human species.[90] In 2010, evidence based on molecular biology was published, revealing unambiguous examples of interbreeding between archaic and modern humans during the Middle Paleolithic and early Upper Paleolithic. It has been demonstrated that interbreeding happened in several independent events that included Neanderthals and Denisovans, as well as several unidentified hominins.[281] Today, approximately 2% of DNA from all non-African populations (including Europeans, Asians, and Oceanians) is Neanderthal,[90] with traces of Denisovan heritage.[282] Also, 4–6% of modern Melanesian genetics are Denisovan.[282] Comparisons of the human genome to the genomes of Neandertals, Denisovans and apes can help identify features that set modern humans apart from other hominin species. In a 2016 comparative genomics study, a Harvard Medical School/UCLA research team made a world map on the distribution and made some predictions about where Denisovan and Neanderthal genes may be impacting modern human biology.[283][284] For example, comparative studies in the mid-2010s found several traits related to neurological, immunological,[285] developmental, and metabolic phenotypes, that were developed by archaic humans to European and Asian environments and inherited to modern humans through admixture with local hominins.[286][287] Although the narratives of human evolution are often contentious, several discoveries since 2010 show that human evolution should not be seen as a simple linear or branched progression, but a mix of related species.[45][4][5][6] In fact, genomic research has shown that hybridization between substantially diverged lineages is the rule, not the exception, in human evolution.[3] Furthermore, it is argued that hybridization was an essential creative force in the emergence of modern humans.[3] Stone tools are first attested around 2.6 million years ago, when hominins in Eastern Africa used so-called core tools, choppers made out of round cores that had been split by simple strikes.[288] This marks the beginning of the Paleolithic, or Old Stone Age; its end is taken to be the end of the last Ice Age, around 10,000 years ago. The Paleolithic is subdivided into the Lower Paleolithic (Early Stone Age), ending around 350,000–300,000 years ago, the Middle Paleolithic (Middle Stone Age), until 50,000–30,000 years ago, and the Upper Paleolithic, (Late Stone Age), 50,000–10,000 years ago. Archaeologists working in the Great Rift Valley in Kenya have discovered the oldest known stone tools in the world. Dated to around 3.3 million years ago, the implements are some 700,000 years older than stone tools from Ethiopia that previously held this distinction.[192][289][290][291] The period from 700,000 to 300,000 years ago is also known as the Acheulean, when H. ergaster (or erectus) made large stone hand axes out of flint and quartzite, at first quite rough (Early Acheulian), later \"retouched\" by additional, more-subtle strikes at the sides of the flakes. After 350,000 BP the more refined so-called Levallois technique was developed, a series of consecutive strikes, by which scrapers, slicers (\"racloirs\"), needles, and flattened needles were made.[288] Finally, after about 50,000 BP, ever more refined and specialized flint tools were made by the Neanderthals and the immigrant Cro-Magnons (knives, blades, skimmers). Bone tools were also made by H. sapiens in Africa by 90–70,000 years ago[199][292] and are also known from early H. sapiens sites in Eurasia by about 50,000 years ago. This list is in chronological order across the table by genus. Some species/subspecies names are well-established, and some are less established – especially in genus Homo. Please see articles for more information. ^ abcThe conventional estimate on the age of H. habilis is at roughly 2.1 to 2.3 million years.[37][165] Suggestions for pushing back the age to 2.8 Mya were made in 2015 based on the discovery of a jawbone.[166] ^There is no general agreement on the line of special descent of H. sapiens from H. erectus. Some of the species depicted in the image may not actually represent a direct evolutionary ancestor to H. sapiens, and may not directly derive from one another, namely: H. heidelbergensis is likely not an ancestor to H. sapiens, nor is H. antecessor.[137] H. ergaster is often considered the next evolutionary ancestor to H. sapiens following H. erectus, however, there is considerable uncertainty as to the accuracy of classifying it as a separate species from H. erectus at all.[138] ^H. erectus in the narrow sense (the Asian species) was extinct by 140,000 years ago, Homo erectus soloensis, found in Java, is considered the latest known survival of H. erectus. Formerly dated to as late as 50,000 to 40,000 years ago, a 2011 study pushed back the date of its extinction of H. e. soloensis to 143,000 years ago at the latest, more likely before 550,000 years ago.[168] ^O'Neil, Dennis. \"Early Modern Homo sapiens\". Evolution of Modern Humans: A Survey of the Biological and Cultural Evolution of Archaic and Modern Homo sapiens (Tutorial). San Marcos, CA: Palomar College. Archived from the original on April 30, 2015. Retrieved April 20, 2015. ^Grine, Frederick E.; Fleagle, John G. (2009), \"The First Humans: A Summary Perspective on the Origin and Early Evolution of the Genus Homo\", The First Humans – Origin and Early Evolution of the Genus Homo, Vertebrate Paleobiology and Paleoanthropology, Springer Netherlands, pp. 197–207, doi:10.1007/978-1-4020-9980-9_17, ISBN978-1-4020-9979-3 ^Dean, MC, Stringer, CB et al, (1986) \"Age at death of the Neanderthal child from Devil's Tower, Gibraltar and the implications for studies of general growth and development in Neanderthals\" (American Journal of Physical Anthropology, Vol 70 Issue 3, July 1986) ^Dirks et al. (2017): between 335 and 236 ka. The lower limit of 236 ka is due to optically stimulated luminescence dating of sediments with U-Th and palaeomagnetic analyses of flowstones; the upper limit of 335 ka is due to U-series and electron spin resonance (US-ESR) dating of two H. naledi teeth, to 253+82 −70 ka, for an estimated age of the fossils of 253+82 −17 ka."}
{"url": "https://en.m.wikipedia.org/wiki/Desmosome", "text": "The extracellular core region containing desmocollin and desmoglein, and the plaque contain desmoplakin, which attaches to keratin intermediate filaments. Desmosome is a intercellular junction in animal cell. Contents Desmosomes are composed of desmosome-intermediate filament complexes (DIFC), a network of cadherin proteins, linker proteins and intermediate filaments.[4] The DIFCs can be broken into three regions: the extracellular core region, or desmoglea, the outer dense plaque, or ODP, and the inner dense plaque, or IDP.[3] The extracellular core region, approximately 34 nm in length, contains desmoglein and desmocollin, which are in the cadherin family of cell adhesion proteins. Both have five extracellular domains, and have calcium-binding motifs. Extracellular calcium helps form the cadherin adhesion by allowing the cadherin extracellular domain on desmoglein and desmocollin to become rigid.[5] They bind to each other via heterophilic interactions in the extracellular space near their N-termini, in contrast with the homophilic binding characteristic of other cadherins.[6] Desmoglein and desmocollin have a single pass transmembrane region plus an intracellular anchor to secure its position in the cell membrane. Desmogleins and the desmocollin isoform \"Dsc-a\" contain an intracellular cadherin domain, which binds to plakoglobin.[3] The outer dense plaque, which is about 15–20 nm in length, contains the intracellular ends of desmocollin and desmoglein, the N-terminus side of desmoplakin, and the armadillo family of mediatory proteins plakoglobin and plakophilin.[3]Armadillo proteins are involved in mediating attachment to intracellular filaments and cell membrane proteins. Armadillo proteins consist of β-catenin, p120-catenin, plakoglobin (γ-catenin), and plakophilins 1-3. In desmosomes, plakoglobin and plakophilin help to anchor desmoplakin and keratin filaments to the desmosome structure. Plakoglobin has 12-arm repeats with a head and tail structure. Plakophilins have 9-arm repeats, and exist in two isoforms: a shorter \"a\" form and longer \"b\" form.[citation needed] The inner dense plaque, also about 15–20 nm in length, contains the C-terminus end of desmoplakin and their attachment to keratin intermediate filaments. Desmoplakin is the most abundant part of the desmosome,[7] as it operates as the mediator between the cadherin proteins in the plasma membrane and the keratin filaments. Desmoplakin has two isoforms that differ in the length of their middle rod domain. All desmoplakins have an N-terminal head, a C-tail consisting of three plakin repeats, and a glycine-serine-arginine rich domain (GSR) at the C-end.[citation needed] Mutations within the desmosome are the main cause of arrhythmogenic cardiomyopathy (ACM), a life-threatening disease caused by mutations usually in desmoglein 2, but sometimes in desmocollin 2. It often afflicts individuals between 20 and 50 years, and has been publicly known as a cause of death in young athletes, although the majority of sudden deaths do not occur in close connection to physical activity. The current incidence within the population is accepted as 1/10,000; however, it is thought that 1/200 may have a mutation that may predispose to ACM.[8] Symptoms of ACM include fainting, shortness of breath, and heart palpitations and the condition is treated by implanting a small defibrillator device. Blistering diseases such as pemphigus vulgaris (PV) and pemphigus foliaceus (PF) are autoimmune diseases in which auto-antibodies target desmogleins. PV is caused by circulating autoantibodies (IgG) that target Dsg3 (Desmoglein 3) and sometimes Dsg1. PV is manifested by suprabasal acantholysis, or blisters in the mucous membrane and blisters in the epidermis. PF patients have autoantibodies that target Dsg1 with superficial blisters on the epidermis with no mucous membrane issues. Both disease result in a loss of keratinocyte adhesion. Pemphigus can also be caused by a bacterial infection: bullous impetigo is an infection caused by a staphylococcus bacterium that releases a toxin that cleaves the Dsg1 extracellular domain.[citation needed] The desmosome was first discovered by Giulio Bizzozero, an Italian pathologist.[3] He named these dense nodules the nodes of Bizzozero. In 1920, the term desmosome was originated by Josef Schaffer. The first combining form, desmo-, Neo-Latin from Greek desmos, bond, carries meaning of binding or bonding things together. Combined with -some, which comes from soma, body, it thus makes a desmosome a binding body."}
{"url": "https://pubmed.ncbi.nlm.nih.gov/11440251/", "text": "Affiliation Abstract Insight into the origin and early evolution of the animal phyla requires an understanding of how animal groups are related to one another. Thus, we set out to explore animal phylogeny by analyzing with maximum parsimony 138 morphological characters from 40 metazoan groups, and 304 18S rDNA sequences, both separately and together. Both types of data agree that arthropods are not closely related to annelids: the former group with nematodes and other molting animals (Ecdysozoa), and the latter group with molluscs and other taxa with spiral cleavage. Furthermore, neither brachiopods nor chaetognaths group with deuterostomes; brachiopods are allied with the molluscs and annelids (Lophotrochozoa), whereas chaetognaths are allied with the ecdysozoans. The major discordance between the two types of data concerns the rooting of the bilaterians, and the bilaterian sister-taxon. Morphology suggests that the root is between deuterostomes and protostomes, with ctenophores the bilaterian sister-group, whereas 18S rDNA suggests that the root is within the Lophotrochozoa with acoel flatworms and gnathostomulids as basal bilaterians, and with cnidarians the bilaterian sister-group. We suggest that this basal position of acoels and gnathostomulids is artifactal because for 1,000 replicate phylogenetic analyses with one random sequence as outgroup, the majority root with an acoel flatworm or gnathostomulid as the basal ingroup lineage. When these problematic taxa are eliminated from the matrix, the combined analysis suggests that the root lies between the deuterostomes and protostomes, and Ctenophora is the bilaterian sister-group. We suggest that because chaetognaths and lophophorates, taxa traditionally allied with deuterostomes, occupy basal positions within their respective protostomian clades, deuterostomy most likely represents a suite of characters plesiomorphic for bilaterians."}
{"url": "https://en.m.wikipedia.org/wiki/Zoophytes", "text": "Zoophyte A zoophyte (animal-plant) is an obsolete term for an organism thought to be intermediate between animals and plants, or an animal with plant-like attributes or appearance. In the 19th century they were reclassified as Radiata which included various taxa, a term superseded by Coelenterata referring more narrowly to the animal phylaCnidaria (coral animals, true jellies, sea anemones, sea pens, and their allies), sponges, and Ctenophora (comb jellies). Tartar lamb illustrationZoophytes in 1833 book. A group of strange creatures that exist somewhere on, or between, the boundaries of plants and animals kingdoms were the subject of considerable debate in the eighteenth century. Some naturalists believed that they were a blend of plant and animal; other naturalists considered them to be entirely either plant or animal (such as sea anemones).[1] Zoophytes are common in medieval and renaissance era herbals, notable examples including the Tartar Lamb, a legendary plant which grew sheep as fruit.[3] Zoophytes appeared in many influential early medical texts, such as Dioscorides'sDe Materia Medica and subsequent adaptations and commentaries on that work, notably Mattioli'sDiscorsi. Zoophytes are frequently seen as medieval attempts to explain the origins of exotic, unknown plants with strange properties (such as cotton, in the case of the Tartar Lamb as theorized by Henry Lee, Fellow of the Linnean Society in the book The Vegetable Lamb of Tartary).[4][5][6] Reports of zoophytes continued into the seventeenth century and were commented on by many influential thinkers of the time period, including Francis Bacon. It was not until 1646 that claims of zoophytes began to be concretely refuted, and skepticism towards claims of zoophytes mounted throughout the seventeenth and eighteenth centuries.[3] As natural history and natural philosophy developed in the 18th century, there was considerable debate and disagreements between naturalists about organisms on or near the boundary between the animal and plant kingdoms, and how to relate them in taxonomy. Interest in the topic began in the 1730s with the research by Abraham Trembley into polyps.[7] When Carl Linnaeus published the 10th edition of Systema Naturae in 1758, marking the start of zoological nomenclature, he set out three divisions of the Kingdom of Nature: rocks, plants and animals, \"though all three exist in the lithophytes\", the corals. He defined zoophytes as \"a composite small organism, with both animal and plant characteristics\". He acknowledged contributions from the coralline expert Ellis by describing him as a \"lynx-eyed discoverer of zoophytes\". In 1761 he wrote to Ellis that \"zoophytes have a mere vegetable life, and are increased every year under their bark, like trees\" as shown by growth rings on the trunk of Gorgonia, they are \"therefore vegetables, with flowers like small animals. As zoophytes are, many of them, covered with a stony coat, the Creator has been pleased that they should receive nourishment by their naked flowers. He has therefore furnished each with a pore, which we call a mouth.\" After wide research, in 1786 Ellis was still unconvinced \"what or where the link is that divides the animal and vegetable kingdoms of Nature\", and pressed Linnaeus to classify most as animals. He subsequently proposed that the animals of the corals construct their own structures, in a book completed by Daniel Solander.[8] Georges Cuvier in his Le Règne Animal of 1817 titled one of his four divisions (Embranchements) of the animal kingdom \"Les Zoophytes ou Animaux Rayonnés\".[9] An 1834 English translation uses the term Radiata, and titles the division \"The Zoophytes, or Animalia Radiata\",[10] an expanded 1840 translation notes that \"Neither of these names is literally applicable, for all the animals in the division are not radiated; and the very name Zoophyte, 'plant - animal,' is a contradiction. In England, the term Zoophyte is much more restricted than in France, but it is equally inapplicable, excepting, perhaps, to those species, about which there are still disputes as to whether they are animals or vegetables.\"[11] Despite its scientific obsolescence, Charles Darwin continued to use the term throughout his studies. ^Cuvier, Georges. 1827-35. The animal kingdom arranged in conformity with its organization. With additional descriptions of all the species hitherto named, and of many not before noticed, by Edward Griffith and others. 16 vols. London: Geo. B. Whittaker. Volume 12./"}
{"url": "https://en.m.wikipedia.org/wiki/Special:BookSources/978-0-7876-5777-2", "text": "This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). If you arrived at this page by clicking an ISBN link in a Wikipedia page, you will find the full range of relevant search links for that specific book by scrolling to the find links below. To search for a different book, type that book's individual ISBN into this ISBN search box. Spaces and hyphens in the ISBN do not matter. Also, the number starts after the colon for \"ISBN-10:\" and \"ISBN-13:\" numbers. An ISBN identifies a specific edition of a book. Any given title may therefore have a number of different ISBNs. See #Find other editions below for finding other editions. An ISBN registration, even one corresponding to a book page on a major book distributor database, is not definite proof that such a book actually exists. A title may have been cancelled or postponed after the ISBN was assigned. Check to see if the book exists or not. Google Books and Amazon.com may be helpful if you want to verify citations in Wikipedia articles, because they often let you search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available). At the Open Library (part of the Internet Archive) you can borrow and read entire books online. Luxembourg Montenegro Netherlands Find this book in the Dutch-Union Catalogue that searches simultaneously in more than 400 Dutch electronic library systems (including regional libraries, university libraries, research libraries and the Royal Dutch library) Book-swapping websites Non-English book sources If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language."}
{"url": "https://en.m.wikipedia.org/wiki/Anomalocaris", "text": "Like other radiodonts, Anomalocaris had swimming flaps running along its body, large compound eyes, and a single pair of segmented, frontal appendages, which in Anomalocaris were used to grasp prey. Measuring up to 38 cm (1.25 ft) long excluding frontal appendages and tail fan,[4]A. canadensis is one of the largest animals of the Cambrian, and thought to be one of the earliest examples of an apex predator,[5][6] though others have been found in older Cambrian lagerstätten deposits. Since the original description in late 19th century,[7] the frontal appendages were the only known fossilized parts and misidentified as the body parts of other animals.[8] Its radiodont affinity was revealed in 1980s, specifically in a 1985 journal article by Harry B. Whittington and Derek Briggs.[9] The trunk and mouth were reconstructed after another radiodont genus until the corrections done in 1996[8] and 2012.[10] It is the type genus of Anomalocarididae, a family which previously included all radiodonts but recently only Anomalocaris and a few closely-related taxa.[3] [The story of Anomalocaris is] a tale of humor, error, struggle, frustration, and more error, culminating in an extraordinary resolution that brought together bits and pieces of three \"phyla\" in a single reconstructed creature, the largest and fiercest of Cambrian organisms.[11] Body or abdominal segments, which, in all the specimens collected, are abnormally flattened laterally, a little higher or deeper than long, broader above than below, the pair of ventral appendages proceeding from each, nearly equal in height or depth to the segment itself... The generic name Anomalocaris (from ανώμαλος, unlike,—καρίς, a shrimp, i.e., unlike other other shrimps) [the species name referring to Canada] is suggested by the unusual shape of the uropods or ventral appendages of the body segments and the relative position of the caudal spine.[7] In 1928, Danish paleontologist Kai Henriksen proposed that Tuzoia, a Burgess Shale arthropod which was known only from the carapace, represented the missing front half of Anomalocaris.[8] The artists Elie Cheverlange and Charles R. Knight followed this interpretation in their depictions of Anomalocaris.[8] Not known to scientists at the time, the body parts of relatives of Anomalocaris had already been described but not recognized as such. The first fossilized mouth of such a kind of animal was discovered by Charles Doolittle Walcott, who mistook it for a jellyfish and placed it in the genus Peytoia. Walcott also discovered a frontal appendage but failed to realize the similarities to Whiteaves' discovery and instead identified it as feeding appendage or tail of the coexisted Sidneyia.[18] In the same publication in which he named Peytoia, Walcott named Laggania, a taxon that he interpreted as a holothurian. In 1966, the Geological Survey of Canada began a comprehensive revision of the Burgess Shale fossil record, led by Cambridge University paleontologist Harry B. Whittington.[8] In the process of this revision, Whittington and his students Simon Conway Morris and Derek Briggs would discover the true nature of Anomalocaris and its relatives, but not without contributing to the history of misinterpretations first.[18] In 1978, Conway Morris recognized that the mouthparts of Laggania were identical to Peytoia, but concluded that Laggania was a composite fossil made up of Peytoia and the sponge Corralio undulata.[19] In 1979, Briggs recognized that the fossils of Anomalocaris were appendages, not abdomens, and proposed that they were the walking legs of a giant arthropod, and that the feeding appendage Walcott had assigned to Sidneyia was the feeding appendage of similar animal, referred to as \"appendage F\".[20] Later, while clearing what he thought was an unrelated specimen, Harry B. Whittington removed a layer of covering stone to discover the unequivocally connected frontal appendage identical to Anomalocaris and mouthpart similar to Peytoia.[18][21] Whittington linked the two species, but it took several more years for researchers to realize that the continuously juxtaposed Peytoia, Laggania and frontal appendages (Anomalocaris and \"appendage F\") actually represented a single group of enormous creatures.[9] The two genera have now been placed into the order Radiodonta[8] and are commonly known as radiodonts or anomalocaridids. Since Peytoia was named first, it is the accepted correct name for the entire animal. However, the original frontal appendage was from a larger species distinct from Peytoia and \"Laggania\" and therefore retains the name Anomalocaris.[10] In 2011[22] and 2020,[23]compound eyes of Anomalocaris were recovered from a paleontological dig at Emu Bay on Kangaroo Island, Australia, proving that Anomalocaris was indeed an arthropod as had been suspected. The find also indicated that advanced arthropod eyes had evolved very early, before the evolution of jointed legs or hardened exoskeletons.[22] For the time in which it lived, Anomalocaris was gigantic. A. canadensis is estimated to be up to 37.8 cm (1.24 ft) long excluding frontal appendages and tail fans.[4][27] Previous estimation up to 1 m (3.3 ft)[20] is unlikely based on the ratio of body parts (body length measured only about 2 times the length of frontal appendage in A. canadensis, respectively[4]) and the size of largest frontal appendage (up to 18 centimetres (7.1 inches) in length when extended).[18][4] Anomalocaris propelled itself through the water by undulating the flexible flaps on the sides of its body.[35] Each flap sloped below the one more posterior to it,[36] and this overlapping allowed the lobes on each side of the body to act as a single \"fin\", maximizing the swimming efficiency.[35] The construction of a remote-controlled model showed this mode of swimming to be intrinsically stable,[37] implying that Anomalocaris would not have needed a complex brain to manage balance while swimming. The body was widest between the third and fifth lobe and narrowed towards the tail, with additional 3 pairs of small flaps on the constricted neck region.[8][38] It is difficult to distinguish lobes near the tail, making an accurate count difficult.[36] For the main trunk flaps, the type species A. canadensis had 13 pairs.[38] Anomalocaris had an unusual disk-like mouth known as oral cone. The oral cone was composed of several plates organized triradially. Three of the plates were quite large. Three to four medium sized plates could be found between each of the large plates, and several small plates between them. Most of the plates wrinkled and have scale-like tubercles near the mouth opening.[10][39] Such an oral cone is very different from those of a typical hurdiid radiodont like Peytoia and Hurdia, which is smooth and tetraradial.[10][33] As a shared character across radiodonts, Anomalocaris also had three sclerites on the top and side of its head.[33] The top one, known as a head shield, dorsal carapace or H-element, was shaped like an laterally-elongated[40] oval, with a distinct rim on the outer edge.[38] The remaining two lateral sclerites, known as P-elements, were also ovoid, but connected by a bar-like outgrowth.[33] The P-elements were previously misinterpreted as two huge compound eyes.[38][33] Frontal appendages of Anomalocaris, with examples from multiple species. Two large frontal appendage were positioned in front of the mouth, at the front of the head.[8] Each frontal appendage of Anomalocaris usually had 14 podomeres (segmental units, at least 1 for shaft and 13 for distal articulated region), with each appendage being laterally-flattened (taller than wide).[38] Most podomeres were tipped with a pair of endites (ventral spines).[38] The endites themselves were both equipped with multiple auxiliary spines, which branches off from the anterior and posterior margin of the endites.[24][39][41][38][1] The tail was a large tail fan, composed of three[8][38] pairs of large, lateral fin-shaped lobes and one terminal lobe-like tailpiece.[38] Previous studies suggest the tail fan was used to propel it through Cambrian waters,[21][35] while further hydrodynamic study rather suggest it was more adapted to provide steering function.[42] The gills of the animal, in the form of long, thin, hair-like structures known as lanceolate blades, were arranged in rows forming setal blades. The setal blades were attached by their margin to the top side of the animal, two setal blades per body segment. A divide ran down the middle, separating the gills.[38] Based on fossilized eyes from the Emu Bay Shale, which belong to the species Anomalocarisdaleyae,[2] the stalked eyes of Anomalocaris were 30 times more powerful than those of trilobites, long thought to have had the most advanced eyes of any contemporary species. With one specimen having over 24,000 lenses in one eye, the resolution of the 3-centimetre-wide (1.2 in) eyes would have been rivalled only by that of the modern dragonfly, which has 28,000 lenses in each eye.[22][23] Additionally, estimation of ecdysozoanopsins suggest that Anomalocaris may have had dichromaticcolor vision.[43] The interpretation of Anomalocaris as an active predator is widely accepted throughout the history of research,[9][8][10] as its raptorial frontal appendages and mid-gut glands strongly suggest a predatory lifestyle.[44][45][5] In the case of A. canadensis, its outstanding size amongst Burgess Shale fauna also make it one of the first apex predators known to exist.[5] However, the long-standing idea that Anomalocaris fed on hard-bodied animals, especially its ability to penetrate mineralized exoskeleton of trilobites, has been questioned, with many recent studies considering it more likely that Anomalocaris exclusively hunted soft-bodied prey.[46][10][5][6] Some Cambrian trilobites have been found with round or W-shaped \"bite\" marks, which were identified as being the same shape as the mouthparts of Peytoia (previously misidentified as those of Anomalocaris[47][10]). Stronger evidence that Anomalocaris ate trilobites comes from coprolite, which contain trilobite parts and are so large that the radiodonts are the only known organism from that period large enough to have produced them.[47] However, since Anomalocaris lacks any mineralized tissue, it seemed unlikely that it would be able to penetrate the hard, calcified exoskeleton of trilobites.[47] Rather, the coprolites may have been produced by different organisms, such as the trilobites of the genus Redlichia.[39] Another suggested possibility was that Anomalocaris fed by grabbing one end of their prey in its oral cone while using its frontal appendages to quickly rock the other end of the animal back and forth. This produced stresses that exploited the weaknesses of arthropod cuticles, causing the prey's exoskeleton to rupture and allowing the predator to access its innards.[47] This behaviour was originally thought to have provided an evolutionary pressure for trilobites to roll up, to avoid being flexed until they snapped.[47] The lack of wear on radiodont mouthparts suggests they did not come into regular contact with mineralized trilobite shells, and were possibly better suited to feeding on smaller, soft-bodied organisms by suction, since they would have experienced structural failure if they were used against the armour of trilobites.[46][39]A. canadensis was suggested to have been capable of feeding on organisms with hard exoskeletons due to the short, robust spines on its frontal appendages.[39][25] However, this conclusion is solely based on the comparison with the fragile frontal appendages of suspension feeding radiodonts (e.g. Echidnacaris and Houcaris spp.).[26] The typical lack of damage to the endites on the frontal appendages of A. canadensis (with damage only present on a single specimen) suggests that they were not used to grasp hard-shelled prey.[6] As opposed to Peytoia whose oral cone is more rectangular with short protruding spines, the oral cone of A. canadensis has a smaller and more irregular opening, not permitting strong biting motions, and indicating a suction-feeding behavior to suck in softer organisms.[10] Three-dimensional modelling of various radiodont frontal appendages also suggest that A. canadensis is more capable to prey on smaller (2–5 cm in diameter), active, soft-bodied animals (e.g. vetulicolian; free-swimming arthropods like isoxyids and hymenocarines; Nectocaris).[5][6] Bicknell et al. (2023) examined the frontal appendages of Anomalocaris, suggesting it was an active nektonic apex predator. Postured with the frontal appendages outstretched, Anomalocaris would have been able to swim with maximized speed, similar to modern predatory water bugs. Its eyes would be suitable to hunt prey in well-lit waters. Anomalocaris would have hunted various free-swimming animals since there are a large diversity of nektonic and pelagic soft-bodied animals. It probably would have not hunted benthic animals like trilobites, considering the possibility of damaging the frontal appendages on the substrate while trying to grab prey from seafloor at speed. Instead, other animals such as other radiodonts (e.g. Hurdia, Cambroraster, Titanokorys, Stanleycaris) and artiopods (e.g. Sidneyia) would have been benthic predators in the Burgess Shale.[5][6] Anomalocaris canadensis lived in the Burgess Shale in relatively great numbers.[1] In the Burgess Shale, Anomalocaris is more common in the older sections, notably the Mount Stephen trilobite beds. However, in the younger sections, such as the Phyllopod bed, Anomalocaris could reach much greater sizes; roughly twice the size of its older, trilobite bed relatives. These rare giant specimens have previously been referred to a separate species, Anomalocaris gigantea; however, the validity of this species has been called into question,[20] and is currently synonymized to A. canadensis.[38] Other unnamed species of Anomalocaris live in vastly different environments.[3] For example, Anomalocaris cf. canadensis (JS-1880) lived in the Maotianshan Shales,[3] a shallow tropical sea or even being delta[48] in what is now modern China. Anomalocaris daleyae (Emu Bay Shale) lived in a comparable environment; the shallow, tropical waters of Cambrian Australia.[3] The Maotianshan Shale and the Emu Bay Shale are very close in proximity, being separated by a small landmass, far from the Burgess Shale.[3] These two locations also included \"Anomalocaris\" kunmingensis and \"Anomalocaris\" briggsi respectively, species that previously attributed[49][50][39][51] but taxonomically unlikely to be a member of Anomalocaris nor even Anomalocarididae.[3][52]"}
{"url": "https://en.m.wikipedia.org/wiki/Insectivore", "text": "The first vertebrate insectivores were amphibians. When they evolved 400 million years ago, the first amphibians were piscivores, with numerous sharp conical teeth, much like a modern crocodile. The same tooth arrangement is however also suited for eating animals with exoskeletons, thus the ability to eat insects is an extension of piscivory.[3] At one time, insectivorous mammals were scientifically classified in an order called Insectivora. This order is now abandoned, as not all insectivorous mammals are closely related. Most of the Insectivora taxa have been reclassified; those that have not yet been reclassified and found to be truly related to each other remain in the order Eulipotyphla. Although individually small, insects exist in enormous numbers. Insects make up a very large part of the animal biomass in almost all non-marine, non-polar environments. It has been estimated that the global insect biomass is in the region of 1012 kg (one billion tons) with an estimated population of 1018 (one billion billion, or quintillion) organisms.[4]: 13 Many creatures depend on insects as their primary diet, and many that do not (and are thus not technically insectivores) nevertheless use insects as a protein supplement, particularly when they are breeding.[5] Insectivorous plants are plants that derive some of their nutrients from trapping and consuming animals or protozoan. The benefit they derive from their catch varies considerably; in some species, it might include a small part of their nutrient intake and in others it might be an indispensable source of nutrients. As a rule, however, such animal food, however valuable it might be as a source of certain critically important minerals, is not the plants' major source of energy, which they generally derive mainly from photosynthesis.[12]: 14 Insectivorous plants might consume insects and other animal material trapped adventitiously. However, most species to which such food represents an important part of their intake are specifically, often spectacularly, adapted to attract and secure adequate supplies. Their prey animals typically, but not exclusively, comprise insects and other arthropods. Plants highly adapted to reliance on animal food use a variety of mechanisms to secure their prey, such as pitfalls, sticky surfaces, hair-trigger snaps, bladder-traps, entangling furriness, and lobster-pot trap mechanisms.[12]: 14–17 Also known as carnivorous plants, they appear adapted to grow in places where the soil is thin or poor in nutrients, especially nitrogen, such as acidic bogs and rock outcroppings.[12]: 13 Insectivorous plants include the Venus flytrap, several types of pitcher plants, butterworts, sundews, bladderworts, the waterwheel plant, brocchinia and many members of the Bromeliaceae. The list is far from complete, and some plants, such as Roridula species, exploit the prey organisms mainly in a mutualistic relationship with other creatures, such as resident organisms that contribute to the digestion of prey. In particular, animal prey organisms supply carnivorous plants with nitrogen, but they also are important sources of various other soluble minerals, such as potassium and trace elements that are in short supply in environments where the plants flourish. This gives them a decisive advantage over other plants, whereas in nutrient-rich soils they tend to be out-competed by plants adapted to aggressive growth where nutrient supplies are not the major constraints. Technically these plants are not strictly insectivorous, as they consume any animal that they can secure and consume; the distinction is trivial, however, because not many primarily insectivorous organisms exclusively consume insects. Most of those that do have such a restrictive diet, such as certain parasitoids and hunting wasps, are specialized to exploit particular species, not insects in general. Indeed, much as large mantids and spiders will do, the larger varieties of pitcher plants have been known to consume vertebrates such as small rodents and lizards.[12]: 13 Charles Darwin wrote the first well-known treatise on carnivorous plants in 1875.[13]"}
{"url": "https://en.m.wikipedia.org/wiki/Equestrianism", "text": "Though there is controversy over the exact date horses were domesticated and when they were first ridden, the best estimate is that horses first were ridden approximately 3500 BC. There is some evidence that about 3,000 BC, near the Dnieper River and the Don River, people were using bits on horses, as a stallion that was buried there shows teeth wear consistent with using a bit.[5] However, the most unequivocal early archaeological evidence of equines put to working use was of horses being driven. Chariot burials about 2500 BC present the most direct hard evidence of horses used as working animals. In ancient times chariot warfare was followed by the use of war horses as light and heavy cavalry. The horse played an important role throughout human history all over the world, both in warfare and in peaceful pursuits such as transportation, trade and agriculture. Horses lived in North America, but died out at the end of the Ice Age. Horses were brought back to North America by European explorers, beginning with the second voyage of Columbus in 1493.[6] Equestrianism was introduced in the 1900 Summer Olympics as an Olympic sport with jumping events.[7] Humans appear to have long expressed a desire to know which horse or horses were the fastest, and horse racing has ancient roots. Gambling on horse races appears to go hand-in hand with racing and has a long history as well. Thoroughbreds have the pre-eminent reputation as a racing breed, but other breeds also race. Steeplechasing involves racing on a track where the horses also jump over obstacles. It is most common in the UK, where it is also called National Hunt racing. In harness: Both light and heavy breeds as well as ponies are raced in harness with a sulky or racing bike. The Standardbred dominates the sport in both trotting and pacing varieties. The United States Trotting Association organizes harness racing in the United States. Harness racing is also found throughout Europe, New Zealand and Australia. Distance racing: Endurance riding, takes place over a given, measured distance and the horses have an even start. Top level races are usually 50 to 100 miles (80 to 161 km), over mountainous or other natural terrain, with scheduled stops to take the horses' vital signs, check soundness and verify that the horse is fit to continue. The first horse to finish and be confirmed by the veterinarian as fit to continue is the winner. Limited distance rides of about 25–20 miles (40–32 km) are offered to newcomers. Variants include Ride and Tie and various forms of long riding.[8] Dressage (\"training\" in French) involves the progressive training of the horse to a high level of impulsion, collection and obedience.[9] Competitive dressage has the goal of showing the horse carrying out, on request, the natural movements that it performs without thinking while running loose. Show jumping comprises a timed event judged on the ability of the horse and rider to jump over a series of obstacles, in a given order and with the fewest refusals or knockdowns of portions of the obstacles. Show jumping is also one of the five events in the modern pentathlon. Eventing, also called combined training, horse trials, the three-day event, the Military or the complete test, puts together the obedience of dressage with the athletic ability of show jumping, the fitness demands the cross-country jumping phase. In the last-named, the horses jump over fixed obstacles, such as logs, stone walls, banks, ditches and water, trying to finish the course under the \"optimum time\". There was also the 'Steeple Chase' Phase, which is now excluded from most major competitions to bring them in line with the Olympic standard. Horse shows are held throughout the world with a tremendous variety of possible events, equipment, attire, and judging standards used. However, most forms of horse show competition can be broken into the following broad categories: Equitation, sometimes called seat and hands or horsemanship, refers to events where the rider is judged on form, style and ability. Pleasure, flat or under saddle classes feature horses who are ridden on the flat (not jumped) and judged on manners, performance, movement, style and quality. Halter, in-hand breeding or conformation classes, where the horse is led by a handler on the ground and judged on conformation and suitability as a breeding animal. Harness classes, where the horse is driven rather than ridden, but still judged on manners, performance and quality. Jumping or Over Fences refers broadly to both show jumping and show hunter, where horses and riders must jump obstacles. In addition to the classical Olympic events, the following forms of competition are seen. In North America they are referred to as \"English riding\" in contrast with western riding; elsewhere in the world, if a distinction is necessary, they are usually described as \"classic riding\": There is no horn. Hunt seat or Hunter classes judge the movement and the form of horses suitable for work over fences. A typical show hunter division would include classes over fences as well as \"Hunter under Saddle\" or \"flat\" classes (sometimes called \"hack\" classes), in which the horse is judged on its performance, manners and movement without having to jump. Hunters have a long, flat-kneed trot, sometimes called \"daisy cutter\" movement, a phrase suggesting a good hunter could slice daisies in a field when it reaches its stride out. The over fences classes in show hunter competition are judged on the form of the horse, its manners and the smoothness of the course. A horse with good jumping form snaps its knees up and jumps with a good bascule. It should also be able to canter or gallop with control while having a stride long enough to make a proper number of strides over a given distance between fences. Hunter classes differ from jumper classes, in which they are not timed, and equitation classes, in which the rider's performance is the focus. Hunter style is based on fox hunting, so jumps in the hunter division are usually more natural colors than the jumps in a jumper division. Eventing, show jumping and dressage, described under \"Olympic disciplines\", above are all \"English\" riding disciplines that in North America sometimes are loosely classified within the \"hunt seat\" category. Saddle seat, is a primarily American discipline, though has recently become somewhat popular in South Africa, was created to show to best advantage the animated movement of high-stepping and gaited breeds such as the American Saddlebred and the Tennessee Walker. Arabians and Morgans may also be shown saddle seat in the United States. There are usually three basic divisions. Park divisions are for the horses with the highest action. Pleasure divisions still emphasis animated action, but to a lesser degree, with manners ranking over animation. Plantation or Country divisions have the least amount of animation (in some breeds, the horses are flat-shod) and the greatest emphasis on manners. Show hack is a competition seen primarily in the United Kingdom, Australia and other nations influenced by British traditions, featuring horses of elegant appearance, with excellent way of going and self-carriage. A related event is riding horse. The most noticeable feature of western style riding is the western saddle, which has a substantial saddle tree that provides support to horse and rider when working long hours in the saddle. The western saddle features a prominent pommel topped by a horn (a knob used for dallying a lariat after roping an animal), wide stirrups, and in some cases, both front and back cinches. The depth of the seat may depend on the activity, a deeper seat used for barrel racing or cutting cows or a more shallow seat for general ranch riding or Steer wrestling. Finished western horses are asked to perform with a loose rein controlled by one hand. The headstall of a western bridle may utilize either a Snaffle bit or curb bit. Bitless headstalls are also seen, such as a bosal-style hackamore on youger horses, or various styles of mechanical hackamore. In Vaquero style training, a combination of a bosal and bit, called a \"two-rein\", is used at some stages of training. The standard western bridle lacks a noseband and usually consists of a single set of reins attached to a curb bit that has somewhat longer shanks than the curb of an English Weymouth bridle or a pelham bit. Western bridles have either a browband or else a \"one ear\" loop (sometimes two) that crosses in front of the horse's ear. Two styles of Western reins developed: The long split reins of the Texas tradition, which are completely separated, or the \"Romal\" reins of the California tradition, which are closed reins with a long single attachment (the romal) that can be used as a quirt. Modern rodeo competitors in timed events sometimes use a closed rein without a romal. Western riders wear a long-sleeved shirt, long pants or jeans, cowboy boots, and a wide-brimmed cowboy hat. A rider may wear protective leather leggings called chaps. Riders may wear brighter colors or finer fabrics in competition than for work. In particular, horse show events such as Western pleasure may much flashier equipment. Saddles, bits and bridles are ornamented with substantial amounts of silver, rider clothing may have vivid colors and even rhinestones or sequins.[12] Horses, ponies, mules and donkeys are driven in harness in many different ways. For working purposes, they can pull a plow or other farm equipment designed to be pulled by animals. In many parts of the world they still pull wagons for basic hauling and transportation. They may draw carriages at ceremonies, in parades or for tourist rides. As noted in \"horse racing\" above, horses can race in harness, pulling a very lightweight cart known as a sulky. At the other end of the spectrum, some draft horses compete in horse pulling competitions, where single or teams of horses and their drivers vie to determine who can pull the most weight for a short distance. In horse show competition, the following general categories of competition are seen: Combined driving, an internationally recognized competition where horses perform an arena-based \"dressage\" class where precision and control are emphasized, a cross-country \"marathon\" section that emphasizes fitness and endurance, and a \"stadium\" or \"cones\" obstacle course. Pleasure driving: Horses and ponies are usually hitched to a light cart shown at a walk and two speeds of trot, with an emphasis on manners. Fine harness: Also called \"Formal driving\", Horses are hitched to a light four-wheeled cart and shown in a manner that emphasizes flashy action and dramatic performance. Roadster: A horse show competition where exhibitors wear racing silks and ride in a sulky in a style akin to harness racing, only without actually racing, but rather focusing on manners and performance. Carriage driving, using somewhat larger two or four wheeled carriages, often restored antiques, judged on the turnout/neatness or suitability of horse and carriage. Barrel racing and pole bending – the timed speed and agility events seen in rodeo as well as gymkhana or O-Mok-See competition. Both men and women compete in speed events at gymkhanas or O-Mok-Sees; however, at most professional, sanctioned rodeos, barrel racing is an exclusively women's sport. In a barrel race, horse and rider gallop around a cloverleaf pattern of barrels, making agile turns without knocking the barrels over. In pole bending, horse and rider run the length of a line of six upright poles, turn sharply and weave through the poles, turn again and weave back, then return to the start. Steer wrestling – Also known as \"Bulldogging\", this is a rodeo event where the rider jumps off his horse onto a steer and 'wrestles' it to the ground by grabbing it by the horns. This is probably the single most physically dangerous event in rodeo for the cowboy, who runs a high risk of jumping off a running horse head first and missing the steer or of having the thrown steer land on top of him, sometimes horns first. Goat tying – usually an event for women or pre-teen girls and boys, a goat is staked out while a mounted rider runs to the goat, dismounts, grabs the goat, throws it to the ground and ties it in the same manner as a calf. This event was designed to teach smaller or younger riders the basics of calf roping without the more complex need to also lasso the animal. Roping includes a number of timed events that are based on the real-life tasks of a working cowboy, who often had to capture calves and adult cattle for branding, medical treatment and other purposes. A lasso or lariat is thrown over the head of a calf or the horns of adult cattle, and the animal is secured in a fashion dictated by its size and age. Calf roping, also called \"tie-down roping\", is an event where a calf is roped around the neck by a lariat, the horse stops and sets back on the rope while the cowboy dismounts, runs to the calf, throws it to the ground and ties three feet together. (If the horse throws the calf, the cowboy must lose time waiting for the calf to get back to its feet so that the cowboy can do the work. The job of the horse is to hold the calf steady on the rope) This activity is still practiced on modern working ranches for branding, medical treatment, and so on. Team roping, also called \"heading and heeling\", is the only rodeo event where men and women riders may compete together. Two people capture and restrain a full-grown steer. One horse and rider, the \"header\", lassos a running steer's horns, while the other horse and rider, the \"heeler\", lassos the steer's two hind legs. Once the animal is captured, the riders face each other and lightly pull the steer between them, so that it loses its balance, thus in the real world allowing restraint for treatment. Breakaway roping – an easier form of calf roping where a very short lariat is used, tied lightly to the saddle horn with string and a flag. When the calf is roped, the horse stops, allowing the calf to run on, flagging the end of time when the string and flag breaks from the saddle. In the United States, this event is primarily for women of all ages and boys under 12, while in some nations where traditional calf roping is frowned upon, riders of both genders compete. In spite of popular myth, most modern \"broncs\" are not in fact wild horses, but are more commonly spoiled riding horses[citation needed] or horses bred specifically as bucking stock. Bronc riding – there are two divisions in rodeo, bareback bronc riding, where the rider rides a bucking horse holding onto a leather surcingle or rigging with only one hand, and saddle bronc riding, where the rider rides a modified western saddle without a horn (for safety) while holding onto a braided lead rope attached to the horse's halter. Bull Riding – though technically not an equestrian event, as the cowboys ride full-grown bulls instead of horses, skills similar to bareback bronc riding are required. Le Trec, which comprises three phases – trail riding, with jumping and correct basic flatwork. Le Trec, which is very popular in Europe, tests the partnership's ability to cope with an all-day ride across varied terrain, route finding, negotiating natural obstacles and hazards, while considering the welfare of the horse, respecting the countryside and enjoying all it has to offer. Competitive trail riding, a pace race held across terrain similar to endurance riding, but shorter in length (25 – 35 miles (56 km), depending on class). Being a form of pace race, the objective is not to finish in the least time. Instead, as in other forms of judged trail riding, each competitor is graded on everything including physical condition, campsite and horse management. Horsemanship also is considered, including how the rider handles the trail and how horse is handled and presented to the judge and vet throughout the ride. The horse is graded on performance, manners, etc. \"Pulse and respiration\" stops check the horse's recovery ability. The judges also set up obstacles along the trail and the horse and rider are graded on how well they perform as a team. The whole point is the partnership between the horse and rider. Cross Country Jumping, a jumping course that contains logs and natural obstacles mostly. The common clothes worn are usually brighter colors and less conservative. Endurance riding, a competition usually of 50 to 100 miles (160 km) or more, over mountainous or other natural terrain, with scheduled stops to take the horses' vital signs, check soundness and verify that the horse is fit to continue. The first horse to finish and be confirmed by the veterinarian as fit to continue is the winner. Additional awards are usually given to the best-conditioned horses who finish in the top 10. Hunter Pacing is a sport where a horse and rider team travel a trail at speeds based the ideal conditions for the horse, with competitors seeking to ride closest to that perfect time. Hunter paces are usually held in a series. Hunter paces are usually a few miles long and covered mostly at a canter or gallop. The horsemanship and management skills of the rider are also considered in the scoring, and periodic stops are required for veterinarians to check the vital signs and overall soundness of the horses. Handling, riding and driving horses have inherent risks. Horses are large prey animals with a well-developed flight or fight instinct able to move quickly and unexpectedly. When mounted, the rider's head may be up to 4 m (13 ft) from the ground, and the horse may travel at a speed of up to 65 km/h (40 mph).[13] The injuries observed range from very minor injuries to fatalities. A study in Germany reported that the relative risk of injury from riding a horse, compared to riding a bicycle, was 9 times higher for adolescents and 5.6 times higher for younger children, but that riding a horse was less risky than riding a moped.[14] In Victoria, Australia, a search of state records found that equestrian sports had the third highest incidence of serious injury, after motor sports and power boating.[15] In Greece, an analysis of a national registry estimated the incidence of equestrian injury to be 21 per 100,000 person-years for farming and equestrian sports combined, and 160 times higher for horse racing personnel. Other findings noted that helmets likely prevent traumatic brain injuries.[16] In the United States each year an estimated 30 million people ride horses, resulting in 50,000 emergency department visits (1 visit per 600 riders per year).[17] A survey of 679 equestrians in Oregon, Washington and Idaho estimated that at some time in their equestrian career one in five will be seriously injured, resulting in hospitalization, surgery or long-term disability.[18] Among survey respondents, novice equestrians had an incidence of any injury that was threefold over intermediates, fivefold over advanced equestrians, and nearly eightfold over professionals. Approximately 100 hours of experience are required to achieve a substantial decline in the risk of injury. The survey authors conclude that efforts to prevent equestrian injury should focus on novice equestrians. The most common injury is falling from the horse, followed by being kicked, trampled and bitten. About 3 out of 4 injuries are due to falling, broadly defined.[19][20] A broad definition of falling often includes being crushed and being thrown from the horse, but when reported separately each of these mechanisms may be more common than being kicked.[21][22] In Canada, a 10-year study of trauma center patients injured while riding reported that although 48% had suffered head injuries, only 9% of these riders had been wearing helmets at the time of their accident. Other injuries involved the chest (54%), abdomen (22%) and extremities (17%).[23] A German study reported that injuries in horse riding are rare compared to other sports, but when they occur they are severe. Specifically, they found that 40% of horse riding injuries were fractures, and only 15% were sprains. Furthermore, the study noted that in Germany, one quarter of all sport related fatalities are caused by horse riding.[24] Most horse related injuries are a result of falling from a horse, which is the cause of 60–80% of all such reported injuries.[19][25] Another common cause of injury is being kicked by a horse, which may cause skull fractures or severe trauma to the internal organs. Some possible injuries resulting from horse riding, with the percent indicating the amounts in relation to all injuries as reported by a New Zealand study,[26] include: Arm fracture or dislocation (31%) Head injury (21%) Leg fracture or dislocation (15%) Chest injury (33%) Among 36 members and employees of the Hong Kong Jockey Club who were seen in a trauma center during a period of 5 years, 24 fell from horses and 11 were kicked by the horse. Injuries comprised: 18 torso; 11 head, face or neck; and 11 limb.[27] The authors of this study recommend that helmets, face shields and body protectors be worn when riding or handling horses. In New South Wales, Australia, a study of equestrians seen at one hospital over a 6-year period found that 81% were wearing a helmet at the time of injury, and that helmet use both increased over time and was correlated with a lower rate of admission.[28] In the second half of the study period, of the equestrians seen at a hospital, only 14% were admitted. In contrast, a study of child equestrians seen at a hospital emergency department in Adelaide reported that 60% were admitted.[29] In the United States, an analysis of National Electronic Injury Surveillance System (NEISS) data performed by the Equestrian Medical Safety Association studied 78,279 horse-related injuries in 2007: \"The most common injuries included fractures (28.5%); contusions/abrasions (28.3%); strain/sprain (14.5%); internal injury (8.1%); lacerations (5.7%); concussions (4.6%); dislocations (1.9%); and hematomas (1.2%). Most frequent injury sites are the lower trunk (19.6%); head (15.0%); upper trunk (13.4%); shoulder (8.2%); and wrist (6.8%). Within this study patients were treated and released (86.2%), were hospitalized (8.7%), were transferred (3.6%), left without being treated (0.8%), remained for observation (0.6%) and arrived at the hospital deceased (0.1%).\"[30] Horseback riding is one of the most dangerous sports, especially in relation to head injury. Statistics from the United States, for example, indicate that about 30 million people ride horses annually.[31] On average, about 67,000 people are admitted to the hospital each year from injuries sustained while working with horses.[32] 15,000 of those admittances are from traumatic brain injuries. Of those, about 60 die each year from their brain injuries.[33] Studies have found horseback riding to be more dangerous than several sports, including skiing, auto racing and football.[23] Horseback riding has a higher hospital admittance rate per hours of riding than motorcycle racing, at 0.49 per thousand hours of riding and 0.14 accidents per thousand hours, respectively.[23] Head injuries are especially traumatic in horseback riding. About two-thirds of all riders requiring hospitalization after a fall have sustained a traumatic brain injury.[34] Falling from a horse without wearing a helmet is comparable to being struck by a car.[35] Most falling deaths are caused by head injury.[35] The use of riding helmets substantially decreases the likelihood and severity of head injuries. When a rider falls with a helmet, he or she is five times less likely to experience a traumatic brain injury than a rider who falls without a helmet.[34] Helmets work by crushing on impact and extending the length of time it takes the head to stop moving.[36] Despite this, helmet usage rates in North America are estimated to be between eight and twenty percent.[37] Once a helmet has sustained an impact from falling, that part of the helmet is structurally weakened, even if no visible damage is present.[38] Helmet manufacturers recommend that a helmet that has undergone impact from a fall be replaced immediately. In addition, helmets should be replaced every three to five years; specific recommendations vary by manufacturer.[39] Many organizations mandate helmet use in competition or on show grounds, and rules have continually moved in the direction of requiring helmet use. In 2011, the United States Equestrian Federation passed a rule making helmet use mandatory while mounted on competition grounds at U.S. nationally rated eventing competitions.[40] Also in 2011, the United States Dressage Federation made helmet use in competition mandatory for all riders under 18 and all riders who are riding any test at Fourth Level and below.[41] If a rider competing at Prix St. Georges and above is also riding a test at Fourth Level or below, he or she must also wear a helmet at all times while mounted. By the 1930s and 1940s most horse riding had become occasional and leisurely or competitive rather than being the common method of transportation it had been for centuries before The idea that riding a horse astride could injure a woman's sex organs is a historic, but sometimes popular even today, misunderstanding or misconception, particularly that riding astride can damage the hymen.[42] Evidence of injury to any female sex organs is scant. In female high-level athletes, trauma to the perineum is rare and is associated with certain sports (see Pelvic floor#Clinical significance). The type of trauma associated with equestrian sports has been termed \"horse riders' perineum\".[43] A case series of 4 female mountain bike riders and 2 female horse riders found both patient-reported perineal pain and evidence of sub-clinical changes in the clitoris;[44] the relevance of these findings to horse riding is unknown. In men, sports-related injuries are among the major causes of testicular trauma. In a small controlled but unblinded study of 52 men, varicocele was significantly more common in equestrians than in non-equestrians.[45] The difference between these two groups was small, however, compared to differences reported between extreme mountain bike riders and non-riders,[46] and also between mountain bike riders and on-road bicycle riders.[47] Horse-riding injuries to the scrotum (contusions) and testes (blunt trauma) were well known to surgeons in the 19th century and early 20th century.[48] Injuries from collision with the pommel of a saddle are mentioned specifically.[48] Horse racing is a popular equestrian sport which is practiced in many nations around the world. It is inextricably associated with gambling, where in certain events, stakes can become very high. Despite its illegality in most competitions, these conditions of extreme competitiveness can lead to the use of performing-enhancing drugs and extreme training techniques, which can result in negative side effects for the horses' well-being. The races themselves have also proved dangerous to the horses – especially steeplechasing, which requires the horse to jump hurdles whilst galloping at full speed. This can result in injury or death to the horse, as well as the jockey.[49] A study by animal welfare group Animal Aid revealed that approximately 375 racehorses die yearly, with 30% of these either during or as a result of injuries from a race.[50] The report also highlighted the increasing frequency of race-related illnesses, including bleeding lungs (exercise-induced pulmonary hemorrhage) and gastric ulcers.[50] Animal rights groups are also primarily concerned that certain sports or training exercises may cause unnecessary pain or injuries to horse athletes. Some specific training or showing practices are so widely condemned that they have been made illegal at the national level and violations can incur criminal penalties. The most well-known is soring, a practice of applying a caustic ointment just above the hooves of a Tennessee Walking Horse to make it pick up its feet higher. However, in spite of a federal law in the United States prohibiting this practice and routine inspections of horse shows by inspectors from the United States Department of Agriculture, soring is still widespread and difficult to eliminate.[51] Some events themselves are also considered so abusive that they are banned in many countries. Among these are horse-tripping, a sport where riders chase and rope a loose-running horse by its front legs, throwing it to the ground.[52] Secondary effects of racing have also recently been uncovered. A 2006 investigation by The Observer in the UK found that each year 6,000–10,000 horses are slaughtered for consumption abroad, a significant proportion of which are horses bred for racing.[53] A boom in the number of foals bred has meant that there is not adequate resources to care for unwanted horses. Demand has increased for this massive breeding programme to be scaled back.[53] Despite over 1000 foals being produced annually by the Thoroughbred horse industry, 66% of those bred for such a purpose were never entered into a race, and despite a life expectancy of 30 years, many are killed before their fifth birthday.[53] Horse riding events have been selected as a main motif in numerous collectors' coins. One of the recent samples is the €10 Greek Horse Riding commemorative coin, minted in 2003 to commemorate the 2004 Summer Olympics. On the composition of the obverse of this coin, the modern horseman is pictured as he jumps over an obstacle, while in the background the ancient horseman is inspired by a representation on a black-figure vase of the 5th century BC. For the 2012 Olympics, the Royal Mint has produced a 50p coin showing a horse jumping a fence.[54]"}
{"url": "https://en.m.wikipedia.org/wiki/File:Polychaeta_(no)_2.jpg", "text": "attribution – You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. share alike – If you remix, transform, or build upon the material, you must distribute your contributions under the same or compatible license as the original."}
{"url": "https://en.m.wikipedia.org/wiki/Pig", "text": "Pig The pig (Sus domesticus), also called swine (pl.: swine) or hog, is an omnivorous, domesticated, even-toed, hoofed mammal. It is named the domestic pig when distinguishing it from other members of the genus Sus. It is considered a subspecies of Sus scrofa (the wild boar or Eurasian boar) by some authorities, but as a distinct species by others. Pigs were domesticated in the Neolithic, both in East Asia and in the Near East. When domesticated pigs arrived in Europe, they extensively interbred with wild boar but retained their domesticated features. Pigs are farmed primarily for meat, called pork. The animal's skin or hide is used for leather. China is the world's largest pork producer, followed by the European Union and then the United States. Around 1.5 billion pigs are raised each year, producing some 120 million tonnes of meat, often cured as bacon. Some are kept as pets. Pigs have featured in human culture since Neolithic times, appearing in art and literature for children and adults, and celebrated in cities such as Bologna for their meat products. Description The pig has a large head, with a long snout strengthened by a special prenasal bone and a disk of cartilage at the tip.[2] The snout is used to dig into the soil to find food and is an acute sense organ. The dental formula of adult pigs is 3.1.4.33.1.4.3, giving a total of 44 teeth. The rear teeth are adapted for crushing. In males, the canine teeth can form tusks, which grow continuously and are sharpened by grinding against each other.[2] There are four hoofed toes on each foot; the two larger central toes bear most of the weight, while the outer two are also used in soft ground.[3] Most pigs have rather sparsely bristled hair on their skin, though there are some woolly-coated breeds such as the Mangalitsa.[4] Adult pigs generally weigh between 140 and 300 kg (310 and 660 lb), though some breeds can exceed this range. Exceptionally, a pig called Big Bill weighed 1,157 kg (2,551 lb) and had a shoulder height of 1.5 m (4.9 ft).[5] Domestication in the Neolithic The initial emergence of wild pigs, followed by the genetic divergence between boars and pigs and the domestication of pigs [20] Archaeological evidence shows that pigs were domesticated from wild boar in the Near East in or around the Tigris Basin,[21] being managed in a semi-wild state much as they are managed by some modern New Guineans.[22] There were pigs in Cyprus more than 11,400 years ago, introduced from the mainland, implying domestication in the adjacent mainland by then.[23] Pigs were separately domesticated in China, starting some 8,000 years ago.[24][25][26] In the Near East, pig husbandry spread for the next few millennia. It reduced gradually during the Bronze Age, as rural populations instead focused on commodity-producing livestock, but it was sustained in cities.[27] Domestication did not involve reproductive isolation with population bottlenecks. Western Asian pigs were introduced into Europe, where they crossed with wild boar. There appears to have been interbreeding with a now extinct ghost population of wild pigs during the Pleistocene. The genomes of domestic pigs show strong selection for genes affecting behavior and morphology. Human selection for domestic traits likely counteracted the homogenizing effect of gene flow from wild boars and created domestication islands in the genome.[28][29] Pigs arrived in Europe from the Near East at least 8,500 years ago. Over the next 3,000 years they interbred with European wild boar until their genome showed less than 5% Near Eastern ancestry, yet retained their domesticated features.[30] DNA evidence from subfossil remains of teeth and jawbones of Neolithic pigs shows that the first domestic pigs in Europe were brought from the Near East. This stimulated the domestication of local European wild boar, resulting in a third domestication event with the Near Eastern genes dying out in European pig stock. More recently there have been complex exchanges, with European domesticated lines being exported, in turn, to the ancient Near East.[31][32] Historical records indicate that Asian pigs were again introduced into Europe during the 18th and early 19th centuries.[25] Feral pigs Pigs have escaped from farms and gone feral in many parts of the world. Feral pigs in the southeastern United States have migrated north to the Midwest, where many state agencies have programs to remove them.[35][36][37] Feral pigs in New Zealand and northern Queensland have caused substantial environmental damage.[38][39] Feral hybrids of the European wild boar with the domestic pig are disruptive to both environment and agriculture, as they destroy crops, spread animal diseases including Foot-and-mouth disease, and consume wildlife such as juvenile seabirds and young tortoises.[40] Feral pig damage is especially an issue in southeastern South America.[41][42] Reproduction Physiology Female pigs reach sexual maturity at 3–12 months of age and come into estrus every 18–24 days if they are not successfully bred. The variation in ovulation rate can be attributed to intrinsic factors such as age and genotype, as well as extrinsic factors like nutrition, environment, and the supplementation of exogenous hormones. The gestation period averages 112–120 days.[43] Piglets keeping warm together Estrus lasts two to three days, and the female's displayed receptiveness to mate is known as standing heat. Standing heat is a reflexive response that is stimulated when the female is in contact with the saliva of a sexually mature boar. Androstenol is one of the pheromones produced in the submaxillary salivary glands of boars that trigger the female's response.[44] The female cervix contains a series of five interdigitating pads, or folds, that hold the boar's corkscrew-shaped penis during copulation.[45] Females have bicornuate uteruses and two conceptuses must be present in both uterine horns to enable pregnancy to proceed.[46] The mother's body recognises that it is pregnant on days 11 to 12 of pregnancy, and is marked by the corpus luteum's producing the sex hormone progesterone.[47] To sustain the pregnancy, the embryo signals to the corpus luteum with the hormones estradiol and prostaglandin E2.[48] This signaling acts on both the endometrium and luteal tissue to prevent the regression of the corpus luteum by activation of genes that are responsible for corpus luteum maintenance.[49] During mid to late pregnancy, the corpus luteum relies primarily on luteinizing hormone for maintenance until birth.[48] Archeological evidence indicates that medieval European pigs farrowed, or bore a litter of piglets, once per year.[50] By the nineteenth century, European piglets routinely double-farrowed, or bore two litters of piglets per year. It is unclear when this shift occurred.[51] Pigs have a maximum life span of about 27 years.[52] Nest-building A characteristic of pigs which they share with carnivores is nest-building. Sows root in the ground to create depressions the size of their body, and then build nest mounds, using twigs and leaves, softer in the middle, in which to give birth. When the mound reaches the desired height, she places large branches, up to 2 metres in length, on the surface. She enters the mound and roots around to create a depression within the gathered material. She then gives birth in a lying position, unlike other artiodactyls which usually stand while birthing.[53] Nest-building occurs during the last 24 hours before the onset of farrowing, and becomes most intense 12 to 6 hours before farrowing.[54] The sow separates from the group and seeks a suitable nest site with well-drained soil and shelter from rain and wind. This provides the offspring with shelter, comfort, and thermoregulation. The nest provides protection against weather and predators, while keeping the piglets close to the sow and away from the rest of the herd. This ensures they do not get trampled on, and prevents other piglets from stealing milk from the sow.[55] The onset of nest-building is triggered by a rise in prolactin level, caused by a decrease in progesterone and an increase in prostaglandin; the gathering of nest material seems to be regulated more by external stimuli such as temperature.[54] Nursing and suckling Pigs have complex nursing and suckling behaviour.[56] Nursing occurs every 50–60 minutes, and the sow requires stimulation from piglets before milk let-down. Sensory inputs (vocalisation, odours from mammary and birth fluids, and hair patterns of the sow) are particularly important immediately post-birth to facilitate teat location by the piglets.[57] Initially, the piglets compete for position at the udder; then the piglets massage around their respective teats with their snouts, during which time the sow grunts at slow, regular intervals. Each series of grunts varies in frequency, tone and magnitude, indicating the stages of nursing to the piglets.[58] The phase of competition for teats and of nosing the udder lasts for about a minute, ending when milk begins to flow. The piglets then hold the teats in their mouths and suck with slow mouth movements (one per second), and the rate of the sow's grunting increases for approximately 20 seconds. The grunt peak in the third phase of suckling does not coincide with milk ejection, but rather the release of oxytocin from the pituitary into the bloodstream.[59] Phase four coincides with the period of main milk flow (10–20 seconds) when the piglets suddenly withdraw slightly from the udder and start sucking with rapid mouth movements of about three per second. The sow grunts rapidly, lower in tone and often in quick runs of three or four, during this phase. Finally, the flow stops and so does the grunting of the sow. The piglets may dart from teat to teat and recommence suckling with slow movements, or nosing the udder. Piglets massage and suckle the sow's teats after milk flow ceases as a way of letting the sow know their nutritional status. This helps her to regulate the amount of milk released from that teat in future sucklings. The more intense the post-feed massaging of a teat, the more milk that teat later releases.[60] Sows typically have 12–14 nipples. A sow with suckling piglets Teat order In pigs, dominance hierarchies are formed at an early age. Piglets are precocious, and attempt to suckle soon after being born. The piglets are born with sharp teeth and fight for the anterior teats, as these produce more milk. Once established, this teat order remains stable; each piglet tends to feed on a particular teat or group of teats.[53] Stimulation of the anterior teats appears to be important in causing milk letdown,[61] so it might be advantageous to the entire litter to have these teats occupied by healthy piglets. Piglets locate teats by sight and then by olfaction.[62] Behaviour Social Pig behaviour is intermediate between that of other artiodactyls and of carnivores.[53] Pigs seek out the company of other pigs, and often huddle to maintain physical contact, but do not naturally form large herds. They live in groups of about 8–10 adult sows, some young individuals, and some single males.[54] Pigs confined in a simplified, crowded, or uncomfortable environment may resort to tail-biting; farmers sometimes dock the tails of pigs to prevent the problem, or may enrich the environment with toys or other objects to reduce the risk.[63][64] Temperature control Because of their relative lack of sweat glands, pigs often control their body temperature using behavioural thermoregulation. Wallowing, coating the body with mud, is a common behaviour.[9] They do not submerge completely under the mud, but vary the depth and duration of wallowing depending on environmental conditions.[9] Adult pigs start wallowing once the ambient temperature is around 17–21 °C (63–70 °F). They cover themselves in mud from head to tail.[9] They may use mud as a sunscreen, or to keep parasites away.[9] Most bristled pigs \"blow their coat\", meaning that they shed most of the longer, coarser stiff hair once a year, usually in spring or early summer, to prepare for the warmer months ahead.[65] Eating, feeding, sleeping Pigs around a rotary feeder Where pigs are allowed to roam freely, they walk roughly 4 km daily, scavenging within a home range of around a hectare. Farmers in Africa often choose such a low-input, free-range production system.[66] If conditions permit, pigs feed continuously for many hours and then sleep for many hours, in contrast to ruminants, which tend to feed for a short time and then sleep for a short time. Pigs are omnivorous and versatile in their feeding behaviour. They primarily eat leaves, stems, roots, fruits, and flowers.[67] Rooting is an instinctual comforting behaviour in pigs characterized by nudging the snout into something. It first happens when piglets are born to obtain their mother's milk, and can become a habitual, obsessive behaviour, most prominent in animals weaned too early. Pigs root and dig into the ground to forage for food. Rooting is also a means of communication.[68] Intelligence Pigs are noticeably intelligent, on a par with dogs. They distinguish each other as individuals; they spend time in play; and they form structured communities. They have good long-term memory, experience emotions, and change their behaviour in response to the emotional states of other pigs. In terms of experimental tasks, pigs can perform tasks that require them to identify the locations of objects; they can solve mazes; and they can work with a simple language of symbols. They display self-recognition in a mirror. Pigs have been trained to associate different sorts of music (Bach and a military march) with food and social isolation respectively, and could communicate the resulting positive or negative emotion to untrained pigs.[70][71] Pigs can be trained to use a joystick with their snout to select a target on screen.[69] Senses Pigs have panoramic vision of approximately 310° and binocular vision of 35° to 50°. It is thought they have no eye accommodation.[72] Other animals that have no accommodation, e.g. sheep, lift their heads to see distant objects.[73] The extent to which pigs have colour vision is still a source of some debate; however, the presence of cone cells in the retina with two distinct wavelength sensitivities (blue and green) suggests that at least some colour vision is present.[74] Pigs have a well-developed sense of smell; this is exploited in Europe where trained pigs find underground truffles.[75] Pigs have 1,113 genes for smell receptors, compared to 1,094 in dogs; this may indicate an acute sense of smell, but against this, insects have only around 50 to 100 such genes but make extensive use of olfaction.[76] Olfactory rather than visual stimuli are used in the identification of other pigs.[77] Hearing is well developed; sounds are localised by moving the head. Pigs use auditory stimuli extensively for communication in all social activities.[78] Alarm or aversive stimuli are transmitted to other pigs not only by auditory cues but also by pheromones.[79] Similarly, recognition between the sow and her piglets is by olfactory and vocal cues.[80] Some parasites of pigs are a public health risk as they can be transmitted to humans in undercooked pork. These are the pork tapeworm Taenia solium; a protozoan, Toxoplasma gondii; and a nematode, Trichinella spiralis. Transmission can be prevented by thorough sanitation on the farm; by meat inspection and careful commercial processing; and by thorough cooking, or alternatively by sufficient freezing and curing.[82] In agriculture Production Pigs have been raised outdoors, and sometimes allowed to forage in woods or pastures. In industrialized nations, pig production has largely switched to large-scale intensive pig farming. This has lowered production costs but has caused concern about possible cruelty. As consumers have become concerned with the humane treatment of livestock, demand for pasture-raised pork in these nations has increased.[83] Most pigs in the US receive ractopamine, a beta-agonist drug, which promotes muscle instead of fat and quicker weight gain, requiring less feed to reach finishing weight, and producing less manure. China has requested that pork exports be ractopamine-free.[84] With a population of around 1 billion individuals, the domesticated pig is one of the most numerous large mammals on the planet.[85][86] Breeds Around 600 breeds of pig have been created by farmers around the world, mainly in Europe and Asia, differing in coloration, shape, and size.[88] According to The Livestock Conservancy, as of 2016, three breeds of pig are critically rare (having a global population of fewer than 2000). They are the Choctaw hog, the Mulefoot, and the Ossabaw Island hog.[89] The smallest known pig breed in the world is the Göttingen minipig, typically weighing about 26 kilograms (57 lb) as a healthy, full-grown adult.[90] As pets Pigs are intelligent, social creatures. They are considered hypoallergenic and are known to do quite well with people who have the usual animal allergies. Since these animals are known to have a life expectancy of 15 to 20 years, they require a long-term commitment. Given pigs are bred primarily as livestock and have not been bred as companion animals for very long, selective breeding for a placid or biddable temperament is not well established. Pigs have radically different psychology to dogs and exhibit fight-or-flight instincts, independent nature, and natural assertiveness.[91] Male and female swine that have not been de-sexed may express unwanted aggressive behavior, and are prone to developing serious health issues.[92] As rooting is found to be comforting, pigs kept in the house may root household objects, furniture or surfaces. Pet pigs should be let outside to allow them to fulfill their natural desire of rooting around. In 2023, China produced more pork than any other country, 55 million tonnes, followed by the European Union with 22.8 million tonnes and the United States with 12.5 million tonnes. Global production in 2023 was 120 million tonnes.[96] India, despite its large population, consumed under 0.3 million tonnes of pork in 2023.[97] International trade in pork (meat not consumed in the producing country) reached 13 million tonnes in 2020.[98] Uses Products Pigs are farmed primarily for meat, called pork. Pork is eaten in the form of pork chops, loin or rib roasts, shoulder joints, steaks, and loin (also called fillet). The many meat products made from pork include ham, bacon (mainly from the back and belly), and sausages.[99] Pork is further made into charcuterie products such as terrines, galantines, pâtés and confits.[100] Some sausages such as salami are fermented and air-dried, to be eaten raw. There are many types, the original Italian varieties including Genovese, Milanese, and Cacciatorino, with spicier kinds from the South of Italy including Calabrese, Napoletano, and Peperone.[101] In medicine The growth in publication of medical research papers using pigs and miniature pigs, and the research done on miniature pigs by organ system[104] Pigs, both as live animals and as a source of post-mortem tissues, are valuable animal models because of their biological, physiological, and anatomical similarities to human beings. For instance, human skin is very similar to the pigskin, therefore pigskin has been used in many preclinical studies.[105][106] Pigs are good non-human candidates for organ donation to humans, and in 2021 became the first animal to successfully donate an organ to a human body.[107][108] The procedure used a donor pig genetically engineered not to have a specific carbohydrate that the human body considers a threat–Galactose-alpha-1,3-galactose.[109] Pigs are good for human donation as the risk of cross-species disease transmission is reduced by the considerable phylogenetic distance from humans.[110] They are readily available, and the danger of creating new human diseases is low as domesticated pigs have been in close contact with humans for thousands of years.[111] On the environment As with the other forms of meat, producing pork is more energy-intensive than plant-based foods, and it is associated with more greenhouse gas emissions per calorie. However, emissions from pork are many times smaller than those of beef, veal and mutton, though larger than of chicken meat.[116] Intensive pig production is also associated with water pollution concerns, as the swine waste is often stored above ground in so-called lagoons. These lagoons typically have high levels of nitrogen and phosphorus, and can contain toxic heavy metals like zinc and copper, microbial pathogens, or hold elevated concentrations of pharmaceuticals from subtherapeutic antibiotic use in swine.[117] This wastewater from lagoons is liable to reach groundwater on farms, though there is little evidence for it reaching deeper into local drinking water supplies.[118] However, lagoon spills, such as from heavy rains in the wake of a hurricane, can lead to fish kills and algal blooms in local rivers.[117] In the United States, 35,000 mi (56,000 km) of river across over 20 states were estimated to have been contaminated by manure leakage as of 2015.[119] There is also evidence that evaporation from lagoons can cause nitrogen and phosphorus to spread through the air as dry particles then reach other water basins when they fall out through dry deposition. This process then also contributes to water eutrophication.[117] On animal welfare Intensive pig production involves practices such as castration, earmarking, tattooing for litter identification, tail docking, which are often done without the use of anesthetic.[120][121] Teeth clipping of piglets is also done to curtail cannibalism, behavioural instability and aggression, and tail biting, which are induced by the cramped environment.[122][123] In indoor farming, pigs are allowed to be kept with less than one square meter of space per pig.[124] Pigs often begin life in a farrowing or gestation crate, which is a small pen with a central cage, designed to allow the piglets to feed from their mother, the sow, while preventing her from moving around, crushing her children, and reducing aggression.[125] The crates are so small that the pigs cannot turn around.[126][127] While wild piglets remain with their mothers for around 12 to 14 weeks, farmed piglets are weaned and removed from their mothers at between two and five weeks old.[128][129] Of the piglets born alive, 10% to 18% will not reach weaning age, instead succumbing to disease, starvation, dehydration, or accidental crushing by their mothers.[122][130] Unusually small runt piglets are typically killed immediately by staff through blunt trauma to the head.[131][132] Further, intensive farming involves sows giving birth to large litter sizes at an unnatural frequency, which increases the rate of stillborn piglets, and causes as many as 25%-50% of sows to die of prolapse[133][134] In literature, both for children[142] and adults, pig characters appear in allegories, comic stories, and serious novels.[136][143][144] In art, pigs have been represented in a wide range of media and styles from the earliest times in many cultures.[145] Pig names are used in idioms and animal epithets, often derogatory, since pigs have long been linked with dirtiness and greed,[146][147] while places such as Swindon are named for their association with swine.[148] The eating of pork is forbidden in Islam and Judaism,[149][150] but pigs are sacred in some other religions.[151][152] ^Jeppesen, L.E. (1982). \"Teat-order in groups of piglets reared on an artificial sow. II. Maintenance of teat order with some evidence for the use of odour cues\". Applied Animal Ethology. 8 (4): 347–355. doi:10.1016/0304-3762(82)90067-0."}
{"url": "https://en.m.wikipedia.org/wiki/West_African_Vodun", "text": "Vodun cosmology centers around the vodun spirits and other elements of divine essence that govern the Earth, a hierarchy that range in power from major deities governing the forces of nature and human society to the spirits of individual streams, trees, and rocks, as well as dozens of ethnic vodun, defenders of a certain clan, tribe, or nation. The vodun are the center of religious life. Perceived similarities with Roman Catholic doctrines such as the intercession of saints and angels allowed Vodun to appear compatible with Catholicism, and helped produce syncretic religions such as Haitian Vodou. Adherents also emphasize ancestor worship and hold that the spirits of the dead live side by side with the world of the living, each family of spirits having its own female priesthood, sometimes hereditary when it is from mother to blood daughter. There is also an underlying philosophical framing underpinning Vodun which, according to Suzanne Preston Blier, who undertook a year of research in 1985–86 in Abomey and the nearby area, highlights the importance of remaining calm in contexts of difficulty and in life more generally. According to Suzanne Preston Blier, Vodun means, “the idea of staying close to a water source, to not rush through life, to take time to attain tranquility.” Her interpretation stems from two area diviners who maintain that its origins lie in the phrase “rest to draw the water,” from the Fon verbs vo “to rest,” and dun “to draw water,” the stoic suggestion of “the need for one to be calm and composed” in the face of adversity.[1] Patterns of Vodun worship follow various dialects, spirits, practices, songs, and rituals. The divine Creator, called variously Mawu or Mahu, is a female being. She is an elder woman, and usually a mother who is gentle and forgiving. She is also seen as the god who owns all other gods and even if there is no temple made in her name, the people continue to pray to her, especially in times of distress. In one tradition, she bore seven children. Sakpata: Vodun of the Earth, Xêvioso (or Xêbioso): Vodun of Thunder, also associated with Divine Justice,[2] Agbe: Vodun of the Sea, Gû: Vodun of Iron and War, Agê: Vodun of Agriculture and Forests, Jo: Vodun of Air, and Lêgba: Vodun of the Unpredictable.[3] The Creator embodies a dual cosmogonic principle of which Mawu the moon and Lisa the sun are respectively the female and male aspects, often portrayed as the twin children of the Creator.[4] In other stories, Mawu-Lisa is depicted as a single hermaphroditic person capable of impregnating herself, with two faces rather than being twins.[5] Lisa is the sun god who brings the day and the heat, and also strength and energy. Mawu, the moon goddess, provides the cool of the night, peace, fertility, and rain. To give this in a summed aspect, a proverb says ‘When Lisa punishes Mawu forgives.[6] In other branches, the Creator and other Vodus are known by a different name, such as Sakpo-Disa(Mawu), Aholu(Sakpata), and Anidoho(Da) among others in Gorovodu.[7] Legba is often represented as a phallus or as a man with a prominent phallus. Known as the youngest son of Mawu, he is the chief of all Vodun divinities;[8] in his Diasporic portrayal, Legba is believed to be a very old man who walks on crutches.[9] Being old he is seen as wise, but when seen as a child he is one who is rebellious. It is only through contact with Legba that it becomes possible to contact the other gods, for he is the guardian at the door of the spirits.[10] Dan, who is Mawu's androgynous son, is represented as a rainbow serpent, and was to remain with her and act as a go-between with her other creations. As the mediator between the spirits and the living, Dan maintains balance, order, peace and communication. All creation is considered divine and therefore contains the power of the divine. This is how medicines such as herbal remedies are understood, and explains the ubiquitous use of mundane objects in religious ritual. Vodun talismans, called \"fetishes\", are objects such as statues or dried animal or human parts that are sold for their healing and spiritually rejuvenating properties. Specifically, they are objects inhabited by spirits. The entities that inhabit a fetish are able to perform different tasks according to their stage of development. Fetish objects are often combined in the construction of \"shrines\", used to call forth specific vodun and their associated powers.[11] The Queen Mother is the first daughter of a matriarchal lineage of a family collective. She holds the right to lead the ceremonies incumbent to the clan: marriages, baptisms and funerals. She is one of the most important members of community. She will lead the women of a village when her family collective is the ruling one. They take part in the organisation and the running of markets and are also responsible for their upkeep. This is vitally important because marketplaces are the focal points for gatherings and social centres in their communities. In the past when the men of the villages would go to war, the Queen Mothers would lead prayer ceremonies in which all the women attended every morning to ensure the safe return of their menfolk. The high priestess is the woman chosen by the oracle to care for the convent. Priestesses, like priests, receive a calling from an oracle, which may come at any moment during their lives. They will then join their clan's convent to pursue spiritual instruction. It is also an oracle that will designate the future high priest and high priestess among the new recruits, establishing an order of succession within the convent. Only blood relatives were allowed in the family convent; strangers are forbidden. In modern days, however, some family members enter what is described as the first circle of worship. Strangers are allowed to worship only the spirits of the standard pantheon. About 17% of the population of Benin, some 1.6 million, people follow Vodun. (This does not count other traditional religions in Benin.) In addition, many of the 41.5% of the population that refer to themselves as \"Christian\" practice a syncretized religion, not dissimilar from Haitian Vodou or Brazilian Candomblé; indeed, many of them are descended from freed Brazilian slaves who settled on the coast near Ouidah.[12] In Togo, about half the population practices indigenous religions, of which Vodun is by far the largest, with some 2.5 million followers; there may be another million Vodunists among the Ewe of Ghana, as a 13% of the total Ghana population of 20 million are Ewe and 38% of Ghanaians practice traditional religion. According to census data, about 14 million people practice traditional religion in Nigeria, most of whom are Yoruba practicing Ifá, but no specific breakdown is available.[12] European colonialism, followed by some of the totalitarian regimes in West Africa, have tried to suppress Vodun as well as other traditional religions.[13] However, because the Vodun deities are born to each clan, tribe, and nation, and their clergy are central to maintaining the moral, social and political order and ancestral foundation of its village, these efforts have not been successful. Recently there have been moves to restore the place of Vodun in national society, such as an annual International Vodun Conference that has been held in the city of Ouidah, Benin since 1991.[14] The syncretism of Vodun and Christianity was created by connecting the traditional West African Vodun and Christianity in Benin.[15] Adherents are mainly found in Benin, Togo and Nigeria.[16] Syncretism in the religious domain is the merging of two or more originally distinct religious traditions.[17] Similar syncretic religions are also found in the surrounding countries,[18] where it is a connection between the Yoruba religion and Christianity,[19] the Odinala religion and Christianity, the Bori religion and Islam, or the Bwiti religion and Christianity.[20] In Benin, in addition to the followers of syncretism, there is a large group of people who profess Vodun and Christianity without mixing.[15] This is a – common in Africa – multiple religious belonging.[18] Various syncretisms and eclecticisms are common in West Africa.[21] In addition to Christian services (mostly, but not exclusively, in Benin), believers also visit Vodun initiates, use traditional household protection fetishes and personal protective gris-gris amulets. They cultivate respect for deceased ancestors and communicate with the spirit world with the help of a Vodun priest (vodunon).[15] The syncretism of Vodun and Christianity arose just like Vodun itself in Benin,[15] but similar syncretisms also arose overseas, when Vodun reached the Caribbean together with slaves, where its syncretism with Catholic Christianity gave rise to Caribbean Voodoo similar to Brazilian Candomble and Cuban Santería.[22][15]"}
{"url": "https://en.m.wikipedia.org/wiki/Angolan_Civil_War", "text": "The MPLA and UNITA had different roots in Angolan society and mutually incompatible leaderships, despite their shared aim of ending colonial rule. A third movement, the National Front for the Liberation of Angola (FNLA), having fought the MPLA with UNITA during the Angolan War of Independence, played almost no role in the Civil War. Additionally, the Front for the Liberation of the Enclave of Cabinda (FLEC), an association of separatist militant groups, fought for the independence of the province of Cabinda from Angola.[citation needed] With the assistance of Cuban soldiers and Soviet support, the MPLA managed to win the initial phase of conventional fighting, oust the FNLA from Luanda, and become the de facto Angolan government.[41] The FNLA disintegrated, but the U.S.- and South Africa-backed UNITA continued its irregular warfare against the MPLA government from its base in the east and south of the country. The 27-year war can be divided roughly into three periods of major fighting – from 1975 to 1991, 1992 to 1994 and from 1998 to 2002 – with fragile periods of peace. By the time the MPLA achieved victory in 2002, between 500,000 and 800,000 people had died and over one million had been internally displaced.[39][42] The war devastated Angola's infrastructure and severely damaged public administration, the economy, and religious institutions. The Angolan Civil War was notable due to the combination of Angola's violent internal dynamics and the exceptional degree of foreign military and political involvement. The war is widely considered a Cold War proxy conflict, as the Soviet Union and the United States, with their respective allies Cuba and South Africa, assisted the opposing factions.[43] The conflict became closely intertwined with the Second Congo War in the neighbouring Democratic Republic of the Congo and the South African Border War. Land mines still litter the countryside and contribute to the ongoing civilian casualties.[39] Main combatants Angola's three rebel movements had their roots in the anti-colonial movements of the 1950s.[43] The MPLA was primarily an urban-based movement in Luanda and its surrounding area.[43] It was largely composed of Mbundu people. By contrast, the other two major anti-colonial movements, the FNLA and UNITA, were rural groups.[43] The FNLA primarily consisted of Bakongo people from Northern Angola. UNITA, an offshoot of the FNLA, was mainly composed of Ovimbundu people, Angola's largest ethnic group, from the Bié Plateau.[43] MPLA Since its formation in the 1950s, the MPLA's main social base has been among the Ambundu people and the multiracial intelligentsia of cities such as Luanda, Benguela and Huambo.[note 3] During its anti-colonial struggle of 1962–1974, the MPLA was supported by several African countries and the Soviet Union. Cuba became the MPLA's strongest ally, sending significant combat and support personnel contingents to Angola. This support, as well as that of several other countries of the Eastern Bloc, e.g. East Germany,[44] was maintained during the Civil War. Yugoslavia provided financial military support for the MPLA, including $14 million in 1977, as well as Yugoslav security personnel in the country and diplomatic training for Angolans in Belgrade.[45] The United States Ambassador to Yugoslavia wrote of the Yugoslav relationship with the MPLA and remarked, \"Tito clearly enjoys his role as patriarch of guerrilla liberation struggle.\" Agostinho Neto, MPLA's leader during the civil war, declared in 1977 that Yugoslav aid was constant and firm and described the help as extraordinary.[46] According to a November 1978 special communique, Portuguese troops were among the 20,000 MPLA troops that participated in a major offensive in central and southern Angola.[47] FNLA The FNLA formed parallel to the MPLA[48] and was initially devoted to defending the interests of the Bakongo people and supporting the restoration of the historical Kongo Empire. It rapidly developed into a nationalist movement, supported in its struggle against Portugal by the government of Mobutu Sese Seko in Zaire. During 1974, the FNLA was also briefly supported by the People's Republic of China; but the aid was quickly withdrawn since China mainly supported the UNITA during the Angolan War of Independence. The United States refused to support the FNLA during the movement's war against Portugal, a NATO member but agreed during the civil war.[citation needed] UNITA UNITA's main social basis were the Ovimbundu of central Angola, who constituted about one-third of the country's population, but the organization also had roots among several less numerous peoples of eastern Angola. UNITA was founded in 1966 by Jonas Savimbi, who until then had been a prominent leader of the FNLA. During the anti-colonial war, UNITA received some support from the People's Republic of China. With the onset of the civil war, the United States decided to support UNITA and considerably augmented their aid to UNITA in the following decades. In the latter period, UNITA's main ally was the apartheid regime of South Africa.[49][50] Angola, like most African countries, became constituted as a nation through colonial intervention. Angola's colonial power was Portugal, which was present and active in the territory, in one way or another, for over four centuries. Ethnic divisions Map of Angola's major ethnic groups, c.1970 The original population of this territory were dispersed Khoisan groups. These were absorbed or pushed southwards, where residual groups still exist, by a massive influx of Bantu people who came from the north and east. The Bantu influx began around 500 BC, and some continued their migrations inside the territory well into the 20th century. They established a number of major political units, of which the most important was the Kongo Empire, whose centre was located in the northwest of what today is Angola and which stretched northwards into the west of the present Democratic Republic of the Congo (DRC), the south and west of the contemporary Republic of Congo and even the southernmost part of Gabon. Also of historical importance were the Ndongo and Matamba kingdoms to the south of the Kongo Empire, in the Ambundu area. Additionally, the Lunda Empire occupied a portion of north-eastern Angola in the south-east of the present-day DRC. In the south of the territory, and the north of present-day Namibia, lay the Kwanyama kingdom, along with minor realms on the central highlands. All these political units were a reflection of ethnic cleavages that slowly developed among the Bantu populations and were instrumental in consolidating these cleavages and fostering the emergence of new and distinct social identities. Portuguese colonialism At the end of the 15th century, Portuguese settlers made contact with the Kongo Empire, maintaining a continuous presence in its territory and enjoying considerable cultural and religious influence after that. In 1575, Portugal established a settlement and fort called Saint Paul of Luanda on the coast south of the Kongo Empire, in an area inhabited by Ambundu people. Another fort, Benguela, was established on the coast further south, in a region inhabited by ancestors of the Ovimbundu people. Neither of these Portuguese settlement efforts was launched for the purpose of territorial conquest. Both gradually came to occupy and farm a broad area around their initial bridgeheads (in the case of Luanda, mostly along the lower Kwanza River). Their main function was in the Atlantic slave trade. Slaves were bought from African intermediaries and sold to Portuguese colonies in Brazil and the Caribbean. In addition, Benguela developed commerce in ivory, wax, and honey, which they bought from Ovimbundu caravans which fetched these goods from among the Ganguela peoples in the eastern part of what is now Angola.[note 4] Nonetheless, the Portuguese presence on the Angolan coast remained limited for much of the colonial period. The degree of real colonial settlement was minor, and, with few exceptions, the Portuguese did not interfere by means other than commercial in the social and political dynamics of the native peoples. There was no real delimitation of territory; Angola, to all intents and purposes, did not yet exist. In the 19th century, the Portuguese began a more serious program of advancing into the continental interior. They wanted a de facto overlordship that allowed them to establish commercial networks and a few settlements. In this context, they also moved further south along the coast and founded the \"third bridgehead\" of Moçâmedes. In the course of this expansion, they entered into conflict with several of the African political units.[51] Territorial occupation only became a central concern for Portugal in the last decades of the 19th century, during the European powers' \"Scramble for Africa\", especially following the 1884 Berlin Conference. Several military expeditions were organized as preconditions for obtaining territory, which roughly corresponded to present-day Angola. By 1906, about 6% of that territory was effectively occupied, and the military campaigns had to continue. By the mid-1920s, the limits of the territory were finally fixed, and the last \"primary resistance\" was quelled in the early 1940s. It is thus reasonable to talk of Angola as a defined territorial entity from this point onwards. Build-up to independence and rising tensions Portuguese Army soldiers operating in the Angolan jungle in the early 1960s In 1961, the FNLA and the MPLA, based in neighbouring countries, began a guerrilla campaign against Portuguese rule on several fronts. The Portuguese Colonial War, which included the Angolan War of Independence, lasted until the Portuguese regime's overthrow in 1974 through a leftist military coup in Lisbon. When the timeline for independence became known, most of the roughly 500,000 ethnic Portuguese Angolans fled the territory during the weeks before or after that deadline. Portugal left behind a newly independent country whose population was mainly composed of Ambundu, Ovimbundu, and Bakongo peoples. The Portuguese that lived in Angola accounted for the majority of the skilled workers in public administration, agriculture, and industry; once they fled the country, the national economy began to sink into depression.[52] The South African government initially became involved in an effort to counter the Chinese presence in Angola, which was feared might escalate the conflict into a local theatre of the Cold War. In 1975, South African Prime Minister B.J. Vorster authorized Operation Savannah,[53] which began as an effort to protect engineers constructing the dam at Calueque after unruly UNITA soldiers took over. The dam, paid for by South Africa, was felt to be at risk.[54] The South African Defence Force (SADF) dispatched an armoured task force to secure Calueque. From this, Operation Savannah escalated; no formal government was in place and thus, no clear lines of authority.[55] The South Africans came to commit thousands of soldiers to the intervention and ultimately clashed with Cuban forces assisting the MPLA. 1970s Independence After the Carnation Revolution in Lisbon and the end of the Angolan War of Independence, the parties of the conflict signed the Alvor Accords on 15 January 1975. In July 1975, the MPLA violently forced the FNLA out of Luanda, and UNITA voluntarily withdrew to its stronghold in the south. By August, the MPLA had control of 11 of the 15 provincial capitals, including Cabinda and Luanda. South Africa intervened on 23 October, sending between 1,500 and 2,000 troops from Namibia into southern Angola in order to support the FNLA and UNITA. Zaire, in a bid to install a pro-Kinshasa government and thwart the MPLA's drive for power, deployed armored cars, paratroopers, and three infantry battalions to Angola in support of the FNLA.[56] Within three weeks, South African and UNITA forces had captured five provincial capitals, including Novo Redondo and Benguela. In response to the South African intervention, Cuba sent 18,000 soldiers as part of a large-scale military intervention nicknamed Operation Carlota in support of the MPLA. Cuba had initially provided the MPLA with 230 military advisers prior to the South African intervention.[57] The Cuban intervention proved decisive in repelling the South African-UNITA advance. The FNLA were likewise routed at the Battle of Quifangondo and forced to retreat towards Zaire.[58][59] The defeat of the FNLA allowed the MPLA to consolidate power over the capital Luanda. In early November 1975, the South African government warned Savimbi and Roberto that the South African Defence Force (SADF) would soon end operations in Angola despite the failure of the coalition to capture Luanda and therefore secure international recognition for their government. Savimbi, desperate to avoid the withdrawal of South Africa, asked General Constand Viljoen to arrange a meeting for him with Prime Minister of South AfricaJohn Vorster, who had been Savimbi's ally since October 1974. On the night of 10 November, the day before the formal declaration of independence, Savimbi secretly flew to Pretoria to meet Vorster. In a reversal of policy, Vorster not only agreed to keep his troops in Angola through November, but also promised to withdraw the SADF only after the OAU meeting on 9 December.[64][65] While Cuban officers led the mission and provided the bulk of the troop force, 60 Soviet officers in the Congo joined the Cubans on 12 November. The Soviet leadership expressly forbade the Cubans from intervening in Angola's civil war, focusing the mission on containing South Africa.[66] In 1975 and 1976 most foreign forces, with the exception of Cuba, withdrew. The last elements of the Portuguese military withdrew in 1975[67] and the South African military withdrew in February 1976.[68] Cuba's troop force in Angola increased from 5,500 in December 1975 to 11,000 in February 1976.[69] Sweden provided humanitarian assistance to both the SWAPO and the MPLA in the mid-1970s,[70][71][72] and regularly raised the issue of UNITA in political discussions between the two movements. Cuban intervention Cuban logistics were primitive, relying on a few aging commercial aircraft, small cargo ships, and large fishing vessels to support a major, long-range military operation.[73] In early September 1975, the Cuban merchant ships Viet Nam Heroico, Isla Coral, and La Plata, loaded with troops, vehicles, and 1,000 tons of gasoline, crossed the Atlantic and sailed to Angola. The United States held a secret, high-level talk with Cuba to express its consternation over Cuba's actions, but this had little effect. The Cuban troops landed in early October. On 7 November, Cuba began a thirteen-day airlift of a 650-man special forces battalion. The Cubans used old Bristol Britannia turboprop aircraft, making refueling stops in Barbados, Guinea-Bissau, and the Congo before landing in Luanda. The troops traveled as \"tourists,\" carrying machine guns in briefcases. They packed 75mm cannons, 82mm mortars, and small arms into the aircraft's cargo holds.[73] Operation Savannah. On 14 October, four South African columns totaling 3,000 troops launched Operation Savannah in an attempt to capture Luanda from the south. The Cubans suffered major reversals, including one at Catofe, where South African forces surprised them and caused numerous casualties.[74] However, the Cubans ultimately halted the South African advance by 26 November.[34] Later, another 4,000 South African soldiers entered southern Angola to establish a buffer zone along the Namibian border. The MPLA received support from 3,000 Katangan exiles, a Mozambican battalion, 3,000 East German personnel, and 1,000 Soviet advisors. The pivotal intervention came from 18,000 Cuban troops, who defeated the FNLA in the north and UNITA in the south, concluding the conventional war by 12 February 1976.[34] In Cabinda, the Cubans launched a series of successful operations against the FLEC separatist movement.[75] By March 1977, the MPLA controlled enough of the country to permit Castro to pay a state visit. However, in May, Nito Alves and José Van Dunem attempted an unsuccessful coup against Agostinho Neto. Cuban troops helped defeat the rebels.[73] In May 1978, South Africa initiated Operation Reindeer, during which an airstrike on a Cuban convoy resulted in the loss of 150 Cuban troops. By July 1978, Cuba had suffered 5,600 casualties in its African wars (Angola and Ethiopia), including 1,000 killed in Angola and 400 killed against Somali forces in the Ethiopian Ogaden.[34] In 1987, 6,000 South African soldiers reentered the Angolan war, clashing with Cuban forces. They defeated four MPLA brigades at the Lomba River in September and laid siege to Cuito Cuanavale for months until 12,000 Cuban troops broke the blockade in March 1988.[34] On 26 June, South African forces engaged Cuban forces at Techipa, killing several Cuban troops. In response, Cuba launched an air strike on SADF positions the following day, killing nearly a dozen South African troops. Both sides promptly withdrew to prevent further escalation of hostilities. Two days before the program's approval, Nathaniel Davis, the Assistant Secretary of State, told Henry Kissinger, the Secretary of State, that he believed maintaining the secrecy of IA Feature would be impossible. Davis correctly predicted the Soviet Union would respond by increasing involvement in the Angolan conflict, leading to more violence and negative publicity for the United States. When Ford approved the program, Davis resigned.[78]John Stockwell, the CIA's station chief in Angola, echoed Davis' criticism saying that success required the expansion of the program, but its size already exceeded what could be hidden from the public eye. Davis' deputy, former U.S. ambassador to Chile Edward Mulcahy, also opposed direct involvement. Mulcahy presented three options for U.S. policy towards Angola on 13 May 1975. Mulcahy believed the Ford administration could use diplomacy to campaign against foreign aid to the communist MPLA, refuse to take sides in factional fighting, or increase support for the FNLA and UNITA. He warned that supporting UNITA would not sit well with Mobutu Sese Seko, the president of Zaire.[76][79] The U.S. government vetoed Angolan entry into the United Nations on 23 June 1976.[86] Zambia forbade UNITA from launching attacks from its territory on 28 December 1976[87] after Angola under MPLA rule became a member of the United Nations.[88] According to Ambassador William Scranton, the United States abstained from voting on the issue of Angola becoming a UN member state \"out of respect for the sentiments expressed by its [our] African friends\".[89] Shaba invasions About 1,500 members of the Congolese National Liberation Front (FNLC) invaded the Shaba Province (modern-day Katanga Province) in Zaire from eastern Angola on 7 March 1977. The FNLC wanted to overthrow Mobutu, and the MPLA government, suffering from Mobutu's support for the FNLA and UNITA, did not try to stop the invasion. The FNLC failed to capture Kolwezi, Zaire's economic heartland, but took Kasaji and Mutshatsha. The Zairean army (the Forces Armées Zaïroises) was defeated without difficulty and the FNLC continued to advance. On 2 April, Mobutu appealed to William Eteki of Cameroon, Chairman of the Organization of African Unity, for assistance. Eight days later, the French government responded to Mobutu's plea and airlifted 1,500 Moroccan troops into Kinshasa. This force worked in conjunction with the Zairean army, the FNLA[90] and Egyptian pilots flying French-made Zairean Mirage fighter aircraft to beat back the FNLC. The counter-invasion force pushed the last of the militants, along with numerous refugees, into Angola and Zambia in April 1977.[91][92][93][94] Mobutu accused the MPLA, Cuban and Soviet governments of complicity in the war.[95] While Neto did support the FNLC, the MPLA government's support came in response to Mobutu's continued support for Angola's FNLA.[96] The Carter Administration, unconvinced of Cuban involvement, responded by offering a meager $15 million-worth of non-military aid. American timidity during the war prompted a shift in Zaire's foreign policy towards greater engagement with France, which became Zaire's largest supplier of arms after the intervention.[97] Neto and Mobutu signed a border agreement on 22 July 1977.[98] John Stockwell, the CIA's station chief in Angola, resigned after the invasion, explaining in the April 1977 The Washington Post article \"Why I'm Leaving the CIA\" that he had warned Secretary of State Henry Kissinger that continued American support for anti-government rebels in Angola could provoke a war with Zaire. He also said that covert Soviet involvement in Angola came after, and in response to, U.S. involvement.[99] The FNLC invaded Shaba again on 11 May 1978, capturing Kolwezi in two days. While the Carter Administration had accepted Cuba's insistence on its non-involvement in Shaba I, and therefore did not stand with Mobutu, the U.S. government now accused Castro of complicity.[100] This time, when Mobutu appealed for foreign assistance, the U.S. government worked with the French and Belgian militaries to beat back the invasion, the first military cooperation between France and the United States since the Vietnam War.[101][102] The French Foreign Legion took back Kolwezi after a seven-day battle and airlifted 2,250 European citizens to Belgium, but not before the FNLC massacred 80 Europeans and 200 Africans. In one instance, the FNLC killed 34 European civilians who had hidden in a room. The FNLC retreated to Zambia, vowing to return to Angola. The Zairean army then forcibly evicted civilians along Shaba's border with Angola. Mobutu, wanting to prevent any chance of another invasion, ordered his troops to shoot on sight.[103] U.S.-mediated negotiations between the MPLA and Zairean governments led to a peace accord in 1979 and an end to support for insurgencies in each other's respective countries. Zaire temporarily cut off support to the FLEC, the FNLA and UNITA, and Angola forbade further activity by the FNLC.[101] Nitistas By the late 1970s, Interior Minister Nito Alves had become a powerful member of the MPLA government. Alves had successfully put down Daniel Chipenda's Eastern Revolt and the Active Revolt during Angola's War of Independence. Factionalism within the MPLA became a major challenge to Neto's power by late 1975 and Neto gave Alves the task of once again clamping down on dissent. Alves shut down the Cabral and Henda Committees while expanding his influence within the MPLA through his control of the nation's newspapers and state-run television. Alves visited the Soviet Union in October 1976, and may have obtained Soviet support for a coup against Neto. By the time he returned, Neto had grown suspicious of Alves' growing power and sought to neutralize him and his followers, the Nitistas. Neto called a plenum meeting of the Central Committee of the MPLA. Neto formally designated the party as Marxist-Leninist, abolished the Interior Ministry (of which Alves was the head), and established a Commission of Enquiry. Neto used the commission to target the Nitistas, and ordered the commission to issue a report of its findings in March 1977. Alves and Chief of Staff José Van-Dunem, his political ally, began planning a coup d'état against Neto.[104] Alves and Van-Dunem planned to arrest Neto on 21 May before he arrived at a meeting of the Central Committee and before the commission released its report on the activities of the Nitistas. The MPLA changed the location of the meeting shortly before its scheduled start, throwing the plotters' plans into disarray. Alves attended anyway. The commission released its report, accusing him of factionalism. Alves fought back, denouncing Neto for not aligning Angola with the Soviet Union. After twelve hours of debate, the party voted 26 to 6 to dismiss Alves and Van-Dunem from their positions.[104] In support of Alves and the coup, the People's Armed Forces for the Liberation of Angola (FAPLA) 8th Brigade broke into São Paulo prison on 27 May, killing the prison warden and freeing more than 150 Nitistas. The 8th brigade then took control of the radio station in Luanda and announced their coup, calling themselves the MPLA Action Committee. The brigade asked citizens to show their support for the coup by demonstrating in front of the presidential palace. The Nitistas captured Bula and Dangereaux, generals loyal to Neto, but Neto had moved his base of operations from the palace to the Ministry of Defence in fear of such an uprising. Cuban troops loyal to Neto retook the palace and marched to the radio station. The Cubans succeeded in taking the radio station and proceeded to the barracks of the 8th Brigade, recapturing it by 1:30 pm. While the Cuban force captured the palace and radio station, the Nitistas kidnapped seven leaders within the government and the military, shooting and killing six.[105] The MPLA government arrested tens of thousands of suspected Nitistas from May to November and tried them in secret courts overseen by Defense Minister Iko Carreira. Those who were found guilty, including Van-Dunem, Jacobo \"Immortal Monster\" Caetano, the head of the 8th Brigade, and political commissar Eduardo Evaristo, were shot and buried in secret graves. At least 2,000 followers (or alleged followers) of Nito Alves were estimated to have been killed by Cuban and MPLA troops in the aftermath, with some estimates claiming as high as 90,000 dead. Amnesty International estimated 30,000 died in the purge.[106][107][108][109] The coup attempt had a lasting effect on Angola's foreign relations. Alves had opposed Neto's foreign policy of non-alignment, evolutionary socialism, and multiracialism, favoring stronger relations with the Soviet Union, which Alves wanted to grant military bases in Angola. While Cuban soldiers actively helped Neto put down the coup, Alves and Neto both believed the Soviet Union opposed Neto. Cuban Armed Forces Minister Raúl Castro sent an additional four thousand troops to prevent further dissension within the MPLA's ranks and met with Neto in August in a display of solidarity. In contrast, Neto's distrust of the Soviet leadership increased and relations with the USSR worsened.[105] In December, the MPLA held its first party Congress and changed its name to the MPLA-Worker's Party (MPLA-PT). The Nitista attempted coup took a toll on the MPLA's membership. In 1975, the MPLA had reached 200,000 members, but after the first party congress, that number decreased to 30,000.[104][110][111][112][113] Replacing Neto The Soviets tried to increase their influence, wanting to establish permanent military bases in Angola,[114] but despite persistent lobbying, especially by the Soviet chargé d'affaires, G. A. Zverev, Neto stood his ground and refused to allow the construction of permanent military bases. With Alves no longer a possibility, the Soviet Union backed Prime Minister Lopo do Nascimento against Neto for the MPLA's leadership.[115] Neto moved swiftly, getting the party's Central Committee to fire Nascimento from his posts as Prime Minister, Secretary of the Politburo, Director of National Television, and Director of Jornal de Angola. Later that month, the positions of Prime Minister and Deputy Prime Minister were abolished.[116] Neto diversified the ethnic composition of the MPLA's political bureau as he replaced the hardline old guard with new blood, including José Eduardo dos Santos.[117] When he died on 10 September 1979, the party's Central Committee unanimously voted to elect dos Santos as president.[citation needed] 1980s South African paratroopers on patrol near the border region, mid-1980s. Under dos Santos's leadership, Angolan troops crossed the border into Namibia for the first time on 31 October, going into Kavango. The next day, dos Santos signed a non-aggression pact with Zambia and Zaire.[118] In the 1980s, fighting spread outward from southeastern Angola, where most of the fighting had taken place in the 1970s, as the National Congolese Army (ANC) and SWAPO increased their activity. The South African government responded by sending troops back into Angola, intervening in the war from 1981 to 1987,[68] prompting the Soviet Union to deliver massive amounts of military aid from 1981 to 1986. The USSR gave the MPLA more than US$2 billion in aid in 1984.[119] In 1981, newly elected United States President Ronald Reagan's U.S. assistant secretary of state for African affairs, Chester Crocker, developed a linkage policy, tying Namibian independence to Cuban withdrawal and peace in Angola.[120][121] Beginning with 1979, Romania trained Angolan guerrillas. Every 3–4 months, Romania sent two airplanes to Angola, each returning with 166 recruits. These were taken back to Angola after they completed their training. In addition to guerrilla training, Romania also instructed young Angolans as pilots. In 1979, under the command of Major General Aurel Niculescu [ro], Romania founded an air academy in Angola. There were around 100 Romanian instructors in this academy, with about 500 Romanian soldiers guarding the base, which supported 50 aircraft used to train Angolan pilots. The aircraft models used were: IAR 826, IAR 836, EL-29, MiG-15 and AN-24.[122][123] Designated as the \"Commander Bula National Military Aviation School\", it was set up on 11 February 1981 in Negage. The facility trained air force pilots, technicians and General Staff officers. The Romanian teaching staff was gradually replaced by Angolans.[124] The South African military attacked insurgents in Cunene Province on 12 May 1980. The Angolan Ministry of Defense accused the South African government of wounding and killing civilians. Nine days later, the SADF attacked again, this time in Cuando-Cubango, and the MPLA threatened to respond militarily. The SADF launched a full-scale invasion of Angola through Cunene and Cuando-Cubango on 7 June, destroying SWAPO's operational command headquarters on 13 June, in what Prime Minister Pieter Willem Botha described as a \"shock attack\". The MPLA government arrested 120 Angolans who were planning to set off explosives in Luanda, on 24 June, foiling a plot purportedly orchestrated by the South African government. Three days later, the United Nations Security Council convened at the behest of Angola's ambassador to the UN, E. de Figuerido, and condemned South Africa's incursions into Angola. President Mobutu of Zaire also sided with the MPLA. The MPLA government recorded 529 instances in which they claim South African forces violated Angola's territorial sovereignty between January and June 1980.[125] In 1984, five Mexican nationals (who are doing missionary work) were kidnapped by UNITA in 1984. The nuns were later released through negotiations by the International Red Cross. In response to the incident, Mexican Foreign Minister Bernardo Sepúlveda Amor had visited Angola in 1985 to support the MPLA to (both diplomatically and militarily) protect Mexican nationals.[20] We, free peoples fighting for our national independence and human rights, assembled at Jamba, declare our solidarity with all freedom movements in the world and state our commitment to cooperate to liberate our nations from the Soviet Imperialists. The United States House of Representatives voted 236 to 185 to repeal the Clark Amendment on 11 July 1985.[128] The MPLA government began attacking UNITA later that month from Luena towards Cazombo along the Benguela Railway in a military operation named Congresso II, taking Cazombo on 18 September. The MPLA government tried unsuccessfully to take UNITA's supply depot in Mavinga from Menongue. While the attack failed, very different interpretations of the attack emerged. UNITA claimed Portuguese-speaking Soviet officers led FAPLA troops while the government said UNITA relied on South African paratroopers to defeat the MPLA attack. The South African government admitted to fighting in the area, but said its troops fought SWAPO militants.[129] War intensifies By 1986, Angola began to assume a more central role in the Cold War, with the Soviet Union, Cuba and other Eastern bloc nations enhancing support for the MPLA government, and American conservatives beginning to elevate their support for Savimbi's UNITA. Savimbi developed close relations with influential American conservatives, who saw Savimbi as a key ally in the U.S. effort to oppose and rollback Soviet-backed, undemocratic governments around the world. The conflict quickly escalated, with both Washington and Moscow seeing it as a critical strategic conflict in the Cold War.[citation needed] Maximum extent of South African and UNITA operations in Angola and Zambia The Soviet Union gave an additional $1 billion in aid to the MPLA government and Cuba sent an additional 2,000 troops to the 35,000-strong force in Angola to protect Chevron oil platforms in 1986.[129] Savimbi had called Chevron's presence in Angola, already protected by Cuban troops, a \"target\" for UNITA in an interview with Foreign Policy magazine on 31 January.[130] In Washington, Savimbi forged close relationships with influential conservatives, including Michael Johns (The Heritage Foundation's foreign policy analyst and a key Savimbi advocate), Grover Norquist (President of Americans for Tax Reform and a Savimbi economic advisor), and others, who played critical roles in elevating escalated U.S. covert aid to Savimbi's UNITA and visited with Savimbi in his Jamba, Angola headquarters to provide the Angolan rebel leader with military, political and other guidance in his war against the MPLA government. With enhanced U.S. support, the war quickly escalated, both in terms of the intensity of the conflict and also in its perception as a key conflict in the overall Cold War.[131][132] In addition to escalating its military support for UNITA, the Reagan administration and its conservative allies also worked to expand recognition of Savimbi as a key U.S. ally in an important Cold War struggle. In January 1986, Reagan invited Savimbi to a meeting at the White House. Following the meeting, Reagan spoke of UNITA as winning a victory that \"electrifies the world\". Two months later, Reagan announced the delivery of Stinger surface-to-air missiles as part of the $25 million in aid UNITA received from the U.S. government.[120][133]Jeremias Chitunda, UNITA's representative to the U.S., became the Vice President of UNITA in August 1986 at the sixth party congress.[134] Fidel Castro made Crocker's proposal—the withdrawal of foreign troops from Angola and Namibia—a prerequisite to Cuban withdrawal from Angola on 10 September. UNITA forces attacked Camabatela in Cuanza Norte province on 8 February 1986. ANGOP alleged UNITA massacred civilians in Damba in Uíge Province later that month, on 26 February. The South African government agreed to Crocker's terms in principle on 8 March. Savimbi proposed a truce regarding the Benguela railway on 26 March, saying MPLA trains could pass through as long as an international inspection group monitored trains to prevent their use for counter-insurgency activity. The government did not respond. In April 1987, Fidel Castro sent Cuba's Fiftieth Brigade to southern Angola, increasing the number of Cuban troops from 12,000 to 15,000.[135] The MPLA and American governments began negotiating in June 1987.[136][137] Cuito Cuanavale and New York Accords UNITA and South African forces attacked the MPLA's base at Cuito Cuanavale in Cuando Cubango province from 13 January to 23 March 1988, in the second-largest battle in the history of Africa,[138] after the Battle of El Alamein,[139] the largest in sub-Saharan Africa since World War II.[140] Cuito Cuanavale's importance came not from its size or its wealth but its location. South African Defence Forces maintained an overwatch on the city using new, G5 artillery pieces. Both sides claimed victory in the ensuing Battle of Cuito Cuanavale.[120][141][142][143] Map of Angola's provinces, with Cuando Cubango province highlighted. After the indecisive results of the Battle of Cuito Cuanavale, Fidel Castro claimed that the increased cost of continuing to fight for South Africa had placed Cuba in its most aggressive combat position of the war, arguing that he was preparing to leave Angola with his opponents on the defensive. According to Cuba, the political, economic and technical cost to South Africa of maintaining its presence in Angola proved too much. Conversely, the South Africans believe that they indicated their resolve to the superpowers by preparing a nuclear test that ultimately forced the Cubans into a settlement.[144] Cuban troops were alleged to have used nerve gas against UNITA troops during the civil war. Belgian criminal toxicologist Dr. Aubin Heyndrickx, studied alleged evidence, including samples of war-gas \"identification kits\" found after the battle at Cuito Cuanavale, claimed that \"there is no doubt anymore that the Cubans were using nerve gases against the troops of Mr. Jonas Savimbi.\"[145] The Cuban government joined negotiations on 28 January 1988, and all three parties held a round of negotiations on 9 March. The South African government joined negotiations on 3 May and the parties met in June and August in New York and Geneva. All parties agreed to a ceasefire on 8 August. Representatives from the governments of Angola, Cuba, and South Africa signed the New York Accords, granting independence to Namibia and ending the direct involvement of foreign troops in the civil war, in New York City on 22 December 1988.[120][137] The United Nations Security Council passed Resolution 626 later that day, creating the United Nations Angola Verification Mission (UNAVEM), a peacekeeping force. UNAVEM troops began arriving in Angola in January 1989.[146] Ceasefire As the Angolan Civil War began to take on a diplomatic component, in addition to a military one, two key Savimbi allies, The Conservative Caucus' Howard Phillips and the Heritage Foundation's Michael Johns visited Savimbi in Angola, where they sought to persuade Savimbi to come to the United States in the spring of 1989 to help the Conservative Caucus, the Heritage Foundation and other conservatives in making the case for continued U.S. aid to UNITA.[147] President Mobutu invited 18 African leaders, Savimbi, and dos Santos to his palace in Gbadolite in June 1989 for negotiations. Savimbi and dos Santos met for the first time and agreed to the Gbadolite Declaration, a ceasefire, on 22 June, paving the way for a future peace agreement.[148][149] President Kenneth Kaunda of Zambia said a few days after the declaration that Savimbi had agreed to leave Angola and go into exile, a claim Mobutu, Savimbi, and the U.S. government disputed.[149] Dos Santos agreed with Kaunda's interpretation of the negotiations, saying Savimbi had agreed to temporarily leave the country.[150] On 23 August, dos Santos complained that the U.S. and South African governments continued to fund UNITA, warning such activity endangered the already fragile ceasefire. The next day Savimbi announced UNITA would no longer abide by the ceasefire, citing Kaunda's insistence that Savimbi leave the country and UNITA disband. The MPLA government responded to Savimbi's statement by moving troops from Cuito Cuanavale, under MPLA control, to UNITA-occupied Mavinga. The ceasefire broke down with dos Santos and the U.S. government blaming each other for the resumption in armed conflict.[151] 1990s Political changes abroad and military victories at home allowed the government to transition from a nominally communist state to a nominally democratic one. Namibia's declaration of independence, internationally recognized on 1 April, eliminated the threat to the MPLA from South Africa, as the SADF withdrew from Namibia.[152] The MPLA abolished the one-party system in June and rejected Marxist-Leninism at the MPLA's third Congress in December, formally changing the party's name from the MPLA-PT to the MPLA.[148] The National Assembly passed law 12/91 in May 1991, coinciding with the withdrawal of the last Cuban troops, defining Angola as a \"democratic state based on the rule of law\" with a multi-party system.[153] Observers met such changes with skepticism. American journalist Karl Maier wrote: \"In the New Angola ideology is being replaced by the bottom line, as security and selling expertise in weaponry have become a very profitable business. With its wealth in oil and diamonds, Angola is like a big swollen carcass and the vultures are swirling overhead. Savimbi's former allies are switching sides, lured by the aroma of hard currency.\"[154] Savimbi also reportedly purged some of those within UNITA whom he may have seen as threats to his leadership or as questioning his strategic course. Among those killed in the purge were Tito Chingunji and his family in 1991. Savimbi denied his involvement in the Chingunji killing and blamed it on UNITA dissidents.[155] Black, Manafort, and Stone Government troops wounded Savimbi in battles in January and February 1990, but not enough to restrict his mobility.[156] He went to Washington, D.C., in December and met with President George H. W. Bush again,[148] the fourth of five trips he made to the United States. Savimbi paid Black, Manafort, Stone, and Kelly, a lobbying firm based in Washington, D.C., $5 million to lobby the Federal government for aid, portray UNITA favorably in Western media, and acquire support among politicians in Washington. Savimbi was highly successful in this endeavour. The weapons he would gain from Bush helped UNITA survive even after U.S. support stopped.[157] Senators Larry Smith and Dante Fascell, a senior member of the firm, worked with the Cuban American National Foundation, Representative Claude Pepper of Florida, Neal Blair's Free the Eagle, and Howard Phillips' Conservative Caucus to repeal the Clark Amendment in 1985.[158] From the amendment's repeal in 1985 to 1992 the U.S. government gave Savimbi $60 million per year, a total of $420 million. A sizable amount of the aid went to Savimbi's personal expenses. Black, Manafort filed foreign lobbying records with the U.S. Justice Department showing Savimbi's expenses during his U.S. visits. During his December 1990 visit he spent $136,424 at the Park Hyatt hotel and $2,705 in tips. He spent almost $473,000 in October 1991 during his week-long visit to Washington and Manhattan. He spent $98,022 in hotel bills, at the Park Hyatt, $26,709 in limousine rides in Washington and another $5,293 in Manhattan. Paul Manafort, a partner in the firm, charged Savimbi $19,300 in consulting and additional $1,712 in expenses. He also bought $1,143 worth of \"survival kits\" from Motorola. When questioned in an interview in 1990 about human rights abuses under Savimbi, Black said, \"Now when you're in a war, trying to manage a war, when the enemy ... is no more than a couple of hours away from you at any given time, you might not run your territory according to New Hampshire town meeting rules.\"[citation needed] Bicesse Accords President dos Santos met with Savimbi in Lisbon, Portugal and signed the Bicesse Accords, the first of three major peace agreements, on 31 May 1991, with the mediation of the Portuguese government. The accords laid out a transition to multi-party democracy under the supervision of the United Nations' UNAVEM II mission, with a presidential election to be held within a year. The agreement attempted to demobilize the 152,000 active fighters and integrate the remaining government troops and UNITA rebels into a 50,000-strong Angolan Armed Forces (FAA). The FAA would consist of a national army with 40,000 troops, navy with 6,000, and air force with 4,000.[159] While UNITA largely did not disarm, the FAA complied with the accord and demobilized, leaving the government disadvantaged.[160] Angola held the first round of its 1992 presidential election on 29–30 September. Dos Santos officially received 49.57% of the vote and Savimbi won 40.6%. As no candidate received 50% or more of the vote, election law dictated a second round of voting between the top two contenders. Savimbi, along with eight opposition parties and many other election observers, said the election had been neither free nor fair.[161] An official observer wrote that there was little UN supervision, that 500,000 UNITA voters were disenfranchised and that there were 100 clandestine polling stations.[161][162] Savimbi sent Jeremias Chitunda, Vice President of UNITA, to Luanda to negotiate the terms of the second round.[163][164] The election process broke down on 31 October, when government troops in Luanda attacked UNITA. Civilians, using guns they had received from police a few days earlier, conducted house-by-house raids with the Rapid Intervention Police, killing and detaining hundreds of UNITA supporters. The government took civilians in trucks to the Camama cemetery and Morro da Luz ravine, shot them, and buried them in mass graves. Assailants attacked Chitunda's convoy on 2 November, pulling him out of his car and shooting him and two others in their faces.[164] The MPLA massacred over ten thousand UNITA and FNLA voters nationwide in a few days in what was known as the Halloween Massacre.[161][165] Savimbi said the election had neither been free nor fair and refused to participate in the second round.[163] He then proceeded to resume armed struggle against the MPLA. Angolan Civil War December 1992 - June 1993. Then, in a series of stunning victories, UNITA regained control over Caxito, Huambo, M'banza Kongo, Ndalatando, and Uíge, provincial capitals it had not held since 1976, and moved against Kuito, Luena, and Malange. Although the U.S. and South African governments had stopped aiding UNITA, supplies continued to come from Mobutu in Zaire.[166] UNITA tried to wrest control of Cabinda from the MPLA in January 1993. Edward DeJarnette, Head of the U.S. Liaison Office in Angola for the Clinton Administration, warned Savimbi that, if UNITA hindered or halted Cabinda's production, the U.S. would end its support for UNITA. On 9 January, UNITA began a 55-day battle over Huambo, the \"War of the Cities\".[167] Hundreds of thousands fled and 10,000 were killed before UNITA gained control on 7 March. The government engaged in an ethnic cleansing of Bakongo, and, to a lesser extent Ovimbundu, in multiple cities, most notably Luanda, on 22 January in the Bloody Friday massacre. UNITA and government representatives met five days later in Ethiopia, but negotiations failed to restore the peace.[168] The United Nations Security Council sanctioned UNITA through Resolution 864 on 15 September 1993, prohibiting the sale of weapons or fuel to UNITA. Angolan Civil War January, - November 1994. Perhaps the clearest shift in U.S. foreign policy emerged when President Bill Clinton issued Executive Order 12865 on 23 September, labeling UNITA a \"continuing threat to the foreign policy objectives of the U.S.\"[169] By August 1993, UNITA had gained control over 70% of Angola, but the government's military successes in 1994 forced UNITA to sue for peace. By November 1994, the government had taken control of 60% of the country. Savimbi called the situation UNITA's \"deepest crisis\" since its creation.[154][170][171] It is estimated that perhaps 120,000 people were killed in the first eighteen months following the 1992 election, nearly half the number of casualties of the previous sixteen years of war.[172] Both sides of the conflict continued to commit widespread and systematic violations of the laws of war with UNITA in particular guilty of indiscriminate shelling of besieged cities resulting in large death toll to civilians. The MPLA government forces used air power in indiscriminate fashion also resulting in high civilian deaths.[173] The Lusaka Protocol of 1994 reaffirmed the Bicesse Accords.[174] Lusaka Protocol Savimbi, unwilling to personally sign an accord, had former UNITA Secretary General Eugenio Manuvakola represent UNITA in his place. Manuvakola and Angolan Foreign Minister Venancio de Moura signed the Lusaka Protocol in Lusaka, Zambia on 31 October 1994, agreeing to integrate and disarm UNITA. Both sides signed a ceasefire as part of the protocol on 20 November.[170][171] Under the agreement the government and UNITA would cease fire and demobilize. 5,500 UNITA members, including 180 militants, would join the Angolan national police, 1,200 UNITA members, including 40 militants, would join the rapid reaction police force, and UNITA generals would become officers in the Angolan Armed Forces. Foreign mercenaries would return to their home countries and all parties would stop acquiring foreign arms. The agreement gave UNITA politicians homes and a headquarters. The government agreed to appoint UNITA members to head the Mines, Commerce, Health, and Tourism ministries, in addition to seven deputy ministers, ambassadors, the governorships of Uige, Lunda Sul, and Cuando Cubango, deputy governors, municipal administrators, deputy administrators, and commune administrators. The government would release all prisoners and give amnesty to all militants involved in the civil war.[170][171]Zimbabwean PresidentRobert Mugabe and South African PresidentNelson Mandela met in Lusaka on 15 November 1994 to boost support symbolically for the protocol. Mugabe and Mandela both said they would be willing to meet with Savimbi and Mandela asked him to come to South Africa, but Savimbi did not come. The agreement created a joint commission, consisting of officials from the Angolan government, UNITA, and the UN with the governments of Portugal, the United States, and Russia observing, to oversee its implementation. Violations of the protocol's provisions would be discussed and reviewed by the commission.[170] The protocol's provisions, integrating UNITA into the military, a ceasefire, and a coalition government, were similar to those of the Alvor Agreement that granted Angola independence from Portugal in 1975. Many of the same environmental problems, mutual distrust between UNITA and the MPLA, loose international oversight, the importation of foreign arms, and an overemphasis on maintaining the balance of power, led to the collapse of the protocol.[171] Arms monitoring In January 1995, U.S. President Clinton sent Paul Hare, his envoy to Angola, to support the Lusaka Protocol and impress the importance of the ceasefire onto the Angolan government and UNITA, both in need of outside assistance.[175] The United Nations agreed to send a peacekeeping force on 8 February.[68] Savimbi met with South African President Mandela in May. Shortly after, on 18 June, the MPLA offered Savimbi the position of Vice President under dos Santos with another Vice President chosen from the MPLA. Savimbi told Mandela he felt ready to \"serve in any capacity which will aid my nation,\" but he did not accept the proposal until 12 August.[176][177] The United States Department of Defense and Central Intelligence Agency's Angola operations and analysis expanded in an effort to halt weapons shipments,[175] a violation of the protocol, with limited success. The Angolan government bought six Mil Mi-17 from Ukraine in 1995.[178] The government bought L-39 attack aircraft from the Czech Republic in 1998 along with ammunition and uniforms from Zimbabwe Defence Industries and ammunition and weapons from Ukraine in 1998 and 1999.[178] U.S. monitoring significantly dropped off in 1997 as events in Zaire, the Congo and then Liberia occupied more of the U.S. government's attention.[175] UNITA purchased more than 20 FROG-7transporter erector launchers (TEL) and three FOX 7 missiles from the North Korean government in 1999.[179] The UN extended its mandate on 8 February 1996. In March, Savimbi and dos Santos formally agreed to form a coalition government.[68] The government deported 2,000 West African and Lebanese Angolans in Operation Cancer Two, in August 1996, on the grounds that dangerous minorities were responsible for the rising crime rate.[180] In 1996 the Angolan government bought military equipment from India, two Mil Mi-24 attack helicopters and three Sukhoi Su-17 from Kazakhstan in December, and helicopters from Slovakia in March.[178] The international community helped install a Government of Unity and National Reconciliation in April 1997, but UNITA did not allow the regional MPLA government to take up residence in 60 cities. The UN Security Council voted on 28 August 1997, to impose sanctions on UNITA through Resolution 1127, prohibiting UNITA leaders from traveling abroad, closing UNITA's embassies abroad, and making UNITA-controlled areas a no-fly zone. The Security Council expanded the sanctions through Resolution 1173 on 12 June 1998, requiring government certification for the purchase of Angolan diamonds and freezing UNITA's bank accounts.[166] During the First Congo War, the Angolan government joined the coalition to overthrow Mobutu's government due to his support for UNITA. Mobutu's government fell to the opposition coalition on 16 May 1997.[181] The Angolan government chose to act primarily through Katangese gendarmes called the Tigres, which were proxy groups formed from the descendants of police units who had been exiled from Zaire and thus were fighting for a return to their homeland.[182] Luanda did also deploy regular troops.[181] In early October 1997, Angola invaded the Republic of the Congo during its civil war, and helped Sassou Nguesso's rebels overthrow the government of Pascal Lissouba. Lissouba's government had allowed UNITA the use of cities in the Republic of Congo in order to circumvent sanctions.[183] Between 11 and 12 October 1997, Angolan air force fighter jets conducted a number of air strikes on government positions within Brazzaville. On 16 October 1997 rebel militia supported by tanks and a force of 1,000 Angolan troops cemented their control of Brazzaville forcing Lisouba to flee.[184][185] Angolan troops remained in the country fighting militia forces loyal to Lissouba engaged in a guerrilla war against the new government.[citation needed] The UN spent $1.6 billion from 1994 to 1998 in maintaining a peacekeeping force.[68] The Angolan military attacked UNITA forces in the Central Highlands on 4 December 1998, the day before the MPLA's fourth Congress. Dos Santos told the delegates the next day that he believed war to be the only way to ultimately achieve peace, rejected the Lusaka Protocol, and asked MONUA to leave. In February 1999, the Security Council withdrew the last MONUA personnel. In late 1998, several UNITA commanders, dissatisfied with Savimbi's leadership, formed UNITA Renovada, a breakaway militant group. Thousands more deserted UNITA in 1999 and 2000.[166] The Angolan military (with the help of the Brazilian Air Force) launched Operation Restore, a massive offensive, in September 1999, recapturing N'harea, Mungo and Andulo and Bailundo, the site of Savimbi's headquarters just one year before. The UN Security Council passed Resolution 1268 on 15 October, instructing United Nations Secretary GeneralKofi Annan to update the Security Council to the situation in Angola every three months. Dos Santos offered an amnesty to UNITA militants on 11 November. By December, Chief of Staff General João de Matos said the Angolan Armed Forces had destroyed 80% of UNITA's militant wing and captured 15,000 tons of military equipment.[166][186] Following the dissolution of the coalition government, Savimbi retreated to his historical base in Moxico and prepared for battle.[187] In order to isolate UNITA, the government forced civilians in countryside areas subject to UNITA influence to relocate to major cities. The strategy was successful isolating in UNITA but had adverse humanitarian consequences.[183] Diamond trade UNITA's ability to mine diamonds and sell them abroad provided funding for the war to continue even as the movement's support in the Western world and among the local populace withered away. De Beers and Endiama, a state-owned diamond-mining monopoly, signed a contract allowing De Beers to handle Angola's diamond exports in 1990.[188] According to the United Nation's Fowler Report, Joe De Deker, a former stockholder in De Beers, worked with the government of Zaire to supply military equipment to UNITA from 1993 to 1997. De Deker's brother, Ronnie, allegedly flew from South Africa to Angola, directing weapons originating in Eastern Europe. In return, UNITA gave Ronnie bushels of diamonds worth $6 million. De Deker sent the diamonds to De Beer's buying office in Antwerp, Belgium. De Beers openly acknowledges spending $500 million on legal and illegal Angolan diamonds in 1992 alone. The United Nations estimates Angolans made between three and four billion dollars through the diamond trade between 1992 and 1998.[169][189] The UN also estimates that out of that sum, UNITA made at least $3.72 billion, or 93% of all diamond sales, despite international sanctions.[190] Executive Outcomes (EO), a South African private military company, played a major role in turning the tide for the MPLA, with one U.S. defence expert calling the EO the \"best fifty or sixty million dollars the Angolan government ever spent.\"[191] Heritage Oil and Gas, and allegedly De Beers, hired EO to protect their operations in Angola.[191] Executive Outcomes trained up to 5,000 troops and 30 combat pilots in camps in Lunda Sul, Cabo Ledo, and Dondo.[192] Cabinda separatism The territory of Cabinda is north of Angola proper, separated by a strip of territory 60 km (37.3 mi) long in the Democratic Republic of the Congo.[193] The Portuguese Constitution of 1933 designated Angola and Cabinda as overseas provinces.[194][195] In the course of administrative reforms during the 1930s to 1950s, Angola was divided into districts, and Cabinda became one of the districts of Angola. The Front for the Liberation of the Enclave of Cabinda (FLEC) formed in 1963 during the broader war for independence from Portugal. Contrary to the organization's name, Cabinda is an exclave, not an enclave. FLEC later split into the Armed Forces of Cabinda (FLEC-FAC) and FLEC-Renovada (FLEC-R). Several other, smaller FLEC factions later broke away from these movements, but FLEC-R remained the most prominent because of its size and its tactics. FLEC-R members cut off the ears and noses of government officials and their supporters, similar to the Revolutionary United Front of Sierra Leone in the 1990s.[196] Despite Cabinda's relatively small size, foreign powers and the nationalist movements coveted the territory for its vast reserves of petroleum, the principal export of Angola then and now.[197] In the war for independence, the division of assimilados versus indigenas peoples masked the inter-ethnic conflict between the various native tribes, a division that emerged in the early 1970s. The Union of Peoples of Angola, the predecessor to the FNLA, only controlled 15% of Angola's territory during the independence war, excluding MPLA-controlled Cabinda. The People's Republic of China openly backed UNITA upon independence despite the mutual support from its adversary South Africa and UNITA's pro-Western tilt. The PRC's support for Savimbi came in 1965, a year after he left the FNLA. China saw Holden Roberto and the FNLA as the stooge of the West and the MPLA as the Soviet Union's proxy. With the Sino-Soviet split, South Africa presented the least odious of allies to the PRC.[198][199] Throughout the 1990s, Cabindan rebels kidnapped and ransomed off foreign oil workers to in turn finance further attacks against the national government. FLEC militants stopped buses, forcing Chevron Oil workers out, and set fire to the buses on 27 March and 23 April 1992. A large-scale battle took place between FLEC and police in Malongo on 14 May, in which 25 mortar rounds accidentally hit a nearby Chevron compound.[200] The government, fearing the loss of their prime source of revenue, began to negotiate with representatives from Front for the Liberation of the Enclave of Cabinda-Renewal (FLEC-R), Armed Forces of Cabinda (FLEC-FAC), and the Democratic Front of Cabinda (FDC) in 1995. Patronage and bribery failed to assuage the anger of FLEC-R and FLEC-FAC and negotiations ended. In February 1997, FLEC-FAC kidnapped two Inwangsa SDN-timber company employees, killing one and releasing the other after receiving a $400,000 ransom. FLEC-FAC kidnapped eleven people in April 1998, nine Angolans and two Portuguese, released for a $500,000 ransom. FLEC-R kidnapped five Byansol-oil engineering employees, two Frenchman, two Portuguese, and an Angolan, in March 1999. While militants released the Angolan, the government complicated the situation by promising the rebel leadership $12.5 million for the hostages. When António Bento Bembe, the President of FLEC-R, showed up, the Angolan army arrested him and his bodyguards. The Angolan army later forcibly freed the other hostages on 7 July. By the end of the year the government had arrested the leadership of all three rebel organizations.[201] 2000s Illicit arms trading characterized much of the later years of the Angolan Civil War, as each side tried to gain the upper hand by buying arms from Eastern Europe and Russia. Israel continued in its role as a proxy arms dealer for the United States.[202] On 21 September 2000, a Russian freighter delivered 500 tons of Ukrainian 7.62 mm ammunition to Simportex, a division of the Angolan government, with the help of a shipping agent in London. The ship's captain declared his cargo \"fragile\" to minimize inspection.[203] The next day, the MPLA began attacking UNITA, winning victories in several battles from 22 to 25 September. The government gained control over military bases and diamond mines in Lunda Norte and Lunda Sul, hurting Savimbi's ability to pay his troops.[68] Angola agreed to trade oil to Slovakia in return for arms, buying six Sukhoi Su-17attack aircraft on 3 April 2000. The Spanish government in the Canary Islands prevented a Ukrainian freighter from delivering 636 tons of military equipment to Angola on 24 February 2001. The captain of the ship had falsely reported his cargo, claiming the ship carried automobile parts. The Angolan government admitted Simportex had purchased arms from Rosvooruzhenie, the Russian state-owned arms company, and acknowledged the captain might have violated Spanish law by misreporting his cargo, a common practice in arms smuggling to Angola.[203] More than 700 villagers trekked 60 kilometres (37 mi) from Golungo Alto to Ndalatando (red dot), fleeing a UNITA attack. They remained uninjured. UNITA carried out several attacks against civilians in May 2001 in a show of strength. UNITA militants attacked Caxito on 7 May, killing 100 people and kidnapping 60 children and two adults. UNITA then attacked Baia-do-Cuio, followed by an attack on Golungo Alto, a city 200 kilometres (124 mi) east of Luanda, a few days later. The militants advanced on Golungo Alto at 2:00 pm on 21 May, staying until 9:00 pm on 22 May when the Angolan military retook the town. They looted local businesses, taking food and alcoholic beverages before singing drunkenly in the streets. More than 700 villagers trekked 60 kilometres (37 mi) from Golungo Alto to Ndalatando, the provincial capital of Cuanza Norte, without injury. According to an aid official in Ndalatando, the Angolan military prohibited media coverage of the incident, so the details of the attack are unknown. Joffre Justino, UNITA's spokesman in Portugal, said UNITA only attacked Gungo Alto to demonstrate the government's military inferiority and the need to cut a deal.[204] Four days later UNITA released the children to a Catholic mission in Camabatela, a city 200 kilometres (124 mi) from where UNITA kidnapped them. The national organization said the abduction violated their policy towards the treatment of civilians. In a letter to the bishops of Angola, Jonas Savimbi asked the Catholic Church to act as an intermediary between UNITA and the government in negotiations.[205] The attacks took their toll on Angola's economy. At the end of May 2001, De Beers, the international diamond mining company, suspended its operations in Angola, ostensibly on the grounds that negotiations with the national government reached an impasse.[206] Militants of unknown affiliation fired rockets at United Nations World Food Program (UNWFP) planes on 8 June near Luena and again near Kuito a few days later. As the first plane, a Boeing 727, approached Luena someone shot a missile at the aircraft, damaging one engine but not critically as the three-man crew landed successfully. The plane's altitude, 5,000 metres (16,404 ft), most likely prevented the assailant from identifying his target. As the citizens of Luena had enough food to last them several weeks, the UNFWP temporarily suspended their flights. When the flights began again a few days later, militants shot at a plane flying to Kuito, the first attack targeting UN workers since 1999.[207] The UNWFP again suspended food aid flights throughout the country. While he did not claim responsibility for the attack, UNITA spokesman Justino said the planes carried weapons and soldiers rather than food, making them acceptable targets. UNITA and the Angolan government both said the international community needed to pressure the other side into returning to the negotiating table. Despite the looming humanitarian crisis, neither side guaranteed UNWFP planes safety. Kuito, which had relied on international aid, only had enough food to feed their population of 200,000 until the end of the week.[208] The UNFWP had to fly in all aid to Kuito and the rest of the Central Highlands because militants ambushed trucks. Further complicating the situation, potholes in the Kuito airport strip slowed aid deliveries. Overall chaos reduced the amount of available oil to the point at which the UN had to import its jet fuel.[209] Government troops captured and destroyed UNITA's Epongoloko base in Benguela province and Mufumbo base in Cuanza Sul in October 2001.[210] The Slovak government sold fighter jets to the Angolan government in 2001 in violation of the European Union Code of Conduct on Arms Exports.[211] Death of Savimbi Government troops killed Jonas Savimbi on 22 February 2002, in Moxico province.[212] UNITA Vice President António Dembo took over, but, weakened by wounds sustained in the same skirmish that killed Savimbi, died from diabetes 12 days later on 3 March, and Secretary-General Paulo Lukamba Gato became UNITA's leader.[213] After Savimbi's death, the government came to a crossroads over how to proceed. After initially indicating the counter-insurgency might continue, the government announced it would halt all military operations on 13 March. Military commanders for UNITA and the MPLA met in Cassamba and agreed to a cease-fire. Carlos Morgado, UNITA's spokesman in Portugal, said the UNITA's Portugal wing had been under the impression General Kamorteiro, the UNITA general who agreed to the ceasefire, had been captured more than a week earlier. Morgado did say that he had not heard from Angola since Savimbi's death. The military commanders signed a Memorandum of Understanding as an addendum to the Lusaka Protocol in Luena on 4 April, with Santos and Lukambo observing.[214][215] The United Nations Security Council passed Resolution 1404 on 18 April, extending the monitoring mechanism of sanctions by six months. Resolutions 1412 and 1432, passed on 17 May and 15 August respectively, suspended the UN travel ban on UNITA officials for 90 days each, finally abolishing the ban through Resolution 1439 on 18 October. UNAVEM III, extended an additional two months by Resolution 1439, ended on 19 December.[216] UNITA's new leadership declared the rebel group a political party and officially demobilized its armed forces in August 2002.[217] That same month, the United Nations Security Council replaced the United Nations Office in Angola with the United Nations Mission in Angola, a larger, non-military, political presence.[218] Aftermath Destroyed road bridge in Angola, 2009 The civil war spawned a disastrous humanitarian crisis in Angola, internally displacing 4.28 million people – one-third of Angola's total population. The United Nations estimated in 2003 that 80% of Angolans lacked access to basic medical care, 60% lacked access to water, and 30% of Angolan children would die before the age of five, with an overall national life expectancy of less than 40 years of age.[219] Over 100,000 children were separated from their families.[220] There was an exodus from rural areas in most of the country. Today the urban population represents slightly more than half of the population, according to the latest census. In many cases, people went into cities outside the traditional area of their ethnic group. There are now important Ovimbundu communities in Luanda, Malanje, and Lubango. There has been a degree of return, but at a slow pace, while many younger people are reluctant to go to a rural life that they never knew.[221] In rural areas, one problem is that some were for years under the control of the MPLA-government, while others were controlled by UNITA.[clarification needed] Some of the population fled to neighbouring countries, while others went into remote mountainous areas.[221] Over 156 people have died since 2018 from 70 landmine accidents and other blasts resulting from explosives installed during the Angolan civil war.[222] The landmine victims do not receive any government support.[223] On the 44th anniversary of the May 27, 1977 attempted coup by Nito Alves, Angolan president João Lourenço apologized for the execution of thousands of Alves' followers by the MPLA in the aftermath of the failed coup and promised to return the remains of the victims to their families.[224] Humanitarian efforts The remnants of a tank in the Angolan countryside, destroyed by a landmine. The government spent $187 million settling internally displaced persons (IDPs) between 4 April 2002, and 2004, after which the World Bank gave $33 million to continue the settling process. The UN Office for the Coordination of Humanitarian Affairs (OCHA) estimated that fighting in 2002 displaced 98,000 people between 1 January and 28 February alone. IDPs comprised 75% of all landmine victims. The IDPs, unacquainted with their surroundings, frequently and predominantly fell victim to these weapons. Militant forces laid approximately 15 million landmines by 2002.[218] The HALO Trust began demining Angola in 1994, and had destroyed 30,000 landmines by July 2007. 1,100 Angolans and seven foreign workers are employed by the HALO Trust in Angola, with demining operations expected to finish by 2014.[225][226] Child soldiers Human Rights Watch estimates UNITA and the government employed more than 6,000 and 3,000 child soldiers, respectively, some forcibly impressed, during the war. Additionally, human rights analysts found that between 5,000 and 8,000 underage girls were married to UNITA militants. Some girls were ordered to go and forage for food to provide for the troops – the girls were denied food if they did not bring back enough to satisfy their commander. After victories, UNITA commanders would be rewarded with women, who were often then sexually abused. The Angolan government and UN agencies identified 190 child soldiers in the Angolan army, and had relocated 70 of them by November 2002, but the government continued to knowingly employ other underage soldiers.[227] The war provides a more comedic background story in the South African comedy The Gods Must Be Crazy 2 as a Cuban and an Angolan soldier repeatedly try to take each other prisoner, but ultimately part on (more or less) amicable terms. The Cuban classic film Caravana was produced on the fictionalized exploits of a Cuban caravan (a military mechanized column) sent to reinforce an isolated Cuban position against an impeding UNITA attack. On the way they need to clear mines and repel continued attacks of Cobra, a special operations section of UNITA troops indirectly monitored by CIA handlers. The film received substantial support from Cuban Armed Forces, included many famous Cuban actors of the time and became a classic of Cuban Cinema.[citation needed] Three additional Cuban films were produced in a loose trilogy, each focused in one significant battle of the war: Kangamba, Sumbe and Cuito Cuanavale.[citation needed] The 2004 film The Hero, produced by Fernando Vendrell and directed by Zézé Gamboa, depicts the life of average Angolans in the aftermath of the civil war. The film follows the lives of three individuals: Vitório, a war veteran crippled by a landmine who returns to Luanda; Manu, a young boy searching for his soldier father; and Joana, a teacher who mentors the boy and begins a love affair with Vitório. The Hero won the 2005 Sundance World Dramatic Cinema Jury Grand Prize. A joint Angolan, Portuguese, and French production, The Hero was filmed entirely in Angola.[234] Notes ^Irritated by UNITA cross-border raids, the Namibian Defence Force retaliated by sending units into southern Angola and destroying a UNITA training camp at Licua in late January 2001.[6] The Namibian troops were not withdrawn from Angola until May 2002.[6] ^The North Korean Military Mission in Angola had about 1,500 personnel attached to FAPLA in 1986, most likely advisers, although their exact duties are uncertain.[11] Their presence in Angola may have been indirectly subsidised by the Soviet Union.[12] ^The results of the 2008 Elections in Angola show that its constituency is by now considerably larger. ^The Dutch conquered and ruled Luanda between 1640 and 1648 as Fort Aardenburgh, but the Portuguese presence was maintained inland, and after the reconquest of Luanda, all trading activities were resumed as before. ^Taylor, Moe (2019). \"Every Citizen a Soldier: The Guyana People's Militia, 1976–1985\". Journal of Global South Studies. 36 (2). University of Florida: 279–311. doi:10.1353/gss.2019.0044. Washington never sought to remove Forbes Burnham from power, despite frequent vexations with his policies. ... However, because of its displeasure with numerous Guyanese policies during the decade, the United States applied pressure in various ways: it suspended economic and food aid, it blocked World Bank loans, and it appeared to side with Venezuela in the ongoing territorial dispute. The October 1976 bombing of Cubana Airlines flight 455, in which eleven Guyanese, five North Koreans and fifty-seven Cubans were killed, was widely seen as retaliation for Guyana and Cuba's coordinated involvement in Angola. ^Hunter, Jane (1987). Israeli Foreign Policy: South Africa and Central America. South End Press. p. 16. In 1975 Israel followed Secretary of State Henry Kissinger's advice and helped South Africa with its invasion of Angola. Even after the passage the following year of the Clark Amendment forbidding U.S. covert involvement in Angola, Israel apparently considered Kissinger's nod a continuing mandate. ^Benjamin Beit-Hallahmi, The Israel Connection: Who Israel Arms and Why, New York: Pantheon Books, 1987, ISBN978-0-394-55922-3, Chapter 5: \"South Africa and Israel: An Alliance of Desperation\" (pp. 108–174). \"The alliance between South Africa and Israel is symbiotic in many areas of military endeavor, with Israel usually the more vital element. Israel is South Africa's closest military ally and its source of inspiration and technology. The Uzi and Galil weapons are as visible in South Africa today as they are in Haiti and Guatemala (Leonard, 1983).\" Onslow, Sue. “The battle of Cuito Cuanavale: Media space and the end of the Cold War in Southern Africa\" in Artemy M. Kalinovsky, Sergey Radchenko. eds., The End of the Cold War and the Third World: New Perspectives on Regional Conflict (2011) pp 277–96. Saney, Isaac, \"African Stalingrad: The Cuban Revolution, Internationalism and the End of Apartheid,\" Latin American Perspectives 33#5 (2006): pp. 81–117. Saunders, Chris. \"The ending of the Cold War and Southern Africa\" in Artemy M. Kalinovsky, Sergey Radchenko. eds., The End of the Cold War and the Third World: New Perspectives on Regional Conflict (2011) pp 264–77."}
{"url": "https://web.archive.org/web/20180612142932/https://www.collinsdictionary.com/dictionary/english/nonhuman", "text": "These crawls are part of an effort to archive pages as they are created and archive the pages that they refer to. That way, as the pages that are referenced are changed or taken from the web, a link to the version that was live when the page was written will be preserved. Then the Internet Archive hopes that references to these archived pages will be put in place of a link that would be otherwise be broken, or a companion link to allow people to see what was originally intended by a page's authors. This is a collection of web page captures from links added to, or changed on, Wikipedia pages. The idea is to bring a reliability to Wikipedia outlinks so that if the pages referenced by Wikipedia articles are changed, or go away, a reader can permanently find what was originally referred to. Example sentences containing 'nonhuman' These examples have been automatically selected and may contain sensitive content. Read more… Farms have long been known for recycling their own nonhuman solidwaste.McKenzie, James F. & Pinger, Robert R. An Introduction to Community Health (1995)Many of the antimicrobials employed in farming and other nonhuman uses can alsopromoteresistance in bacteria common to humans and animals. Times, Sunday Times (2013) Trends of 'nonhuman' Used Occasionally. nonhuman is one of the 30000 most commonly used words in the Collins dictionary"}
{"url": "https://en.m.wikipedia.org/wiki/List_of_ethnic_groups_of_Africa", "text": "The official population count of the various ethnic groups in Africa is highly uncertain, both due to limited infrastructure to perform censuses and due to the rapid population growth. There have also been accusations of deliberate misreporting in order to give selected ethnicities numerical superiority (as in the case of Nigeria's Hausa, Fulani, Yoruba, and Igbo peoples).[1][2][3] A 2009 genetic clustering study, which genotyped 1327 polymorphic markers in various African populations, identified six ancestral clusters. The clustering corresponded closely with ethnicity, culture, and language.[4] A 2018 whole genome sequencing study of the world's populations observed similar clusters among the populations in Africa. At K=9, distinct ancestral components defined the Afroasiatic-speaking populations inhabiting North Africa and Northeast Africa; the Nilo-Saharan-speaking populations in Northeast Africa and East Africa; the Ari populations in Northeast Africa; the Niger-Congo-speaking populations in West-Central Africa, West Africa, East Africa, and Southern Africa; the Pygmy populations in Central Africa; and the Khoisan populations in Southern Africa.[5] ^Tishkoff, SA; et al. (2009). \"The Genetic Structure and History of Africans and African Americans\"(PDF). Science. 324 (5930): 1037–39. Bibcode:2009Sci...324.1035T. doi:10.1126/science.1172257. PMC2947357. PMID19407144. We incorporated geographic data into a Bayesian clustering analysis, assuming no admixture (TESS software) (25) and distinguished six clusters within continental Africa (Fig. 5A). The most geographically widespread cluster (orange) extends from far Western Africa (the Mandinka) through central Africa to the Bantu speakers of South Africa (the Venda and Xhosa) and corresponds to the distribution of the Niger-Kordofanian language family, possibly reflecting the spread of Bantu-speaking populations from near the Nigerian/Cameroon highlands across eastern and southern Africa within the past 5000 to 3000 years (26,27). Another inferred cluster includes the Pygmy and SAK populations (green), with a noncontiguous geographic distribution in central and southeastern Africa, consistent with the STRUCTURE (Fig. 3) and phylogenetic analyses (Fig. 1). Another geographically contiguous cluster extends across northern Africa (blue) into Mali (the Dogon), Ethiopia, and northern Kenya. With the exception of the Dogon, these populations speak an Afroasiatic language. Chadic-speaking and Nilo-Saharan–speaking populations from Nigeria, Cameroon, and central Chad, as well as several Nilo-Saharan–speaking populations from southern Sudan, constitute another cluster (red). Nilo-Saharan and Cushitic speakers from the Sudan, Kenya, and Tanzania, as well as some of the Bantu speakers from Kenya, Tanzania, and Rwanda (Hutu/Tutsi), constitute another cluster (purple), reflecting linguistic evidence for gene flow among these populations over the past ~5000 years (28,29). Finally, the Hadza are the sole constituents of a sixth cluster (yellow), consistent with their distinctive genetic structure identified by PCA and STRUCTURE. ^Peter Austin, One Thousand Languages (2008), p. 75, https://books.google.com/books?isbn=0520255607:\"Kanuri is a major Saharan language spoken in the Lake Chad Basin in the Borno area of northeastern Nigeria, as well as in Niger, Cameroon, and Chad (where the variety is known as Kanembul[)].\""}
