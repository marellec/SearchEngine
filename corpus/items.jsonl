{"url": "https://en.m.wikipedia.org/wiki/List_of_common_misconceptions", "text": "List of common misconceptions Each entry on this list of common misconceptions is worded as a correction; the misconceptions themselves are implied rather than stated. These entries are concise summaries of the main subject articles, which can be consulted for more detail. Federal legal tender laws in the United States do not state that a private business, a person, or an organization must accept cash for payment, though it must be regarded as valid payment for debts when tendered to a creditor.[1] The common image of Santa Claus (Father Christmas) as a jolly old man in red robes was not created by The Coca-Cola Company as an advertising gimmick. Santa Claus had already taken this form in American popular culture and advertising by the late 19th century, long before Coca-Cola used his image in the 1930s.[8] PepsiCo never owned the \"6th most powerful navy\" in the world after a deal with the Soviet Union. In 1989, Pepsi acquired several decommissioned warships as part of a barter deal.[10][11] The oil tankers were leased out and the other ships sold for scrap.[12] A follow-on deal involved another 10 ships.[13] Searing does not seal moisture in meat; in fact, it causes it to lose some moisture. Meat is seared to brown it, to affect its color, flavor, and texture.[14] Twinkies, an American snack cake generally considered to be \"junk food\", have a shelf life of around 45 days, despite the common claim (usually facetious) that they remain edible for decades.[15][16] Twinkies, with only sorbic acid as an added preservative, normally remain on a store shelf for 7 to 10 days.[17][18] With the exception of some perishables, properly stored foods can safely be eaten past their \"expiration\" dates.[19][20] The vast majority of expiration dates in the United States are regulated by state governments and refer to food quality, not safety; the \"use by\" date represents the last day the manufacturer warrants the quality of their product. Hydrox is not a knock-off of Oreos. Hydrox, invented in 1908, predates Oreos by four years and outsold it until the 1950s, when Oreos raised prices and the name \"Hydrox\" became increasingly unappealing due to being said to sound like a laundry detergent brand, after similar new brands of the decade.[31][32][33] Spices were not used in the Middle Ages to mask the flavor of rotten meat before refrigeration. Spices were an expensive luxury item; those who could afford them could afford good meat, and there are no contemporaneous documents calling for spices to disguise the taste of bad meat.[43] Microwave ovens do not cook food from the inside out. 2.45 GHz microwaves can only penetrate approximately 1 centimeter (3⁄8 inch) into most foods. The inside portions of thicker foods are mainly heated by heat conducted from the outer portions.[54] Microwave ovens do not cause cancer, as microwave radiation is non-ionizing and therefore does not have the cancer risks associated with ionizing radiation such as X-rays. No studies have found that microwave radiation causes cancer, even with exposure levels far greater than normal radiation leakage.[55] Microwaving food does not reduce its nutritive value and may preserve it better than other cooking processes due to shorter cooking times.[56] Ronald Reagan was never seriously considered for the role of Rick Blaine in the 1942 film Casablanca, eventually played by Humphrey Bogart. An early studio press release mentioned Reagan, but the studio already knew that Reagan was unavailable because of his upcoming military service.[57] Indeed, the producer had always wanted Bogart for the part.[58] Irregardless is a word.[82][83]Nonstandard, slang, or colloquial terms used by English speakers are sometimes alleged not to be real words, despite appearing in numerous dictionaries. All words in English became accepted by being commonly used for a certain period of time; thus, there are many vernacular words currently not accepted as part of the standard language, or regarded as inappropriate in formal speech or writing, but the idea that they are not words is a misconception.[84] Other examples of words that are sometimes alleged not to be words include burglarize, licit,[85] and funnest[86] which appear in numerous dictionaries as English words.[87] The word crap did not originate as a back-formation of British plumber Thomas Crapper's aptronymous surname, nor does his name originate from the word crap.[94] The surname \"Crapper\" is a variant of \"Cropper\", which originally referred to someone who harvested crops.[95] The word crap ultimately comes from Medieval Latincrappa.[96] The word fuck did not originate in the Middle Ages as an acronym for either \"fornicating under consent of king\" or \"for unlawful carnal knowledge\", either as a sign posted above adulterers in the stocks, or as a sign on houses visible from the road during the Black Death. Nor did it originate as a corruption of \"pluck yew\" (an idiom falsely attributed to the English for drawing a longbow).[100] It is most likely derived from Middle Dutch or other Germanic languages, where it either meant \"to thrust\" or \"to copulate with\" (fokken in Middle Dutch), \"to copulate\", or \"to strike, push, copulate\" or \"penis\".[100][101] Either way, these variations would have been derived from the Indo-European root word -peuk, meaning \"to prick\".[100] The expression \"rule of thumb\" did not originate from an English law allowing a man to beat his wife with a stick no thicker than his thumb, and there is no evidence that such a law ever existed.[102] The false etymology has been broadly reported in media including Time magazine (1983), The Washington Post (1989) and CNN (1993).[103] The expression originates from the seventeenth century from various trades where quantities were measured by comparison to the width or length of a thumb.[104][105] The word the was never pronounced or spelled \"ye\" in Old or Middle English.[106] The confusion, seen in the common stock phrase \"ye olde\", derives from the use of the character thorn (þ), which in Middle English represented the sound now represented in Modern English by \"th\". Early printing presses often lacked types for the letter þ, meaning that \"þͤ\" () and \"þe\" were substituted with the visually similar \"yͤ\" and \"ye\", respectively.[107] The anti-Italian slur wop did not originate from an acronym for \"without papers\" or \"without passport\";[108] it is actually derived from the term guappo (roughly meaning thug or \"dandy\"), from Spanish guapo.[109] \"Xmas\", along with a modern Santa Claus, used on a Christmas postcard (1910) Xmas did not originate as a secular plan to \"take the Christ out of Christmas\".[110]X represents the Greek letter chi, the first letter of Χριστός (Christós), \"Christ\" in Greek,[111] as found in the chi-rho symbol ΧΡ since the 4th century. In English, \"X\" was first used as a scribal abbreviation for \"Christ\" in 1100; \"X'temmas\" is attested in 1551, and \"Xmas\" in 1721.[112] It is not necessary to wait 24 hours before filing a missing person report. When there is evidence of violence or of an unusual absence, it is important to start an investigation promptly.[113][114]Criminology experts say the first 72 hours in a missing person investigation are the most critical.[115] The US Armed Forces have generally forbidden military enlistment as a form of deferred adjudication (that is, an option for convicts to avoid jail time) since the 1980s. US Navy protocols discourage the practice, while the other four branches have specific regulations against it.[117] The United States does not require police officers to identify themselves as police in the case of a sting or other undercover work, and police officers may lie when engaged in such work.[118] Claiming entrapment as a defense instead focuses on whether the defendant was induced by undue pressure (such as threats) or deception from law enforcement to commit crimes they would not have otherwise committed.[119] Violent crime rates in the United States declined significantly between 1994 and 2003. Neither the Mafia nor other criminal organizations regularly use or have used cement shoes to drown their victims.[129] There are only two documented cases of this method being used in murders: one in 1964 and one in 2016 (although, in the former, the victim had concrete blocks tied to his legs rather than being enclosed in cement).[130] The French Army did use cement shoes on Algerians killed in death flights during the Algerian War.[131] In the United States, a defendant may not have their case dismissed simply because they were not read their Miranda rights at the time of their arrest. Miranda warnings cover the rights of a person when they are taken into custody and then interrogated by law enforcement.[132][133] If a person is not given a Miranda warning before the interrogation is conducted, statements made by them during the interrogation may not be admissible in a trial. The prosecution may still present other forms of evidence, or statements made during interrogations where the defendant was read their Miranda rights, to get a conviction.[134] Chewing gum is not punishable by caning in Singapore. Although importing and selling chewing gum has been illegal in Singapore since 1992, and corporal punishment is still an applicable penalty for certain offenses in the country, the two facts are unrelated; chewing gum-related offenses have always been only subject to fines, and the possession or consumption of chewing gum itself is not illegal.[135][136] No cases have been proven of strangers killing or permanently injuring children by intentionally hiding poisons, drugs, or sharp objects such as razor blades in candy during Halloween trick-or-treating.[140] However, in rare cases, adult family members have spread this story to cover up filicide or accidental deaths. Folklorists, scholars, and law enforcement experts say that the story that strangers put poison into candy and give that candy to trick-or-treating children has been \"thoroughly debunked\".[141][140] Mary Shelley's 1818 novel Frankenstein is named after the fictional scientist Victor Frankenstein, who created the sapient creature in the novel, not the creature itself, which is never named and is called Frankenstein's monster. However, as later adaptations started to refer to the monster itself as Frankenstein, this usage became well-established, and some no longer regard it as erroneous.[143][144] Listening to Mozart or classical music does not enhance intelligence (or IQ). A study from 1993 reported a short-term improvement in spatial reasoning.[156][157] However, the weight of subsequent evidence supports either a null effect or short-term effects related to increases in mood and arousal, with mixed results published after the initial report in Nature.[158][159][160][161] The Beatles' 1965 appearance at Shea Stadium was not the first time that a rock concert was played at a large, outdoor sports stadium in the U.S. Such venues were employed by Elvis Presley in the 1950s and the Beatles themselves in 1964.[170] Phil Collins did not write his 1981 hit \"In the Air Tonight\" about witnessing someone drowning and then confronting the person in the audience who let it happen. According to Collins himself, it was about his emotions when divorcing from his first wife.[172] Jesus was most likely not born on December 25, when his birth is traditionally celebrated as Christmas. It is more likely that his birth was in either the season of spring or perhaps summer. Although the Common Era ostensibly counts the years since the birth of Jesus,[174] it is unlikely that he was born in either AD 1 or 1 BC, as such a numbering system would imply. Modern historians estimate a date closer to between 6 BC and 4 BC.[175] The Bible does not say that exactly three magi came to visit the baby Jesus, nor that they were kings, or rode on camels, or that their names were Caspar, Melchior, and Balthazar, nor what color their skin was. Three magi are inferred because three gifts are described, but the Bible says only that there was more than one magus.[176][177][178][179][180][181] Paul the Apostle did not change his name from Saul. He was born a Jew, with Roman citizenship inherited from his father, and thus carried both a Hebrew and a Greco-Roman name from birth, as mentioned by Luke in Acts 13:9: \"...Saul, who also is called Paul...\".[184] Roman Catholic dogma does not say that the pope is either sinless or always infallible.[187] Catholic dogma since 1870 does state that a dogmatic teaching contained in divine revelation that is promulgated by the pope (deliberately, and under certain very specific circumstances; generally called ex cathedra) is free from error, although official invocation of papal infallibility is rare. Most theologians state that canonizations meet the requisites.[188] Otherwise, even when speaking in his official capacity, dogma does not hold that he is always free from error. Saint Augustine did not say \"God created hell for inquisitive people\".[194] He actually said: \"I do not give the answer that someone is said to have given (evading by a joke the force of the objection), 'He was preparing hell for those who pry into such deep subjects.' ... I do not answer in this way. I would rather respond, 'I do not know,' concerning what I do not know than say something for which a man inquiring about such profound matters is laughed at, while the one giving a false answer is praised.\"[195] So Augustine is saying that he would not say this and that he does not know the answer to the question. Most Muslim women do not wear a burqa (also transliterated as burka or burkha), which covers the body, head, and face, with a mesh grille to see through. Many Muslim women cover their hair and face (excluding the eyes) with a niqāb, or just their hair with a hijab[198] and many Muslim women wear neither face nor head coverings of any kind.[199] The word \"jihad\" does not always mean \"holy war\"; its literal meaning in Arabic is \"struggle\". While there is such a thing as \"jihad bil saif\", or jihad \"by the sword\",[202] it can be any spiritual or moral effort or struggle,[203][204] such as seeking knowledge, putting others before oneself, and inviting others to Islam.[205] The Quran does not promise martyrs 72 virgins in heaven. It does mention that virgin female companions,[206]houri, are given to all people, martyr or not, in heaven, but no number is specified. The source for the 72 virgins is a hadith in Sunan al-Tirmidhi by Imam Tirmidhi.[207][208] Hadiths are sayings and acts of Muhammad as reported by others, not part of the Quran itself.[209][207] The name golf is not an acronym for \"Gentlemen Only, Ladies Forbidden\".[213][214][215] It may have come from the Dutch word kolf or kolve, meaning \"club\",[214] or from the Scottish word goulf or gowf meaning \"to strike or cuff\".[213] The black belt in martial arts does not necessarily indicate expert level or mastery. It was introduced for judo in the 1880s to indicate competency at all of the basic techniques of the sport. Promotion beyond 1st dan (the first black belt rank) varies among different martial arts.[217] India did not withdraw from the 1950 FIFA World Cup because their squad played barefoot, which was against FIFA regulations.[220] In reality, India withdrew because the country's managing body, the All India Football Federation (AIFF), was insufficiently prepared for the team's participation and gave various reasons for withdrawing, including a lack of funding and prioritizing the Olympics.[221] There is no definitive proof that violent video games cause people to become violent. Some studies have found no link between aggression and violent video games,[222][223] and the popularity of gaming has coincided with a decrease in youth violence.[224][225] The moral panic surrounding video games in the 1980s through to the 2020s, alongside several studies and incidents of violence and legislation in many countries, likely contributed to proliferating this idea.[226][227] The Pyramids of Egypt were not constructed with slave labor. Archaeological evidence shows that the laborers were a combination of skilled workers and poor farmers working in the off-season with the participants paid in high-quality food and tax exemptions.[239][240][241] The idea that slaves were used originated with Herodotus, and the idea that they were Israelites arose centuries after the pyramids were constructed.[240][241][242] Galleys in ancient times were not commonly operated by chained slaves or prisoners, as depicted in films such as Ben Hur, but by paid laborers or soldiers,[243] with slaves used only in times of crisis, in some cases even gaining freedom after the crisis was averted. Ptolemaic Egypt was a possible exception.[244] Other types of vessel, such as merchant vessels (usually sailing vessels) were manned by slaves, sometimes even with slaves as ship's master.[245] The ancient Greeks did not use the word \"idiot\" (Ancient Greek: ἰδιώτης, romanized: idiṓtēs) to disparage people who did not take part in civic life or who did not vote. An ἰδιώτης was simply a private citizen as opposed to a government official. Later, the word came to mean any sort of non-expert or layman, then someone uneducated or ignorant, and much later to mean stupid or mentally deficient.[250] Julius Caesar was not born via caesarean section. Such a procedure would have been fatal to the mother at the time, and Caesar's mother was still alive when Caesar was 45 years old.[255][256] The name \"caesarean\" probably comes from the Latin verb caedere 'to cut'.[257] While modern life expectancies are much higher than those in the Middle Ages and earlier,[259] adults in the Middle Ages did not die in their 30s or 40s on average. That was the life expectancy at birth, which was skewed by high infant and adolescent mortality. The life expectancy among adults was much higher;[260] a 21-year-old man in medieval England, for example, could expect to live to the age of 64.[261][260] However, in various places and eras, life expectancy was noticeably lower, as in medieval London, where 90% of people in general died before the age of 45[262] and one study estimated that 36 percent of men and 56 percent of women in medieval urban areas passed before the age of 35.[263] Monks in this time period often died in their 20s or 30s.[263] In the tale of King Canute and the tide, the king did not command the tide to reverse in a fit of delusional arrogance.[268] According to the story, his intent was to prove a point to members of his privy council that no man is all-powerful, and that all people must bend to forces beyond their control, such as the tides. Marco Polo did not import pasta from China,[269] a misconception that originated with the Macaroni Journal, published by an association of food industries to promote the use of pasta in the United States.[270] Marco Polo describes a food similar to \"lasagna\" in his Travels, but he uses a term with which he was already familiar. There is no evidence that iron maidens were used for torture, or even yet invented, in the Middle Ages. Instead they were pieced together in the 18th century from several artifacts found in museums, arsenals and the like to create spectacular objects intended for commercial exhibition.[271] Spiral staircases in castles were not designed in a clockwise direction to hinder right-handed attackers.[272][273] While clockwise spiral staircases are more common in castles than anti-clockwise, they were even more common in medieval structures without a military role, such as religious buildings.[274][272] The plate armor of European soldiers did not stop soldiers from moving around or necessitate a crane to get them into a saddle. They would routinely fight on foot and could mount and dismount without help.[275] However, armor used in tournaments in the late Middle Ages was significantly heavier than that used in warfare,[276] which may have contributed to this misconception. Whether chastity belts, devices designed to prevent women from having sexual intercourse, were invented in medieval times is disputed by modern historians. Most existing chastity belts are now thought to be deliberate fakes or anti-masturbatory devices from the 19th and early 20th centuries.[277] Medieval cartographers did not regularly write \"here be dragons\" on their maps. The only maps from this era that have the phrase inscribed on them are the Hunt-Lenox Globe and the Ostrich Egg Globe, next to a coast in Southeast Asia for both of them. Maps instead were more likely to have \"here are lions\" inscribed. Maps in this period did occasionally have illustrations of mythical beasts like dragons and sea serpents, as well as exotic animals like elephants, on them.[279] The Mexica people of the Aztec Empire did not mistake Hernán Cortés and his landing party for gods during Cortés' conquest of the empire. This notion came from Francisco López de Gómara, who never went to Mexico and concocted the myth while working for the retired Cortés in Spain years after the conquest.[284] Shah Jahan, the Indian Mughal Emperor who commissioned the Taj Mahal, did not cut off the hands of the rumored 40,000 workers or lead designers so as to not allow the construction of another monument more beautiful than the Taj Mahal. This is an urban myth that goes back to the 1960s.[285][286][287] The early settlers of the Plymouth Colony in North America usually did not wear all black, and their capotains (hats) were shorter and rounder than the widely depicted tall hat with a buckle on it. Instead, their fashion was based on that of the late Elizabethan era.[288] The traditional image was formed in the 19th century when buckles were a kind of emblem of quaintness.[289] (The Puritans, who also settled in Massachusetts near the same time, did frequently wear all black.)[290] The familiar story that Isaac Newton was inspired to research the nature of gravity when an apple fell on his head is almost certainly apocryphal. All Newton himself ever said was that the idea came to him as he sat \"in a contemplative mood\" and \"was occasioned by the fall of an apple\".[291] Marie Antoinette did not say \"let them eat cake\" when she heard that the French peasantry were starving due to a shortage of bread. The phrase was first published in Rousseau's Confessions, written when Marie Antoinette was only nine years old and not attributed to her, just to \"a great princess\". It was first attributed to her in 1843.[293] George Washington did not have wooden teeth. His dentures were made of lead, gold, hippopotamus ivory, the teeth of various animals, including horse and donkey teeth,[294][295] and human teeth, possibly bought from slaves or poor people.[296][297] The possible origin of this myth is that ivory teeth quickly became stained and may have had the appearance of wood to observers.[295] Napoleon Bonaparte was not especially short for a Frenchman of his time. He was the height of an average French male in 1800, but short for an aristocrat or officer.[302] After his death in 1821, the French emperor's height was recorded as 5 feet 2 inches in French feet, which in English measurements is 5 feet 7 inches (1.70 m).[303][304] Albert Einstein did not fail mathematics classes in school. Einstein remarked, \"I never failed in mathematics.... Before I was fifteen I had mastered differential and integral calculus.\"[308] Einstein did, however, fail his first entrance exam into the Swiss Federal Polytechnic School (ETH) in 1895, when he was two years younger than his fellow students, but scored exceedingly well in the mathematics and science sections, and then passed on his second attempt.[309] Grigori Rasputin was not assassinated by being fed cyanide-laced cakes and wine, shot multiple times, and then thrown into the Little Nevka river when he survived the former two. A contemporary autopsy reported that he was just killed with gunshots. A sensationalized account from the memoirs of co-conspirator PrinceFelix Yusupov is the only source of this story.[311][312][313] The Italian dictator Benito Mussolini did not \"make the trains run on time\". Much of the repair work had been performed before he and the Fascist Party came to power in 1922. Moreover, the Italian railways' supposed adherence to timetables was more propaganda than reality.[314] The Nazis did not use the term \"Nazi\" to refer to themselves. The full name of the Nazi Party was Nationalsozialistische Deutsche Arbeiterpartei (National Socialist German Workers' Party), and members referred to themselves as Nationalsozialisten (National Socialists) or Parteigenossen (party comrades). The term \"Nazi\" was in use prior to the rise of the Nazis as a colloquial and derogatory word for a backwards farmer or peasant. Opponents of the National Socialists abbreviated their name as \"Nazi\" for derogatory effect and the term was popularized by German exiles outside of Germany.[316] Although popularly known as the \"red telephone\", the Moscow–Washington hotline was never a telephone line, nor were red phones used. The first implementation of the hotline used teletype equipment, which was replaced by facsimile (fax) machines in 1988. Since 2008, the hotline has been a secure computer link over which the two countries exchange email.[323] Moreover, the hotline links the Kremlin to the Pentagon, not the White House.[324] The Alaska Purchase was generally viewed as positive or neutral in the United States, both among the public and the press. The opponents of the purchase who characterized it as \"Seward's Folly\", alluding to William H. Seward, the Secretary of State who negotiated it, represented a minority opinion at the time.[341][342] There is no evidence that Frederic Remington, on assignment to Cuba in 1897, telegraphed William Randolph Hearst: \"There will be no war. I wish to return,\" nor that Hearst responded: \"Please remain. You furnish the pictures, and I'll furnish the war\". The anecdote was originally included in a book by James Creelman and probably never happened.[346] Immigrants' last names were not Americanized (voluntarily, mistakenly, or otherwise) upon arrival at Ellis Island. Officials there kept no records other than checking ship manifests created at the point of origin, and there was simply no paperwork that would have let them recast surnames, let alone any law. At the time in New York, anyone could change the spelling of their name simply by using that new spelling.[348] These names are often referred to as an \"Ellis Island Special\". Prohibition did not make drinking alcohol illegal in the United States. The Eighteenth Amendment and the subsequent Volstead Act prohibited the production, sale, and transport of \"intoxicating liquors\" within the United States, but their possession and consumption were never outlawed.[349] U.S. Senator George Smathers never gave a speech to a rural audience describing his opponent, Claude Pepper, as an \"extrovert\" whose sister was a \"thespian\", in the apparent hope they would confuse them with similar-sounding words like \"pervert\" and \"lesbian\". Smathers offered US$10,000 to anyone who could prove he had made the speech; it was never claimed.[358] Rosa Parks was not sitting in the front (\"white\") section of the bus during the event that made her famous and incited the Montgomery bus boycott. Rather, she was sitting in the front of the back (\"colored\") section of the bus, where African Americans were expected to sit, and rejected an order from the driver to vacate her seat in favor of a white passenger when the \"white\" section of the bus had become full.[359] When Kitty Genovese was murdered outside her apartment in 1964, there were not 38 neighbors standing idly by and watching who failed to call the police until after she was dead, as was initially reported[363] to widespread public outrage that persisted for years and even became the basis of a theory in social psychology. In fact, witnesses only heard brief portions of the attack and did not realize what was occurring, and only six or seven actually saw anything. One witness, who had called the police, said when interviewed by officers at the scene, \"I didn't want to get involved\",[364] an attitude later attributed to all the neighbors.[365] While it was praised by one architectural magazine before it was built as \"the best high apartment of the year\", the Pruitt–Igoehousing project in St. Louis, Missouri, considered to epitomize the failures of urban renewal in American cities after it was demolished in the early 1970s, never won any awards for its design.[366] The architectural firm that designed the buildings did win an award for an earlier St. Louis project, which may have been confused with Pruitt–Igoe.[367] There is little contemporary documentary evidence for the notion that US Vietnam veterans were spat upon by anti-war protesters upon return to the United States. This belief was detailed in some biographical accounts and was later popularized by films such as Rambo.[368][369][370] Women did not burn their bras outside the Miss America contest in 1969 as a protest in support of women's liberation. They did symbolically throw bras in a trash can, along with other articles seen as emblematic of women's position in American society such as mops, make-up, and high-heeled shoes. The myth of bra burning came when a journalist hypothetically suggested that women may do so in the future, as men of the era burned their draft cards.[371] Black holes have the same gravitational effects as any other equal mass in their place. They will draw objects nearby towards them, just as any other celestial body does, except at very close distances to the black hole, comparable to its Schwarzschild radius.[390] If, for example, the Sun were replaced by a black hole of equal mass, the orbits of the planets would be essentially unaffected. A black hole can pull in a substantial inflow of surrounding matter, but only if the star from which it formed was already doing so.[391] Egg balancing is possible on every day of the year, not just the vernal equinox,[398] and there is no relationship between any astronomical phenomenon and the ability to balance an egg.[399] The Fisher Space Pen was not commissioned by NASA at a cost of millions of dollars, while the Soviets used pencils. It was independently developed by Paul C. Fisher, founder of the Fisher Pen Company, with $1 million of his own funds.[400] NASA tested and approved the pen for space use, then purchased 400 pens at $6 per pen.[401] The Soviet Union subsequently also purchased the Space Pen for its Soyuz spaceflights.[402] The Sun is not yellow; rather, it emits light across the full spectrum of visible colors, and this combined light appears white when outside of Earth's atmosphere. Earth's atmosphere scatters shorter wavelengths of light, particularly blues and violets, more than longer wavelengths like reds and yellows, and this scattering is why the Sun appears yellow during the day or orange or red during sunrise and sunset.[405][406] A satellite image of a section of the Great Wall of China, running diagonally from lower left to upper right (not to be confused with the much more prominent river running from upper left to lower right). The region pictured is 12 by 12 kilometers (7.5 mi × 7.5 mi). The Big Bang model does not fully explain the origin of the universe. It does not describe how energy, time, and space were caused, but rather it describes the emergence of the present universe from an ultra-dense and high-temperature initial state.[408] Bulls are not enraged by the color red, used in capes by professional bullfighters. Cattle are dichromats, so red does not stand out as a bright color. It is not the color of the cape, but the perceived threat by the bullfighter that incites it to charge.[410] Lemmings do not engage in mass suicidal dives off cliffs when migrating. The scenes of lemming suicides in the 1958 Disney documentary film White Wilderness, which popularized this idea, were completely fabricated. The lemmings in the film were actually purchased from Inuit children for 25 cents apiece, transported to the filming location in Canmore, Alberta, and repeatedly shoved off a nearby cliff by the filmmakers to create the illusion of a mass suicide.[411][412] The misconception itself is much older, dating back to at least the late 19th century, though its exact origins are uncertain.[413] Dogs do not consistently age seven times as quickly as humans. Aging in dogs varies widely depending on the breed; certain breeds, such as giant dog breeds and English bulldogs, have much shorter lifespans than average. Most dogs age consistently across all breeds in the first year of life, reaching adolescence[clarification needed] by one year old; smaller and medium-sized breeds begin to age more slowly in adulthood.[416] The phases of the Moon have no effect on the vocalizations of wolves, and wolves do not howl at the Moon.[417] Wolves howl to assemble the pack usually before and after hunts, to pass on an alarm particularly at a den site, to locate each other during a storm, while crossing unfamiliar territory, and to communicate across great distances.[418] There is no such thing as an \"alpha\" in a wolf pack. An early study that coined the term \"alpha wolf\" had only observed unrelated adult wolves living in captivity. In the wild, wolf packs operate like families: parents are in charge until the young grow up and start their own families, and younger wolves do not overthrow an \"alpha\" to become the new leader.[419][420] Bats are not blind. While about 70% of bat species, mainly in the microbat family, use echolocation to navigate, all bat species have eyes and are capable of sight. In addition, almost all bats in the megabat or fruit bat family cannot echolocate and have excellent night vision.[421] Sharks can get cancer. The misconception that sharks do not get cancer was spread by the 1992 book Sharks Don't Get Cancer, which was used to sell extracts of shark cartilage as cancer prevention treatments. Reports of carcinomas in sharks exist, and current data do not support any conclusions about the incidence of tumors in sharks.[425] Great white sharks do not mistake human divers for seals or other pinnipeds. When attacking pinnipeds, the shark surfaces quickly and attacks violently. In contrast, attacks on humans are slower and less violent: the shark charges at a normal pace, bites, and swims off. Great white sharks have efficient eyesight and color vision; the bite is not predatory, but rather for identification of an unfamiliar object.[426] Snake jaws cannot unhinge. The posterior end of the lower jaw bones contains a quadrate bone, allowing jaw extension. The anterior tips of the lower jaw bones are joined by a flexible ligament allowing them to bow outwards, increasing the mouth gape.[427][428] Porcupines do not shoot their quills. They can detach, and porcupines will deliberately back into attackers to impale them, but their quills do not project.[431][432][433] Mice do not have a special appetite for cheese, and will eat it only for lack of better options; they actually favor sweet, sugary foods. The myth may have come from the fact that before the advent of refrigeration, cheese was usually stored outside and was therefore an easy food for mice to reach.[434] There is no credible evidence that the candiru, a South American parasitic catfish, can swim up a human urethra if one urinates in the water in which it lives. The sole documented case of such an incident, written in 1997, has been heavily criticized upon peer review, and this phenomenon is now largely considered a myth.[435] Piranhas do not eat only meat but are omnivorous, and they only swim in schools to defend themselves from predators and not to attack. They very rarely attack humans, only when under stress and feeling threatened, and even then, bites typically only occur on hands and feet.[438] The hippopotamus does not produce pink milk, nor does it sweat blood. The skin secretions of the hippopotamus are red due to the presence of hipposudoric acid, a red pigment which acts as a natural sunscreen, and is neither sweat or blood. It does not affect the color of their milk, which is white or beige.[439] A human touching or handling eggs or baby birds will not cause the adult birds to abandon them.[443] The same is generally true for other animals having their young touched by humans as well, with the possible exception of rabbits (as rabbits will sometimes abandon their nest after an event they perceive as traumatizing).[444] The bold, powerful cry commonly associated with the bald eagle in popular culture is actually that of a red-tailed hawk. Bald eagle vocalizations are much softer and chirpier, and bear far more resemblance to the calls of gulls.[451][452] Ostriches do not stick their heads in the sand to hide from enemies or to sleep.[453] This misconception's origins are uncertain but it was probably popularized by Pliny the Elder (23–79 CE), who wrote that ostriches \"imagine, when they have thrust their head and neck into a bush, that the whole of their body is concealed\".[454] A duck's quack actually does echo,[455] although the echo may be difficult to hear for humans under some circumstances.[456] Despite this, a British panel show compiling interesting facts has been given the name Duck Quacks Don't Echo. The skin of a chameleon is not adapted solely for camouflage purposes, nor can a chameleon change its skin colour to match any background.[459] Rabbits are not specially partial to carrots. Their diet in the wild primarily consists of dark green vegetables such as grasses and clovers, and excessive carrot consumption is unhealthy for them due to containing high levels of sugar. This misconception originated from Bugs Bunny cartoons, whose carrot-chomping habit was meant as a reference to a minor character in It Happened One Night.[460][461][462] Houseflies have an average lifespan of 20 to 30 days, not 24 hours.[465] The misconception may arise from confusion with mayflies, which, in one species, have an adult lifespan of as little as 5 minutes.[466] The daddy longlegs spider (Pholcidae) is not the most venomous spider in the world. Their fangs are capable of piercing human skin, but the tiny amount of venom they carry causes only a mild burning sensation for a few seconds.[467] Other species such as harvestmen, crane flies, and male mosquitoes are also called daddy longlegs in some regional dialects, and share the misconception of being highly venomous but unable to pierce the skin of humans.[468][469] People do not swallow large numbers of spiders during sleep. A sleeping person makes noises that warn spiders of danger.[470][471] Most people also wake up from sleep when they have a spider on their face.[472] Earwigs are not known to purposely climb into external ear canals, though there have been anecdotal reports of earwigs being found in the ear.[478] The name may be a reference to the appearance of their hindwings, which are unique and distinctive among insects, and resemble a human ear when unfolded.[479][480] While certainly critical to the pollination of many plant species, European honey bees are not essential to human food production, despite claims that without their pollination, humanity would starve or die out \"within four years\".[481] In fact, many important crops need no insect pollination at all. The ten most important crops,[482] accounting for 60% of all human food energy,[483] all fall into this category. Ticks do not jump or fall from trees onto their hosts. Instead, they lie in wait to grasp and climb onto any passing host or otherwise trace down hosts via, for example, olfactory stimuli, the host's body heat, or carbon dioxide in the host's breath.[484][485] Poinsettias are not highly toxic to humans or cats. While it is true that they are mildly irritating to the skin or stomach,[497] and may sometimes cause diarrhea and vomiting if eaten, they rarely cause serious medical problems.[498] Sunflowers do not always point to the Sun. Flowering sunflowers face a fixed direction (often east) all day long, but do not necessarily face the Sun.[499] However, in an earlier developmental stage, before the appearance of flower heads, the immature buds do track the Sun (a phenomenon called heliotropism), and the fixed alignment of the mature flowers toward a certain direction is often the result.[500] The word theory in \"the theory of evolution\" does not imply scientific doubt regarding its validity; the concepts of theory and hypothesis have specific meanings in a scientific context. While theory in colloquial usage may denote a hunch or conjecture, a scientific theory is a set of principles that explains an observable phenomenon in natural terms.[504][505] \"Scientific fact and theory are not categorically separable\",[506] and evolution is a theory in the same sense as germ theory or the theory of gravitation.[507] The theory of evolution does not attempt to explain the origin of life[508] or the origin and development of the universe. The theory of evolution deals primarily with changes in successive generations over time after life has already originated.[509] The scientific model concerned with the origin of the first organisms from organic or inorganic molecules is known as abiogenesis, and the prevailing theory for explaining the early development of the universe is the Big Bang model. Mutations are not entirely random, nor do they occur at the same frequency everywhere in the genome. Certain regions of an organism's genome will be more or less likely to undergo mutation depending on the presence of DNA repair mechanisms and other mutation biases. For instance, in a study on Arabidopsis thaliana, biologically important regions of the plant's genome were found to be protected from mutations, and beneficial mutations were found to be more likely, i.e. mutation was \"non-random in a way that benefits the plant\".[522][523][524] Dimetrodon is often mistakenly called a dinosaur or considered to be a contemporary of dinosaurs in popular culture, but it became extinct some 40 million years before the first appearance of dinosaurs. Being a synapsid, Dimetrodon is actually more closely related to mammals than to dinosaurs, birds, lizards, or other diapsids.[535][536][537][538] Humans and aviandinosaurs currently coexist, but humans and non-avian dinosaurs did not coexist at any point.[540] The last of the non-avian dinosaurs died 66 million years ago in the course of the Cretaceous–Paleogene extinction event, whereas the earliest members of the genus Homo (humans) evolved between 2.3 and 2.4 million years ago. This places a 63-million-year expanse of time between the last non-avian dinosaurs and the earliest humans. Humans did coexist with woolly mammoths and saber-toothed cats: extinct mammals often erroneously depicted alongside non-avian dinosaurs.[541] Most diamonds are not formed from highly compressed coal. More than 99% of diamonds ever mined have formed in the conditions of extreme heat and pressure about 140 kilometers (87 mi) below the earth's surface. Coal is formed from prehistoric plants buried much closer to the surface, and is unlikely to migrate below 3.2 kilometers (2.0 mi) through common geological processes. Most diamonds that have been dated are older than the first land plants, and are therefore older than coal.[569] Diamonds are not infinitely hard, and are subject to wear and scratching: although they are the hardest known material on the Mohs Scale, they can be scratched by other diamonds[570] and worn down even by much softer materials, such as vinyl records.[571] Although the core of a wooden pencil is commonly referred to as \"lead\", wooden pencils do not contain the chemical element lead, nor have they ever contained it; \"black lead\" was formerly a name of graphite, which is commonly used for pencil leads.[573] The deep web is not primarily full of pornography, illegal drug trade websites, and stolen bank details. This information is primarily found in a small portion of the deep web known as the \"dark web\". Much of the deep web consists of academic libraries, databases, and anything that is not indexed by normal search engines.[576] Total population living in extreme poverty, by world region 1987 to 2015[583] The total number of people living in extremeabsolute poverty globally, by the widely used metric of $1.00/day (in 1990 U.S. dollars) has decreased over the last several decades, but most people surveyed in several countries incorrectly think it has increased or stayed the same.[584] However, this depends on the poverty line calculation used. For instance, if the metric used is instead one that prioritizes meeting a standard life expectancy that no longer significantly rises with additional consumption enabled by income, the number of individuals in poverty has risen by nearly 1 billion.[585][586] Human population growth is decreasing and the world population is expected to peak and then begin falling during the 21st century. Improvements in agricultural productivity and technology are expected to be able to meet anticipated increased demand for resources, making a global human overpopulation scenario unlikely.[587][588][589] For any given production set, there is not a set amount of labor input (a \"lump of labor\") to produce that output. This fallacy is commonly seen in Luddite and later, related movements as an argument either that automation causes permanent, structural unemployment, or that labor-limiting regulation can decrease unemployment. In fact, changes in capital allocation, efficiency, and economies of learning can change the amount of labor input for a given set of production.[590] Income is not a direct factor in determining credit score in the United States. Rather, credit score is affected by the amount of unused available credit, which is in turn affected by income.[591] Income is also considered when evaluating creditworthiness more generally. In the US, an increase in gross income will never reduce a taxpayer's post-tax earnings (net income) by putting them in a higher tax bracket. Tax brackets specify marginal tax rates: only income earned in the higher tax bracket is taxed at the higher rate.[593] An increase in gross income can reduce net income in a welfare cliff, however, when benefits are withdrawn when passing a certain income threshold.[594] Prevalence of the misconception varies by political party affiliation.[595] Constructing new housing decreases the cost of rent or of buying a home in both the immediate neighborhood and in the city as a whole. In real estate economics, \"supply skepticism\" leads many Americans to misunderstand the effect of increasing the supply of housing on housing costs. The misconception is unique to the housing market.[596][597] Earthquake strength (or magnitude) is not commonly measured using the Richter scale. Although the Richter scale was used historically to measure earthquake magnitude (although, notably, not earthquake damage), it was found in the 1970s that it does not reliably represent the magnitude of large earthquakes. It has therefore been largely replaced by the moment magnitude scale,[619] although very small earthquakes are still sometimes measured using the Richter scale.[620] Nevertheless, earthquake magnitude is still widely misattributed to the Richter scale.[621][622][623]Death rates from air pollution and accidents related to energy production, measured in deaths in the past per terawatt hours (TWh) Lightning can, and often does, strike the same place twice. Lightning in a thunderstorm is more likely to strike objects and spots that are more prominent or conductive. For instance, lightning strikes the Empire State Building in New York City on average 23 times per year.[624] Heat lightning does not exist as a distinct phenomenon. What is mistaken for \"heat lightning\" is usually ordinary lightning from storms too distant to hear the associated thunder.[625] The Earth's interior is not molten rock. This misconception may originate from a misunderstanding based on the fact that the Earth's mantle convects, and the incorrect assumption that only liquids and gases can convect. In fact, a solid with a large Rayleigh number can also convect, given enough time, which is what occurs in the solid mantle due to the very large thermal gradient across it.[630][631] There are small pockets of molten rock in the upper mantle, but these make up a tiny fraction of the mantle's volume.[632] The Earth's outer coreis liquid, but it is liquid metal, not rock.[633] The Amazon rainforest does not provide 20% of Earth's oxygen. This is a misinterpretation of a 2010 study which found that approximately 34% of photosynthesis by terrestrial plants occurs in tropical rainforests (so the Amazon rainforest would account for approximately half of this). Due to respiration by the resident organisms, all ecosystems (including the Amazon rainforest) have a net output of oxygen of approximately zero. The oxygen currently present in the atmosphere was accumulated over billions of years.[634] Rivers do not predominantly flow from north to south. Rivers flow downhill in all compass directions, often changing direction along their course.[636][637] Indeed, many major rivers flow northward, including the Nile, the Yenisey, the Ob, the Rhine, the Lena, and the Orinoco.[638][639] Waking up a sleepwalker does not harm them. Sleepwalkers may be confused or disoriented for a short time after awakening, but the health risks associated with sleepwalking are from injury or insomnia, not from being awakened.[641] Seizures cannot cause a person to swallow their own tongue,[642] and it is dangerous to attempt to place a foreign object into a convulsing person's mouth. Instead it is recommended to gently lay a convulsing person on their side to minimize the risk of asphyxiation.[643] Drowning is often inconspicuous to onlookers.[644] In most cases, the instinctive drowning response prevents the victim from waving or yelling (known as \"aquatic distress\"),[644] which are therefore not dependable signs of trouble; indeed, most drowning victims undergoing the response do not show prior evidence of distress.[645] Human blood in veins is not actually blue. Blood is red due to the presence of hemoglobin; deoxygenated blood (in veins) has a deep red color, and oxygenated blood (in arteries) has a light cherry-red color. Veins below the skin can appear blue or green due to subsurface scattering of light through the skin, and aspects of human color perception. Many medical diagrams also use blue to show veins, and red to show arteries, which contributes to this misconception.[646] Exposure to a vacuum, or experiencing all but the most extreme uncontrolled decompression, does not cause the body to explode or internal fluids to boil (although the fluids in the mouth and lungs will indeed boil at altitudes above the Armstrong limit); rather, it will lead to a loss of consciousness once the body has depleted the supply of oxygen in the blood, followed by death from hypoxia within minutes.[647] Exercise-induced delayed onset muscle soreness is not caused by lactic acid build-up. Muscular lactic acid levels return to normal levels within an hour after exercise; delayed onset muscle soreness is thought to be due to microtrauma from unaccustomed or strenuous exercise.[648] Cremated remains are not ashes in the usual sense. After the incineration is completed, the dry bone fragments are swept out of the retort and pulverized by a machine called a cremulator (essentially a high-capacity, high-speed blender) to process them into \"ashes\" or \"cremated remains\".[652] Half of body heat is not lost through the head, and covering the head is no more effective at preventing heat loss than covering any other portion of the body. Heat is lost from the body in proportion to the amount of exposed skin.[654][655] The head accounts for around 7–9% of the body's surface, and studies have shown that having one's head submerged in cold water only causes a person to lose 10% more heat overall.[656] This myth likely comes from a flawed United States military experiment in 1950, involving a prototype Arctic survival suit where the head was one of the few body parts left exposed.[657] The misconception was further perpetuated by a 1970 military field manual that claimed \"40–45%\" of heat is lost through the head, based on the 1950 study.[655][657] Adrenochrome is not harvested from living people and has no use as a recreational drug. Hunter S. Thompson conceived a fictional drug of the same name in his book Fear and Loathing in Las Vegas, apparently as a metaphor and unaware that a real substance by that name existed; it is Thompson's fictional adrenochrome, and not the real chemical compound, that is the source of numerous conspiracy theories revolving around human trafficking to harvest the fictional drug.[658][659] The use of cotton swabs (aka cotton buds or Q-Tips) in the ear canal has no associated medical benefits and poses definite medical risks.[661] The idea that a precise number of stages of grief exist is not supported in peer-reviewed research or objective clinical observation, let alone the five stages of grief model.[662] The model was originally based on uncredited work and originally applied to the terminally ill instead of the grieving or bereaved.[663] The common cold and the common flu are caused by viruses, not exposure to cold temperatures. However, low temperatures may somewhat weaken the immune system, and someone already infected with a cold or influenza virus but showing no symptoms can become symptomatic after they are exposed to low temperatures.[664][665] Viruses are more likely to spread during the winter for a variety of reasons such as dry air, less air circulation in homes, people spending more time indoors, and lower vitamin D levels in humans.[666][667][668] There is little to no evidence that any illnesses are curable through essential oils or aromatherapy. Fish oil has not been shown to cure dementia, though there is evidence to support the effectiveness of lemon oil as a way to reduce agitation in patients with dementia.[672] In those with the common cold, the color of the sputum or nasal secretion may vary from clear to yellow to green and does not indicate the class of agent causing the infection.[673] The color of the sputum is determined by immune cells fighting an infection in the nasal area.[674] Vitamin Cdoes not prevent or treat the common cold, although it may have a protective effect during intense cold-weather exercise. If taken daily, it may slightly reduce the duration and severity of colds, but it has no effect if taken after the cold starts.[675] In people with eczema, bathing does not dry the skin as long as a moisturizer is applied soon after. If moisturizer is not applied after bathing, then the evaporation of water from the skin can result in dryness.[678] There have never been any programs in the US that provide access to dialysis machines in exchange for pull tabs on beverage cans.[679] This rumor has existed since at least the 1970s, and usually cites the National Kidney Foundation as the organization offering the program. The Foundation itself has denied the rumor, noting that dialysis machines are primarily funded by Medicare.[680] High dietary protein intake is not associated with kidney disease in healthy people.[681] While significantly increased protein intake in the short-term is associated with changes in renal function, there is no evidence to suggest this effect persists in the long-term and results in kidney damage or disease.[682] Leprosy is not auto-degenerative as commonly supposed, meaning that it will not (on its own) cause body parts to be damaged or fall off.[684] Leprosy causes rashes to form and may degrade cartilage and, if untreated, inflame tissue. In addition, leprosy is only mildly contagious, partly because 95% of those infected with the mycobacteria that causes leprosy do not develop the disease.[685][684]Tzaraath, a Biblical disease that disfigures the skin is often identified as leprosy, and may be the source of many myths about the disease.[686] Rust does not cause tetanus infection. The Clostridium tetani bacterium is generally found in dirty environments. Since the same conditions that harbor tetanus bacteria also promote rusting of metal, many people associate rust with tetanus. C. tetani requires anoxic conditions to reproduce and these are found in the permeable layers of rust that form on oxygen-absorbing, unprotected ironwork.[687] Quarantine has never been a standard procedure for those with severe combined immunodeficiency, despite the condition's popular nickname (\"bubble boy syndrome\") and its portrayal in films. A bone marrow transplant in the earliest months of life is the standard course of treatment. The exceptional case of David Vetter, who lived much of his life encased in a sterile environment because he would not receive a transplant until age 12, was an inspirations for the \"bubble boy\" trope.[688] Gunnison, Colorado, did not avoid the 1918 flu pandemic by using protective sequestration. The implementation of protective sequestration did prevent the virus from spreading outside a single household after a single carrier came into the town while it was in effect, but it was not sustainable and had to be lifted in February 1919. A month later, the flu killed five residents and infected dozens of others.[689] Statements in medication package inserts listing the frequency of side effects describe how often the effect occurs after taking a drug, but are not making any assertion that there is a causal connection between taking the drug and the occurrence of the side effect. In other words, what is being reported on is correlation, not necessarily causation.[690] A dog's mouth is not significantly cleaner than a human's mouth. A dog's mouth contains almost as much bacteria as a human mouth.[691][692] Drinking milk or consuming other dairy products does not increase mucus production.[702] As a result, they do not need to be avoided by those with the flu or cold congestion. However, milk and saliva in one's mouth mix to create a thick liquid that can briefly coat the mouth and throat. The sensation that lingers may be mistaken for increased phlegm.[703] Drinking eight glasses (2–3 liters) of water a day is not needed to maintain health.[704] The amount of water needed varies by person, weight, diet, activity level, clothing, and the ambient heat and humidity. Water does not actually need to be drunk in pure form, and can be derived from liquids such as juices, tea, milk, soups, etc., and from foods including fruits and vegetables.[704][705] Drinking coffee and other caffeinated beverages does not cause dehydration for regular drinkers, although it can for occasional drinkers.[706][705] Eating less than an hour before swimming does not increase the risk of experiencing muscle cramps or drowning. One study shows a correlation between alcohol consumption and drowning, but not between eating and stomach cramps.[712] Spinach is not a particularly good source of dietary iron. While it does contain more iron than many vegetables such as asparagus, Swiss chard, kale, or arugula, it contains only about one-third to one-fifth of the iron in lima beans, chickpeas, apricots, or wheat germ. Additionally, the non-heme iron found in spinach and other vegetables is not as readily absorbed as the heme iron found in meats and fish.[725][726][727] Most cases of obesity are not related to slower resting metabolism. Resting metabolic rate does not vary much between people. Overweight people tend to underestimate the amount of food they eat, and underweight people tend to overestimate. In fact, overweight people tend to have faster metabolic rates due to the increased energy required by the larger body.[728] Alcoholic beverages do not make the entire body warmer.[730] Alcoholic drinks create the sensation of warmth because they cause blood vessels to dilate and stimulate nerve endings near the surface of the skin with an influx of warm blood. This can actually result in making the core body temperature lower, as it allows for easier heat exchange with a cold external environment.[731] Alcohol does not necessarily kill brain cells.[732] Alcohol can, however, lead indirectly to the death of brain cells in two ways. First, in chronic, heavy alcohol users whose brains have adapted to the effects of alcohol, abrupt ceasing following heavy use can cause excitotoxicity leading to cellular death in multiple areas of the brain.[733] Second, in alcoholics who get most of their daily calories from alcohol, a deficiency of thiamine can produce Korsakoff's syndrome, which is associated with serious brain damage.[734] The order in which different types of alcoholic beverages are consumed (\"Grape or grain but never the twain\" and \"Beer before liquor never sicker; liquor before beer in the clear\") does not affect intoxication or create adverse side effects.[735] Authentic absinthe has no hallucinogenic properties, and is no more dangerous than any other alcoholic beverage of equivalent proof.[736] This misconception stems from late-19th- and early-20th-century distillers who produced cheap knockoff versions of absinthe, which used copper salts to recreate the distinct green color of true absinthe, and some also reportedly adulterated cheap absinthe with poisonous antimony trichloride, reputed to enhance the louching effect.[737] Examination of the hymen is not an accurate or reliable indicator that a woman or girl has had penetrative sex, because the tearing of the hymen may have been the result of some other event, and some women are born without one.[739][740][741] Traditional virginity tests, such as the \"two-finger\" test, are widely considered to be unscientific.[742][743][744] While pregnancies from sex between first cousins do carry a slightly elevated risk of birth defects, this risk is often exaggerated.[748] The risk is 5–6% (similar to that of a woman in her early 40s giving birth),[748][749] compared with a baseline risk of 3–4%.[749] The effects of inbreeding depression, while still relatively small compared to other factors (and thus difficult to control for in a scientific experiment), become more noticeable if isolated and maintained for several generations.[750] Having sex before a sporting event or contest is not physiologically detrimental to performance.[751] In fact it has been suggested that sex prior to sports activity can elevate male testosterone levels, which could potentially enhance performance for male athletes.[752] There is no definitive proof of the existence of the vaginal G-spot, and the general consensus is that no such spot exists on the female body.[753] A person's hair and fingernails do not continue to grow after death. Rather, the skin dries and shrinks away from the bases of hairs and nails, giving the appearance of growth.[760] Shaving does not cause terminal hair to grow back thicker or darker. This belief is thought to be due to the fact that hair that has never been cut has a tapered end, so after cutting, the base of the hair is blunt and appears thicker and feels coarser. That short hairs are less flexible than longer hairs contributes to this effect.[761] MC 1 R, the gene mostly responsible for red hair, is not becoming extinct, nor will the gene for blond hair do so, although both are recessivealleles. Redheads and blonds may become rarer but will not die out unless everyone who carries those alleles dies without passing their hair color genes on to their children.[762] Acne is mostly caused by genetics, and is not caused by a lack of hygiene or eating fatty foods, though certain medication or a carbohydrate-rich diet may worsen it.[763] Dandruff is not caused by poor hygiene, though infrequent hair-washing can make it more obvious. The exact causes of dandruff are uncertain, but they are believed to be mostly genetic and environmental factors.[764] James Watt did not invent the steam engine,[765] nor were his ideas on steam engine power inspired by a kettle lid pressured open by steam.[766] Watt improved upon the already commercially successful Newcomen atmospheric engine (invented in 1712) in the 1760s and 1770s, making certain improvements critical to its future usage, particularly the external condenser, increasing its efficiency, and later the mechanism for transforming reciprocating motion into rotary motion; his new steam engine later gained huge fame as a result.[767] Thomas Edison did not invent the light bulb.[774] He did, however, develop the first practical light bulb in 1880 (employing a carbonized bamboo filament), shortly prior to Joseph Swan, who invented an even more efficient bulb in 1881 (which used a cellulose filament). Henry Ford did not invent either the automobile or the assembly line. He did improve the assembly line process substantially, sometimes through his own engineering but more often through sponsoring the work of his employees, and he was the main person behind the introduction of the Model T, regarded as the first affordable automobile.[775]Karl Benz (co-founder of Mercedes-Benz) is credited with the invention of the first modern automobile,[776] and the assembly line has existed throughout history. The repeating decimal commonly written as 0.999... represents exactly the same quantity as the number one. Despite having the appearance of representing a smaller number, 0.999... is a symbol for the number 1 in exactly the same way that 0.333... is an equivalent notation for the number represented by the fraction 1⁄3.[790] The p-value is not the probability that the null hypothesis is true, or the probability that the alternative hypothesis is false; it is the probability of obtaining results at least as extreme as the results actually observed under the assumption that the null hypothesis was correct, which can indicate the incompatibility of results with the specific statistical model assumed in the null hypothesis.[791] This misconception, and similar ones like it, contributes to the common misuse of p-values in education and research.[791][792] An illustration of the (incorrect) equal-transit-time explanation of aerofoil lift The lift force is not generated by the air taking the same time to travel above and below an aircraft's wing.[794] This misconception, sometimes called the equal transit-time fallacy, is widespread among textbooks and non-technical reference books, and even appears in pilot training materials. In fact, the air moving over the top of an aerofoil generating lift is always moving much faster than the equal transit theory would imply,[794] as described in the incorrect and correct explanations of lift force. Blowing over a curved piece of paper does not demonstrate Bernoulli's principle. Although a common classroom experiment is often explained this way,[795] Bernoulli's principle only applies within a flow field, and the air above and below the paper is in different flow fields.[796] The paper rises because the air follows the curve of the paper and a curved streamline will develop pressure differences perpendicular to the airflow.[797][798] The Coriolis effect does not cause water to consistently drain from basins in a clockwise/counter-clockwise direction depending on the hemisphere. The common myth often refers to the draining action of flush toilets and bathtubs. In fact, rotation is determined by whatever minor rotation is initially present at the time the water starts to drain, as the magnitude of the coriolis acceleration is negligibly small compared to the inertial acceleration of flow within a typical basin.[799] A penny dropped from the Empire State Building would not kill a person or crack the sidewalk. A penny is too light and has too much air resistance to acquire enough speed to do much damage since it reaches terminal velocity after falling about 50 feet. Heavier or more aerodynamic objects could cause significant damage if dropped from that height.[802][803] Using a programmable thermostat's setback feature to limit heating or cooling in a temporarily unoccupied building does not waste as much energy as leaving the temperature constant. Using setback saves energy (5–15%) because heat transfer across the surface of the building is roughly proportional to the temperature difference between its inside and the outside.[804][805] It is not possible for a person to completely submerge in quicksand, as commonly depicted in fiction,[806] although sand entrapment in the nearshore of a body of water can be a drowning hazard as the tide rises.[807] True photographic memory (the ability to remember endless images, particularly pages or numbers, with such a high degree of precision that the image mimics a photo) has never been demonstrated to exist in any individual,[812] although a small number of young children have eidetic memory, where they can recall an object with high precision for a few minutes after it is no longer present.[813] Many people have claimed to have a photographic memory, but those people have been shown to have high precision memories as a result of mnemonic devices rather than a natural capacity for detailed memory encoding.[814] There are rare cases of individuals with exceptional memory, but none of them have a memory that mimics that of a camera. The phase of the Moon does not influence fertility, cause a fluctuation in crime, or affect the stock market. There is no correlation between the lunar cycle and human biology or behavior. However, the increased amount of illumination during the full moon may account for increased epileptic episodes, motorcycle accidents, or sleep disorders.[815] Dyslexia is not defined or diagnosed as mirror writing or reading letters or words backwards.[817][818] Mirror writing and reading letters or words backwards are behaviors seen in many children (dyslexic or not) as they learn to read and write.[817][818]Dyslexia is a neurodevelopmental disorder of people who have at least average intelligence and who have difficulty in reading and writing that is not otherwise explained by low intelligence.[819] Self-harm is not generally an attention-seeking behavior. People who engage in self-harm are typically very self-conscious of their wounds and scars and feel guilty about their behavior, leading them to go to great lengths to conceal it from others.[820] They may offer alternative explanations for their injuries, or conceal their scars with clothing.[821][822] There is no evidence that a chemical imbalance or neurotransmitter deficiency is the sole factor in depression and other mental disorders, but rather a combination of biological, psychological, and social factors.[823][824] Schizophrenia is characterized by continuous or relapsing episodes of psychosis. Major symptoms include hallucinations (typically hearing voices), delusions, paranoia, and disorganized thinking. Other symptoms include social withdrawal, decreased emotional expression, and apathy.[825] The term was coined from the Greek roots schizein and phrēn, \"to split\" and \"mind\", in reference to a \"splitting of mental functions\" seen in schizophrenia, not a splitting of the personality.[826] It does not involve split or multiple personalities—a split or multiple personality is dissociative identity disorder.[827] Broad generalizations are often made in popular psychology about certain brain functions being lateralized, or more predominant in one hemisphere than the other. These claims are often inaccurate or overstated.[828] The human brain, particularly the prefrontal cortex, does not reach \"full maturity\" at any particular age (e.g. 18, 21, or 25 years of age). Changes in structure and myelination of gray matter are recorded to continue with relative consistency all throughout adult life. Some mental abilities peak and begin to decline around high school graduation while others do not peak until much later (i.e. 40s or later).[829] Golgi-stained neurons in human hippocampal tissue. It is commonly believed that humans will not grow new brain cells, but research has shown that some neurons can reform in humans. Humans do not generate all of the brain cells they will ever have by the age of two years. Although this belief was held by medical experts until 1998, it is now understood that new neurons can be created after infancy in some parts of the brain into late adulthood.[830] People do not use only 10% of their brains.[831][832] While it is true that a small minority of neurons in the brain are actively firing at any one time, a healthy human will normally use most of their brain over the course of a day, and the inactive neurons are important as well. The idea that activating 100% of the brain would allow someone to achieve their maximum potential and/or gain various psychic abilities is common in folklore and fiction,[832][833][834] but doing so in real life would likely result in a fatal seizure.[835][836] This misconception was attributed to late 19th century leading thinker William James, who apparently used the expression only metaphorically.[833] Although Phineas Gage's brain injuries, caused by a several-foot-long tamping rod driven completely through his skull, caused him to become temporarily disabled, many fanciful descriptions of his aberrant behavior in later life are without factual basis or contradicted by known facts.[837] All different tastes can be detected on all parts of the tongue by taste buds,[840] with slightly increased sensitivities in different locations depending on the person; the tongue map showing the contrary is fallacious.[841] Swallowing gasoline does not generally require special emergency treatment, as long as it goes into the stomach and not the lungs, and inducing vomiting can make it worse.[850] A chloroform-soaked rag cannot instantly incapacitate a person.[851] It takes at least five minutes of inhaling an item soaked in chloroform to render a person unconscious. Most criminal cases involving chloroform also involve another drug being co-administered, such as alcohol or diazepam, or the victim being found to have been complicit in its administration. The misconception that chloroform can be used as an incapacitating agent[852] has been popularized by crime fiction authors. Toilet waste is never intentionally jettisoned from an aircraft. All waste is collected in tanks and emptied into toilet waste vehicles.[858]Blue ice is caused by accidental leakage from the waste tank. Passenger train toilets, on the other hand, have indeed historically flushed onto the tracks; modern trains in most developed countries usually have retention tanks on board and therefore do not dispose of waste in such a manner. Automotive batteries stored on a concrete floor do not discharge any faster than they would on other surfaces,[859] in spite of worry among Americans that concrete harms batteries.[860] Early batteries with porous, leaky cases may have been susceptible to moisture from floors, but for many years lead–acid car batteries have had impermeable polypropylene cases.[861] While most modern automotive batteries are sealed, and do not leak battery acid when properly stored and maintained,[862] the sulfuric acid in them can leak out and stain, etch, or corrode concrete floors if their cases crack or tip over or their vent-holes are breached by floods.[863] ^Myre G (February 28, 2018). \"A Brief History Of The AR-15\". National Public Radio. Archived from the original on May 13, 2023. Retrieved November 20, 2021. AR\" comes from the name of the gun's original manufacturer, ArmaLite, Inc. The letters stand for ArmaLite Rifle — and not for \"assault rifle\" or \"automatic rifle. ^Rhoads C (January 19, 2008). \"The Hydrox Cookie Is Dead, and Fans Won't Get Over It\". The Wall Street Journal. Retrieved July 6, 2022. In college, when friends ridiculed her for preferring the cheaper knock-off Hydrox to the real thing, she did some research. Among her findings: Hydrox was created in 1908 by what would later become Sunshine Biscuits Inc. That was four years before the National Biscuit Co. (later called Nabisco) came up with the similar Oreo. Oreo was the knock-off. The Hydrox name came from combining the words hydrogen and oxygen, which Sunshine executives thought evoked purity. Others thought it sounded more like a laundry detergent. ^Wheeling K (January 2021). \"A Brief History of Peanut Butter\". Smithsonian Magazine. Retrieved June 24, 2022. North Americans weren't the first to grind peanuts—the Inca beat us to it by a few hundred years—but peanut butter reappeared in the modern world because of an American, the doctor, nutritionist and cereal pioneer John Harvey Kellogg, who filed a patent for a proto-peanut butter in 1895. ^McElwain A (June 17, 2019). \"Did Tayto really invent cheese and onion crisps?\". Irish Times. Retrieved June 23, 2022. One of the oldest known published recipes for crisps is by William Kitchiner, an optician who doubled up as a Georgian-era celebrity chef. His book, A Cook's Oracle, published in 1817, was a big hit in the UK and a young America. Kitchiner's recipe – Potatoes fried in Slices or Shavings – calls for slivers of potato fried in \"lard or dripping\" and \"served with a very little salt sprinkled over them\". ^Burhans D (2008). \"Creation Myths\". Crunch!: A History of the Great American Potato Chip. University of Wisconsin Press. pp. 17–20. ISBN978-0-299-22770-8. ^Di Placido D (July 19, 2017). \"The Evolution Of The Zombie\". Forbes. Retrieved July 3, 2022. George A. Romero's first zombie film Night of the Living Dead is credited with popularizing the zombie, though it never actually uses that word. The \"ghouls\" in the film are mindless flesh-eaters that have little in common with the Haitian zombie other than rising from the grave. ^ abEschner K (October 31, 2017). \"Zombie Movies Are Never Really About Zombies\". Smithsonian Magazine. Retrieved July 3, 2022. In the 1960s and 70s, filmmaker George Romero brought the zombie film into the mainstream with Night of the Living Dead and Dawn of the Dead. The first of these was technically about \"ghouls.\" Romero didn't start calling them \"zombies\" until his second film. But his now-iconic films helped to erase enslaved people from zombie history. ^Sisterson D (March 28, 2017). \"Magic Wilderness: El Apóstol & Peludópolis\". Skwigly. Retrieved June 22, 2022. As we all know, Disney's Snow White and the Seven Dwarfs is usually cited as the first animated feature, but as most of us who read this site are no doubt aware, it wasn't. It was preceded by Lotte Reiniger's The Adventures of Prince Achmed, Ladislas Starevitch's The Tale of the Fox, and two features by the Argentinian animator Quirino Cristiani – all films which could scracely [sic] be more different from the Disney mode. ^Bendazzi G (2017). \"The First Feature Length Animated Film in History\". Twice the First: Quirino Cristiani and the Animated Feature Film. CRC Press. p. 36. ISBN978-1-351-37179-7. On the other hand, the movie was not widely successful, and appealed to a small portion of the population. It was strictly for a Buenos Aires audience: nobody in the provinces even saw it because it was not distributed there. And likewise, given the subject, it was not possible to export the film to other nations, not even to a close cousin similar to Uruguay. ^US 941960 A, Smith, George Albert, \"Kinematograph apparatus for the production of colored pictures\", issued 1909-11-30 ^ \"Irregardless originated in dialectal American speech in the early 20th century... The most frequently repeated remark about it is that \"there is no such word.\" There is such a word, however.\" Merriam Webster Dictionary \"Definition of IRREGARDLESS\". Archived from the original on May 8, 2014. Retrieved October 27, 2011. ^Churchwell S (June 23, 2019). \"For sale, baby shoes, never worn — the myth of Ernest Hemingway's short story\". The Times. Retrieved April 5, 2023. For a long time, legend held that this was one of the world's great short stories, by one of the world's great short-story writers: Ernest Hemingway. There were different versions of the myth . . . None of it is true. And for those who think the internet is a cesspool of lies, it is an interesting experiment to google those six words today. The top items, including a Wikipedia entry on the myth, debunk it as the urban legend it is. ^William Pryse-Phillips (2003). Companion to Clinical Neurology. Oxford University Press. ISBN0-19-515938-1., p. 611 defines the term as \"Slight and transient improvement in spational[sic] reasoning skills detected in normal subjects as a result of exposure to the music of Mozart, specifically his sonata for two pianos (K448).\" ^Maurice Hinson (2004). The Pianist's Dictionary. Indiana University Press. p. 114. ISBN978-0-253-21682-3. Retrieved October 2, 2010. This piece bears an erroneous nickname since the story long associated with this nickname presumes the pianist is supposed to play the piece in one minute. The word \"minute\" means small or little waltz. ^Kennedy S (December 2, 2005). \"Dragon Quest vs. America\". 1up. Archived from the original on July 28, 2012. Retrieved June 26, 2022. Predating Xbox 360 hysteria by years, several fans were mugged on their way home with their new prize, and the situation became so bad that it was brought before the Japanese Diet. Although tales of a law requiring Dragon Quest games only be released on the mornings of weekends or holidays are the stuff of urban legend, each new title is as highly anticipated as the launch of a new console. ^Walker M (August 19, 2012). \"Dragon Quest X Online: Mezameshi Itsutsu no Shuzoku\". Nintendo World Report. Retrieved June 26, 2022. Its Thursday release is unheard of for a Dragon Quest game, which are generally released over the weekend so people don't take work off in droves to play them. ^ abWatterson B (1997). \"The Era of Pyramid-builders\". The Egyptians. Blackwell. p. 63. Herodotus claimed that the Great Pyramid at Giza was built with the labour of 100,000 slaves working in three-monthly shifts, a charge that cannot be substantiated. Much of the non-skilled labour on the pyramids was undertaken by peasants working during the Inundation season when they could not farm their lands. In return for their services they were given rations of food, a welcome addition to the family diet. ^ a. Neer R (2012). Art and Archaeology of the Greek World. Thames and Hudson. p. 37. ISBN978-0-500-05166-5. \"...popular associations of the eruption with a legend of Atlantis should be dismissed...nor is there good evidence to suggest that the eruption...brought about the collapse of Minoan Crete b. Manning S (2012). \"Eruption of Thera/Santorini\". In Cline E (ed.). The Oxford Handbook of the Bronze Age Aegean. Oxford University Press. pp. 457–454. doi:10.1093/oxfordhb/9780199873609.013.0034. ISBN978-0-19-987360-9. Marinatos (1939) famously suggested that the eruption might even have caused the destruction of Minoan Crete (also Page 1970). Although this simple hypothesis has been negated by the findings of excavation and other research since the late 1960s... which demonstrate that the eruption occurred late in the Late Minoan IA ceramic period, whereas the destructions of the Cretan palaces and so on are some time subsequent (late in the following Late Minoan IB ceramic period) c. Brouwers J (2021). \"Did Atlantis Exist?\". Bad Ancient. Retrieved August 30, 2023. ^\"National Pasta Association\". Archived from the original on March 20, 2012. article FAQs section \"Who \"invented\" pasta?\"; \"The story that it was Marco Polo who imported noodles to Italy and thereby gave birth to the country's pasta culture is the most pervasive myth in the history of Italian food.\" (Dickie 2008, p. 48). ^Crabtree S (July 6, 1999). \"New Poll Gauges Americans' General Knowledge Levels\". Gallup News Service. Archived from the original on March 27, 2014. Retrieved January 13, 2011. Fifty-five percent say it commemorates the signing of the Declaration of Independence (this is a common misconception, and close to being accurate; July 4th is actually the date in 1776 when the Continental Congress approved the Declaration, which was officially signed on August 2nd.) Another 32 percent give a more general answer, saying that July 4th celebrates Independence Day. ^ a. Craig L, Young K (2008). \"Beyond White Pride: Identity, Meaning and Contradiction in the Canadian Skinhead Subculture*\". Canadian Review of Sociology/Revue Canadienne de Sociologie. 34 (2): 175–206. doi:10.1111/j.1755-618x.1997.tb00206.x. Retrieved July 2, 2022. b. Borgeson K, Valeri R (Fall 2005). \"Examining Differences in Skinhead Ideology and Culture Through an Analysis of Skinhead Websites\". Michigan Sociological Review. 19: 45–62. JSTOR40969104. c. Lambert C (November 12, 2017). \"'Black Skinhead': The politics of New Kanye\". Daily Dot. Retrieved July 2, 2022. \"Skinhead\" was a term originally used to describe a 1960s British working-class subculture that revolved around fashion and music and that would heavily inspire the punk rock scene. While it has harmless roots, the skinhead movement fell into polemic politics. Nowadays, it's popularly associated with neo-Nazis, despite having split demographics of far-right, far-left, and apolitical. ^Krause CA (December 17, 1978). \"Jonestown Is an Eerie Ghost Town Now\". Washington Post. Retrieved June 20, 2022. A pair of woman's eyelasses, a towel, a pair of shorts, packets of unopened Flavor-Aid lie scattered about waiting for the final cleanup that may one day return Jonestown to the tidy, if overcrowded, little community it once was. ^Gudger E (January 1930). \"On the alleged penetration of the human urethra by an Amazonian catfish called candiru with a review of the allied habits of other members of the family pygidiidae\". The American Journal of Surgery (Print). 8 (1). Elsevier Inc.: 170–188. doi:10.1016/S0002-9610(30)90912-9. ISSN0002-9610. ^\"Have You Heard the Calls from Cook County's 12 Frog and Toad Species?\". Forest Preserves of Cook County. May 25, 2022. Retrieved June 25, 2022. Here's a bonus fact: you might notice that none of these species says, \"ribbit.\" In fact, the \"ribbit\" call is unique to the Pacific tree frog, which lives along the Pacific coast, and, notably, in Hollywood, California, where the largest volume of early frog recordings took place. ^Bittel J (September 22, 2019). \"Think you know what bunnies and bears eat? Their diets may surprise you\". Washington Post. Archived from the original on September 26, 2019. In the wild, rabbits aren't in the habit of digging up root vegetables such as carrots, potatoes and beets. They much prefer wild greens, such as grasses and clover. In fact, carrots may actually be bad for rabbits, because although the vegetables are high in good nutrients, including beta carotene, they are also relatively high in sugar. This means that feeding a rabbit lots of carrots could lead to tooth decay or other health issues. ^Curtin C (February 2007). \"Fact or Fiction?: Glass Is a (Supercooled) Liquid\". Scientific American. Archived from the original on December 14, 2013. Glass, however, is actually neither a liquid—supercooled or otherwise—nor a solid. It is an amorphous solid—a state somewhere between those two states of matter. And yet glass's liquidlike properties are not enough to explain the thicker-bottomed windows, because glass atoms move too slowly for changes to be visible. ^Begley S (August 13, 2007). \"The Truth About Denial\". Newsweek. Archived from the original on October 21, 2007. (MSNBC single page version, archived 20 August 2007) \"If you think those who have long challenged the mainstream scientific findings about global warming recognize that the game is over, think again. ... outside Hollywood, Manhattan and other habitats of the chattering classes, the denial machine is running at full throttle—and continuing to shape both government policy and public opinion. Since the late 1980s, this well-coordinated, well-funded campaign by contrarian scientists, free-market think tanks and industry has created a paralyzing fog of doubt around climate change. Through advertisements, op-eds, lobbying and media attention, greenhouse doubters (they hate being called deniers) argued first that the world is not warming; measurements indicating otherwise are flawed, they said. Then they claimed that any warming is natural, not caused by human activities. Now they contend that the looming warming will be minuscule and harmless. 'They patterned what they did after the tobacco industry,' says former senator Tim Wirth.\" ^ a. \"Lightning Myths and Facts\". National Weather Service. Fact: Lightning often strikes the same place repeatedly, especially if it's a tall, pointy, isolated object. The Empire State Building is hit an average of 23 times a year b. \"Lightning Often Strikes Twice\". NASA Spinoff. Office of the Chief Technologist, NASA. Archived from the original on March 25, 2012. Retrieved June 23, 2010. c. WeatherBug Meteorologists (May 17, 2010). \"The Myths and Facts of Lightning\". WeatherBug. Earth Works. Archived from the original on July 11, 2010. Retrieved June 23, 2010. d. Tristan Simpson (April 29, 2022). \"Can lightning strike the same place twice?\". The Weather Network. Believe it or not, this long-held myth is far from the truth. While the odds of being struck by lightning are low, the chances of lightning striking the same place twice are high. Lightning can, and often will, hit the same spot multiple times. ^ a. \"Ask an Astrophysicist\". NASA. Archived from the original on June 4, 2012. If you don't try to hold your breath, exposure to space for half a minute or so is unlikely to produce permanent injury. Holding your breath is likely to damage your lungs, ... but theory predicts—and animal experiments confirm—that otherwise, exposure to vacuum causes no immediate injury. You do not explode. Your blood does not boil. You do not freeze. You do not instantly lose consciousness b. \"Exploding Body in Vacuum\". ABC Science. April 6, 2005. Archived from the original on June 4, 2012. ...will we humans explode in the full vacuum of space, as urban legends claim? The answer is that we won't explode, and if the exposure is short enough, we can even survive. ^Dresden D (March 12, 2020). \"How many ribs do humans have? Men, women, and anatomy\". Medical News Today. Retrieved June 5, 2022. Although many people might think that males have fewer ribs than females — most likely sparked by the biblical story of Adam and Eve — there is no factual evidence. ^Spellman, Frank R; Price-Bayer, Joni. (2010). In Defense of Science: Why Scientific Literacy Matters. The Scarecrow Press. p. 81. ISBN978-1-60590-735-2 \"There is no scientific evidence that crystal healing has any effect. It has been called a pseudoscience. Pleasant feelings or the apparent successes of crystal healing can be attributed to the placebo effect or cognitive bias—a believer wanting it to be true.\" The literature about Biodiversity and the GE food/feed consumption has sometimes resulted in animated debate regarding the suitability of the experimental designs, the choice of the statistical methods or the public accessibility of data. Such debate, even if positive and part of the natural process of review by the scientific community, has frequently been distorted by the media and often used politically and inappropriately in anti-GE crops campaigns. b. \"State of Food and Agriculture 2003–2004. Agricultural Biotechnology: Meeting the Needs of the Poor. Health and environmental impacts of transgenic crops\". Food and Agriculture Organization of the United Nations. Retrieved August 30, 2019. Currently available transgenic crops and foods derived from them have been judged safe to eat and the methods used to test their safety have been deemed appropriate. These conclusions represent the consensus of the scientific evidence surveyed by the ICSU (2003) and they are consistent with the views of the World Health Organization (WHO, 2002). These foods have been assessed for increased risks to human health by several national regulatory authorities (inter alia, Argentina, Brazil, Canada, China, the United Kingdom and the United States) using their national food safety procedures (ICSU). To date no verifiable untoward toxic or nutritionally deleterious effects resulting from the consumption of foods derived from genetically modified crops have been discovered anywhere in the world (GM Science Review Panel). Many millions of people have consumed foods derived from GM plants – mainly maize, soybean and oilseed rape – without any observed adverse effects (ICSU). c. Ronald P (May 1, 2011). \"Plant Genetics, Sustainable Agriculture and Global Food Security\". Genetics. 188 (1): 11–20. doi:10.1534/genetics.111.128553. PMC3120150. PMID21546547. There is broad scientific consensus that genetically engineered crops currently on the market are safe to eat. After 14 years of cultivation and a cumulative total of 2 billion acres planted, no adverse health or environmental effects have resulted from commercialization of genetically engineered crops (Board on Agriculture and Natural Resources, Committee on Environmental Impacts Associated with Commercialization of Transgenic Plants, National Research Council and Division on Earth and Life Studies 2002). Both the U.S. National Research Council and the Joint Research Centre (the European Union's scientific and technical research laboratory and an integral part of the European Commission) have concluded that there is a comprehensive body of knowledge that adequately addresses the food safety issue of genetically engineered crops (Committee on Identifying and Assessing Unintended Effects of Genetically Engineered Foods on Human Health and National Research Council 2004; European Commission Joint Research Centre 2008). These and other recent reports conclude that the processes of genetic engineering and conventional breeding are no different in terms of unintended consequences to human health and the environment (European Commission Directorate-General for Research and Innovation 2010). ^ See also: Domingo JL, Bordonaba JG (2011). \"A literature review on the safety assessment of genetically modified plants\"(PDF). Environment International. 37 (4): 734–742. Bibcode:2011EnInt..37..734D. doi:10.1016/j.envint.2011.01.003. PMID21296423. In spite of this, the number of studies specifically focused on safety assessment of GM plants is still limited. However, it is important to remark that for the first time, a certain equilibrium in the number of research groups suggesting, on the basis of their studies, that a number of varieties of GM products (mainly maize and soybeans) are as safe and nutritious as the respective conventional non-GM plant, and those raising still serious concerns, was observed. Moreover, it is worth mentioning that most of the studies demonstrating that GM foods are as nutritional and safe as those obtained by conventional breeding, have been performed by biotechnology companies or associates, which are also responsible of commercializing these GM plants. Anyhow, this represents a notable advance in comparison with the lack of studies published in recent years in scientific journals by those companies.Krimsky S (2015). \"An Illusory Consensus behind GMO Health Assessment\". Science, Technology, & Human Values. 40 (6): 883–914. doi:10.1177/0162243915598381. S 2 CID40855100. I began this article with the testimonials from respected scientists that there is literally no scientific controversy over the health effects of GMOs. My investigation into the scientific literature tells another story. And contrast: Panchin AY, Tuzhikov AI (January 14, 2016). \"Published GMO studies find no evidence of harm when corrected for multiple comparisons\". Critical Reviews in Biotechnology. 37 (2): 213–217. doi:10.3109/07388551.2015.1130684. ISSN0738-8551. PMID26767435. S 2 CID11786594. Here, we show that a number of articles some of which have strongly and negatively influenced the public opinion on GM crops and even provoked political actions, such as GMO embargo, share common flaws in the statistical evaluation of the data. Having accounted for these flaws, we conclude that the data presented in these articles does not provide any substantial evidence of GMO harm. The presented articles suggesting possible harm of GMOs received high public attention. However, despite their claims, they actually weaken the evidence for the harm and lack of substantial equivalency of studied GMOs. We emphasize that with over 1783 published articles on GMOs over the last 10 years it is expected that some of them should have reported undesired differences between GMOs and conventional crops even if no such differences exist in reality. and Yang Y, Chen B (2016). \"Governing GMOs in the USA: science, law and public health\". Journal of the Science of Food and Agriculture. 96 (4): 1851–1855. Bibcode:2016JSFA...96.1851Y. doi:10.1002/jsfa.7523. PMID26536836. It is therefore not surprising that efforts to require labeling and to ban GMOs have been a growing political issue in the USA (citing Domingo and Bordonaba, 2011). Overall, a broad scientific consensus holds that currently marketed GM food poses no greater risk than conventional food... Major national and international science and medical associations have stated that no adverse human health effects related to GMO food have been reported or substantiated in peer-reviewed literature to date. Despite various concerns, today, the American Association for the Advancement of Science, the World Health Organization, and many independent international science organizations agree that GMOs are just as safe as other foods. Compared with conventional breeding techniques, genetic engineering is far more precise and, in most cases, less likely to create an unexpected outcome. ^ a. \"Statement by the AAAS Board of Directors On Labeling of Genetically Modified Foods\"(PDF). American Association for the Advancement of Science. October 20, 2012. Retrieved August 30, 2019. The EU, for example, has invested more than €300 million in research on the biosafety of GMOs. Its recent report states: \"The main conclusion to be drawn from the efforts of more than 130 research projects, covering a period of more than 25 years of research and involving more than 500 independent research groups, is that biotechnology, and in particular GMOs, are not per se more risky than e.g. conventional plant breeding technologies.\" The World Health Organization, the American Medical Association, the U.S. National Academy of Sciences, the British Royal Society, and every other respected organization that has examined the evidence has come to the same conclusion: consuming foods containing ingredients derived from GM crops is no riskier than consuming the same foods containing ingredients from crop plants modified by conventional plant improvement techniques. b. Pinholster G (October 25, 2012). \"AAAS Board of Directors: Legally Mandating GM Food Labels Could \"Mislead and Falsely Alarm Consumers\"\"(PDF). American Association for the Advancement of Science. Retrieved August 30, 2019. c. European Commission. Directorate-General for Research (2010). A decade of EU-funded GMO research (2001–2010)(PDF). Directorate-General for Research and Innovation. Biotechnologies, Agriculture, Food. European Commission, European Union. doi:10.2777/97784. ISBN978-92-79-16344-9. Retrieved August 30, 2019. d. \"ISAAA Summary of AMA Report on Genetically Modified Crops and Foods\". ISAAA. January 2001. Retrieved August 30, 2019. A report issued by the scientific council of the American Medical Association (AMA) says that no long-term health effects have been detected from the use of transgenic crops and genetically modified foods, and that these foods are substantially equivalent to their conventional counterparts. e. \"Featured CSA Report: Genetically Modified Crops and Foods (I-00) Full Text\". American Medical Association. Archived from the original on June 10, 2001. Crops and foods produced using recombinant DNA techniques have been available for fewer than 10 years and no long-term effects have been detected to date. These foods are substantially equivalent to their conventional counterparts. f. \"Report 2 of the Council on Science and Public Health (A-12): Labeling of Bioengineered Foods\"(PDF). American Medical Association. 2012. Archived from the original(PDF) on September 7, 2012. Retrieved August 30, 2019. \"Bioengineered foods have been consumed for close to 20 years, and during that time, no overt consequences on human health have been reported and/or substantiated in the peer-reviewed literature\". g. \"Restrictions on Genetically Modified Organisms: United States. Public and Scholarly Opinion\". Library of Congress. June 30, 2015. Retrieved August 30, 2019. Several scientific organizations in the US have issued studies or statements regarding the safety of GMOs indicating that there is no evidence that GMOs present unique safety risks compared to conventionally bred products. These include the National Research Council, the American Association for the Advancement of Science, and the American Medical Association. Groups in the US opposed to GMOs include some environmental organizations, organic farming organizations, and consumer organizations. A substantial number of legal academics have criticized the US's approach to regulating GMOs. h. National Academies Of Sciences E, Division on Earth Life Studies, Board on Agriculture Natural Resources, Committee on Genetically Engineered Crops: Past Experience Future Prospects (2016). Genetically Engineered Crops: Experiences and Prospects. The National Academies of Sciences, Engineering, and Medicine (US). p. 149. doi:10.17226/23395. ISBN978-0-309-43738-7. PMID28230933. Retrieved August 30, 2019. Overall finding on purported adverse effects on human health of foods derived from GE crops: On the basis of detailed examination of comparisons of currently commercialized GE with non-GE foods in compositional analysis, acute and chronic animal toxicity tests, long-term data on health of livestock fed GE foods, and human epidemiological data, the committee found no differences that implicate a higher risk to human health from GE foods than from their non-GE counterparts. ^Fullerton-Smith J (2007). The Truth About Food. Bloomsbury. pp. 115–17. ISBN978-0-7475-8685-2. Most parents assume that children plus sugary foods equals raucous and uncontrollable behaviour. ... according to nutrition experts, the belief that children experience a 'sugar high' is a myth. ^ a. Jesse Galef (August 29, 2011). \"Lies and Debunked Legends about the Golden Ratio\". Archived from the original on April 27, 2014. Retrieved April 10, 2013. b. \"Two other beliefs about [the golden ratio] are often mentioned in magazines and books: that the ancient Greeks believed it was the proportion of the rectangle the eye finds most pleasing and that they accordingly incorporated the rectangle in many of their buildings, including the famous Parthenon. These two equally persistent beliefs are likewise assuredly false and, in any case, are completely without any evidence.\" Devlin K (2008). The Unfinished Game: Pascal, Fermat, and the Seventeenth-Century Letter that Made the World Modern. Basic Books. p. 35. ^ a. Donald E. Simanek. \"Fibonacci Flim-Flam\". Archived from the original on February 1, 2010. Retrieved April 9, 2013. b. Devlin K (May 2007). \"The Myth That Will Not Go Away\". Archived from the original on July 1, 2013. Retrieved April 10, 2013. Part of the process of becoming a mathematics writer is, it appears, learning that you cannot refer to the golden ratio without following the first mention by a phrase that goes something like 'which the ancient Greeks and others believed to have divine and mystical properties.' Almost as compulsive is the urge to add a second factoid along the lines of 'Leonardo Da Vinci believed that the human form displays the golden ratio.' There is not a shred of evidence to back up either claim, and every reason to assume they are both false. Yet both claims, along with various others in a similar vein, live on. ^a. \"This occurs because of Bernoulli's principle – fast-moving air has lower pressure than non-moving air\". Make Magazine. Archived from the original on January 3, 2013. Retrieved September 5, 2012. b. \"Paper Lift\". Physics Force. University of Minnesota. Archived from the original on November 12, 2020. Retrieved January 7, 2021. ... When the demonstrator holds the paper in front of his mouth and blows across the top, he is creating an area of faster-moving air. The slower-moving air under the paper now has higher pressure, thus pushing the paper up, towards the area of lower pressure.. c. \"Educational Packet\"(PDF). Tall Ships Festival: Channel Islands Harbor. Archived from the original(PDF) on December 3, 2013. Retrieved June 25, 2012. Bernoulli's Principle states that faster moving air has lower pressure... You can demonstrate Bernoulli's Principle by blowing over a piece of paper held horizontally across your lips.\" ^a. Craig GM. \"Physical Principles of Winged Flight\"(PDF). Archived from the original(PDF) on March 7, 2021. Retrieved September 5, 2012. If the lift in figure A were caused by \"Bernoulli principle,\" then the paper in figure B should droop further when air is blown beneath it. However, as shown, it raises when the upward pressure gradient in downward-curving flow adds to atmospheric pressure at the paper lower surface. b. Babinsky H (2003). \"How Do Wings Work\". Physics Education. 38 (6): 497–503. Bibcode:2003PhyEd..38..497B. doi:10.1088/0031-9120/38/6/001. S 2 CID1657792. Retrieved January 7, 2021. In fact, the pressure in the air blown out of the lungs is equal to that of the surrounding air... Blowing over a piece of paper does not demonstrate Bernoulli's equation. While it is true that a curved paper lifts when flow is applied on one side, this is not because air is moving at different speeds on the two sides... It is false to make a connection between the flow on the two sides of the paper using Bernoulli's equation. c. Eastwell P (2007). \"Bernoulli? Perhaps, but What About Viscosity?\"(PDF). The Science Education Review. 6 (1). Archived from the original(PDF) on March 18, 2018. Retrieved September 10, 2023. ...air does not have a reduced lateral pressure (or static pressure...) simply because it is caused to move, the static pressure of free air does not decrease as the speed of the air increases, it misunderstanding Bernoulli's principle to suggest that this is what it tells us, and the behavior of the curved paper is explained by other reasoning than Bernoulli's principle. ... An explanation based on Bernoulli's principle is not applicable to this situation, because this principle has nothing to say about the interaction of air masses having different speeds... Also, while Bernoulli's principle allows us to compare fluid speeds and pressures along a single streamline and... along two different streamlines that originate under identical fluid conditions, using Bernoulli's principle to compare the air above and below the curved paper in Figure 1 is nonsensical; in this case, there aren't any streamlines at all below the paper! d. Raskin J. \"Coanda Effect: Understanding Why Wings Work\". Make a strip of writing paper about 5 cm X 25 cm. Hold it in front of your lips so that it hangs out and down making a convex upward surface. When you blow across the top of the paper, it rises. Many books attribute this to the lowering of the air pressure on top solely to the Bernoulli effect. Now use your fingers to form the paper into a curve that it is slightly concave upward along its whole length and again blow along the top of this strip. The paper now bends downward...an often-cited experiment which is usually taken as demonstrating the common explanation of lift does not do so... e. Auerbach D (2000). \"Why Aircraft Fly\". European Journal of Physics. 21 (4): 289. Bibcode:2000EJPh...21..289A. doi:10.1088/0143-0807/21/4/302. S 2 CID250821727. The well-known demonstration of the phenomenon of lift by means of lifting a page cantilevered in one's hand by blowing horizontally along it is probably more a demonstration of the forces inherent in the Coanda effect than a demonstration of Bernoulli's law; for, here, an air jet issues from the mouth and attaches to a curved (and, in this case pliable) surface. The upper edge is a complicated vortex-laden mixing layer and the distant flow is quiescent, so that Bernoulli's law is hardly applicable. f. Smith NF (November 1972). \"Bernoulli and Newton in Fluid Mechanics\". The Physics Teacher. 10 (8): 451–455. Bibcode:1972PhTea..10..451S. doi:10.1119/1.2352317. Millions of children in science classes are being asked to blow over curved pieces of paper and observe that the paper \"lifts\"... They are then asked to believe that Bernoulli's theorem is responsible... Unfortunately, the \"dynamic lift\" involved...is not properly explained by Bernoulli's theorem. ^ a. Anderson DF, Eberhardt S (2000). Understanding Flight. McGraw Hill Professional. p. 229. ISBN978-0-07-138666-1 – via Google Books. Demonstrations\" of Bernoulli's principle are often given as demonstrations of the physics of lift. They are truly demonstrations of lift, but certainly not of Bernoulli's principle. b. Feil M. The Aeronautics File. As an example, take the misleading experiment most often used to \"demonstrate\" Bernoulli's principle. Hold a piece of paper so that it curves over your finger, then blow across the top. The paper will rise. However most people do not realize that the paper would NOT rise if it was flat, even though you are blowing air across the top of it at a furious rate. Bernoulli's principle does not apply directly in this case. This is because the air on the two sides of the paper did not start out from the same source. The air on the bottom is ambient air from the room, but the air on the top came from your mouth where you actually increased its speed without decreasing its pressure by forcing it out of your mouth. As a result the air on both sides of the flat paper actually has the same pressure, even though the air on the top is moving faster. The reason that a curved piece of paper does rise is that the air from your mouth speeds up even more as it follows the curve of the paper, which in turn lowers the pressure according to Bernoulli. ^ a. Colbeck SC (1995). \"Pressure melting and ice skating\". American Journal of Physics. 63 (10): 888. Bibcode:1995AmJPh..63..888C. doi:10.1119/1.18028. Pressure melting cannot be responsible for the low friction of ice. The pressure needed to reach the melting temperature is above the compressive failure stress...\" b. Kenneth Chang (February 21, 2006). \"Explaining Ice: The Answers Are Slippery\". The New York Times. According to the frequently cited — if incorrect — explanation of why ice is slippery under an ice skate, the pressure exerted along the blade lowers the melting temperature of the top layer of ice, the ice melts and the blade glides on a thin layer of water that refreezes to ice as soon as the blade passes... But the explanation fails... because the pressure-melting effect is small. c. Robert Rosenberg (December 2005). \"Why is Ice slippery?\"(PDF). Physics Today: 50–55. ^ abHandler SM, Fierson WM, Section on O, Council on Children with D, American Academy of O, American Association for Pediatric Ophthalmology and S, et al. (March 2011). \"Learning disabilities, dyslexia, and vision\". Pediatrics. 127 (3): e818–56. doi:10.1542/peds.2010-3670. PMID21357342. A common misconception is that dyslexia is a problem of letter or word reversals. Reversals of letters or words and mirror writing occur normally in early readers and writers. Children with dyslexia are not unusually prone to reversals. Although they do occur, reversal of letters or words, or mirror writing, is not included in the definition of dyslexia. ^ abRadford B (March–April 1999). \"The Ten-Percent Myth\". Skeptical Inquirer. ISSN0194-6730. Archived from the original on October 30, 2013. Retrieved April 15, 2009. It's the old myth heard time and again about how people use only ten percent of their brains ^ abBeyerstein BL (1999). \"Whence Cometh the Myth that We Only Use 10% of our Brains?\". In Sergio Della Sala (ed.). Mind Myths: Exploring Popular Assumptions About the Mind and Brain. Wiley. pp. 3–24. ISBN978-0-471-98303-3. ^ a. Eisenbud M, Gesell TF (1997). Environmental radioactivity: from natural, industrial, and military sources. Academic Press. pp. 171–172. ISBN978-0-12-235154-9. It is important to recognize that the potassium content of the body is under strict homeostatic control and is not influenced by variations in environmental levels. For this reason, the dose from 40K in the body is constant. b. U. S. Environmental Protection Agency (1999), Federal Guidance Report 13, page 16: \"For example, the ingestion coefficient risk for 40K would not be appropriate for an application to ingestion of 40K in conjunction with an elevated intake of natural potassium. This is because the biokinetic model for potassium used in this document represents the relatively slow removal of potassium (biological half-time 30 days) that is estimated to occur for typical intakes of potassium, whereas an elevated intake of potassium would result in excretion of a nearly equal mass of natural potassium, and hence of 40K, over a short period.\" c. Maggie Koerth-Baker (August 27, 2010). \"Bananas are radioactive—But they aren't a good way to explain radiation exposure\". Retrieved May 25, 2011.. Attributes the title statement to Geoff Meggitt, former UK Atomic Energy Authority. Gullotta DN (2017). \"On Richard Carrier's Doubts: A Response to Richard Carrier's On the Historicity of Jesus: Why We Might Have Reason for Doubt\". Journal for the Study of the Historical Jesus. 15 (2–3): 310–346. doi:10.1163/17455197-01502009. O'Conner PT, Kellerman S (2009). Origins of the Specious: Myths and Misconceptions of the English Language. New York: Random House. ISBN978-1-4000-6660-5. Smith FJ (January 1, 1979). \"Some aspects of the tritone and the semitritone in the Speculum Musicae: the non-emergence of the diabolus in musica\". Journal of Musicological Research. 3 (1–2): 63–74. doi:10.1080/01411897908574507. ISSN0141-1896."}
{"url": "https://en.m.wikipedia.org/wiki/Berliner_(doughnut)", "text": "Contents Sugar was very costly until the 16th century, and early doughnuts were usually stuffed with savory fillings like cheese, meat and mushroom. When imports from Caribbean sugar plantations made sugar more affordable, fruit preserves gained in popularity. In 1485, the first German-language cookbook to be published in printed form Kuechenmeisterei was published in Nuremberg and remained in print at least until 1674 with 20 editions [1] (it was later translated into Polish in 1532). It was one of the first cookbooks printed using the Gutenberg press and contains the first known recipe for a jelly doughnut, called Gefüllte Krapfen made with jam-filled yeasted bread dough deep-fried in lard. It's unknown whether this innovation was the author's[2] own or simply a record of an existing practice.[3] The yeast dough contains a good deal of eggs, milk and butter. For the classical Pfannkuchen made in Berlin the dough is rolled into a ball, deep-fried in lard, whereby the distinctive bright bulge occurs, and then filled with jam. The filling is related to the topping:[citation needed] for plum-butter, powdered sugar; for raspberry, strawberry and cherry jam, sugar; for all other fillings, sugar icing, sometimes flavoured with rum. Today the filling usually is injected with a large syringe or pastry bag after the dough is fried in one piece. The jelly-filled Krapfen were called Berliners in the 1800s, based on the legend of a patriotic baker from Berlin who became a regimental baker after he was deemed unfit for combat by the Prussian Army. When the army was in the field, he \"baked\" the doughnuts the old-fashioned way, by frying them over an open fire. According to the tale, the soldiers called the pastry Berliner after the baker's hometown.[3] Colorfully decorated Krapfen doughnuts with different fillings often eaten at the carnival The term Bismarcken (for Otto von Bismarck) came into use by the end of the 19th century. Immigrants from Central Europe settled in the United States in large numbers during the 19th century, and jelly doughnuts are called \"bismarcks\" in some parts of the Midwestern United States, Boston, and Alberta and Saskatchewan in Canada.[3] The terminology used to refer to this delicacy differs greatly in various areas of modern Germany. While called Berliner Ballen or simply Berliner in Northern and Western Germany, as well as in Switzerland, the Berliners themselves and residents of Brandenburg, Western Pomerania, Saxony-Anhalt and Saxony know them as Pfannkuchen, which translates literally and wrongly to \"pancakes\". A pancake in the rest of Germany is indeed a Pfannkuchen, in Austria and sometimes Southern Germany called Palatschinken.[6] The people of Berlin call their pancakes Eierkuchen, which translates to \"egg cakes\". In parts of southern and central Germany (Bavaria), as well as in much of Austria, they are a variety of Krapfen (derived from Old High Germankraffo and furthermore related to Gothic languagekrappa), sometimes called Fastnachtskrapfen or Faschingskrapfen to distinguish them from Bauernkrapfen. In Hesse they are referred to as Kräppel or Kreppel. Residents of the Palatinate call them also Kreppel or Fastnachtsküchelchen (\"little carnival cakes\"), hence the English term for a pastry called \"Fasnacht\"; further south, the Swabians use the equivalent term in their distinctive dialect: Fasnetskiachla. In South Tyrol, Triveneto and other parts of northern Italy, the food is called kraffen or krapfen, while in the southern parts it can be referred as bomba or bombolone. In Slovenia, it is krof; in Portugal it is \"bola de Berlim\" (Berlin ball) or malasada (from \"mal-assada\" = \"badly-baked\"); in Croatia, it is krafne; while in Bosnia and Herzegovina and Serbia, it is called krofne. In Poland, they are known as pączki, in Ukraine, as \"pampushky [uk]\"; and in the Czech Republic as kobliha. In Hungary, it is called bécsi fánk, meaning Viennese doughnut, as it was transited by Austria to the Hungarian kitchen.[7] The pastry is called Berlinerbol in the Netherlands and Suriname, Berlijnse bol and boule de Berlin in Belgium, hillomunkki or (glazed) berliininmunkki or piispanmunkki in Finland, berlinerbolle in Norway, sufganiyot in Israel, Berlínarbollur in Iceland, šiška in Slovakia, and gogoși in Romania. In Denmark, it is called \"Berliner\". In Turkey, they are known as Alman Pastası (German Pie). All of these are similar preparations. In Israel, a version of the pastry called sufganiyah (Hebrew: סופגנייה) is traditionally consumed during the Jewish holiday of Hanukkah.[3] In Southeast Europe, they are called Krofne, Krafne or Krofi. They are the same size and often filled with jam as well, but unlike its German counterpart, chocolate fillings are also very common there. They are not to be confused with \"princes krofne\" which is a Serbo-Croatian name for profiteroles. Bola de Berlim from Portugal In Portugal, Berliners are slightly bigger than their German counterparts. They are known as bolas de Berlim (lit. Berlin ball) or malasada, and the filling is frequently an egg-yolk-based yellow cream called creme pasteleiro (lit. confectioner's cream).[11] The filling is inserted after a half-length cut and is always visible. Regular sugar is used to sprinkle it. They can be found in almost every pastry shop in the country. It's a typical beach food, provided by street vendors. Such versions are also found in Latin American countries with German descended populations, such as in Mexico (berlinesas), Chile (Berlín), Paraguay (bollo), Venezuela (bomba), Uruguay and Argentina (bola de fraile or suspiro de monja or berlinesa), where it is filled not only with custard (called \"crema pastelera\"), but also with jam (especially red ones), dulce de leche, or manjar blanco. In Brazil, they are known as sonho (dream) and are also widely consumed in the country. Their commercialization began in the 1920s in bakeries in São Paulo, with the use of leftover bread dough. They are presented filled, usually with pastry cream, chocolate, or dulce de leche.[12] In Finland, berliininmunkki (lit. Berlin's doughnut) is a commonly consumed pastry, although unlike a traditional Berliner, this variant has pink caramel colored frosting on top as opposed to regular or powdered sugar. In Tromsø, Norway, Berliners are eaten to celebrate the return of the sun at the end of the polar night on January 21. They are called a solbolle (lit. sun bun), and around 60,000 Berliners, roughly one per capita, are consumed in Tromsø on this day. In recent years, bakeries have also made a special type of Berliner called a mørketidsbolle (lit. polar night bun), with a yellow custard filing and a dark chocolate covering (to symbolize darkness covering the sun). This Berliner is eaten in the build up to and during the polar night period, from the end of the September until the start of Christmas.[13] Portuguese style Malasadas are also very popular in Hawaii. In 1878, Portuguese laborers from Madeira and the Azores went to Hawaii to work in the plantations. These immigrants brought their traditional foods with them, including a fried dough pastry called \"malasada.\"[14] Today, there are numerous bakeries in the Hawaiian islands specializing in malasadas. Mardi Gras (\"Fat Tuesday\"), the day before Lent, is Malasada Day in Hawaii. Being predominantly Catholic, Portuguese immigrants would need to use up all their butter and sugar prior to Lent. They did so by making large batches of malasadas, which they would subsequently share with friends from all the other ethnic groups in the plantation camps. John F. Kennedy's words \"Ich bin ein Berliner\" are standard German for \"I am a Berliner\", meaning a person from Berlin. Mentioned in Len Deighton's 1983 novel Berlin Game, an urban legend has it that due to his use of the indefinite article ein, Berliner is translated as \"jelly doughnut\", and that the population of Berlin was amused by the supposed mistake. This is incorrect, insofar as when leaving out ein, the meaning only changes slightly (compare I am Berliner and I am a Berliner). The normal convention when stating a nationality or, for instance, saying one is from Berlin, would be to leave out the indefinite article ein. However, Kennedy used the indefinite article here correctly to emphasize his relation to Berlin.[15][16] Additionally, the word Berliner is not used in Berlin to refer to the Berliner Pfannkuchen. These are simply called Pfannkuchen there[17] and therefore no Berliner would mistake Berliner for a doughnut. Throughout the 1980s, the legend was spread even by reputable media like The New York Times, The Guardian, BBC and NBC.[citation needed]"}
{"url": "https://en.m.wikipedia.org/wiki/Deep_web", "text": "Since then, after their use in the media's reporting on the black-market website Silk Road, media outlets have generally used 'deep web' synonymously with the dark web or darknet, a comparison some reject as inaccurate[10] and consequently has become an ongoing source of confusion.[11]Wired reporters Kim Zetter[12] and Andy Greenberg[13] recommend the terms be used in distinct fashions. While the deep web is a reference to any site that cannot be accessed by a traditional search engine, the dark web is a portion of the deep web that has been hidden intentionally and is inaccessible by standard browsers and methods.[14][15][16][17][18] Bergman, in a paper on the deep web published in The Journal of Electronic Publishing, mentioned that Jill Ellsworth used the term Invisible Web in 1994 to refer to websites that were not registered with any search engine.[19] Bergman cited a January 1996 article by Frank Garcia:[20] It would be a site that's possibly reasonably designed, but they didn't bother to register it with any of the search engines. So, no one can find them! You're hidden. I call that the invisible Web. Another early use of the term Invisible Web was by Bruce Mount and Matthew B. Koll of Personal Library Software, in a description of the No. 1 Deep Web program found in a December 1996 press release.[21] The first use of the specific term deep web, now generally accepted, occurred in the aforementioned 2001 Bergman study.[19] Dynamic content: dynamic pages, which are returned in response to a submitted query or accessed only through a form, especially if open-domain input elements (such as text fields) are used; such fields are hard to navigate without domain knowledge. Limited access content: sites that limit access to their pages in a technical manner (e.g., using the Robots Exclusion Standard or CAPTCHAs, or no-store directive, which prohibit search engines from browsing them and creating cached copies).[22] Sites may feature an internal search engine for exploring such pages.[23][24] Scripted content: pages that are accessible only by links produced by JavaScript as well as content dynamically downloaded from Web servers via Flash or Ajax solutions. Software: certain content is hidden intentionally from the regular Internet, accessible only with special software, such as Tor, I 2 P, or other darknet software. For example, Tor allows users to access websites using the .onion server address anonymously, hiding their IP address. Unlinked content: pages which are not linked to by other pages, which may prevent web crawling programs from accessing the content. This content is referred to as pages without backlinks (also known as inlinks). Also, search engines do not always detect all backlinks from searched web pages. Web archives: Web archival services such as the Wayback Machine enable users to see archived versions of web pages across time, including websites that have become inaccessible and are not indexed by search engines such as Google. The Wayback Machine may be termed a program for viewing the deep web, as web archives that are not from the present cannot be indexed, as past versions of websites are impossible to view by a search. All websites are updated at some time, which is why web archives are considered Deep Web content.[25] While it is not always possible to discover directly a specific web server's content so that it may be indexed, a site potentially can be accessed indirectly (due to computer vulnerabilities). To discover content on the web, search engines use web crawlers that follow hyperlinks through known protocol virtual port numbers. This technique is ideal for discovering content on the surface web but is often ineffective at finding deep web content. For example, these crawlers do not attempt to find dynamic pages that are the result of database queries due to the indeterminate number of queries that are possible.[26] It has been noted that this can be overcome (partially) by providing links to query results, but this could unintentionally inflate the popularity of a site of the deep web. Researchers have been exploring how the deep web can be crawled in an automatic fashion, including content that can be accessed only by special software such as Tor. In 2001, Sriram Raghavan and Hector Garcia-Molina (Stanford Computer Science Department, Stanford University)[29][30] presented an architectural model for a hidden-Web crawler that used important terms provided by users or collected from the query interfaces to query a Web form and crawl the Deep Web content. Alexandros Ntoulas, Petros Zerfos, and Junghoo Cho of UCLA created a hidden-Web crawler that automatically generated meaningful queries to issue against search forms.[31] Several form query languages (e.g., DEQUEL[32]) have been proposed that, besides issuing a query, also allow extraction of structured data from result pages. Another effort is DeepPeep, a project of the University of Utah sponsored by the National Science Foundation, which gathered hidden-web sources (web forms) in different domains based on novel focused crawler techniques.[33][34] Commercial search engines have begun exploring alternative methods to crawl the deep web. The Sitemap Protocol (first developed, and introduced by Google in 2005) and OAI-PMH are mechanisms that allow search engines and other interested parties to discover deep web resources on particular web servers. Both mechanisms allow web servers to advertise the URLs that are accessible on them, thereby allowing automatic discovery of resources that are not linked directly to the surface web. Google's deep web surfacing system computes submissions for each HTML form and adds the resulting HTML pages into the Google search engine index. The surfaced results account for a thousand queries per second to deep web content.[35] In this system, the pre-computation of submissions is done using three algorithms: selecting input values for text search inputs that accept keywords, identifying inputs that accept only values of a specific type (e.g., date) and selecting a small number of input combinations that generate URLs suitable for inclusion into the Web search index. In 2008, to facilitate users of Tor hidden services in their access and search of a hidden .onion suffix, Aaron Swartz designed Tor 2 web—a proxy application able to provide access by means of common web browsers.[36] Using this application, deep web links appear as a random sequence of letters followed by the .onion top-level domain. ^\"Elsevier to Retire Popular Science Search Engine\". library.bldrdoc.gov. December 2013. Archived from the original on June 23, 2015. Retrieved June 22, 2015. by end of January 2014, Elsevier will be discontinuing Scirus, its free science search engine. Scirus has been a wide-ranging research tool, with over 575 million items indexed for searching, including webpages, pre-print articles, patents, and repositories."}
{"url": "https://www.pbs.org/kcts/videogamerevolution/impact/myths.html", "text": "A large gap exists between the public's perception of video games and what the research actually shows. The following is an attempt to separate fact from fiction. 1. The availability of video games has led to an epidemic of youth violence. According to federal crime statistics, the rate of juvenile violent crime in the United States is at a 30-year low. Researchers find that people serving time for violent crimes typically consume less media before committing their crimes than the average person in the general population. It's true that young offenders who have committed school shootings in America have also been game players. But young people in general are more likely to be gamers — 90 percent of boys and 40 percent of girls play. The overwhelming majority of kids who play do NOT commit antisocial acts. According to a 2001 U.S. Surgeon General's report, the strongest risk factors for school shootings centered on mental stability and the quality of home life, not media exposure. The moral panic over violent video games is doubly harmful. It has led adult authorities to be more suspicious and hostile to many kids who already feel cut off from the system. It also misdirects energy away from eliminating the actual causes of youth violence and allows problems to continue to fester. 2. Scientific evidence links violent game play with youth aggression. Claims like this are based on the work of researchers who represent one relatively narrow school of research, \"media effects.\" This research includes some 300 studies of media violence. But most of those studies are inconclusive and many have been criticized on methodological grounds. In these studies, media images are removed from any narrative context. Subjects are asked to engage with content that they would not normally consume and may not understand. Finally, the laboratory context is radically different from the environments where games would normally be played. Most studies found a correlation, not a causal relationship, which means the research could simply show that aggressive people like aggressive entertainment. That's why the vague term \"links\" is used here. If there is a consensus emerging around this research, it is that violent video games may be one risk factor - when coupled with other more immediate, real-world influences — which can contribute to anti-social behavior. But no research has found that video games are a primary factor or that violent video game play could turn an otherwise normal person into a killer. 3. Children are the primary market for video games. While most American kids do play video games, the center of the video game market has shifted older as the first generation of gamers continues to play into adulthood. Already 62 percent of the console market and 66 percent of the PC market is age 18 or older. The game industry caters to adult tastes. Meanwhile, a sizable number of parents ignore game ratings because they assume that games are for kids. One quarter of children ages 11 to 16 identify an M-Rated (Mature Content) game as among their favorites. Clearly, more should be done to restrict advertising and marketing that targets young consumers with mature content, and to educate parents about the media choices they are facing. But parents need to share some of the responsibility for making decisions about what is appropriate for their children. The news on this front is not all bad. The Federal Trade Commission has found that 83 percent of game purchases for underage consumers are made by parents or by parents and children together. 4. Almost no girls play computer games. Historically, the video game market has been predominantly male. However, the percentage of women playing games has steadily increased over the past decade. Women now slightly outnumber men playing Web-based games. Spurred by the belief that games were an important gateway into other kinds of digital literacy, efforts were made in the mid-90s to build games that appealed to girls. More recent games such as The Sims were huge crossover successes that attracted many women who had never played games before. Given the historic imbalance in the game market (and among people working inside the game industry), the presence of sexist stereotyping in games is hardly surprising. Yet it's also important to note that female game characters are often portrayed as powerful and independent. In his book Killing Monsters, Gerard Jones argues that young girls often build upon these representations of strong women warriors as a means of building up their self confidence in confronting challenges in their everyday lives. 5. Because games are used to train soldiers to kill, they have the same impact on the kids who play them. Former military psychologist and moral reformer David Grossman argues that because the military uses games in training (including, he claims, training soldiers to shoot and kill), the generation of young people who play such games are similarly being brutalized and conditioned to be aggressive in their everyday social interactions. Grossman's model only works if: we remove training and education from a meaningful cultural context. we assume learners have no conscious goals and that they show no resistance to what they are being taught. we assume that they unwittingly apply what they learn in a fantasy environment to real world spaces. The military uses games as part of a specific curriculum, with clearly defined goals, in a context where students actively want to learn and have a need for the information being transmitted. There are consequences for not mastering those skills. That being said, a growing body of research does suggest that games can enhance learning. In his recent book, What Video Games Have to Teach Us About Learning and Literacy, James Gee describes game players as active problem solvers who do not see mistakes as errors, but as opportunities for improvement. Players search for newer, better solutions to problems and challenges, he says. And they are encouraged to constantly form and test hypotheses. This research points to a fundamentally different model of how and what players learn from games. 6. Video games are not a meaningful form of expression. On April 19, 2002, U.S. District Judge Stephen N. Limbaugh Sr. ruled that video games do not convey ideas and thus enjoy no constitutional protection. As evidence, Saint Louis County presented the judge with videotaped excerpts from four games, all within a narrow range of genres, and all the subject of previous controversy. Overturning a similar decision in Indianapolis, Federal Court of Appeals Judge Richard Posner noted: \"Violence has always been and remains a central interest of humankind and a recurrent, even obsessive theme of culture both high and low. It engages the interest of children from an early age, as anyone familiar with the classic fairy tales collected by Grimm, Andersen, and Perrault are aware.\" Posner adds, \"To shield children right up to the age of 18 from exposure to violent descriptions and images would not only be quixotic, but deforming; it would leave them unequipped to cope with the world as we know it.\" Many early games were little more than shooting galleries where players were encouraged to blast everything that moved. Many current games are designed to be ethical testing grounds. They allow players to navigate an expansive and open-ended world, make their own choices and witness their consequences. The Sims designer Will Wright argues that games are perhaps the only medium that allows us to experience guilt over the actions of fictional characters. In a movie, one can always pull back and condemn the character or the artist when they cross certain social boundaries. But in playing a game, we choose what happens to the characters. In the right circumstances, we can be encouraged to examine our own values by seeing how we behave within virtual space. 7. Video game play is socially isolating. Much video game play is social. Almost 60 percent of frequent gamers play with friends. Thirty-three percent play with siblings and 25 percent play with spouses or parents. Even games designed for single players are often played socially, with one person giving advice to another holding a joystick. A growing number of games are designed for multiple players — for either cooperative play in the same space or online play with distributed players. Sociologist Talmadge Wright has logged many hours observing online communities interact with and react to violent video games, concluding that meta-gaming (conversation about game content) provides a context for thinking about rules and rule-breaking. In this way there are really two games taking place simultaneously: one, the explicit conflict and combat on the screen; the other, the implicit cooperation and comradeship between the players. Two players may be fighting to death on screen and growing closer as friends off screen. Social expectations are reaffirmed through the social contract governing play, even as they are symbolically cast aside within the transgressive fantasies represented onscreen. 8. Video game play is desensitizing. Classic studies of play behavior among primates suggest that apes make basic distinctions between play fighting and actual combat. In some circumstances, they seem to take pleasure wrestling and tousling with each other. In others, they might rip each other apart in mortal combat. Game designer and play theorist Eric Zimmerman describes the ways we understand play as distinctive from reality as entering the \"magic circle.\" The same action — say, sweeping a floor — may take on different meanings in play (as in playing house) than in reality (housework). Play allows kids to express feelings and impulses that have to be carefully held in check in their real-world interactions. Media reformers argue that playing violent video games can cause a lack of empathy for real-world victims. Yet, a child who responds to a video game the same way he or she responds to a real-world tragedy could be showing symptoms of being severely emotionally disturbed. Here's where the media effects research, which often uses punching rubber dolls as a marker of real-world aggression, becomes problematic. The kid who is punching a toy designed for this purpose is still within the \"magic circle\" of play and understands her actions on those terms. Such research shows us only that violent play leads to more violent play."}
{"url": "https://en.m.wikipedia.org/wiki/Alternating_current", "text": "Alternating current (green curve). The horizontal axis measures time (it also represents zero voltage/current); the vertical, current or voltage. The usual waveform of alternating current in most electric power circuits is a sine wave, whose positive half-period corresponds with positive direction of the current and vice versa (the full period is called a cycle). In certain applications, like guitar amplifiers, different waveforms are used, such as triangular waves or square waves. Audio and radio signals carried on electrical wires are also examples of alternating current. These types of alternating current carry information such as sound (audio) or images (video) sometimes carried by modulation of an AC carrier signal. These currents typically alternate at higher frequencies than those used in power transmission. A schematic representation of long distance electric power transmission. From left to right: G=generator, U=step-up transformer, V=voltage at beginning of transmission line, Pt=power entering transmission line, I=current in wires, R=total resistance in wires, Pw=power lost in transmission line, Pe=power reaching the end of the transmission line, D=step-down transformer, C=consumers. Electrical energy is distributed as alternating current because AC voltage may be increased or decreased with a transformer. This allows the power to be transmitted through power lines efficiently at high voltage, which reduces the energy lost as heat due to resistance of the wire, and transformed to a lower, safer voltage for use. Use of a higher voltage leads to significantly more efficient transmission of power. The power losses (Pw{\\displaystyle P_{\\rm {w}}}) in the wire are a product of the square of the current ( I ) and the resistance (R) of the wire, described by the formula: Pw=I 2 R.{\\displaystyle P_{\\rm {w}}=I^{2}R\\,.} This means that when transmitting a fixed power on a given wire, if the current is halved (i.e. the voltage is doubled), the power loss due to the wire's resistance will be reduced to one quarter. The power transmitted is equal to the product of the current and the voltage (assuming no phase difference); that is, Pt=IV.{\\displaystyle P_{\\rm {t}}=IV\\,.} Consequently, power transmitted at a higher voltage requires less loss-producing current than for the same power at a lower voltage. Power is often transmitted at hundreds of kilovolts on pylons, and transformed down to tens of kilovolts to be transmitted on lower level lines, and finally transformed down to 100 V – 240 V for domestic use. Three-phase high-voltage transmission lines use alternating currents to distribute power over long distances between electric generation plants and consumers. The lines in the picture are located in eastern Utah. High voltages have disadvantages, such as the increased insulation required, and generally increased difficulty in their safe handling. In a power plant, energy is generated at a convenient voltage for the design of a generator, and then stepped up to a high voltage for transmission. Near the loads, the transmission voltage is stepped down to the voltages used by equipment. Consumer voltages vary somewhat depending on the country and size of load, but generally motors and lighting are built to use up to a few hundred volts between phases. The voltage delivered to equipment such as lighting and motor loads is standardized, with an allowable range of voltage over which equipment is expected to operate. Standard power utilization voltages and percentage tolerance vary in the different mains power systems found in the world. High-voltage direct-current (HVDC) electric power transmission systems have become more viable as technology has provided efficient means of changing the voltage of DC power. Transmission with high voltage direct current was not feasible in the early days of electric power transmission, as there was then no economically viable way to step the voltage of DC down for end user applications such as lighting incandescent bulbs. Three-phase electrical generation is very common. The simplest way is to use three separate coils in the generator stator, physically offset by an angle of 120° (one-third of a complete 360° phase) to each other. Three current waveforms are produced that are equal in magnitude and 120° out of phase to each other. If coils are added opposite to these (60° spacing), they generate the same phases with reverse polarity and so can be simply wired together. In practice, higher \"pole orders\" are commonly used. For example, a 12-pole machine would have 36 coils (10° spacing). The advantage is that lower rotational speeds can be used to generate the same frequency. For example, a 2-pole machine running at 3600 rpm and a 12-pole machine running at 600 rpm produce the same frequency; the lower speed is preferable for larger machines. If the load on a three-phase system is balanced equally among the phases, no current flows through the neutral point. Even in the worst-case unbalanced (linear) load, the neutral current will not exceed the highest of the phase currents. Non-linear loads (e.g. the switch-mode power supplies widely used) may require an oversized neutral bus and neutral conductor in the upstream distribution panel to handle harmonics. Harmonics can cause neutral conductor current levels to exceed that of one or all phase conductors. For three-phase at utilization voltages a four-wire system is often used. When stepping down three-phase, a transformer with a Delta (3-wire) primary and a Star (4-wire, center-earthed) secondary is often used so there is no need for a neutral on the supply side. For smaller customers (just how small varies by country and age of the installation) only a single phase and neutral, or two phases and neutral, are taken to the property. For larger installations all three phases and neutral are taken to the main distribution panel. From the three-phase main panel, both single and three-phase circuits may lead off. Three-wire single-phase systems, with a single center-tapped transformer giving two live conductors, is a common distribution scheme for residential and small commercial buildings in North America. This arrangement is sometimes incorrectly referred to as \"two phase\". A similar method is used for a different reason on construction sites in the UK. Small power tools and lighting are supposed to be supplied by a local center-tapped transformer with a voltage of 55 V between each power conductor and earth. This significantly reduces the risk of electric shock in the event that one of the live conductors becomes exposed through an equipment fault whilst still allowing a reasonable voltage of 110 V between the two conductors for running the tools. A third wire, called the bond (or earth) wire, is often connected between non-current-carrying metal enclosures and earth ground. This conductor provides protection from electric shock due to accidental contact of circuit conductors with the metal chassis of portable appliances and tools. Bonding all non-current-carrying metal parts into one complete system ensures there is always a low electrical impedance path to ground sufficient to carry any fault current for as long as it takes for the system to clear the fault. This low impedance path allows the maximum amount of fault current, causing the overcurrent protection device (breakers, fuses) to trip or burn out as quickly as possible, bringing the electrical system to a safe state. All bond wires are bonded to ground at the main service panel, as is the neutral/identified conductor if present. The frequency of the electrical system varies by country and sometimes within a country; most electric power is generated at either 50 or 60 Hertz. Some countries have a mixture of 50 Hz and 60 Hz supplies, notably electricity power transmission in Japan. A low frequency eases the design of electric motors, particularly for hoisting, crushing and rolling applications, and commutator-type traction motors for applications such as railways. However, low frequency also causes noticeable flicker in arc lamps and incandescent light bulbs. The use of lower frequencies also provided the advantage of lower transmission losses, which are proportional to frequency. The original Niagara Falls generators were built to produce 25 Hz power, as a compromise between low frequency for traction and heavy induction motors, while still allowing incandescent lighting to operate (although with noticeable flicker). Most of the 25 Hz residential and commercial customers for Niagara Falls power were converted to 60 Hz by the late 1950s, although some[which?] 25 Hz industrial customers still existed as of the start of the 21st century. 16.7 Hz power (formerly 16 2/3 Hz) is still used in some European rail systems, such as in Austria, Germany, Norway, Sweden and Switzerland. Off-shore, military, textile industry, marine, aircraft, and spacecraft applications sometimes use 400 Hz, for benefits of reduced weight of apparatus or higher motor speeds. Computer mainframe systems were often powered by 400 Hz or 415 Hz for benefits of ripple reduction while using smaller internal AC to DC conversion units.[citation needed] A direct current flows uniformly throughout the cross-section of a homogeneous electrically conducting wire. An alternating current of any frequency is forced away from the wire's center, toward its outer surface. This is because an alternating current (which is the result of the acceleration of electric charge) creates electromagnetic waves (a phenomenon known as electromagnetic radiation). Electric conductors are not conducive to electromagnetic waves (a perfect electric conductor prohibits all electromagnetic waves within its boundary), so a wire that is made of a non-perfect conductor (a conductor with finite, rather than infinite, electrical conductivity) pushes the alternating current, along with their associated electromagnetic fields, away from the wire's center. The phenomenon of alternating current being pushed away from the center of the conductor is called skin effect, and a direct current does not exhibit this effect, since a direct current does not create electromagnetic waves. At very high frequencies, the current no longer flows in the wire, but effectively flows on the surface of the wire, within a thickness of a few skin depths. The skin depth is the thickness at which the current density is reduced by 63%. Even at relatively low frequencies used for power transmission (50 Hz – 60 Hz), non-uniform distribution of current still occurs in sufficiently thick conductors. For example, the skin depth of a copper conductor is approximately 8.57 mm at 60 Hz, so high current conductors are usually hollow to reduce their mass and cost. This tendency of alternating current to flow predominantly in the periphery of conductors reduces the effective cross-section of the conductor. This increases the effective AC resistance of the conductor, since resistance is inversely proportional to the cross-sectional area. A conductor's AC resistance is higher than its DC resistance, causing a higher energy loss due to ohmic heating (also called I 2 R loss). For low to medium frequencies, conductors can be divided into stranded wires, each insulated from the others, with the relative positions of individual strands specially arranged within the conductor bundle. Wire constructed using this technique is called Litz wire. This measure helps to partially mitigate skin effect by forcing more equal current throughout the total cross section of the stranded conductors. Litz wire is used for making high-Qinductors, reducing losses in flexible conductors carrying very high currents at lower frequencies, and in the windings of devices carrying higher radio frequency current (up to hundreds of kilohertz), such as switch-mode power supplies and radio frequencytransformers. At frequencies up to about 1 GHz, pairs of wires are twisted together in a cable, forming a twisted pair. This reduces losses from electromagnetic radiation and inductive coupling. A twisted pair must be used with a balanced signalling system, so that the two wires carry equal but opposite currents. Each wire in a twisted pair radiates a signal, but it is effectively cancelled by radiation from the other wire, resulting in almost no radiation loss. Coaxial cables are commonly used at audio frequencies and above for convenience. A coaxial cable has a conductive wire inside a conductive tube, separated by a dielectric layer. The current flowing on the surface of the inner conductor is equal and opposite to the current flowing on the inner surface of the outer tube. The electromagnetic field is thus completely contained within the tube, and (ideally) no energy is lost to radiation or coupling outside the tube. Coaxial cables have acceptably small losses for frequencies up to about 5 GHz. For microwave frequencies greater than 5 GHz, the losses (due mainly to the dielectric separating the inner and outer tubes being a non-ideal insulator) become too large, making waveguides a more efficient medium for transmitting energy. Coaxial cables often use a perforated dielectric layer to separate the inner and outer conductors in order to minimize the power dissipated by the dielectric. Waveguides are similar to coaxial cables, as both consist of tubes, with the biggest difference being that waveguides have no inner conductor. Waveguides can have any arbitrary cross section, but rectangular cross sections are the most common. Because waveguides do not have an inner conductor to carry a return current, waveguides cannot deliver energy by means of an electric current, but rather by means of a guidedelectromagnetic field. Although surface currents do flow on the inner walls of the waveguides, those surface currents do not carry power. Power is carried by the guided electromagnetic fields. The surface currents are set up by the guided electromagnetic fields and have the effect of keeping the fields inside the waveguide and preventing leakage of the fields to the space outside the waveguide. Waveguides have dimensions comparable to the wavelength of the alternating current to be transmitted, so they are feasible only at microwave frequencies. In addition to this mechanical feasibility, electrical resistance of the non-ideal metals forming the walls of the waveguide causes dissipation of power (surface currents flowing on lossy conductors dissipate power). At higher frequencies, the power lost to this dissipation becomes unacceptably large. At frequencies greater than 200 GHz, waveguide dimensions become impractically small, and the ohmic losses in the waveguide walls become large. Instead, fiber optics, which are a form of dielectric waveguides, can be used. For such frequencies, the concepts of voltages and currents are no longer used. The peak-to-peak value of an AC voltage is defined as the difference between its positive peak and its negative peak. Since the maximum value of sin⁡(x){\\displaystyle \\sin(x)} is +1 and the minimum value is −1, an AC voltage swings between +Vpeak{\\displaystyle +V_{\\text{peak}}} and −Vpeak{\\displaystyle -V_{\\text{peak}}}. The peak-to-peak voltage, usually written as Vpp{\\displaystyle V_{\\text{pp}}} or VP-P{\\displaystyle V_{\\text{P-P}}}, is therefore Vpeak−(−Vpeak)=2Vpeak{\\displaystyle V_{\\text{peak}}-(-V_{\\text{peak}})=2V_{\\text{peak}}}. where the trigonometric identitysin2⁡(x)=1−cos⁡(2x)2{\\displaystyle \\sin ^{2}(x)={\\frac {1-\\cos(2x)}{2}}} has been used and the factor 2{\\displaystyle {\\sqrt {2}}} is called the crest factor, which varies for different waveforms. Rather than using instantaneous power, p(t){\\displaystyle p(t)}, it is more practical to use a time-averaged power (where the averaging is performed over any integer number of cycles). Therefore, AC voltage is often expressed as a root mean square (RMS) value, written as Vrms{\\displaystyle V_{\\text{rms}}}, because To illustrate these concepts, consider a 230 V AC mains supply used in many countries around the world. It is so called because its root mean square value is 230 V. This means that the time-averaged power delivered Paverage{\\displaystyle P_{\\text{average}}} is equivalent to the power delivered by a DC voltage of 230 V. To determine the peak voltage (amplitude), we can rearrange the above equation to: For 230 V AC, the peak voltage Vpeak{\\displaystyle V_{\\text{peak}}} is therefore 230 V×2{\\displaystyle 230{\\text{ V}}\\times {\\sqrt {2}}}, which is about 325 V, and the peak power Ppeak{\\displaystyle P_{\\text{peak}}} is 230×R×W×2{\\displaystyle 230\\times R\\times W\\times 2}, that is 460 RW. During the course of one cycle (two cycle as the power) the voltage rises from zero to 325 V, the power from zero to 460 RW, and both falls through zero. Next, the voltage descends to reverse direction, -325 V, but the power ascends again to 460 RW, and both returns to zero. Alternating current is used to transmit information, as in the cases of telephone and cable television. Information signals are carried over a wide range of AC frequencies. POTS telephone signals have a frequency of about 3 kHz, close to the baseband audio frequency. Cable television and other cable-transmitted information currents may alternate at frequencies of tens to thousands of megahertz. These frequencies are similar to the electromagnetic wave frequencies often used to transmit the same types of information over the air. In 1876, Russian engineer Pavel Yablochkov invented a lighting system where sets of induction coils were installed along a high voltage AC line. Instead of changing voltage, the primary windings transferred power to the secondary windings which were connected to one or several 'electric candles' (arc lamps) of his own design,[5][6] used to keep the failure of one lamp from disabling the entire circuit.[5] In 1878, the Ganz factory, Budapest, Hungary, began manufacturing equipment for electric lighting and, by 1883, had installed over fifty systems in Austria-Hungary. Their AC systems used arc and incandescent lamps, generators, and other equipment.[7] Alternating current systems can use transformers to change voltage from low to high level and back, allowing generation and consumption at low voltages but transmission, possibly over great distances, at high voltage, with savings in the cost of conductors and energy losses. A bipolar open-core power transformer developed by Lucien Gaulard and John Dixon Gibbs was demonstrated in London in 1881, and attracted the interest of Westinghouse. They also exhibited the invention in Turin in 1884. However, these early induction coils with open magnetic circuits are inefficient at transferring power to loads. Until about 1880, the paradigm for AC power transmission from a high voltage supply to a low voltage load was a series circuit. Open-core transformers with a ratio near 1:1 were connected with their primaries in series to allow use of a high voltage for transmission while presenting a low voltage to the lamps. The inherent flaw in this method was that turning off a single lamp (or other electric device) affected the voltage supplied to all others on the same circuit. Many adjustable transformer designs were introduced to compensate for this problematic characteristic of the series circuit, including those employing methods of adjusting the core or bypassing the magnetic flux around part of a coil.[8] The direct current systems did not have these drawbacks, giving it significant advantages over early AC systems. In the UK, Sebastian de Ferranti, who had been developing AC generators and transformers in London since 1882, redesigned the AC system at the Grosvenor Gallery power station in 1886 for the London Electric Supply Corporation (LESCo) including alternators of his own design and open core transformer designs with serial connections for utilization loads - similar to Gaulard and Gibbs.[9] In 1890, he designed their power station at Deptford[10] and converted the Grosvenor Gallery station across the Thames into an electrical substation, showing the way to integrate older plants into a universal AC supply system.[11] In the autumn[ambiguous] of 1884, Károly Zipernowsky, Ottó Bláthy and Miksa Déri (ZBD), three engineers associated with the Ganz Works of Budapest, determined that open-core devices were impractical, as they were incapable of reliably regulating voltage.[12] Bláthy had suggested the use of closed cores, Zipernowsky had suggested the use of parallel shunt connections, and Déri had performed the experiments;[13] In their joint 1885 patent applications for novel transformers (later called ZBD transformers), they described two designs with closed magnetic circuits where copper windings were either wound around a ring core of iron wires or else surrounded by a core of iron wires.[8] In both designs, the magnetic flux linking the primary and secondary windings traveled almost entirely within the confines of the iron core, with no intentional path through air (see toroidal cores). The new transformers were 3.4 times more efficient than the open-core bipolar devices of Gaulard and Gibbs.[14] The Ganz factory in 1884 shipped the world's first five high-efficiency AC transformers.[15] This first unit had been manufactured to the following specifications: 1,400 W, 40 Hz, 120:72 V, 11.6:19.4 A, ratio 1.67:1, one-phase, shell form.[15] The ZBD patents included two other major interrelated innovations: one concerning the use of parallel connected, instead of series connected, utilization loads, the other concerning the ability to have high turns ratio transformers such that the supply network voltage could be much higher (initially 1400 V to 2000 V) than the voltage of utilization loads (100 V initially preferred).[16][17] When employed in parallel connected electric distribution systems, closed-core transformers finally made it technically and economically feasible to provide electric power for lighting in homes, businesses and public spaces.[18][19] The other essential milestone was the introduction of 'voltage source, voltage intensive' (VSVI) systems'[20] by the invention of constant voltage generators in 1885.[21] In early 1885, the three engineers also eliminated the problem of eddy current losses with the invention of the lamination of electromagnetic cores.[22] Ottó Bláthy also invented the first AC electricity meter.[23][24][25][26] The AC power system was developed and adopted rapidly after 1886 due to its ability to distribute electricity efficiently over long distances, overcoming the limitations of the direct current system. In 1886, the ZBD engineers designed the world's first power station that used AC generators to power a parallel-connected common electrical network, the steam-powered Rome-Cerchi power plant.[27] The reliability of the AC technology received impetus after the Ganz Works electrified a large European metropolis: Rome in 1886.[27] Building on the advancement of AC technology in Europe,[28]George Westinghouse founded the Westinghouse Electric in Pittsburgh, Pennsylvania, on January 8, 1886.[29] The new firm became active in developing alternating current (AC) electric infrastructure throughout the United States. The Edison Electric Light Company held an option on the US rights for the Ganz ZBD transformers, requiring Westinghouse to pursue alternative designs on the same principles. George Westinghouse had bought Gaulard and Gibbs' patents for $50,000 in February 1886.[30] He assigned to William Stanley the task of redesigning the Gaulard and Gibbs transformer for commercial use in United States.[31] On March 20, 1886, Stanley conducted a demonstrative experiment in Great Barrington: A Siemens generator's voltage of 500 volts was converted into 3000 volts, and then the voltage was stepped down to 500 volts by six Westinghouse transformers. With this setup, the Westinghouse company successfully powered thirty 100-volt incandescent bulbs in twenty shops along the main street of Great Barrington.[32] The spread of Westinghouse and other AC systems triggered a push back in late 1887 by Thomas Edison (a proponent of direct current), who attempted to discredit alternating current as too dangerous in a public campaign called the \"war of the currents\". In 1888, alternating current systems gained further viability with introduction of a functional AC motor, something these systems had lacked up till then. The design, an induction motor, was independently invented by Galileo Ferraris and Nikola Tesla (with Tesla's design being licensed by Westinghouse in the US). This design was independently further developed into the modern practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown in Germany on one side,[33] and Jonas Wenström in Sweden on the other, though Brown favoured the two-phase system. The Ames Hydroelectric Generating Plant and the original Niagara Falls Adams Power Plant were among the first hydroelectric alternating current power plants. The first long distance transmission of single-phase electricity was from a hydroelectric generating plant in Oregon at Willamette Falls which in 1890 sent power fourteen miles downriver to downtown Portland for street lighting.[34] In 1891, a second transmission system was installed in Telluride Colorado.[35] The San Antonio Canyon Generator was the third commercial single-phase hydroelectric AC power plant in the United States to provide long-distance electricity. It was completed on December 31, 1892, by Almarian William Decker to provide power to the city of Pomona, California, which was 14 miles away. Meanwhile, the possibility of transferring electrical power from a waterfall at a distance was explored at the Grängesberg mine in Sweden. A 45 m fall at Hällsjön, Smedjebackens kommun, where a small iron work had been located, was selected. In 1893, a three-phase 9.5 kv system was used to transfer 400 horsepower a distance of 15 km, becoming the first commercial application.[36] In 1893, Decker designed the first American commercial three-phase power plant using alternating current—the hydroelectric Mill Creek No. 1 Hydroelectric Plant near Redlands, California. Decker's design incorporated 10 kV three-phase transmission and established the standards for the complete system of generation, transmission and motors used in USA today. The Jaruga Hydroelectric Power Plant in Croatia was set in operation on 28 August 1895. The two generators (42 Hz, 550 kW each) and the transformers were produced and installed by the Hungarian company Ganz. The transmission line from the power plant to the City of Šibenik was 11.5 kilometers (7.1 mi) long on wooden towers, and the municipal distribution grid 3000 V/110 V included six transforming stations."}
{"url": "https://en.m.wikipedia.org/wiki/Kotaku", "text": "Contents Kotaku was first launched in October 2004 with Matthew Gallant as its lead writer, with an intended target audience of young men.[3][4] About a month later, Brian Crecente was brought in to try to save the failing site.[5] Since then, the site has launched several country-specific sites for Australia, Japan, Brazil and the UK. Crecente was named one of the 20 most influential people in the video game industry over the past 20 years by GamePro in 2009[6] and one of gaming's Top 50 journalists by Edge in 2006. The site has made CNET's \"Blog 100\" list[7] and was ranked 50th on PC Magazine's \"Top 100 Classic Web Sites\" list.[8] Its name comes from the Japanese otaku (obsessive fan) and the prefix \"ko-\" (small in size).[9] Kotaku was one of several websites that was purchased by Univision Communications in their acquisition of Gawker Media in August 2016; Gizmodo Media Group was subsequently founded to house the Gawker acquisitions, operating under the Fusion Media Group, a division of Univision.[14] The Gizmodo Media Group was later acquired by the private equity firm Great Hill Partners in April 2019, and renamed G/O Media.[15] The transition to G/O Media led to several departures from the site, as well as from other sister sites under the former Gawker Media label due to conflicts with G/O Media's management. Cecilia D'Anastasio left Kotaku in December 2019 to become a journalist for Wired.[19] Joshua Rivera and Gita Jackson left in January 2020 stating it was impossible to work with the new management.[20]Jason Schreier, one of Kotaku's writers since 2012 known for his investigative in-depth coverage of working conditions at various studios and development histories for various video games, announced his departure from the site on April 16, 2020, citing the issues surrounding G/O Media which filtered into disruptions at their sister website Deadspin around October 2019. Schreier subsequently took a position at Bloomberg News.[21] In May 2020, senior writer Harper Jay MacIntyre[a] departed from Kotaku, similarly citing conflicts with management, and joined Double Fine Productions as their content and community manager.[24] Totilo announced he was departing as editor in chief on February 5, 2021, though will remain in games journalism elsewhere.[12] Riley MacLeod served as interim editor in chief following Totilo's departure, before Patricia Hernandez commenced her tenure as editor in chief from June 2, 2021.[26] Jen Glennon was appointed editor of the site in October 2023, after previous editor Patricia Hernandez was reportedly fired following a \"personal disagreement\" in August 2023.[27][28] In November 2023, G/O Media announced it was laying off 23 people across Kotaku and the company's other websites.[29][30] Jen Glennon resigned her position as editor in March 2024, citing an opposition to G/O Media's desire for the site to deprioritize news and instead focus on producing game guides.[31] In 2007, attorney Jack Thompson sued Gawker Media and site editor Brian Crecente over concerns that Kotaku declined to remove threatening user comments,[32] but the lawsuit was dismissed the next day.[33] On October 9, 2021, Kotaku published an article about Metroid Dread, which had been released a day prior, running on Nintendo Switch emulators. The article praised the game's performance on emulators (said to be better than on the Nintendo Switch itself), thanked \"pirates, emulators, modders, and hackers\", and suggested readers emulate older or expensive games themselves.[35] The article was criticized for promoting piracy, especially of newly released games, but was also noted to have sparked wider discussions about the role of emulation in video game preservation.[36][37][38][39] On October 10, Kotaku revised the article to clarify they were referring to game preservation[40] and, after a complaint from Nintendo on a later date, removed all mentions of piracy from the article.[41]Kotaku also issued an apology and stated that, though they believed emulation was \"a vital part of the world of gaming\", they did not condone using it to acquire games illegally.[41] In 2023, Kotaku was blacklisted by Nintendo, reportedly over articles that covered leaks of unreleased Nintendo games. Further controversy followed when then senior writer Luke Plunkett posted a picture of a fighter plane with victory markings featuring the Imperial Japanese flag.[46] In 2014, Kotaku was part of the accusations that instigated the harassment campaign known as Gamergate when a writer from the site, Nathan Grayson, was falsely accused of writing a favorable review of the game Depression Quest as a result of his relationship with its developer, Zoë Quinn. After conducting an internal review, it was discovered that no review of Depression Quest existed and he had only written one article that mentioned Quinn in passing before their relationship began.[47][48] The subreddit/r/KotakuInAction became a hub for the Gamergate community.[49][50] Its creator attempted to shut it down in 2018, claiming that it had become \"infested with racism and sexism\", but it was reinstated by a Reddit administrator due to the site's guidelines.[51][52]"}
{"url": "https://en.m.wikipedia.org/wiki/Mosquito", "text": "The mosquito life cycle consists of four stages: egg, larva, pupa, and adult. Eggs are laid on the water surface; they hatch into motile larvae that feed on aquatic algae and organic material. These larvae are important food sources for many freshwater animals, such as dragonfly nymphs, many fish, and some birds. Adult females of many species have mouthparts adapted to pierce the skin of a host and feed on blood of a wide range of vertebrate hosts, and some invertebrates, primarily other arthropods. Some species only produce eggs after a blood meal. The mosquito's saliva is transferred to the host during the bite, and can cause an itchy rash. In addition, blood-feeding species can ingest pathogens while biting, and transmit them to other hosts. Those species include vectors of parasitic diseases such as malaria and filariasis, and arboviral diseases such as yellow fever and dengue fever. By transmitting diseases, mosquitoes cause the deaths of over 725,000 people each year. Description and life cycle Like all flies, mosquitoes go through four stages in their life cycles: egg, larva, pupa, and adult. The first three stages—egg, larva, and pupa—are largely aquatic,[4] the eggs usually being laid in stagnant water.[5] They hatch to become larvae, which feed, grow, and molt until they change into pupae. The adult mosquito emerges from the mature pupa as it floats at the water surface. Mosquitoes have adult lifespans ranging from as short as a week to around a month. Some species overwinter as adults in diapause.[6] Adult Mosquitoes have one pair of wings, with distinct scales on the surface. Their wings are long and narrow, while the legs are long and thin. The body, usually grey or black, is slender, and typically 3–6 mm long. When at rest, mosquitoes hold their first pair of legs outwards, whereas the somewhat similar Chironomid midges hold these legs forwards.[7]Anopheles mosquitoes can fly for up to four hours continuously at 1 to 2 km/h (0.62 to 1.24 mph),[8] traveling up to 12 kilometres (7.5 mi) in a night. Males beat their wings between 450 and 600 times per second, driven indirectly by muscles which vibrate the thorax.[9][10] Mosquitoes are mainly small flies; the largest are in the genus Toxorhynchites, at up to 18 mm (0.71 in) in length and 24 mm (0.94 in) in wingspan.[11] Those in the genus Aedes are much smaller, with a wingspan of 2.8 to 4.4 mm (0.11 to 0.17 in).[12] Mosquitoes can develop from egg to adult in hot weather in as few as five days, but it may take up to a month.[13] At dawn or dusk, within days of pupating, males assemble in swarms, mating when females fly in.[14] The female mates only once in her lifetime, attracted by the pheromones emitted by the male.[15][16] In species that need blood for the eggs to develop, the female finds a host and drinks a full meal of blood. She then rests for two or three days to digest the meal and allow her eggs to develop. She is then ready to lay the eggs and repeat the cycle of feeding and laying.[14] Females can live for up to three weeks in the wild, depending on temperature, humidity, their ability to obtain a blood meal, and avoiding being killed by their vertebrate hosts.[14][17] Eggs The eggs of most mosquitoes are laid in stagnant water, which may be a pond, a marsh, a temporary puddle, a water-filled hole in a tree, or the water-trapping leaf axils of a bromeliad. Some lay near the water's edge while others attach their eggs to aquatic plants. A few, like Opifex fuscus, can breed in salt-marshes.[5]Wyeomyia smithii breeds in the pitchers of pitcher plants, its larvae feeding on decaying insects that have drowned there.[18] Oviposition, egg-laying, varies between species. Anopheles females fly over the water, touching down or dapping to place eggs on the surface one at a time; their eggs are roughly cigar-shaped and have floats down their sides. A female can lay 100–200 eggs in her lifetime.[14]Aedes females drop their eggs singly, on damp mud or other surfaces near water; their eggs hatch only when they are flooded.[19] Females in genera such as Culex, Culiseta, and Uranotaenia lay their eggs in floating rafts.[20][21]Mansonia females in contrast lay their eggs in arrays, attached usually to the under-surfaces of waterlily pads.[22] Clutches of eggs of most mosquito species hatch simultaneously, but Aedes eggs in diapause hatch irregularly over an extended period.[19] Larva The mosquito larva's head has prominent mouth brushes used for feeding, a large thorax with no legs, and a segmented abdomen. It breathes air through a siphon on its abdomen, so must come to the surface frequently. It spends most of its time feeding on algae, bacteria, and other microbes in the water's surface layer. It dives below the surface when disturbed. It swims either by propelling itself with its mouth brushes, or by jerkily wriggling its body. It develops through several stages, or instars, molting each time, after which it metamorphoses into a pupa.[13]Aedes larvae, except when very young, can withstand drying; they go into diapause for several months if their pond dries out.[19] Pupa The head and thorax of the pupa are merged into a cephalothorax, with the abdomen curving around beneath it. The pupa or \"tumbler\" can swim actively by flipping its abdomen. Like the larva, the pupa of most species must come to the surface frequently to breathe, which they do through a pair of respiratory trumpets on their cephalothoraxes. They do not feed; they pass much of their time hanging from the surface of the water by their respiratory trumpets. If alarmed, they swim downwards by flipping their abdomens in much the same way as the larvae. If undisturbed, they soon float up again. The adult emerges from the pupa at the surface of the water and flies off.[13] Mosquito pupae, shortly before the adults emerged. The head and thorax are fused into the cephalothorax. Feeding by adults Diet Both male and female mosquitoes feed on nectar, aphid honeydew, and plant juices,[17] but in many species the females are also blood-suckingectoparasites. In some of those species, a blood meal is essential for egg production; in others, it just enables the female to lay more eggs.[23] Both plant materials and blood are useful sources of energy in the form of sugars. Blood supplies more concentrated nutrients, such as lipids, but the main function of blood meals is to obtain proteins for egg production.[24][25] Mosquitoes like Toxorhynchites reproduce autogenously, not needing blood meals. Disease vector mosquitoes like Anopheles and Aedes are anautogenous, requiring blood to lay eggs. Many Culex species are partially anautogenous, needing blood only for their second and subsequent clutches of eggs.[26] Finding hosts Blood-feeding female mosquitoes find their hosts using multiple cues, including exhaled carbon dioxide, heat, and many different odorants. Most mosquito species are crepuscular, feeding at dawn or dusk, and resting in a cool place through the heat of the day.[31] Some species, such as the Asian tiger mosquito, are known to fly and feed during daytime.[32] Female mosquitoes hunt for hosts by smelling substances such as carbon dioxide (CO2) and 1-octen-3-ol (mushroom alcohol, found in exhaled breath) produced from the host, and through visual recognition.[33] The semiochemical that most strongly attracts Culex quinquefasciatus is nonanal.[34] Another attractant is sulcatone.[35] A large part of the mosquito's sense of smell, or olfactory system, is devoted to sniffing out blood sources. Of 72 types of odor receptors on its antennae, at least 27 are tuned to detect chemicals found in perspiration.[36] In Aedes, the search for a host takes place in two phases. First, the mosquito flies about until it detects a host's odorants; then it flies towards them, using the concentration of odorants as its guide.[37] Mosquitoes prefer to feed on people with type O blood, an abundance of skin bacteria, high body heat, and pregnant women.[38][39] Individuals' attractiveness to mosquitoes has a heritable, genetically controlled component.[40] Mouthparts Female mosquito mouthparts are highly adapted to piercing skin and sucking blood. Males only drink sugary fluids, and have less specialized mouthparts.[41] Externally, the most obvious feeding structure of the mosquito is the proboscis, composed of the labium, U-shaped in section like a rain gutter, which sheaths a bundle (fascicle) of six piercing mouthparts or stylets. These are two mandibles, two maxillae, the hypopharynx, and the labrum. The labium bends back into a bow when the mosquito begins to bite, staying in contact with the skin and guiding the stylets downwards. The extremely sharp tips of the labrum and maxillae are moved backwards and forwards to saw their way into the skin, with just one thousandth of the force that would be needed to penetrate the skin with a needle, resulting in a painless insertion.[42][43][44] Evolution of mosquito mouthparts, with grasshopper mouthparts (shown both in situ and separately) representing a more primitive condition. All the mouthparts except the labium are stylets, formed into a fascicle or bundle. Mouthparts of a female mosquito while feeding on blood, showing the flexible labium sheath supporting the piercing and sucking tube which penetrates the host's skin Egg development and blood digestion An Anopheles stephensi female is engorged with blood and beginning to pass unwanted liquid fractions to make room in its gut for more of the solid nutrients. Females of many blood-feeding species need a blood meal to begin the process of egg development. A sufficiently large blood meal triggers a hormonal cascade that leads to egg development.[58] Upon completion of feeding, the mosquito withdraws her proboscis, and as the gut fills up, the stomach lining secretes a peritrophic membrane that surrounds the blood. This keeps the blood separate from anything else in the stomach. Like many Hemiptera that survive on dilute liquid diets, many adult mosquitoes excrete surplus liquid even when feeding. This permits females to accumulate a full meal of nutrient solids. The blood meal is digested over a period of several days.[59] Once blood is in the stomach, the midgut synthesizes protease enzymes, primarily trypsin assisted by aminopeptidase, that hydrolyze the blood proteins into free amino acids. These are used in the synthesis of vitellogenin, which in turn is made into egg yolk protein.[60] Ecology and distribution Distribution Mosquitoes have a cosmopolitan distribution, occurring in every land region except Antarctica and a few islands with polar or subpolar climates, such as Iceland, which is essentially free of mosquitoes.[61] This absence is probably caused by Iceland's climate. Its weather is unpredictable, freezing but often warming suddenly in mid-winter, making mosquitoes emerge from pupae in diapause, and then freezing again before they can complete their life cycle.[62][63] Eggs of temperate zone mosquitoes are more tolerant of cold than the eggs of species indigenous to warmer regions.[64][65] Many can tolerate subzero temperatures, while adults of some species can survive winter by sheltering in microhabitats such as buildings or hollow trees.[66] In warm and humid tropical regions, some mosquito species are active for the entire year, but in temperate and cold regions they hibernate or enter diapause. Arctic or subarctic mosquitoes, like some other arctic midges in families such as Simuliidae and Ceratopogonidae may be active for only a few weeks annually as melt-water pools form on the permafrost. During that time, though, they emerge in huge numbers in some regions and may take up to 300 ml of blood per day from each animal in a caribou herd.[67] For a mosquito to transmit disease, there must be favorable seasonal conditions,[68] primarily humidity, temperature, and precipitation.[69]El Niño affects the location and number of outbreaks in East Africa, Latin America, Southeast Asia and India. Climate change impacts the seasonal factors and in turn the dispersal of mosquitoes.[70] Climate models can use historic data to recreate past outbreaks and to predict the risk of vector-borne disease, based on an area's forecasted climate.[71] Mosquito-borne diseases have long been most prevalent in East Africa, Latin America, Southeast Asia, and India. An emergence in Europe was observed early in the 21st century. It is predicted that by 2030, the climate of southern Great Britain will be suitable for transmission of Plasmodium vivax malaria for two months of the year, and that by 2080, the same will be true for southern Scotland.[72][73] Pollination Several flowers including members of the Asteraceae, Rosaceae and Orchidaceae are pollinated by mosquitoes, which visit to obtain sugar-rich nectar. They are attracted to flowers by a range of semiochemicals such as alcohols, aldehydes, ketones, and terpenes. Mosquitoes have visited and pollinated flowers since the Cretaceous period. It is possible that plant-sucking preadapted mosquitoes to blood-sucking.[17] Parasitism Ecologically, blood-feeding mosquitoes are micropredators, small animals that feed on larger animals without immediately killing them. Evolutionary biologists see this as a form of parasitism; in Edward O. Wilson's phrase \"Parasites ... are predators that eat prey in units of less than one.\"[76] Micropredation is one of six major evolutionarily stable strategies within parasitism. It is distinguished by leaving the host still able to reproduce, unlike the activity of parasitic castrators or parasitoids; and having multiple hosts, unlike conventional parasites.[77][78] From this perspective, mosquitoes are ectoparasites, feeding on blood from the outside of their hosts, using their piercing mouthparts, rather than entering their bodies. Unlike some other ectoparasites such as fleas and lice, mosquitoes do not remain constantly on the body of the host, but visit only to feed.[78] Evolution Fossil record Culex malariager mosquito infected with the malarial parasite Plasmodium dominicana, in Dominican amber of Miocene age, 15–20 million years ago[79] The oldest known mosquitoes are Libanoculex intermedius found in Lebanese amber, dating to the Barremian stage of the Early Cretaceous, around 125 million years ago. The mouthparts of male individuals of this species are similar to living female mosquitoes, indicating that they consumed blood, unlike living male mosquitoes.[80] Three other species of Cretaceous mosquito are known. Burmaculex antiquus and Priscoculex burmanicus are known from Burmese amber from Myanmar, which dates to the earliest part of the Cenomanian stage of the Late Cretaceous, around 99 million years ago.[81][82]Paleoculicis minutus, is known from Canadian amber from Alberta, Canada, which dates to the Campanian stage of the Late Cretaceous, around 79 million years ago.[83]P. burmanicus has been assigned to the Anophelinae, indicating that the split between this subfamily and the Culicinae took place over 99 million years ago.[82] Molecular estimates suggest that this split occurred 197.5 million years ago, during the Early Jurassic, but that major diversification did not take place until the Cretaceous.[84] Taxonomy Over 3,600 species of mosquitoes in 112 genera have been described. They are traditionally divided into two subfamilies, the Anophelinae and the Culicinae, which carry different diseases. Roughly speaking, protozoal diseases like malaria are transmitted by anophelines, while viral diseases such as yellow fever and dengue fever are transmitted by culicines.[85] The name Culicidae was introduced by the German entomologist Johann Wilhelm Meigen in his seven-volume classification published in 1818–1838.[86] Mosquito taxonomy was advanced in 1901 when the English entomologist Frederick Vincent Theobald published his 5-volume monograph on the Culicidae.[87] He had been provided with mosquito specimens sent in to the British Museum (Natural History) from around the world, on the 1898 instruction of the Secretary of State for the Colonies, Joseph Chamberlain, who had written that \"in view of the possible connection of Malaria with mosquitoes, it is desirable to obtain exact knowledge of the different species of mosquitoes and allied insects in the various tropical colonies. I will therefore ask you ... to have collections made of the winged insects in the Colony which bite men or animals.\"[88] Internal Kyanne Reidenbach and colleagues analysed mosquito phylogenetics in 2009, using both nuclear DNA and morphology of 26 species. They note that Anophelinae is confirmed to be rather basal, but that the deeper parts of the tree are not well resolved.[92] Origin myths The peoples of Siberia have origin myths surrounding the mosquito. One Ostiak myth tells of a man-eating giant, Punegusse, who is killed by a hero but will not stay dead. The hero eventually burns the giant, but the ashes of the fire become mosquitoes that continue to plague mankind. Other myths from the Yakuts, Goldes (Nanai people), and Samoyed have the insect arising from the ashes or fragments of some giant creature or demon. Similar tales found in Native North American myth, with the mosquito arising from the ashes of a man-eater, suggest a common origin. The Tatars of the Altai had a variant of the same myth, involving the fragments of the dead giant, Andalma-Muus, becoming mosquitoes and other insects.[125] Lafcadio Hearn tells that in Japan, mosquitoes are seen as reincarnations of the dead, condemned by the errors of their former lives to the condition of Jiki-ketsu-gaki, or \"blood-drinking pretas\".[126] Modern era How a Mosquito Operates (1912) Winsor McCay's 1912 film How a Mosquito Operates was one of the earliest works of animation. It has been described as far ahead of its time in technical quality.[127] It depicts a giant mosquito tormenting a sleeping man.[128] Twelve ships of the Royal Navy have borne the name HMS Mosquito or the archaic form of the name, HMS Musquito.[129] ^Tyagi, B.K. (2004). The Invincible Deadly Mosquitoes. Scientific Publishers. p. 79. ISBN978-93-87741-30-0. Archived from the original on 29 January 2022. Retrieved 6 April 2021. Only female mosquitoes require a blood meal (protein)...The number of egg formation and development in ovary of the female is directly dependent on quantum and nature supply of blood meal. ^\"Biology\". mosquito.org. American Mosquito Control Association. Archived from the original on 29 March 2021. Retrieved 6 April 2021. Acquiring a blood meal (protein) is essential for egg production, but mostly both male and female mosquitoes are nectar feeders for their nutrition. ^Wilson, Edward O. (2014). The Meaning of Human Existence. W. W. Norton & Company. p. 112. ISBN978-0-87140-480-0. Parasites, in a phrase, are predators that eat prey in units of less than one. Tolerable parasites are those that have evolved to ensure their own survival and reproduction but at the same time with minimum pain and cost to the host."}
{"url": "https://en.m.wikipedia.org/wiki/Mongols", "text": "The Mongols are bound together by a common heritage and ethnic identity. Their indigenous dialects are collectively known as the Mongolian language. The contiguous geographical area in which the Mongols primarily live is referred to as the Mongol heartland, especially in history books. The ancestors of the modern-day Mongols are referred to as Proto-Mongols. In various times Mongolic peoples have been equated with the Scythians, the Magog, and the Tungusic peoples. Based on Chinese historical texts the ancestry of the Mongolic peoples can be traced back to the Donghu, a nomadic confederation occupying eastern Mongolia and Manchuria. The Donghu neighboured the Xiongnu, whose identity is still debated today. Although some scholars maintain that they were proto-Mongols, they were more likely a multi-ethnic group of Mongolic and Turkic tribes.[19][full citation needed] It has been suggested that the language of the Huns was related to the Xiongnu.[20] The Donghu, however, can be much more easily labeled proto-Mongol since the Chinese histories trace only Mongolic tribes and kingdoms (Xianbei and Wuhuan peoples) from them, although some historical texts claim a mixed Xiongnu-Donghu ancestry for some tribes (e.g. the Khitan).[21][22] The Xianbei formed part of the Donghu confederation, and possibly had in earlier times some independence within the Donghu confederation as well as from the Zhou dynasty. During the Warring States the poem \"The Great Summons\" (Chinese: 大招; pinyin: Dà zhāo) in the anthology Verses of Chu mentions small-waisted and long-necked Xianbei women,[26] and possibly also the book Discourses of the States, which states that during the reign of King Cheng of Zhou (reigned 1042–1021 BCE) the Xianbei came to participate at a meeting of Zhou subject-lords at Qiyang (岐阳) (now Qishan County) but were only allowed to perform the fire ceremony under the supervision of Chu since they were not vassals (诸侯) by enfeoffment and establishment. The Xianbei chieftain was appointed joint guardian of the ritual torch along with Chu viscount Xiong Yi.[27][28][b] After the Donghu were defeated by Xiongnu king Modu Chanyu, the Xianbei and Wuhuan survived as the main remnants of the confederation. Tadun Khan of the Wuhuan (died 207 AD) was the ancestor of the proto-Mongolic Kumo Xi.[44] The Wuhuan are of the direct Donghu royal line and the New Book of Tang says that in 209 BCE, Modu Chanyu defeated the Wuhuan instead of using the word Donghu. The Xianbei, however, were of the lateral Donghu line and had a somewhat separate identity, although they shared the same language with the Wuhuan. In 49 CE the Xianbei ruler Bianhe (Bayan Khan?) raided and defeated the Xiongnu, killing 2000, after having received generous gifts from Emperor Guangwu of Han. The Xianbei reached their peak under Tanshihuai Khan (reigned 156–181) who expanded the vast, but short lived, Xianbei state (93–234). Three prominent groups split from the Xianbei state as recorded by the Chinese histories: the Rouran (claimed by some to be the Pannonian Avars), the Khitan people and the Shiwei (a subtribe called the \"Shiwei Menggu\" is held to be the origin of the Genghisid Mongols).[45] Besides these three Xianbei groups, there were others such as the Murong, Duan and Tuoba. Their culture was nomadic, their religion shamanism or Buddhism and their military strength formidable. There is still no direct evidence that the Rouran spoke Mongolic languages, although most scholars agree that they were Proto-Mongolic.[46] The Khitan, however, had two scripts of their own and many Mongolic words are found in their half-deciphered writings. Geographically, the Tuoba Xianbei ruled the southern part of Inner Mongolia and northern China, the Rouran (Yujiulü Shelun was the first to use the title khagan in 402) ruled eastern Mongolia, western Mongolia, the northern part of Inner Mongolia and northern Mongolia, the Khitan were concentrated in eastern part of Inner Mongolia north of Korea and the Shiwei were located to the north of the Khitan. These tribes and kingdoms were soon overshadowed by the rise of the First Turkic Khaganate in 555, the Uyghur Khaganate in 745 and the Yenisei Kirghiz states in 840. The Tuoba were eventually absorbed into China. The Rouran fled west from the Göktürks and either disappeared into obscurity or, as some say, invaded Europe as the Avars under their Khan, Bayan I. Some Rouran under Tatar Khan migrated east, founding the Tatar confederation, who became part of the Shiwei. The Khitans, who were independent after their separation from the Kumo Xi (of Wuhuan origin) in 388, continued as a minor power in Manchuria until one of them, Abaoji (872–926), established the Liao dynasty (916–1125). Mongol Empire The destruction of Uyghur Khaganate by the Kirghiz resulted in the end of Turkic dominance in Mongolia. According to historians, Kirghiz were not interested in assimilating newly acquired lands; instead, they controlled local tribes through various manaps (tribal leaders). The Khitans occupied the areas vacated by the Turkic Uyghurs bringing them under their control. The Yenisei Kirghiz state was centered on Khakassia and they were expelled from Mongolia by the Khitans in 924. Beginning in the 10th century, the Khitans, under the leadership of Abaoji, prevailed in several military campaigns against the Tang dynasty's border guards, and the Xi, Shiwei and Jurchen nomadic groups.[47] ... from Chinggis up high down to the common people, all are shaven in the style pojiao. As with small boys in China, they leave three locks, one hanging from the crown of their heads. When it has grown some, they clip it; the strands lower on both sides they plait to hang down on the shoulders.[49] In 1434, Eastern Mongol Taisun Khan's (1433–1452) Oirat prime minister Togoon Taish reunited the Mongols after killing Adai Khan in Khorchin. Togoon died in 1439 and his son Esen Taish became ruler of Northern Yuan dynasty. Esen later unified the Mongol tribes. The Ming dynasty attempted to invade the Northern Yuan in the 14–16th centuries, however, the Ming dynasty was defeated by the Oirat, Southern Mongol, Eastern Mongol and united Mongol armies. Esen's 30,000 cavalries defeated 500,000 Chinese soldiers in the 1449 Tumu Crisis. Within eighteen months of his defeat of the titular Khan Taisun, in 1453, Esen himself took the title of Great Khan (1454–1455) of the Great Yuan.[51] The Khalkha emerged during the reign of Dayan Khan (1479–1543) as one of the six tumens of the Eastern Mongolic peoples. They quickly became the dominant Mongolic clan in Mongolia proper.[52][53] He reunited the Mongols again. In 1550, Altan Khan led a Khalkha Mongol raid on Beijing. The Mongols voluntarily reunified during Eastern Mongolian Tümen Zasagt Khan rule (1558–1592) for the last time (the Mongol Empire united all Mongols before this). The last Mongol khagan was Ligdan in the early 17th century. He got into conflicts with the Manchus over the looting of Chinese cities, and managed to alienate most Mongol tribes. In 1618, Ligdan signed a treaty with the Ming dynasty to protect their northern border from the Manchus attack in exchange for thousands of taels of silver. By the 1620s, only the Chahars remained under his rule. Qing era Map showing wars between Qing dynasty and Dzungar KhanateA Dzungar soldier called Ayusi from the high Qing era, by Giuseppe Castiglione, 1755The Battle of Oroi-Jalatu in 1755 between the Qing (that ruled China at the time) and Mongol Dzungar armies. The fall of the Dzungar Khanate The Chahar army was defeated in 1625 and 1628 by the Inner Mongol and Manchu armies due to Ligdan's faulty tactics. The Qing forces secured their control over Inner Mongolia by 1635, and the army of the last khan Ligdan moved to battle against Tibetan Gelugpa sect (Yellow Hat sect) forces. The Gelugpa forces supported the Manchus, while Ligdan supported Kagyu sect (Red Hat sect) of Tibetan Buddhism. Ligden died in 1634 on his way to Tibet. By 1636, most Inner Mongolian nobles had submitted to the Qing dynasty founded by the Manchus. Inner Mongolian Tengis noyan revolted against the Qing in the 1640s and the Khalkha battled to protect Sunud. Western Mongol Oirats and Eastern Mongolian Khalkhas vied for domination in Mongolia since the 15th century and this conflict weakened Mongol strength. In 1688, the Western Mongol Dzungar Khanate's king Galdan Boshugtu attacked Khalkha after murder of his younger brother by Tusheet Khan Chakhundorj (main or Central Khalkha leader) and the Khalkha-Oirat War began. Galdan threatened to kill Chakhundorj and Zanabazar (Javzandamba Khutagt I, spiritual head of Khalkha) but they escaped to Sunud (Inner Mongolia). Many Khalkha nobles and folks fled to Inner Mongolia because of the war. Few Khalkhas fled to the Buryat region and Russia threatened to exterminate them if they did not submit, but many of them submitted to Galdan Boshugtu. In 1683 Galdan's armies reached Tashkent and the Syr Darya and crushed two armies of the Kazakhs. After that Galdan subjugated the Black Khirgizs and ravaged the Fergana Valley. From 1685 Galdan's forces aggressively pushed the Kazakhs. While his general Rabtan took Taraz, and his main force forced the Kazakhs to migrate westwards.[54] In 1687, he besieged the City of Turkistan. Under the leadership of Abul Khair Khan, the Kazakhs won major victories over the Dzungars at the Bulanty River in 1726, and at the Battle of Anrakay in 1729.[55] The Khalkha eventually submitted to Qing rule in 1691 by Zanabazar's decision, thus bringing all of today's Mongolia under the rule of the Qing dynasty but Khalkha de facto remained under the rule of Galdan Boshugtu Khaan until 1696. The Mongol-Oirat's Code (a treaty of alliance) against foreign invasion between the Oirats and Khalkhas was signed in 1640, however, the Mongols could not unite against foreign invasions. Chakhundorj fought against Russian invasion of Outer Mongolia until 1688 and stopped Russian invasion of Khövsgöl Province. Zanabazar struggled to bring together the Oirats and Khalkhas before the war. Galdan Boshugtu sent his army to \"liberate\" Inner Mongolia after defeating the Khalkha's army and called Inner Mongolian nobles to fight for Mongolian independence. Some Inner Mongolian nobles, Tibetans, Kumul Khanate and some Moghulistan's nobles supported his war against the Manchus, however, Inner Mongolian nobles did not battle against the Qing. There were three khans in Khalkha and Zasagt Khan Shar (Western Khalkha leader) was Galdan's ally. Tsetsen Khan (Eastern Khalkha leader) did not engage in this conflict. While Galdan was fighting in Eastern Mongolia, his nephew Tseveenravdan seized the Dzungarian throne in 1689 and this event made Galdan impossible to fight against the Qing Empire. The Russian and Qing Empires supported his action because this coup weakened Western Mongolian strength. Galdan Boshugtu's army was defeated by the outnumbering Qing army in 1696 and he died in 1697. The Mongols who fled to the Buryat region and Inner Mongolia returned after the war. Some Khalkhas mixed with the Buryats. The Buryats fought against Russian invasion since the 1620s and thousands of Buryats were massacred. The Buryat region was formally annexed to Russia by treaties in 1689 and 1727, when the territories on both the sides of Lake Baikal were separated from Mongolia. In 1689 the Treaty of Nerchinsk established the northern border of Manchuria north of the present line. The Russians retained Trans-Baikalia between Lake Baikal and the Argun River north of Mongolia. The Treaty of Kyakhta (1727), along with the Treaty of Nerchinsk, regulated the relations between Russian and Qing empires until the mid-nineteenth century, and established the Mongolia-Russia border. Oka Buryats revolted in 1767 and Russia completely conquered the Buryat region in the late 18th century. Russia and Qing were rival empires until the early 20th century, however, both empires carried out united policy against Central Asians. The Qing Empire conquered Upper Mongolia or the Oirat's Khoshut Khanate in the 1720s and 80,000 people were killed.[56] By that period, Upper Mongolian population reached 200,000. The Dzungar Khanate conquered by the Qing dynasty in 1755–1758 because of their leaders and military commanders conflicts. Some scholars estimate that about 80% of the Dzungar population were destroyed by a combination of warfare and disease during the Qing conquest of the Dzungar Khanate in 1755–1758.[57] Mark Levene, a historian whose recent research interests focus on genocide,[58] has stated that the extermination of the Dzungars was \"arguably the eighteenth century genocide par excellence.\"[59] The Dzungar population reached 600,000 in 1755. About 200,000–250,000 Oirats migrated from western Mongolia to Volga River in 1607 and established the Kalmyk Khanate.The Torghuts were led by their Tayishi, Kho Orluk. Russia was concerned about their attack but the Kalmyks became a Russian ally and a treaty to protect the southern Russian border was signed between the Kalmyk Khanate and Russia. In 1724 the Kalmyks came under control of Russia. By the early 18th century, there were approximately 300,000–350,000 Kalmyks and 15,000,000 Russians.[citation needed] The Tsardom of Russia gradually chipped away at the autonomy of the Kalmyk Khanate. These policies, for instance, encouraged the establishment of Russian and German settlements on pastures the Kalmyks used to roam and feed their livestock. In addition, the Tsarist government imposed a council on the Kalmyk Khan, thereby diluting his authority, while continuing to expect the Kalmyk Khan to provide cavalry units to fight on behalf of Russia. The Russian Orthodox church, by contrast, pressured Buddhist Kalmyks to adopt Orthodoxy. In January 1771, approximately 200,000 (170,000)[60] Kalmyks began the migration from their pastures on the left bank of the Volga to Dzungaria, through the territories of their Bashkir and Kazakh enemies. The last Kalmyk khan Ubashi led the migration to restore Mongolian independence. Ubashi Khan sent his 30,000 cavalries to the Russo-Turkish War (1768–74) to gain weapon before the migration. The Empress Catherine the Great ordered the Russian army, Bashkirs and Kazakhs to exterminate all migrants and the Empress abolished the Kalmyk Khanate.[60][61][62][63][64] The Kyrgyzs attacked them near Balkhash Lake. About 100,000–150,000 Kalmyks who settled on the west bank of the Volga River could not cross the river because the river did not freeze in the winter of 1771 and Catherine the Great executed influential nobles of them. After seven months of travel, only one-third (66,073)[60] of the original group reached Dzungaria (Balkhash Lake, western border of the Qing Empire).[65] The Qing Empire transmigrated the Kalmyks to five different areas to prevent their revolt and influential leaders of the Kalmyks died soon (killed by the Manchus). Russia states that Buryatia voluntarily merged with Russia in 1659 due to Mongolian oppression and the Kalmyks voluntarily accepted Russian rule in 1609 but only Georgia voluntarily accepted Russian rule.[66][67] In the early 20th century, the late Qing government encouraged Han Chinese settlement of Mongolian lands under the name of \"New Policies\" or \"New Administration\" (xinzheng). As a result, some Mongol leaders, especially those of Outer Mongolia, decided to seek Mongolian independence. After the Xinhai Revolution, the Mongolian Revolution on 30 November 1911 in Outer Mongolia ended an over 200-year rule of the Qing dynasty. Post-Qing era With the independence of Outer Mongolia, the Mongolian army controlled Khalkha and Khovd regions (modern day Uvs, Khovd, and Bayan-Ölgii provinces), but Northern Xinjiang (the Altai and Ili regions of the Qing empire), Upper Mongolia, Barga and Inner Mongolia came under control of the newly formed Republic of China. On February 2, 1913, the Bogd Khanate of Mongolia sent Mongolian cavalries to \"liberate\" Inner Mongolia from China. Russia refused to sell weapons to the Bogd Khanate, and the Russian czar, Nicholas II, referred to it as \"Mongolian imperialism\". Additionally, the United Kingdom urged Russia to abolish Mongolian independence as it was concerned that \"if Mongolians gain independence, then Central Asians will revolt\". 10,000 Khalkha and Inner Mongolian cavalries (about 3,500 Inner Mongols) defeated 70,000 Chinese soldiers and controlled almost all of Inner Mongolia; however, the Mongolian army retreated due to lack of weapons in 1914. 400 Mongol soldiers and 3,795 Chinese soldiers died in this war. The Khalkhas, Khovd Oirats, Buryats, Dzungarian Oirats, Upper Mongols, Barga Mongols, most Inner Mongolian and some Tuvan leaders sent statements to support Bogd Khan's call of Mongolian reunification. In reality however, most of them were too prudent or irresolute to attempt joining the Bogd Khan regime.[68] Russia encouraged Mongolia to become an autonomous region of China in 1914. Mongolia lost Barga, Dzungaria, Tuva, Upper Mongolia and Inner Mongolia in the 1915 Treaty of Kyakhta. In October 1919, the Republic of China occupied Mongolia after the suspicious deaths of Mongolian patriotic nobles. On 3 February 1921 the White Russian army—led by Baron Ungern and mainly consisting of Mongolian volunteer cavalries, and Buryat and Tatar cossacks—liberated Ulaanbaatar. Baron Ungern's purpose was to find allies to defeat the Soviet Union. The Statement of Reunification of Mongolia was adopted by Mongolian revolutionaries in 1921. The Soviet, however, considered Mongolia to be Chinese territory in 1924 during a secret meeting with the Republic of China. However, the Soviets officially recognized Mongolian independence in 1945 but carried out various policies (political, economic and cultural) against Mongolia until its fall in 1991 to prevent Pan-Mongolism and other irredentistmovements. On 10 April 1932, Mongolians revolted against the government's new policy and Soviets. The government and Soviet soldiers defeated the rebels in October. The Buryats started to migrate to Mongolia in the 1900s due to Russian oppression. Joseph Stalin's regime stopped the migration in 1930 and started a campaign of ethnic cleansing against newcomers and Mongolians. During the Stalinist repressions in Mongolia, almost all adult Buryat men and 22,000–33,000 Mongols (3–5% of the total population; common citizens, monks, Pan-Mongolists, nationalists, patriots, hundreds of military officers, nobles, intellectuals and elite people) were shot dead under Soviet orders.[69][70] Some authors also offer much higher estimates, up to 100,000 victims.[70] Around the late 1930s the Mongolian People's Republic had an overall population of about 700,000 to 900,000 people. By 1939, Soviet said \"We repressed too many people, the population of Mongolia is only hundred thousands\". The proportion of victims in relation to the population of the country is much higher than the corresponding figures of the Great Purge in the Soviet Union. The Manchukuo (1932–1945), puppet state of the Empire of Japan (1868–1947) invaded Barga and some part of Inner Mongolia with Japanese help. The Mongolian army advanced to the Great Wall of China during the Soviet–Japanese War of 1945 (Mongolian name: Liberation War of 1945). Japan forced Inner Mongolian and Barga people to fight against Mongolians but they surrendered to Mongolians and started to fight against their Japanese and Manchu allies. Marshal Khorloogiin Choibalsan called Inner Mongolians and Xinjiang Oirats to migrate to Mongolia during the war but the Soviet Army blocked Inner Mongolian migrants' way. It was a part of a Pan-Mongolian plan and few Oirats and Inner Mongols (Huuchids, Bargas, Tümeds, about 800 Uzemchins) arrived. Inner Mongolian leaders carried out active policy to merge Inner Mongolia with Mongolia since 1911. They founded the Inner Mongolian Army in 1929 but the Inner Mongolian Army disbanded after ending World War II. The Japanese Empire supported Pan-Mongolism since the 1910s but there have never been active relations between Mongolia and Imperial Japan due to Russian resistance. The nominally independent Inner Mongolian Mengjiang state (1936–1945) was established with support of Japan in 1936; also, some Buryat and Inner Mongol nobles founded a Pan-Mongolist government with the support of Japan in 1919. The Inner Mongols established the short-lived Republic of Inner Mongolia in 1945. Another part of Choibalsan's plan was to merge Inner Mongolia and Dzungaria with Mongolia. By 1945, Chinese communist leader Mao Zedong requested the Soviets to stop Pan-Mongolism because China lost its control over Inner Mongolia and without Inner Mongolian support the Communists were unable to defeat Japan and Kuomintang.[citation needed] Mongolia and Soviets supported the Uyghur and Kazakhseparatist movement during the 1930s and 1940s. By 1945, the Soviets refused to support them after its alliance with the Chinese Communist Party and Mongolia interrupted its relations with the separatists under pressure. Xinjiang Oirat militant groups operated together the Turkic peoples but the Oirats did not have the leading role due to their small population. Basmachis or Turkic and Tajik militants fought to liberate Soviet Central Asia until 1942.[citation needed] On October 27, 1961, the United Nations recognized Mongolian independence and granted the nation full membership in the organization. The powerful states of Russia and China have committed many abuses against Mongols in their homeland, including war crimes and crimes against humanity, sometimes characterized as cultural genocide, with targets among the Mongol language, culture, tradition, history, religion, and ethnic identity. Peter the Great said: \"The headwaters of the Yenisei River must be Russian land\".[71] The Russian Empire sent the Kalmyks and Buryats to war to reduce the populations (World War I and other wars). During the 20th century, Soviet scientists attempted to convince the Kalmyks and Buryats that they're not Mongols during (demongolization policy). 35,000 Buryats were killed during a rebellion in 1927, and around one-third of the Buryat population in Russia died in the 1900s–1950s.[72][73] 10,000 Buryats of the Buryat-Mongol Autonomous Soviet Socialist Republic were massacred by Stalin's order in the 1930s.[74] In 1919 the Buryats established a small theocratic Balagad state in Kizhinginsky District of Russia and it fell in 1926. In 1958, the name \"Mongol\" was removed from the name of the Buryat-Mongol Autonomous Soviet Socialist Republic. On 22 January 1922 Mongolia proposed to migrate the Kalmyks during the Kalmykian Famine but bolshevik Russia refused. 71,000–72,000 (93,000?; around half of the population) Kalmyks died during the Russian famine of 1921–22.[75] The Kalmyks revolted against the Soviet Union in 1926, 1930 and 1942–1943. In 1913, Nicholas II, tsar of Russia, said: \"We need to prevent from Volg Tatars. But the Kalmyks are more dangerous than them because they are the Mongols so send them to war to reduce the population\".[76] On 23 April 1923 Joseph Stalin, communist leader of Russia, said: \"We are carrying out wrong policy on the Kalmyks who related to the Mongols. Our policy is too peaceful\".[76] In March 1927, Soviet deported 20,000 Kalmyks to Siberia, the tundra and Karelia.The Kalmyks founded the sovereign Republic of Oirat-Kalmyk on 22 March 1930.[76] The Oirats' state had a small army and 200 Kalmyk soldiers defeated 1,700 Soviet soldiers in Durvud province of Kalmykia but the Oirats' state was destroyed by the Soviet Army in 1930. Kalmykian nationalists and Pan-Mongolists attempted to migrate Kalmyks to Mongolia in the 1920s. Mongolia suggested to migrate the Soviet Union's Mongols to Mongolia in the 1920s but Russia refused the suggestion. Stalin deported all Kalmyks to Siberia in 1943 and around half of the (97,000–98,000) Kalmyks deported to Siberia died before being allowed to return home in 1957.[77] The government of the Soviet Union forbade teaching the Kalmyk language during the deportation. The Kalmyks' main purpose was to migrate to Mongolia and many Kalmyks joined the German Army. Marshal Khorloogiin Choibalsan attempted to migrate the deportees to Mongolia and he met with them in Siberia during his visit to Russia. Under the Law of the Russian Federation of April 26, 1991 \"On Rehabilitation of Exiled Peoples,\" repressions against Kalmyks and other peoples were qualified as acts of genocide. Language Mongolian is the official national language of Mongolia, where it is spoken by nearly 2.8 million people (2010 estimate),[82] and the official provincial language of China's Inner Mongolia Autonomous Region, where there are at least 4.1 million ethnic Mongols.[83] Across the whole of China, the language is spoken by roughly half of the country's 5.8 million ethnic Mongols (2005 estimate)[82] However, the exact number of Mongolian speakers in China is unknown, as there is no data available on the language proficiency of that country's citizens. The use of Mongolian in China, specifically in Inner Mongolia, has witnessed periods of decline and revival over the last few hundred years. The language experienced a decline during the late Qing period, a revival between 1947 and 1965, a second decline between 1966 and 1976, a second revival between 1977 and 1992, and a third decline between 1995 and 2012.[84] However, in spite of the decline of the Mongolian language in some of Inner Mongolia's urban areas and educational spheres, the ethnic identity of the urbanized Chinese-speaking Mongols is most likely going to survive due to the presence of urban ethnic communities.[85] The multilingual situation in Inner Mongolia does not appear to obstruct efforts by ethnic Mongols to preserve their language.[86][87] Although an unknown number of Mongols in China, such as the Tumets, may have completely or partially lost the ability to speak their language, they are still registered as ethnic Mongols and continue to identify themselves as ethnic Mongols.[82][88] The children of inter-ethnic Mongol-Chinese marriages also claim to be and are registered as ethnic Mongols.[89] The specific origin of the Mongolic languages and associated tribes is unclear. Linguists have traditionally proposed a link to the Tungusic and Turkic language families, included alongside Mongolic in the broader group of Altaic languages, though this remains controversial. Today the Mongolian peoples speak at least one of several Mongolic languages including Mongolian, Buryat, Oirat, Dongxiang, Tu and Bonan. Additionally, many Mongols speak either Russian or Mandarin Chinese as languages of inter-ethnic communication. Religion The original religion of the Mongolic peoples was Mongolian shamanism. The Xianbei came in contact with Confucianism and Daoism but eventually adopted Buddhism. However, the Xianbeis and some other people in Mongolia and Rourans followed a form of shamanism.[90] In the 5th century the Buddhist monk Dharmapriya was proclaimed \"State Teacher\" of the Rouran Khaganate and 3,000 families and some Rouran nobles became Buddhists. In 511 the Rouran Douluofubadoufa Khan sent Hong Xuan to the Tuoba court with a pearl-encrusted statue of the Buddha as a gift. The Tuoba Xianbei and Khitans were mostly Buddhists, although they still retained their original Shamanism. The Tuoba had a \"sacrificial castle\" to the west of their capital where ceremonies to spirits took place. Wooden statues of the spirits were erected on top of this sacrificial castle. One ritual involved seven princes with milk offerings who ascended the stairs with 20 female shamans and offered prayers, sprinkling the statues with the sacred milk. The Khitan had their holiest shrine on Mount Muye where portraits of their earliest ancestor Qishou Khagan, his wife Kedun and eight sons were kept in two temples. Mongolic peoples were also exposed to Zoroastrianism, Manicheism, Nestorianism, Eastern Orthodoxy and Islam from the west. The Mongolic peoples, in particular the Borjigin, had their holiest shrine on Mount Burkhan Khaldun where their ancestor Börte Chono (Blue Wolf) and Goo Maral (Beautiful Doe) had given birth to them. Genghis Khan usually fasted, prayed and meditated on this mountain before his campaigns. As a young man he had thanked the mountain for saving his life and prayed at the foot of the mountain sprinkling offerings and bowing nine times to the east with his belt around his neck and his hat held at his chest. Genghis Khan kept a close watch on the Mongolic supreme shaman Kokochu Teb who sometimes conflicted with his authority. Later, Tengrism, the imperial cult of Genghis Khan centered on the eight white gers and nine white banners in Ordos grew into a highly organized indigenous religion with scriptures in the Mongolian script.[91] Indigenous moral precepts of the Mongolic peoples were enshrined in oral wisdom sayings (now collected in several volumes), the anda (blood-brother) system and ancient texts such as the Chinggis-un Bilig (Wisdom of Genghis) and Oyun Tulkhuur (Key of Intelligence). These moral precepts were expressed in poetic form and mainly involved truthfulness, fidelity, help in hardship, unity, self-control, fortitude, veneration of nature, veneration of the state and veneration of parents. The western Khanates, however, eventually adopted Islam (under Berke and Ghazan) and the Turkic languages (because of its commercial importance), although allegiance to the Great Khan and limited use of the Mongolic languages can be seen even in the 1330s. In 1521 the first Mughal emperor Babur took part in a military banner milk-sprinkling ceremony in the Chagatai Khanate where the Mongolian language was still used. Al-Adil Kitbugha (reigned 1294–1296), a Mongol Sultan of Egypt, and the half-Mongol An-Nasir Muhammad (reigned till 1341) built the Madrassa of Al-Nasir Muhammad in Cairo, Egypt. An-Nasir's Mongol mother was Ashlun bint Shaktay. The Mongolic nobility during the Yuan dynasty studied Confucianism, built Confucian temples (including Beijing Confucius Temple) and translated Confucian works into Mongolic but mainly followed the Sakya school of Tibetan Buddhism under Phags-pa Lama. The general populace still practised Shamanism. Dongxiang and Bonan people adopted Islam, as did Moghol-speaking peoples in Afghanistan. In the 1576 the Gelug school of Tibetan Buddhism became the state religion of Mongolia. The Red Hat school of Tibetan Buddhism coexisted with the Gelug Yellow Hat school which was founded by the half-Mongol Je Tsongkhapa (1357–1419). Shamanism was absorbed into the state religion while being marginalized in its purer forms, later only surviving in far northern Mongolia. Monks were some of the leading intellectuals in Mongolia, responsible for much of the literature and art of the pre-modern period. Many Buddhist philosophical works lost in Tibet and elsewhere are preserved in older and purer form in Mongolian ancient texts (e.g. the Mongol Kanjur). Zanabazar (1635–1723), Zaya Pandita (1599–1662) and Danzanravjaa (1803–1856) are among the most famous Mongol holy men. The 4th Dalai Lama Yonten Gyatso (1589–1617), a Mongol himself, is recognized as the only non-Tibetan Dalai Lama although the current 14th Dalai Lama is of Mongolic Monguor extraction.[92] The name is a combination of the Mongolian word dalai meaning \"ocean\" and the Tibetan word (bla-ma) meaning \"guru, teacher, mentor\".[1] Many Buryats became Orthodox Christians due to the Russian expansion. During the socialist period religion was officially banned, although it was practiced in clandestine circles. Today, a sizable proportion of Mongolic peoples are atheist or agnostic. In the most recent census in Mongolia, almost forty percent of the population reported as being atheist, while the majority religion was Tibetan Buddhism, with 53%.[93] Having survived suppression by the Communists, Buddhism among the Eastern, Northern, Southern and Western Mongols is today primarily of the Gelugpa school of Tibetan Buddhism. There is a strong shamanistic influence in the Gelugpa sect among the Mongols.[94] Kinship and family life The traditional Mongol family was patriarchal, patrilineal and patrilocal. Wives were brought for each of the sons, while daughters were married off to other clans. Wife-taking clans stood in a relation of inferiority to wife-giving clans. Thus wife-giving clans were considered \"elder\" or \"bigger\" in relation to wife-taking clans, who were considered \"younger\" or \"smaller\".[95][96] This distinction, symbolized in terms of \"elder\" and \"younger\" or \"bigger\" and \"smaller\", was carried into the clan and family as well, and all members of a lineage were terminologically distinguished by generation and age, with senior superior to junior. In the traditional Mongolian family, each son received a part of the family herd as he married, with the elder son receiving more than the younger son. The youngest son would remain in the parental tent caring for his parents, and after their death he would inherit the parental tent in addition to his own part of the herd. This inheritance system was mandated by law codes such as the Yassa, created by Genghis Khan.[97] Likewise, each son inherited a part of the family's camping lands and pastures, with the elder son receiving more than the younger son. The eldest son inherited the farthest camping lands and pastures, and each son in turn inherited camping lands and pastures closer to the family tent until the youngest son inherited the camping lands and pastures immediately surrounding the family tent. Family units would often remain near each other and in close cooperation, though extended families would inevitably break up after a few generations. It is probable that the Yasa simply put into written law the principles of customary law. It is apparent that in many cases, for example in family instructions, the yasa tacitly accepted the principles of customary law and avoided any interference with them. For example, Riasanovsky said that killing the man or the woman in case of adultery is a good illustration. Yasa permitted the institutions of polygamy and concubinage so characteristic of southerly nomadic peoples. Children born of concubines were legitimate. Seniority of children derived their status from their mother. Eldest son received more than the youngest after the death of father. But the latter inherited the household of the father. Children of concubines also received a share in the inheritance, in accordance with the instructions of their father (or with custom). — Nilgün Dalkesen, Gender roles and women's status in Central Asia and Anatolia between the thirteenth and sixteenth centuries[98] After the family, the next largest social units were the subclan and clan. These units were derived from groups claiming patrilineal descent from a common ancestor, ranked in order of seniority (the \"conical clan\"). By the Chingissid era this ranking was symbolically expressed at formal feasts, in which tribal chieftains were seated and received particular portions of the slaughtered animal according to their status.[99] The lineage structure of Central Asia had three different modes. It was organized on the basis of genealogical distance, or the proximity of individuals to one another on a graph of kinship; generational distance, or the rank of generation in relation to a common ancestor, and birth order, the rank of brothers in relation to each another.[100] The paternal descent lines were collaterally ranked according to the birth of their founders, and were thus considered senior and junior to each other. Of the various collateral patrilines, the senior in order of descent from the founding ancestor, the line of eldest sons, was the most noble. In the steppe, no one had his exact equal; everyone found his place in a system of collaterally ranked lines of descent from a common ancestor.[101] It was according to this idiom of superiority and inferiority of lineages derived from birth order that legal claims to superior rank were couched.[102] The Mongol kinship is one of a particular patrilineal type classed as Omaha, in which relatives are grouped together under separate terms that crosscut generations, age, and even sexual difference. Thus, one uses different terms for a man's father's sister's children, his sister's children, and his daughter's children. A further attribute is strict terminological differentiation of siblings according to seniority. The anthropologist Herbert Harold Vreeland visited three Mongol communities in 1920 and published a highly detailed book with the results of his fieldwork, Mongol community and kinship structure.[103] Mural of a Mongol family, Yuan dynastyThe Mughal emperor Babur and his heir Humayun. The word Mughal is derived from the Persian word for Mongol. The royal clan of the Mongols is the Borjigin clan descended from Bodonchar Munkhag (c. 850–900). This clan produced Khans and princes for Mongolia and surrounding regions until the early 20th century. All the Great Khans of the Mongol Empire, including its founder Genghis Khan, were of the Borjigin clan. The royal family of Mongolia was called the Altan Urag (Golden Lineage) and is synonymous with Genghisid. After the fall of the Northern Yuan dynasty in 1635 the Dayan Khanid aristocracy continued the Genghisid legacy in Mongolia until 1937 when most were killed during the Stalinist purges. The four hereditary Khans of the Khalkha (Tüsheet Khan, Setsen Khan, Zasagt Khan and Sain Noyan Khan) were all descended from Dayan Khan (1464–1543) through Abtai Sain Khan, Sholoi Khan, Laikhur Khan and Tumenkhen Sain Noyan respectively. Dayan Khan was himself raised to power by Queen Mandukhai (c. 1449–1510) during the crisis of the late 15th century when the line of Kublai Khan, the grandson of Genghis Khan, was on the verge of dying out.[citation needed] The Khongirad was the main consort clan of the Borjigin and provided numerous Empresses and consorts. There were five minor non-Khonggirad inputs from the maternal side which passed on to the Dayan Khanid aristocracy of Mongolia and Inner Mongolia. The first was the Keraite lineage added through Kublai Khan's mother Sorghaghtani Beki which linked the Borjigin to the Nestorian Christian tribe of Cyriacus Buyruk Khan. The second was the Turkic Karluk lineage added through Toghon Temur Khan's mother Mailaiti which linked the Borjigin to Bilge Kul Qadir Khan (840–893) of the Kara-Khanid Khanate and ultimately to the Lion-Karluks as well as the Ashina tribe of the 6th century Göktürks. The third was the Korean lineage added through Biligtü Khan's mother Empress Gi (1315–370) which linked the Borjigin to the Haengju Gi clan and ultimately to King Jun of Gojeoson (262–184 BC) and possibly even further to King Tang of Shang (1675–1646 BCE) through Jizi. The fourth was the Esen Taishi lineage added through Bayanmunkh Jonon's mother Tsetseg Khatan which linked the Borjigin more firmly to the Oirats. The fifth was the Aisin-Gioro lineage added during the Qing dynasty. To the west, Genghisid Khans received daughters of the Byzantine emperor in marriage, such as when the Byzantine princess Maria Palaiologina married to Abaqa Khan (1234–1282), while there were also connections with European royalty through Russia, where, for example, Prince Gleb (1237–1278) married Feodora Sartaqovna the daughter of Sartaq Khan, a great-grandson of Genghis Khan.[citation needed] The Dayan Khanid aristocracy still held power during the Bogd Khanate of Mongolia (1911–1919) and the Constitutional Monarchy period (1921–1924). They were accused of collaboration with the Japanese and executed in 1937 while their counterparts in Inner Mongolia were severely persecuted during the Cultural Revolution. Ancestral shrines of Genghis Khan were destroyed by the Red Guards during the 1960s and the Horse-Tail Banner of Genghis Khan disappeared.[citation needed] Sister groups The Buryats are mainly concentrated in their homeland, the Buryat Republic, a federal subject of Russia. They are the major northern subgroup of the Mongols.[112] The Barga Mongols are mainly concentrated in Inner Mongolia, China, along with the Buryats and Hamnigan. Mongolia In modern-day Mongolia, Mongols make up approximately 95% of the population, with the largest ethnic group being Khalkha Mongols, followed by Buryats, both belonging to the Eastern Mongolian peoples. They are followed by Oirats, who belong to the Western Mongolian peoples. China The 2010 census of the People's Republic of China counted more than 7 million people of various Mongolic groups. The 1992 census of China counted only 3.6 million ethnic Mongols.[citation needed] The 2010 census counted roughly 5.8 million ethnic Mongols, 621,500 Dongxiangs, 289,565 Mongours, 132,000 Daurs, 20,074 Baoans, and 14,370 Yugurs.[citation needed] Most of them live in the Inner Mongolia Autonomous Region, followed by Liaoning. Small numbers can also be found in provinces near those two. There were 669,972 Mongols in Liaoning in 2011, making up 11.52% of Mongols in China.[113] The closest Mongol area to the sea is the Dabao Mongol Ethnic Township (大堡蒙古族乡) in Fengcheng, Liaoning. With 8,460 Mongols (37.4% of the township population)[citation needed] it is located 40 km (25 mi) from the North Korean border and 65 km (40 mi) from Korea Bay of the Yellow Sea. Another contender for closest Mongol area to the sea would be Erdaowanzi Mongol Ethnic Township (二道湾子蒙古族乡) in Jianchang County, Liaoning. With 5,011 Mongols (20.7% of the township population)[citation needed] it is located around 65 km (40 mi) from the Bohai Sea. Other peoples speaking Mongolic languages are the Daur, Sogwo Arig, Monguor people, Dongxiangs, Bonans, Sichuan Mongols and eastern part of the Yugur people. Those do not officially count as part of the Mongol ethnicity, but are recognized as ethnic groups of their own. The Mongols lost their contact with the Mongours, Bonan, Dongxiangs, Yunnan Mongols since the fall of the Yuan dynasty. Mongolian scientists and journalists met with the Dongxiangs and Yunnan Mongols in the 2000s.[citation needed] Notes ^Zhang Zhengming (2017) accepts the reading 鮮卑[29] (also seen in the early 19th century version published by Jinzhang bookstore (錦章図書局) in Shanghai[30]) as the ethnonym of the people who accompanied the Chu. However, 鮮卑 Xianbei is likely a scribal error for 鮮牟 Xianmou (as in other versions like Sibu Congkan (四部叢刊),[31] or Siku Quanshu (四庫全書)[32]). Eastern Wu scholar Wei Zhao states that the 鮮牟 Xianmou were an Eastern Yi nation,[33][34] while the 鮮卑 Xianbei were of Mountain Rong origin.[35][36] The apparent scribal error results in contradicting statements, apparently by Wei Zhao, that the Xianbei were an Eastern Yi nation[37] and a people of Mountain Rong origin.[38]Huang Pilie (1763-1825) states that the reading 鮮卑 Xianbei was inauthentic and identifies the 鮮牟 Xianmou with 根牟 Genmou, an Eastern Yi nation conquered by the Lu state in the 9th year of Duke Xuan of Lu's reign (600 BCE).[39][40][41] ^\"Mongolian office to ride into Taipei by end of the year\". Taipei Times. 2002-10-11. Archived from the original on 2009-02-10. Retrieved 2009-05-28. In October 1945, the people of Outer Mongolia voted for independence, gaining the recognition of many countries, including the Republic of China. (...) Due to a souring of relations with the Soviet Union in the early 1950s, however, the ROC revoked recognition of Outer Mongolia, reclaiming it as ROC territory."}
{"url": "https://en.m.wikipedia.org/wiki/History_of_assembly_lines", "text": "Assembly line An assembly line is a manufacturing process (often called a progressive assembly) in which parts (usually interchangeable parts) are added as the semi-finished assembly moves from workstation to workstation where the parts are added in sequence until the final assembly is produced. By mechanically moving the parts to the assembly work and moving the semi-finished assembly from work station to work station, a finished product can be assembled faster and with less labor than by having workers carry parts to a stationary piece for assembly. Assembly lines are designed for the sequential organization of workers, tools or machines, and parts. The motion of workers is minimized to the extent possible. All parts or assemblies are handled either by conveyors or motorized vehicles such as forklifts, or gravity, with no manual trucking. Heavy lifting is done by machines such as overhead cranes or forklifts. Each worker typically performs one simple operation unless job rotation strategies are applied. (1) Place the tools and the men in the sequence of the operation so that each component part shall travel the least possible distance while in the process of finishing. (2) Use work slides or some other form of the carrier so that when a workman completes his operation, he drops the part always in the same place—which place must always be the most convenient place to his hand—and if possible have gravity carry the part to the next workman for his own. (3) Use sliding assembling lines by which the parts to be assembled are delivered at convenient distances.[2] Designing assembly lines is a well-established mathematical challenge, referred to as an assembly line balancing problem.[3] In the simple assembly line balancing problem the aim is to assign a set of tasks that need to be performed on the workpiece to a sequence of workstations. Each task requires a given task duration for completion. The assignment of tasks to stations is typically limited by two constraints: (1) a precedence graph which indicates what other tasks need to be completed before a particular task can be initiated (e.g. not putting in a screw before drilling the hole) and (2) a cycle time which restricts the sum of task processing times which can be completed at each workstation before the work-piece is moved to the next station by the conveyor belt. Major planning problems for operating assembly lines include supply chain integration, inventory control and production scheduling.[4] Consider the assembly of a car: assume that certain steps in the assembly line are to install the engine, install the hood, and install the wheels (in that order, with arbitrary interstitial steps); only one of these steps can be done at a time. In traditional production, only one car would be assembled at a time. If engine installation takes 20 minutes, hood installation takes five minutes, and wheels installation takes 10 minutes, then a car can be produced every 35 minutes. In an assembly line, car assembly is split between several stations, all working simultaneously. When a station is finished with a car, it passes it on to the next. By having three stations, three cars can be operated on at the same time, each at a different stage of assembly. After finishing its work on the first car, the engine installation crew can begin working on the second car. While the engine installation crew works on the second car, the first car can be moved to the hood station and fitted with a hood, then to the wheels station and be fitted with wheels. After the engine has been installed on the second car, the second car moves to the hood assembly. At the same time, the third car moves to the engine assembly. When the third car's engine has been mounted, it then can be moved to the hood station; meanwhile, subsequent cars (if any) can be moved to the engine installation station. Assuming no loss of time when moving a car from one station to another, the longest stage on the assembly line determines the throughput (20 minutes for the engine installation) so a car can be produced every 20 minutes, once the first car taking 35 minutes has been produced. Before the Industrial Revolution, most manufactured products were made individually by hand. A single craftsman or team of craftsmen would create each part of a product. They would use their skills and tools such as files and knives to create the individual parts. They would then assemble them into the final product, making cut-and-try changes in the parts until they fit and could work together (craft production). The Venetian Arsenal, dating to about 1104, operated similar to a production line. Ships moved down a canal and were fitted by the various shops they passed. At the peak of its efficiency in the early 16th century, the Arsenal employed some 16,000 people who could apparently produce nearly one ship each day and could fit out, arm, and provision a newly built galley with standardized parts on an assembly-line basis. Although the Arsenal lasted until the early Industrial Revolution, production line methods did not become common even then. The automatic flourmill built by Oliver Evans in 1785 was called the beginning of modern bulk material handling by Roe (1916). Evans's mill used a leather belt bucket elevator, screw conveyors, canvas belt conveyors, and other mechanical devices to completely automate the process of making flour. The innovation spread to other mills and breweries.[7][8] One of the earliest examples of an almost modern factory layout, designed for easy material handling, was the Bridgewater Foundry. The factory grounds were bordered by the Bridgewater Canal and the Liverpool and Manchester Railway. The buildings were arranged in a line with a railway for carrying the work going through the buildings. Cranes were used for lifting the heavy work, which sometimes weighed in the tens of tons. The work passed sequentially through to erection of framework and final assembly.[10] The Bridgewater Foundry, pictured in 1839, one of the earliest factories to use an almost modern layout, workflow, and material-handling system The first flow assembly line was initiated at the factory of Richard Garrett & Sons, Leiston Works in Leiston in the English county of Suffolk for the manufacture of portable steam engines. The assembly line area was called 'The Long Shop' on account of its length and was fully operational by early 1853. The boiler was brought up from the foundry and put at the start of the line, and as it progressed through the building it would stop at various stages where new parts would be added. From the upper level, where other parts were made, the lighter parts would be lowered over a balcony and then fixed onto the machine on the ground level. When the machine reached the end of the shop, it would be completed. [11] Steam-powered conveyor lifts began being used for loading and unloading ships some time in the last quarter of the 19th century.[13] Hounshell (1984) shows a c. 1885 sketch of an electric-powered conveyor moving cans through a filling line in a canning factory. The meatpacking industry of Chicago is believed to be one of the first industrial assembly lines (or disassembly lines) to be utilized in the United States starting in 1867.[14] Workers would stand at fixed stations and a pulley system would bring the meat to each worker and they would complete one task. Henry Ford and others have written about the influence of this slaughterhouse practice on the later developments at Ford Motor Company.[15] Ford assembly line, 1913. The magneto assembly line was the first.[16][17]1913 Experimenting with the mounting body on Model T chassis. Ford tested various assembly methods to optimize the procedures before permanently installing the equipment. The actual assembly line used an overhead crane to mount the body.Ford Model T assembly line c. 1919Ford Model T assembly line c. 1924Ford assembly line c. 1930Ford assembly line c. 1947 At Ford Motor Company, the assembly line was introduced by William \"Pa\" Klann upon his return from visiting Swift & Company's slaughterhouse in Chicago and viewing what was referred to as the \"disassembly line\", where carcasses were butchered as they moved along a conveyor. The efficiency of one person removing the same piece over and over without moving to another station caught his attention. He reported the idea to Peter E. Martin, soon to be head of Ford production, who was doubtful at the time but encouraged him to proceed. Others at Ford have claimed to have put the idea forth to Henry Ford, but Pa Klann's slaughterhouse revelation is well documented in the archives at the Henry Ford Museum[20] and elsewhere, making him an important contributor to the modern automated assembly line concept. Ford was appreciative, having visited the highly automated 40-acre Searsmail order handling facility around 1906. At Ford, the process was an evolution by trial and error[17] of a team consisting primarily of Peter E. Martin, the factory superintendent; Charles E. Sorensen, Martin's assistant; Clarence W. Avery; C. Harold Wills, draftsman and toolmaker; Charles Ebender; and József Galamb. Some of the groundwork for such development had recently been laid by the intelligent layout of machine tool placement that Walter Flanders had been doing at Ford up to 1908. In 1922, Ford (through his ghostwriter Crowther) said of his 1913 assembly line: I believe that this was the first moving line ever installed. The idea came in a general way from the overhead trolley that the Chicago packers use in dressing beef.[24] Charles E. Sorensen, in his 1956 memoir My Forty Years with Ford, presented a different version of development that was not so much about individual \"inventors\" as a gradual, logical development of industrial engineering: What was worked out at Ford was the practice of moving the work from one worker to another until it became a complete unit, then arranging the flow of these units at the right time and the right place to a moving final assembly line from which came a finished product. Regardless of earlier uses of some of these principles, the direct line of succession of mass production and its intensification into automation stems directly from what we worked out at Ford Motor Company between 1908 and 1913. Henry Ford is generally regarded as the father of mass production. He was not. He was the sponsor of it.[25] As a result of these developments in method, Ford's cars came off the line in three-minute intervals or six feet per minute.[26] This was much faster than previous methods, increasing production by eight to one (requiring 12.5 man-hours before, 1 hour 33 minutes after), while using less manpower.[6] It was so successful, paint became a bottleneck. Only japan black would dry fast enough, forcing the company to drop the variety of colours available before 1914, until fast-drying Ducolacquer was developed in 1926.[6] The assembly line technique was an integral part of the diffusion of the automobile into American society. Decreased costs of production allowed the cost of the Model T to fall within the budget of the American middle class. In 1908, the price of a Model T was around $825, and by 1912 it had decreased to around $575. This price reduction is comparable to a reduction from $15,000 to $10,000 in dollar terms from the year 2000. In 1914, an assembly line worker could buy a Model T with four months' pay.[6] Ford's complex safety procedures—especially assigning each worker to a specific location instead of allowing them to roam about—dramatically reduced the rate of injury. The combination of high wages and high efficiency is called \"Fordism\", and was copied by most major industries. The efficiency gains from the assembly line also coincided with the take-off of the United States. The assembly line forced workers to work at a certain pace with very repetitive motions which led to more output per worker while other countries were using less productive methods. In the automotive industry, its success was dominating, and quickly spread worldwide. Ford France and Ford Britain in 1911, Ford Denmark 1923, Ford Germany and Ford Japan 1925; in 1919, Vulcan (Southport, Lancashire) was the first native European manufacturer to adopt it. Soon, companies had to have assembly lines, or risk going broke by not being able to compete; by 1930, 250 companies which did not had disappeared.[6] The massive demand for military hardware in World War II prompted assembly-line techniques in shipbuilding and aircraft production. Thousands of Liberty ships were built making extensive use of prefabrication, enabling ship assembly to be completed in weeks or even days. After having produced fewer than 3,000 planes for the United States Military in 1939, American aircraft manufacturers built over 300,000 planes in World War II.[citation needed]Vultee pioneered the use of the powered assembly line for aircraft manufacturing. Other companies quickly followed. As William S. Knudsen (having worked at Ford,[17] GM and the National Defense Advisory Commission) observed, \"We won because we smothered the enemy in an avalanche of production, the like of which he had never seen, nor dreamed possible.\"[27][28] In his 1922 autobiography,[2] Henry Ford mentions several benefits of the assembly line including: Workers do not do any heavy lifting. No stooping or bending over. No special training was required. There are jobs that almost anyone can do. Provided employment to immigrants. The gains in productivity allowed Ford to increase worker pay from $1.50 per day to $5.00 per day once employees reached three years of service on the assembly line. Ford continued on to reduce the hourly work week while continuously lowering the Model T price. These goals appear altruistic; however, it has been argued that they were implemented by Ford in order to reduce high employee turnover: when the assembly line was introduced in 1913, it was discovered that \"every time the company wanted to add 100 men to its factory personnel, it was necessary to hire 963\" in order to counteract the natural distaste the assembly line seems to have inspired.[29] Karl Marx expressed in his theory of alienation the belief that, in order to achieve job satisfaction, workers need to see themselves in the objects they have created, that products should be \"mirrors in which workers see their reflected essential nature\". Marx viewed labour as a chance for people to externalize facets of their personalities. Marxists argue that performing repetitive, specialized tasks causes a feeling of disconnection between what a worker does all day, who they really are, and what they would ideally be able to contribute to society. Furthermore, Marx views these specialised jobs as insecure, since the worker is expendable as soon as costs rise and technology can replace more expensive human labour.[31] ^ abcdWeber, Austin (2013-10-01). \"The Moving Assembly Line Turns 100\". Assembly Magazine. Archived from the original on 2016-08-26. Retrieved 2017-03-26. The assembly line ... was the result of a long period of trial and error. The assembly line wasn't a planned development; rather, it emerged in 1913 from a dynamic situation. People such as Carl Emde, William Klann and William Knudsen all played key roles in early automation efforts at Ford's Highland Park factory. Two individuals were essential to the success of the moving assembly line: Clarence Avery and Charles Sorensen. constant redesign of the Model T. Many components was tweaked regularly to make the vehicle easier to assemble. In 1913 alone, Ford made more than 100 design changes every month. Continuous experimentation was the rule rather than the exception at Ford's Highland Park plant. Ford engineers were constantly redesigning and tweaking jigs and fixtures, and planning new machine tools or fixing old ones, to achieve higher production."}
{"url": "https://en.m.wikipedia.org/wiki/Wolf_communication", "text": "Wolf communication Wolves communicate using vocalizations, body postures, scent, touch, and taste.[1] The lunar phases have no effect on wolf vocalisation. Despite popular belief, wolves do not howl at the Moon.[2] Gray wolves howl to assemble the pack, usually before and after hunts, to pass on an alarm particularly at a den site, to locate each other during a storm or while crossing unfamiliar territory, and to communicate across great distances.[3] Other vocalisations include growls, barks and whines.[4] Wolves do not bark as loudly or continuously as dogs do but they bark a few times and then retreat from a perceived danger.[4] Aggressive or self-assertive wolves are characterized by their slow and deliberate movements, high body posture and raised hackles, while submissive ones carry their bodies low, sleeken their fur, and lower their ears and tail.[5]Raised leg urination is considered to be one of the most important forms of scent communication in the wolf, making up 60–80% of all scent marks observed.[6] Contents The gray wolf's expressive behavior is more complex than that of the coyote and golden jackal, as necessitated by its group living and hunting habits. While less gregarious canids generally possess simple repertoires of visual signals, wolves have more varied signals that subtly inter grade in intensity.[7][8] When neutral, the legs are not stiffened, the tail hangs down loosely, the face is smooth, the lips untensed, and the ears point in no particular direction.[9] Postural communication in wolves consists of a variety of facial expressions, tail positions and piloerection.[10] Aggressive, or self-assertive wolves are characterized by their slow and deliberate movements, high body posture and raised hackles, while submissive ones carry their bodies low, sleeken their fur and lower their ears and tail.[5] When a breeding male encounters a subordinate family member, it may stare at it, standing erect and still with the tail horizontal to its spine.[11] Two forms of submissive behavior are recognized: passive and active. Passive submission usually occurs as a reaction to the approach of a dominant animal, and consists of the submissive wolf lying partly on its back and allowing the dominant wolf to sniff its anogenital area. Active submission occurs often as a form of greeting, and involves the submissive wolf approaching another in a low posture, and licking the other wolf's face.[12] When wolves are together, they commonly indulge in behaviors such as nose pushing, jaw wrestling, cheek rubbing and facial licking. The mouthing of each other's muzzles is a friendly gesture, while clamping on the muzzle with bared teeth is a dominance display.[13] Similar to humans, gray wolves have facial color patterns in which the gaze direction can be easily identified, although this is often not the case in other canid species. In 2014, a study compared the facial color pattern across 25 canid species. The results suggested that the facial color pattern of canid species is related to their gaze communication, and that especially gray wolves use the gaze signal in conspecific communication.[14] Gray wolves howl to assemble the pack (usually before and after hunts), to pass on an alarm (particularly at a den site), to locate each other during a storm or unfamiliar territory and to communicate across great distances.[3] Wolf howls can under certain conditions be heard over areas of up to 130 km2 (50 sq mi).[15] Wolf howls are generally indistinguishable from those of large dogs.[16] Male wolves give voice through an octave, passing to a deep bass with a stress on \"O\", while females produce a modulated nasal baritone with stress on \"U\". Pups almost never howl, while yearling wolves produce howls ending in a series of dog-like yelps.[17] Howling consists of a fundamental frequency that may lie between 150 and 780 Hz, and consists of up to 12 harmonically related overtones. The pitch usually remains constant or varies smoothly, and may change direction as many as four or five times.[18] Howls used for calling pack mates to a kill are long, smooth sounds similar to the beginning of the cry of a great horned owl. When pursuing prey, they emit a higher pitched howl, vibrating on two notes. When closing in on their prey, they emit a combination of a short bark and a howl.[16] When howling together, wolves harmonize rather than chorus on the same note, thus creating the illusion of there being more wolves than there actually are.[3] Lone wolves typically avoid howling in areas where other packs are present.[19] Wolves from different geographic locations may howl in different fashions: according to Erik Zimen, the howls of European wolves are much more protracted and melodious than those of North American wolves, whose howls are louder and have a stronger emphasis on the first syllable.[20] Other vocalisations of wolves are usually divided into three categories: growls, barks and whines.[4] Barking has a fundamental frequency between 320–904 Hz,[18] and is usually emitted by startled wolves. Wolves do not bark as loudly or continuously as dogs do, but bark a few times and retreat from perceived danger.[4] Growling has a fundamental frequency of 380–450 Hz,[18] and is usually emitted during food challenges. Pups commonly growl when playing. One variation of the howl is accompanied by a high pitched whine, which precedes a lunging attack.[3] Whining is associated with situations of anxiety, curiosity, inquiry and intimacy such as greeting, feeding pups and playing.[4] Olfaction is probably the wolf's most acute sense, and plays a fundamental role in communication. The wolf has a large number of apocrine sweat glands on the face, lips, back, and between the toes. The odor produced by these glands varies according to the individual wolf's microflora and diet, giving each a distinct \"odor fingerprint\". A combination of apocrine and eccrine sweat glands on the feet allows the wolf to deposit its scent whilst scratching the ground, which usually occurs after urine marking and defecation during the breeding season. The follicles present on the guard hairs from the wolf's back have clusters of apocrine and sebaceous glands at their bases. As the skin on the back is usually folded, this provides a microclimate for bacterial propagation around the glands. During piloerection, the guard hairs on the back are raised and the skin folds spread, thus releasing scent.[21] The precaudal scent glands may play a role in expressing aggression, as combative wolves raise the base of their tails whilst drooping the tip, thus positioning the scent glands at the highest point.[16] The wolf possesses a pair of anal sacs beneath the rectum, which contain both apocrine and sebaceous glands. The components of anal sac secretions vary according to season and gender, thus indicating that the secretions provide information related to gender and reproductive state. The secretions of the preputial glands may advertise hormonal condition or social position, as dominant wolves have been observed to stand over subordinates, apparently presenting the genital area for investigation,[21] which may include genital licking.[22] During the breeding season, female wolves secrete substances from the vagina, which communicate the females' reproductive state, and can be detected by males from long distances. Urine marking is the best-studied means of olfactory communication in wolves. Its exact function is debated, though most researchers agree that its primary purpose is to establish boundaries. Wolves urine mark more frequently and vigorously in unfamiliar areas, or areas of intrusion, where the scent of other wolves or canids is present. So-called raised leg urination (RLU) is more common in male wolves than in females, and may serve the purpose of maximizing the possibility of detection by conspecifics, as well as reflect the height of the marking wolf. Only dominant wolves typically use RLU, with subordinate males continuing to use the juvenile standing posture throughout adulthood.[21] RLU is considered to be one of the most important forms of scent communication in the wolf, making up 60–80% of all scent marks observed.[24] Their urine contains pyrazine analogs that act as kairomones, repelling their prey.[25]"}
