{"url": "https://en.wikipedia.org/wiki/List_of_common_misconceptions", "text": "Each entry on this list of common misconceptions is worded as a correction; the misconceptions themselves are implied rather than stated. These entries are concise summaries of the main subject articles, which can be consulted for more detail. Federal legal tender laws in the United States do not state that a private business, a person, or an organization must accept cash for payment, though it must be regarded as valid payment for debts when tendered to a creditor.[1] The common image of Santa Claus (Father Christmas) as a jolly old man in red robes was not created by The Coca-Cola Company as an advertising gimmick. Santa Claus had already taken this form in American popular culture and advertising by the late 19th century, long before Coca-Cola used his image in the 1930s.[8] PepsiCo never owned the \"6th most powerful navy\" in the world after a deal with the Soviet Union. In 1989, Pepsi acquired several decommissioned warships as part of a barter deal.[10][11] The oil tankers were leased out and the other ships sold for scrap.[12] A follow-on deal involved another 10 ships.[13] Searing does not seal moisture in meat; in fact, it causes it to lose some moisture. Meat is seared to brown it, to affect its color, flavor, and texture.[14] Twinkies, an American snack cake generally considered to be \"junk food\", have a shelf life of around 45 days, despite the common claim (usually facetious) that they remain edible for decades.[15][16] Twinkies, with only sorbic acid as an added preservative, normally remain on a store shelf for 7 to 10 days.[17][18] With the exception of some perishables, properly stored foods can safely be eaten past their \"expiration\" dates.[19][20] The vast majority of expiration dates in the United States are regulated by state governments and refer to food quality, not safety; the \"use by\" date represents the last day the manufacturer warrants the quality of their product. Hydrox is not a knock-off of Oreos. Hydrox, invented in 1908, predates Oreos by four years and outsold it until the 1950s, when Oreos raised prices and the name \"Hydrox\" became increasingly unappealing due to being said to sound like a laundry detergent brand, after similar new brands of the decade.[31][32][33] Spices were not used in the Middle Ages to mask the flavor of rotten meat before refrigeration. Spices were an expensive luxury item; those who could afford them could afford good meat, and there are no contemporaneous documents calling for spices to disguise the taste of bad meat.[43] Microwave ovens do not cook food from the inside out. 2.45 GHz microwaves can only penetrate approximately 1 centimeter (3⁄8 inch) into most foods. The inside portions of thicker foods are mainly heated by heat conducted from the outer portions.[54] Microwave ovens do not cause cancer, as microwave radiation is non-ionizing and therefore does not have the cancer risks associated with ionizing radiation such as X-rays. No studies have found that microwave radiation causes cancer, even with exposure levels far greater than normal radiation leakage.[55] Microwaving food does not reduce its nutritive value and may preserve it better than other cooking processes due to shorter cooking times.[56] Ronald Reagan was never seriously considered for the role of Rick Blaine in the 1942 film Casablanca, eventually played by Humphrey Bogart. An early studio press release mentioned Reagan, but the studio already knew that Reagan was unavailable because of his upcoming military service.[57] Indeed, the producer had always wanted Bogart for the part.[58] Irregardless is a word.[86][87]Nonstandard, slang, or colloquial terms used by English speakers are sometimes alleged not to be real words, despite appearing in numerous dictionaries. All words in English became accepted by being commonly used for a certain period of time; thus, there are many vernacular words currently not accepted as part of the standard language, or regarded as inappropriate in formal speech or writing, but the idea that they are not words is a misconception.[88] Other examples of words that are sometimes alleged not to be words include burglarize, licit,[89] and funnest[90] which appear in numerous dictionaries as English words.[91] The word crap did not originate as a back-formation of British plumber Thomas Crapper's aptronymous surname, nor does his name originate from the word crap.[98] The surname \"Crapper\" is a variant of \"Cropper\", which originally referred to someone who harvested crops.[99] The word crap ultimately comes from Medieval Latincrappa.[100] The word fuck did not originate in the Middle Ages as an acronym for either \"fornicating under consent of king\" or \"for unlawful carnal knowledge\", either as a sign posted above adulterers in the stocks, or as a sign on houses visible from the road during the Black Death. Nor did it originate as a corruption of \"pluck yew\" (an idiom falsely attributed to the English for drawing a longbow).[104] It is most likely derived from Middle Dutch or other Germanic languages, where it either meant \"to thrust\" or \"to copulate with\" (fokken in Middle Dutch), \"to copulate\", or \"to strike, push, copulate\" or \"penis\".[104][105] Either way, these variations would have been derived from the Indo-European root word -peuk, meaning \"to prick\".[104] The expression \"rule of thumb\" did not originate from an English law allowing a man to beat his wife with a stick no thicker than his thumb, and there is no evidence that such a law ever existed.[106] The false etymology has been broadly reported in media including Time magazine (1983), The Washington Post (1989) and CNN (1993).[107] The expression originates from the seventeenth century from various trades where quantities were measured by comparison to the width or length of a thumb.[108][109] The word the was never pronounced or spelled \"ye\" in Old or Middle English.[110] The confusion, seen in the common stock phrase \"ye olde\", derives from the use of the character thorn (þ), which in Middle English represented the sound now represented in Modern English by \"th\". Early printing presses often lacked types for the letter þ, meaning that \"þͤ\" () and \"þe\" were substituted with the visually similar \"yͤ\" and \"ye\", respectively.[111] The anti-Italian slur wop did not originate from an acronym for \"without papers\" or \"without passport\";[112] it is actually derived from the term guappo (roughly meaning thug or \"dandy\"), from Spanish guapo.[113] \"Xmas\", along with a modern Santa Claus, used on a Christmas postcard (1910) Xmas did not originate as a secular plan to \"take the Christ out of Christmas\".[114]X represents the Greek letter chi, the first letter of Χριστός (Christós), \"Christ\" in Greek,[115] as found in the chi-rho symbol ΧΡ since the 4th century. In English, \"X\" was first used as a scribal abbreviation for \"Christ\" in 1100; \"X'temmas\" is attested in 1551, and \"Xmas\" in 1721.[116] It is not necessary to wait 24 hours before filing a missing person report. When there is evidence of violence or of an unusual absence, it is important to start an investigation promptly.[117][118]Criminology experts say the first 72 hours in a missing person investigation are the most critical.[119] The US Armed Forces have generally forbidden military enlistment as a form of deferred adjudication (that is, an option for convicts to avoid jail time) since the 1980s. US Navy protocols discourage the practice, while the other four branches have specific regulations against it.[121] The United States does not require police officers to identify themselves as police in the case of a sting or other undercover work, and police officers may lie when engaged in such work.[122] Claiming entrapment as a defense instead focuses on whether the defendant was induced by undue pressure (such as threats) or deception from law enforcement to commit crimes they would not have otherwise committed.[123] Violent crime rates in the United States declined significantly between 1994 and 2003. Neither the Mafia nor other criminal organizations regularly use or have used cement shoes to drown their victims.[133] There are only two documented cases of this method being used in murders: one in 1964 and one in 2016 (although, in the former, the victim had concrete blocks tied to his legs rather than being enclosed in cement).[134] The French Army did use cement shoes on Algerians killed in death flights during the Algerian War.[135] In the United States, a defendant may not have their case dismissed simply because they were not read their Miranda rights at the time of their arrest. Miranda warnings cover the rights of a person when they are taken into custody and then interrogated by law enforcement.[136][137] If a person is not given a Miranda warning before the interrogation is conducted, statements made by them during the interrogation may not be admissible in a trial. The prosecution may still present other forms of evidence, or statements made during interrogations where the defendant was read their Miranda rights, to get a conviction.[138] Chewing gum is not punishable by caning in Singapore. Although importing and selling chewing gum has been illegal in Singapore since 1992, and corporal punishment is still an applicable penalty for certain offenses in the country, the two facts are unrelated; chewing gum-related offenses have always been only subject to fines, and the possession or consumption of chewing gum itself is not illegal.[139][140] No cases have been proven of strangers killing or permanently injuring children by intentionally hiding poisons, drugs, or sharp objects such as razor blades in candy during Halloween trick-or-treating.[144] However, in rare cases, adult family members have spread this story to cover up filicide or accidental deaths. Folklorists, scholars, and law enforcement experts say that the story that strangers put poison into candy and give that candy to trick-or-treating children has been \"thoroughly debunked\".[145][144] Mary Shelley's 1818 novel Frankenstein is named after the fictional scientist Victor Frankenstein, who created the sapient creature in the novel, not the creature itself, which is never named and is called Frankenstein's monster. However, as later adaptations started to refer to the monster itself as Frankenstein, this usage became well-established, and some no longer regard it as erroneous.[147][148] Listening to Mozart or classical music does not enhance intelligence (or IQ). A study from 1993 reported a short-term improvement in spatial reasoning.[160][161] However, the weight of subsequent evidence supports either a null effect or short-term effects related to increases in mood and arousal, with mixed results published after the initial report in Nature.[162][163][164][165] The Beatles' 1965 appearance at Shea Stadium was not the first time that a rock concert was played at a large, outdoor sports stadium in the U.S. Such venues were employed by Elvis Presley in the 1950s and the Beatles themselves in 1964.[174] Phil Collins did not write his 1981 hit \"In the Air Tonight\" about witnessing someone drowning and then confronting the person in the audience who let it happen. According to Collins himself, it was about his emotions when divorcing from his first wife.[176] Jesus was most likely not born on December 25, when his birth is traditionally celebrated as Christmas. It is more likely that his birth was in either the season of spring or perhaps summer. Although the Common Era ostensibly counts the years since the birth of Jesus,[178] it is unlikely that he was born in either AD 1 or 1 BC, as such a numbering system would imply. Modern historians estimate a date closer to between 6 BC and 4 BC.[179] The Bible does not say that exactly three magi came to visit the baby Jesus, nor that they were kings, or rode on camels, or that their names were Caspar, Melchior, and Balthazar, nor what color their skin was. Three magi are inferred because three gifts are described, but the Bible says only that there was more than one magus.[180][181][182][183][184][185] Paul the Apostle did not change his name from Saul. He was born a Jew, with Roman citizenship inherited from his father, and thus carried both a Hebrew and a Greco-Roman name from birth, as mentioned by Luke in Acts 13:9: \"...Saul, who also is called Paul...\".[188] Roman Catholic dogma does not say that the pope is either sinless or always infallible.[191] Catholic dogma since 1870 does state that a dogmatic teaching contained in divine revelation that is promulgated by the pope (deliberately, and under certain very specific circumstances; generally called ex cathedra) is free from error, although official invocation of papal infallibility is rare. Most theologians state that canonizations meet the requisites.[192] Otherwise, even when speaking in his official capacity, dogma does not hold that he is always free from error. Saint Augustine did not say \"God created hell for inquisitive people\".[198] He actually said: \"I do not give the answer that someone is said to have given (evading by a joke the force of the objection), 'He was preparing hell for those who pry into such deep subjects.' ... I do not answer in this way. I would rather respond, 'I do not know,' concerning what I do not know than say something for which a man inquiring about such profound matters is laughed at, while the one giving a false answer is praised.\"[199] So Augustine is saying that he would not say this and that he does not know the answer to the question. Most Muslim women do not wear a burqa (also transliterated as burka or burkha), which covers the body, head, and face, with a mesh grille to see through. Many Muslim women cover their hair and face (excluding the eyes) with a niqāb, or just their hair with a hijab[202] and many Muslim women wear neither face nor head coverings of any kind.[203] The word \"jihad\" does not always mean \"holy war\"; its literal meaning in Arabic is \"struggle\". While there is such a thing as \"jihad bil saif\", or jihad \"by the sword\",[206] it can be any spiritual or moral effort or struggle,[207][208] such as seeking knowledge, putting others before oneself, and inviting others to Islam.[209] The Quran does not promise martyrs 72 virgins in heaven. It does mention that virgin female companions,[210]houri, are given to all people, martyr or not, in heaven, but no number is specified. The source for the 72 virgins is a hadith in Sunan al-Tirmidhi by Imam Tirmidhi.[211][212] Hadiths are sayings and acts of Muhammad as reported by others, not part of the Quran itself.[213][211] The name golf is not an acronym for \"Gentlemen Only, Ladies Forbidden\".[217][218][219] It may have come from the Dutch word kolf or kolve, meaning \"club\",[218] or from the Scottish word goulf or gowf meaning \"to strike or cuff\".[217] The black belt in martial arts does not necessarily indicate expert level or mastery. It was introduced for judo in the 1880s to indicate competency at all of the basic techniques of the sport. Promotion beyond 1st dan (the first black belt rank) varies among different martial arts.[221] India did not withdraw from the 1950 FIFA World Cup because their squad played barefoot, which was against FIFA regulations.[224] In reality, India withdrew because the country's managing body, the All India Football Federation (AIFF), was insufficiently prepared for the team's participation and gave various reasons for withdrawing, including a lack of funding and prioritizing the Olympics.[225] There is no definitive proof that violent video games cause people to become violent. Some studies have found no link between aggression and violent video games,[226][227] and the popularity of gaming has coincided with a decrease in youth violence.[228][229] The moral panic surrounding video games in the 1980s through to the 2020s, alongside several studies and incidents of violence and legislation in many countries, likely contributed to proliferating this idea.[230][231] The Pyramids of Egypt were not constructed with slave labor. Archaeological evidence shows that the laborers were a combination of skilled workers and poor farmers working in the off-season with the participants paid in high-quality food and tax exemptions.[243][244][245] The idea that slaves were used originated with Herodotus, and the idea that they were Israelites arose centuries after the pyramids were constructed.[244][245][246] Galleys in ancient times were not commonly operated by chained slaves or prisoners, as depicted in films such as Ben Hur, but by paid laborers or soldiers,[247] with slaves used only in times of crisis, in some cases even gaining freedom after the crisis was averted. Ptolemaic Egypt was a possible exception.[248] Other types of vessel, such as merchant vessels (usually sailing vessels) were manned by slaves, sometimes even with slaves as ship's master.[249] The ancient Greeks did not use the word \"idiot\" (Ancient Greek: ἰδιώτης, romanized: idiṓtēs) to disparage people who did not take part in civic life or who did not vote. An ἰδιώτης was simply a private citizen as opposed to a government official. Later, the word came to mean any sort of non-expert or layman, then someone uneducated or ignorant, and much later to mean stupid or mentally deficient.[254] Julius Caesar was not born via caesarean section. Such a procedure would have been fatal to the mother at the time, and Caesar's mother was still alive when Caesar was 45 years old.[259][260] The name \"caesarean\" probably comes from the Latin verb caedere 'to cut'.[261] While modern life expectancies are much higher than those in the Middle Ages and earlier,[263] adults in the Middle Ages did not die in their 30s or 40s on average. That was the life expectancy at birth, which was skewed by high infant and adolescent mortality. The life expectancy among adults was much higher;[264] a 21-year-old man in medieval England, for example, could expect to live to the age of 64.[265][264] However, in various places and eras, life expectancy was noticeably lower, as in medieval London, where 90% of people in general died before the age of 45[266] and one study estimated that 36 percent of men and 56 percent of women in medieval urban areas passed before the age of 35.[267] Monks in this time period often died in their 20s or 30s.[267] In the tale of King Canute and the tide, the king did not command the tide to reverse in a fit of delusional arrogance.[272] According to the story, his intent was to prove a point to members of his privy council that no man is all-powerful, and that all people must bend to forces beyond their control, such as the tides. Marco Polo did not import pasta from China,[273] a misconception that originated with the Macaroni Journal, published by an association of food industries to promote the use of pasta in the United States.[274] Marco Polo describes a food similar to \"lasagna\" in his Travels, but he uses a term with which he was already familiar. There is no evidence that iron maidens were used for torture, or even yet invented, in the Middle Ages. Instead they were pieced together in the 18th century from several artifacts found in museums, arsenals and the like to create spectacular objects intended for commercial exhibition.[275] Spiral staircases in castles were not designed in a clockwise direction to hinder right-handed attackers.[276][277] While clockwise spiral staircases are more common in castles than anti-clockwise, they were even more common in medieval structures without a military role, such as religious buildings.[278][276] The plate armor of European soldiers did not stop soldiers from moving around or necessitate a crane to get them into a saddle. They would routinely fight on foot and could mount and dismount without help.[279] However, armor used in tournaments in the late Middle Ages was significantly heavier than that used in warfare,[280] which may have contributed to this misconception. Whether chastity belts, devices designed to prevent women from having sexual intercourse, were invented in medieval times is disputed by modern historians. Most existing chastity belts are now thought to be deliberate fakes or anti-masturbatory devices from the 19th and early 20th centuries.[281] Medieval cartographers did not regularly write \"here be dragons\" on their maps. The only maps from this era that have the phrase inscribed on them are the Hunt-Lenox Globe and the Ostrich Egg Globe, next to a coast in Southeast Asia for both of them. Maps instead were more likely to have \"here are lions\" inscribed. Maps in this period did occasionally have illustrations of mythical beasts like dragons and sea serpents, as well as exotic animals like elephants, on them.[283] The Mexica people of the Aztec Empire did not mistake Hernán Cortés and his landing party for gods during Cortés' conquest of the empire. This notion came from Francisco López de Gómara, who never went to Mexico and concocted the myth while working for the retired Cortés in Spain years after the conquest.[288] Shah Jahan, the Indian Mughal Emperor who commissioned the Taj Mahal, did not cut off the hands of the rumored 40,000 workers or lead designers so as to not allow the construction of another monument more beautiful than the Taj Mahal. This is an urban myth that goes back to the 1960s.[289][290][291] The early settlers of the Plymouth Colony in North America usually did not wear all black, and their capotains (hats) were shorter and rounder than the widely depicted tall hat with a buckle on it. Instead, their fashion was based on that of the late Elizabethan era.[292] The traditional image was formed in the 19th century when buckles were a kind of emblem of quaintness.[293] (The Puritans, who also settled in Massachusetts near the same time, did frequently wear all black.)[294] The familiar story that Isaac Newton was inspired to research the nature of gravity when an apple fell on his head is almost certainly apocryphal. All Newton himself ever said was that the idea came to him as he sat \"in a contemplative mood\" and \"was occasioned by the fall of an apple\".[295] Marie Antoinette did not say \"let them eat cake\" when she heard that the French peasantry were starving due to a shortage of bread. The phrase was first published in Rousseau's Confessions, written when Marie Antoinette was only nine years old and not attributed to her, just to \"a great princess\". It was first attributed to her in 1843.[297] George Washington did not have wooden teeth. His dentures were made of lead, gold, hippopotamus ivory, the teeth of various animals, including horse and donkey teeth,[298][299] and human teeth, possibly bought from slaves or poor people.[300][301] The possible origin of this myth is that ivory teeth quickly became stained and may have had the appearance of wood to observers.[299] Napoleon Bonaparte was not especially short for a Frenchman of his time. He was the height of an average French male in 1800, but short for an aristocrat or officer.[306] After his death in 1821, the French emperor's height was recorded as 5 feet 2 inches in French feet, which in English measurements is 5 feet 7 inches (1.70 m).[307][308] Albert Einstein did not fail mathematics classes in school. Einstein remarked, \"I never failed in mathematics.... Before I was fifteen I had mastered differential and integral calculus.\"[312] Einstein did, however, fail his first entrance exam into the Swiss Federal Polytechnic School (ETH) in 1895, when he was two years younger than his fellow students, but scored exceedingly well in the mathematics and science sections, and then passed on his second attempt.[313] Grigori Rasputin was not assassinated by being fed cyanide-laced cakes and wine, shot multiple times, and then thrown into the Little Nevka river when he survived the former two. A contemporary autopsy reported that he was just killed with gunshots. A sensationalized account from the memoirs of co-conspirator PrinceFelix Yusupov is the only source of this story.[315][316][317] The Italian dictator Benito Mussolini did not \"make the trains run on time\". Much of the repair work had been performed before he and the Fascist Party came to power in 1922. Moreover, the Italian railways' supposed adherence to timetables was more propaganda than reality.[318] The Nazis did not use the term \"Nazi\" to refer to themselves. The full name of the Nazi Party was Nationalsozialistische Deutsche Arbeiterpartei (National Socialist German Workers' Party), and members referred to themselves as Nationalsozialisten (National Socialists) or Parteigenossen (party comrades). The term \"Nazi\" was in use prior to the rise of the Nazis as a colloquial and derogatory word for a backwards farmer or peasant. Opponents of the National Socialists abbreviated their name as \"Nazi\" for derogatory effect and the term was popularized by German exiles outside of Germany.[320] Although popularly known as the \"red telephone\", the Moscow–Washington hotline was never a telephone line, nor were red phones used. The first implementation of the hotline used teletype equipment, which was replaced by facsimile (fax) machines in 1988. Since 2008, the hotline has been a secure computer link over which the two countries exchange email.[327] Moreover, the hotline links the Kremlin to the Pentagon, not the White House.[328] The Alaska Purchase was generally viewed as positive or neutral in the United States, both among the public and the press. The opponents of the purchase who characterized it as \"Seward's Folly\", alluding to William H. Seward, the Secretary of State who negotiated it, represented a minority opinion at the time.[345][346] There is no evidence that Frederic Remington, on assignment to Cuba in 1897, telegraphed William Randolph Hearst: \"There will be no war. I wish to return,\" nor that Hearst responded: \"Please remain. You furnish the pictures, and I'll furnish the war\". The anecdote was originally included in a book by James Creelman and probably never happened.[350] Immigrants' last names were not Americanized (voluntarily, mistakenly, or otherwise) upon arrival at Ellis Island. Officials there kept no records other than checking ship manifests created at the point of origin, and there was simply no paperwork that would have let them recast surnames, let alone any law. At the time in New York, anyone could change the spelling of their name simply by using that new spelling.[352] These names are often referred to as an \"Ellis Island Special\". Prohibition did not make drinking alcohol illegal in the United States. The Eighteenth Amendment and the subsequent Volstead Act prohibited the production, sale, and transport of \"intoxicating liquors\" within the United States, but their possession and consumption were never outlawed.[353] U.S. Senator George Smathers never gave a speech to a rural audience describing his opponent, Claude Pepper, as an \"extrovert\" whose sister was a \"thespian\", in the apparent hope they would confuse them with similar-sounding words like \"pervert\" and \"lesbian\". Smathers offered US$10,000 to anyone who could prove he had made the speech; it was never claimed.[362] Rosa Parks was not sitting in the front (\"white\") section of the bus during the event that made her famous and incited the Montgomery bus boycott. Rather, she was sitting in the front of the back (\"colored\") section of the bus, where African Americans were expected to sit, and rejected an order from the driver to vacate her seat in favor of a white passenger when the \"white\" section of the bus had become full.[363] When Kitty Genovese was murdered outside her apartment in 1964, there were not 38 neighbors standing idly by and watching who failed to call the police until after she was dead, as was initially reported[367] to widespread public outrage that persisted for years and even became the basis of a theory in social psychology. In fact, witnesses only heard brief portions of the attack and did not realize what was occurring, and only six or seven actually saw anything. One witness, who had called the police, said when interviewed by officers at the scene, \"I didn't want to get involved\",[368] an attitude later attributed to all the neighbors.[369] While it was praised by one architectural magazine before it was built as \"the best high apartment of the year\", the Pruitt–Igoehousing project in St. Louis, Missouri, considered to epitomize the failures of urban renewal in American cities after it was demolished in the early 1970s, never won any awards for its design.[370] The architectural firm that designed the buildings did win an award for an earlier St. Louis project, which may have been confused with Pruitt–Igoe.[371] There is little contemporary documentary evidence for the notion that US Vietnam veterans were spat upon by anti-war protesters upon return to the United States. This belief was detailed in some biographical accounts and was later popularized by films such as Rambo.[372][373][374] Women did not burn their bras outside the Miss America contest in 1969 as a protest in support of women's liberation. They did symbolically throw bras in a trash can, along with other articles seen as emblematic of women's position in American society such as mops, make-up, and high-heeled shoes. The myth of bra burning came when a journalist hypothetically suggested that women may do so in the future, as men of the era burned their draft cards.[375] Black holes have the same gravitational effects as any other equal mass in their place. They will draw objects nearby towards them, just as any other celestial body does, except at very close distances to the black hole, comparable to its Schwarzschild radius.[394] If, for example, the Sun were replaced by a black hole of equal mass, the orbits of the planets would be essentially unaffected. A black hole can pull in a substantial inflow of surrounding matter, but only if the star from which it formed was already doing so.[395] Egg balancing is possible on every day of the year, not just the vernal equinox,[402] and there is no relationship between any astronomical phenomenon and the ability to balance an egg.[403] The Fisher Space Pen was not commissioned by NASA at a cost of millions of dollars, while the Soviets used pencils. It was independently developed by Paul C. Fisher, founder of the Fisher Pen Company, with $1 million of his own funds.[404] NASA tested and approved the pen for space use, then purchased 400 pens at $6 per pen.[405] The Soviet Union subsequently also purchased the Space Pen for its Soyuz spaceflights.[406] The Sun is not yellow; rather, it emits light across the full spectrum of visible colors, and this combined light appears white when outside of Earth's atmosphere. Earth's atmosphere scatters shorter wavelengths of light, particularly blues and violets, more than longer wavelengths like reds and yellows, and this scattering is why the Sun appears yellow during the day or orange or red during sunrise and sunset.[409][410] A satellite image of a section of the Great Wall of China, running diagonally from lower left to upper right (not to be confused with the much more prominent river running from upper left to lower right). The region pictured is 12 by 12 kilometers (7.5 mi × 7.5 mi). The Big Bang model does not fully explain the origin of the universe. It does not describe how energy, time, and space were caused, but rather it describes the emergence of the present universe from an ultra-dense and high-temperature initial state.[412] Bulls are not enraged by the color red, used in capes by professional bullfighters. Cattle are dichromats, so red does not stand out as a bright color. It is not the color of the cape, but the perceived threat by the bullfighter that incites it to charge.[414] Lemmings do not engage in mass suicidal dives off cliffs when migrating. The scenes of lemming suicides in the 1958 Disney documentary film White Wilderness, which popularized this idea, were completely fabricated. The lemmings in the film were actually purchased from Inuit children for 25 cents apiece, transported to the filming location in Canmore, Alberta, and repeatedly shoved off a nearby cliff by the filmmakers to create the illusion of a mass suicide.[415][416] The misconception itself is much older, dating back to at least the late 19th century, though its exact origins are uncertain.[417] Dogs do not consistently age seven times as quickly as humans. Aging in dogs varies widely depending on the breed; certain breeds, such as giant dog breeds and English bulldogs, have much shorter lifespans than average. Most dogs age consistently across all breeds in the first year of life, reaching adolescence[clarification needed] by one year old; smaller and medium-sized breeds begin to age more slowly in adulthood.[420] The phases of the Moon have no effect on the vocalizations of wolves, and wolves do not howl at the Moon.[421] Wolves howl to assemble the pack usually before and after hunts, to pass on an alarm particularly at a den site, to locate each other during a storm, while crossing unfamiliar territory, and to communicate across great distances.[422] There is no such thing as an \"alpha\" in a wolf pack. An early study that coined the term \"alpha wolf\" had only observed unrelated adult wolves living in captivity. In the wild, wolf packs operate like families: parents are in charge until the young grow up and start their own families, and younger wolves do not overthrow an \"alpha\" to become the new leader.[423][424] Bats are not blind. While about 70% of bat species, mainly in the microbat family, use echolocation to navigate, all bat species have eyes and are capable of sight. In addition, almost all bats in the megabat or fruit bat family cannot echolocate and have excellent night vision.[425] Sharks can get cancer. The misconception that sharks do not get cancer was spread by the 1992 book Sharks Don't Get Cancer, which was used to sell extracts of shark cartilage as cancer prevention treatments. Reports of carcinomas in sharks exist, and current data do not support any conclusions about the incidence of tumors in sharks.[429] Great white sharks do not mistake human divers for seals or other pinnipeds. When attacking pinnipeds, the shark surfaces quickly and attacks violently. In contrast, attacks on humans are slower and less violent: the shark charges at a normal pace, bites, and swims off. Great white sharks have efficient eyesight and color vision; the bite is not predatory, but rather for identification of an unfamiliar object.[430] Snake jaws cannot unhinge. The posterior end of the lower jaw bones contains a quadrate bone, allowing jaw extension. The anterior tips of the lower jaw bones are joined by a flexible ligament allowing them to bow outwards, increasing the mouth gape.[431][432] Porcupines do not shoot their quills. They can detach, and porcupines will deliberately back into attackers to impale them, but their quills do not project.[435][436][437] Mice do not have a special appetite for cheese, and will eat it only for lack of better options; they actually favor sweet, sugary foods. The myth may have come from the fact that before the advent of refrigeration, cheese was usually stored outside and was therefore an easy food for mice to reach.[438] There is no credible evidence that the candiru, a South American parasitic catfish, can swim up a human urethra if one urinates in the water in which it lives. The sole documented case of such an incident, written in 1997, has been heavily criticized upon peer review, and this phenomenon is now largely considered a myth.[439] Piranhas do not eat only meat but are omnivorous, and they only swim in schools to defend themselves from predators and not to attack. They very rarely attack humans, only when under stress and feeling threatened, and even then, bites typically only occur on hands and feet.[442] The hippopotamus does not produce pink milk, nor does it sweat blood. The skin secretions of the hippopotamus are red due to the presence of hipposudoric acid, a red pigment which acts as a natural sunscreen, and is neither sweat or blood. It does not affect the color of their milk, which is white or beige.[443] A human touching or handling eggs or baby birds will not cause the adult birds to abandon them.[447] The same is generally true for other animals having their young touched by humans as well, with the possible exception of rabbits (as rabbits will sometimes abandon their nest after an event they perceive as traumatizing).[448] The bold, powerful cry commonly associated with the bald eagle in popular culture is actually that of a red-tailed hawk. Bald eagle vocalizations are much softer and chirpier, and bear far more resemblance to the calls of gulls.[455][456] Ostriches do not stick their heads in the sand to hide from enemies or to sleep.[457] This misconception's origins are uncertain but it was probably popularized by Pliny the Elder (23–79 CE), who wrote that ostriches \"imagine, when they have thrust their head and neck into a bush, that the whole of their body is concealed\".[458] A duck's quack actually does echo,[459] although the echo may be difficult to hear for humans under some circumstances.[460] Despite this, a British panel show compiling interesting facts has been given the name Duck Quacks Don't Echo. The skin of a chameleon is not adapted solely for camouflage purposes, nor can a chameleon change its skin colour to match any background.[463] Rabbits are not specially partial to carrots. Their diet in the wild primarily consists of dark green vegetables such as grasses and clovers, and excessive carrot consumption is unhealthy for them due to containing high levels of sugar. This misconception originated from Bugs Bunny cartoons, whose carrot-chomping habit was meant as a reference to a minor character in It Happened One Night.[464][465][466] Houseflies have an average lifespan of 20 to 30 days, not 24 hours.[469] The misconception may arise from confusion with mayflies, which, in one species, have an adult lifespan of as little as 5 minutes.[470] The daddy longlegs spider (Pholcidae) is not the most venomous spider in the world. Their fangs are capable of piercing human skin, but the tiny amount of venom they carry causes only a mild burning sensation for a few seconds.[471] Other species such as harvestmen, crane flies, and male mosquitoes are also called daddy longlegs in some regional dialects, and share the misconception of being highly venomous but unable to pierce the skin of humans.[472][473] People do not swallow large numbers of spiders during sleep. A sleeping person makes noises that warn spiders of danger.[474][475] Most people also wake up from sleep when they have a spider on their face.[476] Earwigs are not known to purposely climb into external ear canals, though there have been anecdotal reports of earwigs being found in the ear.[482] The name may be a reference to the appearance of their hindwings, which are unique and distinctive among insects, and resemble a human ear when unfolded.[483][484] While certainly critical to the pollination of many plant species, European honey bees are not essential to human food production, despite claims that without their pollination, humanity would starve or die out \"within four years\".[485] In fact, many important crops need no insect pollination at all. The ten most important crops,[486] accounting for 60% of all human food energy,[487] all fall into this category. Ticks do not jump or fall from trees onto their hosts. Instead, they lie in wait to grasp and climb onto any passing host or otherwise trace down hosts via, for example, olfactory stimuli, the host's body heat, or carbon dioxide in the host's breath.[488][489] Poinsettias are not highly toxic to humans or cats. While it is true that they are mildly irritating to the skin or stomach,[501] and may sometimes cause diarrhea and vomiting if eaten, they rarely cause serious medical problems.[502] Sunflowers do not always point to the Sun. Flowering sunflowers face a fixed direction (often east) all day long, but do not necessarily face the Sun.[503] However, in an earlier developmental stage, before the appearance of flower heads, the immature buds do track the Sun (a phenomenon called heliotropism), and the fixed alignment of the mature flowers toward a certain direction is often the result.[504] The word theory in \"the theory of evolution\" does not imply scientific doubt regarding its validity; the concepts of theory and hypothesis have specific meanings in a scientific context. While theory in colloquial usage may denote a hunch or conjecture, a scientific theory is a set of principles that explains an observable phenomenon in natural terms.[508][509] \"Scientific fact and theory are not categorically separable\",[510] and evolution is a theory in the same sense as germ theory or the theory of gravitation.[511] The theory of evolution does not attempt to explain the origin of life[512] or the origin and development of the universe. The theory of evolution deals primarily with changes in successive generations over time after life has already originated.[513] The scientific model concerned with the origin of the first organisms from organic or inorganic molecules is known as abiogenesis, and the prevailing theory for explaining the early development of the universe is the Big Bang model. Mutations are not entirely random, nor do they occur at the same frequency everywhere in the genome. Certain regions of an organism's genome will be more or less likely to undergo mutation depending on the presence of DNA repair mechanisms and other mutation biases. For instance, in a study on Arabidopsis thaliana, biologically important regions of the plant's genome were found to be protected from mutations, and beneficial mutations were found to be more likely, i.e. mutation was \"non-random in a way that benefits the plant\".[526][527][528] Dimetrodon is often mistakenly called a dinosaur or considered to be a contemporary of dinosaurs in popular culture, but it became extinct some 40 million years before the first appearance of dinosaurs. Being a synapsid, Dimetrodon is actually more closely related to mammals than to dinosaurs, birds, lizards, or other diapsids.[539][540][541][542] Humans and aviandinosaurs currently coexist, but humans and non-avian dinosaurs did not coexist at any point.[544] The last of the non-avian dinosaurs died 66 million years ago in the course of the Cretaceous–Paleogene extinction event, whereas the earliest members of the genus Homo (humans) evolved between 2.3 and 2.4 million years ago. This places a 63-million-year expanse of time between the last non-avian dinosaurs and the earliest humans. Humans did coexist with woolly mammoths and saber-toothed cats: extinct mammals often erroneously depicted alongside non-avian dinosaurs.[545] Most diamonds are not formed from highly compressed coal. More than 99% of diamonds ever mined have formed in the conditions of extreme heat and pressure about 140 kilometers (87 mi) below the earth's surface. Coal is formed from prehistoric plants buried much closer to the surface, and is unlikely to migrate below 3.2 kilometers (2.0 mi) through common geological processes. Most diamonds that have been dated are older than the first land plants, and are therefore older than coal.[573] Diamonds are not infinitely hard, and are subject to wear and scratching: although they are the hardest known material on the Mohs Scale, they can be scratched by other diamonds[574] and worn down even by much softer materials, such as vinyl records.[575] Although the core of a wooden pencil is commonly referred to as \"lead\", wooden pencils do not contain the chemical element lead, nor have they ever contained it; \"black lead\" was formerly a name of graphite, which is commonly used for pencil leads.[577] The deep web is not primarily full of pornography, illegal drug trade websites, and stolen bank details. This information is primarily found in a small portion of the deep web known as the \"dark web\". Much of the deep web consists of academic libraries, databases, and anything that is not indexed by normal search engines.[580] Total population living in extreme poverty, by world region 1987 to 2015[587] The total number of people living in extremeabsolute poverty globally, by the widely used metric of $1.00/day (in 1990 U.S. dollars) has decreased over the last several decades, but most people surveyed in several countries incorrectly think it has increased or stayed the same.[588] However, this depends on the poverty line calculation used. For instance, if the metric used is instead one that prioritizes meeting a standard life expectancy that no longer significantly rises with additional consumption enabled by income, the number of individuals in poverty has risen by nearly 1 billion.[589][590] Human population growth is decreasing and the world population is expected to peak and then begin falling during the 21st century. Improvements in agricultural productivity and technology are expected to be able to meet anticipated increased demand for resources, making a global human overpopulation scenario unlikely.[591][592][593] For any given production set, there is not a set amount of labor input (a \"lump of labor\") to produce that output. This fallacy is commonly seen in Luddite and later, related movements as an argument either that automation causes permanent, structural unemployment, or that labor-limiting regulation can decrease unemployment. In fact, changes in capital allocation, efficiency, and economies of learning can change the amount of labor input for a given set of production.[594] Income is not a direct factor in determining credit score in the United States. Rather, credit score is affected by the amount of unused available credit, which is in turn affected by income.[595] Income is also considered when evaluating creditworthiness more generally. In the US, an increase in gross income will never reduce a taxpayer's post-tax earnings (net income) by putting them in a higher tax bracket. Tax brackets specify marginal tax rates: only income earned in the higher tax bracket is taxed at the higher rate.[597] An increase in gross income can reduce net income in a welfare cliff, however, when benefits are withdrawn when passing a certain income threshold.[598] Prevalence of the misconception varies by political party affiliation.[599] Constructing new housing decreases the cost of rent or of buying a home in both the immediate neighborhood and in the city as a whole. In real estate economics, \"supply skepticism\" leads many Americans to misunderstand the effect of increasing the supply of housing on housing costs. The misconception is unique to the housing market.[600][601] Earthquake strength (or magnitude) is not commonly measured using the Richter scale. Although the Richter scale was used historically to measure earthquake magnitude (although, notably, not earthquake damage), it was found in the 1970s that it does not reliably represent the magnitude of large earthquakes. It has therefore been largely replaced by the moment magnitude scale,[623] although very small earthquakes are still sometimes measured using the Richter scale.[624] Nevertheless, earthquake magnitude is still widely misattributed to the Richter scale.[625][626][627]Death rates from air pollution and accidents related to energy production, measured in deaths in the past per terawatt hours (TWh) Lightning can, and often does, strike the same place twice. Lightning in a thunderstorm is more likely to strike objects and spots that are more prominent or conductive. For instance, lightning strikes the Empire State Building in New York City on average 23 times per year.[628] Heat lightning does not exist as a distinct phenomenon. What is mistaken for \"heat lightning\" is usually ordinary lightning from storms too distant to hear the associated thunder.[629] The Earth's interior is not molten rock. This misconception may originate from a misunderstanding based on the fact that the Earth's mantle convects, and the incorrect assumption that only liquids and gases can convect. In fact, a solid with a large Rayleigh number can also convect, given enough time, which is what occurs in the solid mantle due to the very large thermal gradient across it.[634][635] There are small pockets of molten rock in the upper mantle, but these make up a tiny fraction of the mantle's volume.[636] The Earth's outer coreis liquid, but it is liquid metal, not rock.[637] The Amazon rainforest does not provide 20% of Earth's oxygen. This is a misinterpretation of a 2010 study which found that approximately 34% of photosynthesis by terrestrial plants occurs in tropical rainforests (so the Amazon rainforest would account for approximately half of this). Due to respiration by the resident organisms, all ecosystems (including the Amazon rainforest) have a net output of oxygen of approximately zero. The oxygen currently present in the atmosphere was accumulated over billions of years.[638] Rivers do not predominantly flow from north to south. Rivers flow downhill in all compass directions, often changing direction along their course.[640][641] Indeed, many major rivers flow northward, including the Nile, the Yenisey, the Ob, the Rhine, the Lena, and the Orinoco.[642][643] Waking up a sleepwalker does not harm them. Sleepwalkers may be confused or disoriented for a short time after awakening, but the health risks associated with sleepwalking are from injury or insomnia, not from being awakened.[645] Seizures cannot cause a person to swallow their own tongue,[646] and it is dangerous to attempt to place a foreign object into a convulsing person's mouth. Instead it is recommended to gently lay a convulsing person on their side to minimize the risk of asphyxiation.[647] Drowning is often inconspicuous to onlookers.[648] In most cases, the instinctive drowning response prevents the victim from waving or yelling (known as \"aquatic distress\"),[648] which are therefore not dependable signs of trouble; indeed, most drowning victims undergoing the response do not show prior evidence of distress.[649] Human blood in veins is not actually blue. Blood is red due to the presence of hemoglobin; deoxygenated blood (in veins) has a deep red color, and oxygenated blood (in arteries) has a light cherry-red color. Veins below the skin can appear blue or green due to subsurface scattering of light through the skin, and aspects of human color perception. Many medical diagrams also use blue to show veins, and red to show arteries, which contributes to this misconception.[650] Exposure to a vacuum, or experiencing all but the most extreme uncontrolled decompression, does not cause the body to explode or internal fluids to boil (although the fluids in the mouth and lungs will indeed boil at altitudes above the Armstrong limit); rather, it will lead to a loss of consciousness once the body has depleted the supply of oxygen in the blood, followed by death from hypoxia within minutes.[651] Exercise-induced delayed onset muscle soreness is not caused by lactic acid build-up. Muscular lactic acid levels return to normal levels within an hour after exercise; delayed onset muscle soreness is thought to be due to microtrauma from unaccustomed or strenuous exercise.[652] Cremated remains are not ashes in the usual sense. After the incineration is completed, the dry bone fragments are swept out of the retort and pulverized by a machine called a cremulator (essentially a high-capacity, high-speed blender) to process them into \"ashes\" or \"cremated remains\".[656] Half of body heat is not lost through the head, and covering the head is no more effective at preventing heat loss than covering any other portion of the body. Heat is lost from the body in proportion to the amount of exposed skin.[658][659] The head accounts for around 7–9% of the body's surface, and studies have shown that having one's head submerged in cold water only causes a person to lose 10% more heat overall.[660] This myth likely comes from a flawed United States military experiment in 1950, involving a prototype Arctic survival suit where the head was one of the few body parts left exposed.[661] The misconception was further perpetuated by a 1970 military field manual that claimed \"40–45%\" of heat is lost through the head, based on the 1950 study.[659][661] Adrenochrome is not harvested from living people and has no use as a recreational drug. Hunter S. Thompson conceived a fictional drug of the same name in his book Fear and Loathing in Las Vegas, apparently as a metaphor and unaware that a real substance by that name existed; it is Thompson's fictional adrenochrome, and not the real chemical compound, that is the source of numerous conspiracy theories revolving around human trafficking to harvest the fictional drug.[662][663] The use of cotton swabs (aka cotton buds or Q-Tips) in the ear canal has no associated medical benefits and poses definite medical risks.[665] The idea that a precise number of stages of grief exist is not supported in peer-reviewed research or objective clinical observation, let alone the five stages of grief model.[666] The model was originally based on uncredited work and originally applied to the terminally ill instead of the grieving or bereaved.[667] The common cold and the common flu are caused by viruses, not exposure to cold temperatures. However, low temperatures may somewhat weaken the immune system, and someone already infected with a cold or influenza virus but showing no symptoms can become symptomatic after they are exposed to low temperatures.[668][669] Viruses are more likely to spread during the winter for a variety of reasons such as dry air, less air circulation in homes, people spending more time indoors, and lower vitamin D levels in humans.[670][671][672] There is little to no evidence that any illnesses are curable through essential oils or aromatherapy. Fish oil has not been shown to cure dementia, though there is evidence to support the effectiveness of lemon oil as a way to reduce agitation in patients with dementia.[676] In those with the common cold, the color of the sputum or nasal secretion may vary from clear to yellow to green and does not indicate the class of agent causing the infection.[677] The color of the sputum is determined by immune cells fighting an infection in the nasal area.[678] Vitamin Cdoes not prevent or treat the common cold, although it may have a protective effect during intense cold-weather exercise. If taken daily, it may slightly reduce the duration and severity of colds, but it has no effect if taken after the cold starts.[679] In people with eczema, bathing does not dry the skin as long as a moisturizer is applied soon after. If moisturizer is not applied after bathing, then the evaporation of water from the skin can result in dryness.[682] There have never been any programs in the US that provide access to dialysis machines in exchange for pull tabs on beverage cans.[683] This rumor has existed since at least the 1970s, and usually cites the National Kidney Foundation as the organization offering the program. The Foundation itself has denied the rumor, noting that dialysis machines are primarily funded by Medicare.[684] High dietary protein intake is not associated with kidney disease in healthy people.[685] While significantly increased protein intake in the short-term is associated with changes in renal function, there is no evidence to suggest this effect persists in the long-term and results in kidney damage or disease.[686] Leprosy is not auto-degenerative as commonly supposed, meaning that it will not (on its own) cause body parts to be damaged or fall off.[688] Leprosy causes rashes to form and may degrade cartilage and, if untreated, inflame tissue. In addition, leprosy is only mildly contagious, partly because 95% of those infected with the mycobacteria that causes leprosy do not develop the disease.[689][688]Tzaraath, a Biblical disease that disfigures the skin is often identified as leprosy, and may be the source of many myths about the disease.[690] Rust does not cause tetanus infection. The Clostridium tetani bacterium is generally found in dirty environments. Since the same conditions that harbor tetanus bacteria also promote rusting of metal, many people associate rust with tetanus. C. tetani requires anoxic conditions to reproduce and these are found in the permeable layers of rust that form on oxygen-absorbing, unprotected ironwork.[691] Quarantine has never been a standard procedure for those with severe combined immunodeficiency, despite the condition's popular nickname (\"bubble boy syndrome\") and its portrayal in films. A bone marrow transplant in the earliest months of life is the standard course of treatment. The exceptional case of David Vetter, who lived much of his life encased in a sterile environment because he would not receive a transplant until age 12, was an inspirations for the \"bubble boy\" trope.[692] Gunnison, Colorado, did not avoid the 1918 flu pandemic by using protective sequestration. The implementation of protective sequestration did prevent the virus from spreading outside a single household after a single carrier came into the town while it was in effect, but it was not sustainable and had to be lifted in February 1919. A month later, the flu killed five residents and infected dozens of others.[693] Statements in medication package inserts listing the frequency of side effects describe how often the effect occurs after taking a drug, but are not making any assertion that there is a causal connection between taking the drug and the occurrence of the side effect. In other words, what is being reported on is correlation, not necessarily causation.[694] A dog's mouth is not significantly cleaner than a human's mouth. A dog's mouth contains almost as much bacteria as a human mouth.[695][696] Drinking milk or consuming other dairy products does not increase mucus production.[706] As a result, they do not need to be avoided by those with the flu or cold congestion. However, milk and saliva in one's mouth mix to create a thick liquid that can briefly coat the mouth and throat. The sensation that lingers may be mistaken for increased phlegm.[707] Drinking eight glasses (2–3 liters) of water a day is not needed to maintain health.[708] The amount of water needed varies by person, weight, diet, activity level, clothing, and the ambient heat and humidity. Water does not actually need to be drunk in pure form, and can be derived from liquids such as juices, tea, milk, soups, etc., and from foods including fruits and vegetables.[708][709] Drinking coffee and other caffeinated beverages does not cause dehydration for regular drinkers, although it can for occasional drinkers.[710][709] Eating less than an hour before swimming does not increase the risk of experiencing muscle cramps or drowning. One study shows a correlation between alcohol consumption and drowning, but not between eating and stomach cramps.[716] Spinach is not a particularly good source of dietary iron. While it does contain more iron than many vegetables such as asparagus, Swiss chard, kale, or arugula, it contains only about one-third to one-fifth of the iron in lima beans, chickpeas, apricots, or wheat germ. Additionally, the non-heme iron found in spinach and other vegetables is not as readily absorbed as the heme iron found in meats and fish.[729][730][731] Most cases of obesity are not related to slower resting metabolism. Resting metabolic rate does not vary much between people. Overweight people tend to underestimate the amount of food they eat, and underweight people tend to overestimate. In fact, overweight people tend to have faster metabolic rates due to the increased energy required by the larger body.[732] Alcoholic beverages do not make the entire body warmer.[734] Alcoholic drinks create the sensation of warmth because they cause blood vessels to dilate and stimulate nerve endings near the surface of the skin with an influx of warm blood. This can actually result in making the core body temperature lower, as it allows for easier heat exchange with a cold external environment.[735] Alcohol does not necessarily kill brain cells.[736] Alcohol can, however, lead indirectly to the death of brain cells in two ways. First, in chronic, heavy alcohol users whose brains have adapted to the effects of alcohol, abrupt ceasing following heavy use can cause excitotoxicity leading to cellular death in multiple areas of the brain.[737] Second, in alcoholics who get most of their daily calories from alcohol, a deficiency of thiamine can produce Korsakoff's syndrome, which is associated with serious brain damage.[738] The order in which different types of alcoholic beverages are consumed (\"Grape or grain but never the twain\" and \"Beer before liquor never sicker; liquor before beer in the clear\") does not affect intoxication or create adverse side effects.[739] Authentic absinthe has no hallucinogenic properties, and is no more dangerous than any other alcoholic beverage of equivalent proof.[740] This misconception stems from late-19th- and early-20th-century distillers who produced cheap knockoff versions of absinthe, which used copper salts to recreate the distinct green color of true absinthe, and some also reportedly adulterated cheap absinthe with poisonous antimony trichloride, reputed to enhance the louching effect.[741] Examination of the hymen is not an accurate or reliable indicator that a female has had penetrative sex,[743] because the tearing of the hymen may have been the result of some other event, and some women are born without one.[744][745] Traditional virginity tests, such as the \"two-finger\" test, are widely considered to be unscientific.[746][747][748] While pregnancies from sex between first cousins do carry a slightly elevated risk of birth defects, this risk is often exaggerated.[752] The risk is 5–6% (similar to that of a woman in her early 40s giving birth),[752][753] compared with a baseline risk of 3–4%.[753] The effects of inbreeding depression, while still relatively small compared to other factors (and thus difficult to control for in a scientific experiment), become more noticeable if isolated and maintained for several generations.[754] Having sex before a sporting event or contest is not physiologically detrimental to performance.[755] In fact it has been suggested that sex prior to sports activity can elevate male testosterone levels, which could potentially enhance performance for male athletes.[756] There is no definitive proof of the existence of the vaginal G-spot, and the general consensus is that no such spot exists on the female body.[757] A person's hair and fingernails do not continue to grow after death. Rather, the skin dries and shrinks away from the bases of hairs and nails, giving the appearance of growth.[764] Shaving does not cause terminal hair to grow back thicker or darker. This belief is thought to be due to the fact that hair that has never been cut has a tapered end, so after cutting, the base of the hair is blunt and appears thicker and feels coarser. That short hairs are less flexible than longer hairs contributes to this effect.[765] MC1R, the gene mostly responsible for red hair, is not becoming extinct, nor will the gene for blond hair do so, although both are recessivealleles. Redheads and blonds may become rarer but will not die out unless everyone who carries those alleles dies without passing their hair color genes on to their children.[766] Acne is mostly caused by genetics, and is not caused by a lack of hygiene or eating fatty foods, though certain medication or a carbohydrate-rich diet may worsen it.[767] Dandruff is not caused by poor hygiene, though infrequent hair-washing can make it more obvious. The exact causes of dandruff are uncertain, but they are believed to be mostly genetic and environmental factors.[768] James Watt did not invent the steam engine,[769] nor were his ideas on steam engine power inspired by a kettle lid pressured open by steam.[770] Watt improved upon the already commercially successful Newcomen atmospheric engine (invented in 1712) in the 1760s and 1770s, making certain improvements critical to its future usage, particularly the external condenser, increasing its efficiency, and later the mechanism for transforming reciprocating motion into rotary motion; his new steam engine later gained huge fame as a result.[771] Thomas Edison did not invent the light bulb.[778] He did, however, develop the first practical light bulb in 1880 (employing a carbonized bamboo filament), shortly prior to Joseph Swan, who invented an even more efficient bulb in 1881 (which used a cellulose filament). Henry Ford did not invent either the automobile or the assembly line. He did improve the assembly line process substantially, sometimes through his own engineering but more often through sponsoring the work of his employees, and he was the main person behind the introduction of the Model T, regarded as the first affordable automobile.[779]Karl Benz (co-founder of Mercedes-Benz) is credited with the invention of the first modern automobile,[780] and the assembly line has existed throughout history. The repeating decimal commonly written as 0.999... represents exactly the same quantity as the number one. Despite having the appearance of representing a smaller number, 0.999... is a symbol for the number 1 in exactly the same way that 0.333... is an equivalent notation for the number represented by the fraction 1⁄3.[794] The p-value is not the probability that the null hypothesis is true, or the probability that the alternative hypothesis is false; it is the probability of obtaining results at least as extreme as the results actually observed under the assumption that the null hypothesis was correct, which can indicate the incompatibility of results with the specific statistical model assumed in the null hypothesis.[795] This misconception, and similar ones like it, contributes to the common misuse of p-values in education and research.[795][796] An illustration of the (incorrect) equal-transit-time explanation of aerofoil lift The lift force is not generated by the air taking the same time to travel above and below an aircraft's wing.[798] This misconception, sometimes called the equal transit-time fallacy, is widespread among textbooks and non-technical reference books, and even appears in pilot training materials. In fact, the air moving over the top of an aerofoil generating lift is always moving much faster than the equal transit theory would imply,[798] as described in the incorrect and correct explanations of lift force. Blowing over a curved piece of paper does not demonstrate Bernoulli's principle. Although a common classroom experiment is often explained this way,[799] Bernoulli's principle only applies within a flow field, and the air above and below the paper is in different flow fields.[800] The paper rises because the air follows the curve of the paper and a curved streamline will develop pressure differences perpendicular to the airflow.[801][802] The Coriolis effect does not cause water to consistently drain from basins in a clockwise/counter-clockwise direction depending on the hemisphere. The common myth often refers to the draining action of flush toilets and bathtubs. In fact, rotation is determined by whatever minor rotation is initially present at the time the water starts to drain, as the magnitude of the coriolis acceleration is negligibly small compared to the inertial acceleration of flow within a typical basin.[803] A penny dropped from the Empire State Building would not kill a person or crack the sidewalk. A penny is too light and has too much air resistance to acquire enough speed to do much damage since it reaches terminal velocity after falling about 50 feet. Heavier or more aerodynamic objects could cause significant damage if dropped from that height.[806][807] Using a programmable thermostat's setback feature to limit heating or cooling in a temporarily unoccupied building does not waste as much energy as leaving the temperature constant. Using setback saves energy (5–15%) because heat transfer across the surface of the building is roughly proportional to the temperature difference between its inside and the outside.[808][809] It is not possible for a person to completely submerge in quicksand, as commonly depicted in fiction,[810] although sand entrapment in the nearshore of a body of water can be a drowning hazard as the tide rises.[811] True photographic memory (the ability to remember endless images, particularly pages or numbers, with such a high degree of precision that the image mimics a photo) has never been demonstrated to exist in any individual,[816] although a small number of young children have eidetic memory, where they can recall an object with high precision for a few minutes after it is no longer present.[817] Many people have claimed to have a photographic memory, but those people have been shown to have high precision memories as a result of mnemonic devices rather than a natural capacity for detailed memory encoding.[818] There are rare cases of individuals with exceptional memory, but none of them have a memory that mimics that of a camera. The phase of the Moon does not influence fertility, cause a fluctuation in crime, or affect the stock market. There is no correlation between the lunar cycle and human biology or behavior. However, the increased amount of illumination during the full moon may account for increased epileptic episodes, motorcycle accidents, or sleep disorders.[819] Dyslexia is not defined or diagnosed as mirror writing or reading letters or words backwards.[821][822] Mirror writing and reading letters or words backwards are behaviors seen in many children (dyslexic or not) as they learn to read and write.[821][822]Dyslexia is a neurodevelopmental disorder of people who have at least average intelligence and who have difficulty in reading and writing that is not otherwise explained by low intelligence.[823] Self-harm is not generally an attention-seeking behavior. People who engage in self-harm are typically very self-conscious of their wounds and scars and feel guilty about their behavior, leading them to go to great lengths to conceal it from others.[824] They may offer alternative explanations for their injuries, or conceal their scars with clothing.[825][826] There is no evidence that a chemical imbalance or neurotransmitter deficiency is the sole factor in depression and other mental disorders, but rather a combination of biological, psychological, and social factors.[827][828] Schizophrenia is characterized by continuous or relapsing episodes of psychosis. Major symptoms include hallucinations (typically hearing voices), delusions, paranoia, and disorganized thinking. Other symptoms include social withdrawal, decreased emotional expression, and apathy.[829] The term was coined from the Greek roots schizein and phrēn, \"to split\" and \"mind\", in reference to a \"splitting of mental functions\" seen in schizophrenia, not a splitting of the personality.[830] It does not involve split or multiple personalities—a split or multiple personality is dissociative identity disorder.[831] Broad generalizations are often made in popular psychology about certain brain functions being lateralized, or more predominant in one hemisphere than the other. These claims are often inaccurate or overstated.[832] The human brain, particularly the prefrontal cortex, does not reach \"full maturity\" at any particular age (e.g. 18, 21, or 25 years of age). Changes in structure and myelination of gray matter are recorded to continue with relative consistency all throughout adult life. Some mental abilities peak and begin to decline around high school graduation while others do not peak until much later (i.e. 40s or later).[833] Golgi-stained neurons in human hippocampal tissue. It is commonly believed that humans will not grow new brain cells, but research has shown that some neurons can reform in humans. Humans do not generate all of the brain cells they will ever have by the age of two years. Although this belief was held by medical experts until 1998, it is now understood that new neurons can be created after infancy in some parts of the brain into late adulthood.[834] People do not use only 10% of their brains.[835][836] While it is true that a small minority of neurons in the brain are actively firing at any one time, a healthy human will normally use most of their brain over the course of a day, and the inactive neurons are important as well. The idea that activating 100% of the brain would allow someone to achieve their maximum potential and/or gain various psychic abilities is common in folklore and fiction,[836][837][838] but doing so in real life would likely result in a fatal seizure.[839][840] This misconception was attributed to late 19th century leading thinker William James, who apparently used the expression only metaphorically.[837] Although Phineas Gage's brain injuries, caused by a several-foot-long tamping rod driven completely through his skull, caused him to become temporarily disabled, many fanciful descriptions of his aberrant behavior in later life are without factual basis or contradicted by known facts.[841] All different tastes can be detected on all parts of the tongue by taste buds,[844] with slightly increased sensitivities in different locations depending on the person; the tongue map showing the contrary is fallacious.[845] Swallowing gasoline does not generally require special emergency treatment, as long as it goes into the stomach and not the lungs, and inducing vomiting can make it worse.[854] A chloroform-soaked rag cannot instantly incapacitate a person.[855] It takes at least five minutes of inhaling an item soaked in chloroform to render a person unconscious. Most criminal cases involving chloroform also involve another drug being co-administered, such as alcohol or diazepam, or the victim being found to have been complicit in its administration. The misconception that chloroform can be used as an incapacitating agent[856] has been popularized by crime fiction authors. Toilet waste is never intentionally jettisoned from an aircraft. All waste is collected in tanks and emptied into toilet waste vehicles.[862]Blue ice is caused by accidental leakage from the waste tank. Passenger train toilets, on the other hand, have indeed historically flushed onto the tracks; modern trains in most developed countries usually have retention tanks on board and therefore do not dispose of waste in such a manner. Automotive batteries stored on a concrete floor do not discharge any faster than they would on other surfaces,[863] in spite of worry among Americans that concrete harms batteries.[864] Early batteries with porous, leaky cases may have been susceptible to moisture from floors, but for many years lead–acid car batteries have had impermeable polypropylene cases.[865] While most modern automotive batteries are sealed, and do not leak battery acid when properly stored and maintained,[866] the sulfuric acid in them can leak out and stain, etch, or corrode concrete floors if their cases crack or tip over or their vent-holes are breached by floods.[867] ^Myre G (February 28, 2018). \"A Brief History Of The AR-15\". National Public Radio. Archived from the original on May 13, 2023. Retrieved November 20, 2021. AR\" comes from the name of the gun's original manufacturer, ArmaLite, Inc. The letters stand for ArmaLite Rifle — and not for \"assault rifle\" or \"automatic rifle. ^Rhoads C (January 19, 2008). \"The Hydrox Cookie Is Dead, and Fans Won't Get Over It\". The Wall Street Journal. Retrieved July 6, 2022. In college, when friends ridiculed her for preferring the cheaper knock-off Hydrox to the real thing, she did some research. Among her findings: Hydrox was created in 1908 by what would later become Sunshine Biscuits Inc. That was four years before the National Biscuit Co. (later called Nabisco) came up with the similar Oreo. Oreo was the knock-off. The Hydrox name came from combining the words hydrogen and oxygen, which Sunshine executives thought evoked purity. Others thought it sounded more like a laundry detergent. ^Wheeling K (January 2021). \"A Brief History of Peanut Butter\". Smithsonian Magazine. Retrieved June 24, 2022. North Americans weren't the first to grind peanuts—the Inca beat us to it by a few hundred years—but peanut butter reappeared in the modern world because of an American, the doctor, nutritionist and cereal pioneer John Harvey Kellogg, who filed a patent for a proto-peanut butter in 1895. ^McElwain A (June 17, 2019). \"Did Tayto really invent cheese and onion crisps?\". Irish Times. Retrieved June 23, 2022. One of the oldest known published recipes for crisps is by William Kitchiner, an optician who doubled up as a Georgian-era celebrity chef. His book, A Cook's Oracle, published in 1817, was a big hit in the UK and a young America. Kitchiner's recipe – Potatoes fried in Slices or Shavings – calls for slivers of potato fried in \"lard or dripping\" and \"served with a very little salt sprinkled over them\". ^Burhans D (2008). \"Creation Myths\". Crunch!: A History of the Great American Potato Chip. University of Wisconsin Press. pp. 17–20. ISBN978-0-299-22770-8. ^Di Placido D (July 19, 2017). \"The Evolution Of The Zombie\". Forbes. Retrieved July 3, 2022. George A. Romero's first zombie film Night of the Living Dead is credited with popularizing the zombie, though it never actually uses that word. The \"ghouls\" in the film are mindless flesh-eaters that have little in common with the Haitian zombie other than rising from the grave. ^ abEschner K (October 31, 2017). \"Zombie Movies Are Never Really About Zombies\". Smithsonian Magazine. Retrieved July 3, 2022. In the 1960s and 70s, filmmaker George Romero brought the zombie film into the mainstream with Night of the Living Dead and Dawn of the Dead. The first of these was technically about \"ghouls.\" Romero didn't start calling them \"zombies\" until his second film. But his now-iconic films helped to erase enslaved people from zombie history. ^Sisterson D (March 28, 2017). \"Magic Wilderness: El Apóstol & Peludópolis\". Skwigly. Retrieved June 22, 2022. As we all know, Disney's Snow White and the Seven Dwarfs is usually cited as the first animated feature, but as most of us who read this site are no doubt aware, it wasn't. It was preceded by Lotte Reiniger's The Adventures of Prince Achmed, Ladislas Starevitch's The Tale of the Fox, and two features by the Argentinian animator Quirino Cristiani – all films which could scracely [sic] be more different from the Disney mode. ^Bendazzi G (2017). \"The First Feature Length Animated Film in History\". Twice the First: Quirino Cristiani and the Animated Feature Film. CRC Press. p. 36. ISBN978-1-351-37179-7. On the other hand, the movie was not widely successful, and appealed to a small portion of the population. It was strictly for a Buenos Aires audience: nobody in the provinces even saw it because it was not distributed there. And likewise, given the subject, it was not possible to export the film to other nations, not even to a close cousin similar to Uruguay. ^US941960A, Smith, George Albert, \"Kinematograph apparatus for the production of colored pictures\", issued 1909-11-30 ^ a. Geoffrey K. Pullum's explanation in Language Log: The list of snow-referring roots to stick [suffixes] on isn't that long [in the Eskimoan language group]: qani- for a snowflake, apu- for snow considered as stuff lying on the ground and covering things up, a root meaning \"slush\", a root meaning \"blizzard\", a root meaning \"drift\", and a few others—very roughly the same number of roots as in English. Nonetheless, the number of distinct words you can derive from them is not 50, or 150, or 1500, or a million, but simply unbounded. Only stamina sets a limit. b. The seven most common English words for snow are snow, hail, sleet, ice, icicle, slush, and snowflake. English also has the related word glacier and the four common skiing terms pack, powder, crud, and crust, so one can say that at least 12 distinct words for snow exist in English. ^David Robson, New Scientist 2896, December 18 2012, Are there really 50 Eskimo words for snow?, \"Yet Igor Krupnik, an anthropologist at the Smithsonian Arctic Studies Center in Washington DC believes that Boas was careful to include only words representing meaningful distinctions. Taking the same care with their own work, Krupnik and others have now charted the vocabulary of about 10 Inuit and Yupik dialects and conclude that there are indeed many more words for snow than in English (SIKU: Knowing Our Ice, 2010). Central Siberian Yupik has 40 such terms, whereas the Inuit dialect spoken in Nunavik, Quebec, has at least 53, including matsaaruti, wet snow that can be used to ice a sleigh's runners, and pukak, for the crystalline powder snow that looks like salt. For many of these dialects, the vocabulary associated with sea ice is even richer.\" ^ \"Irregardless originated in dialectal American speech in the early 20th century... The most frequently repeated remark about it is that \"there is no such word.\" There is such a word, however.\" Merriam Webster Dictionary \"Definition of IRREGARDLESS\". Archived from the original on May 8, 2014. Retrieved October 27, 2011. ^Churchwell S (June 23, 2019). \"For sale, baby shoes, never worn — the myth of Ernest Hemingway's short story\". The Times. Retrieved April 5, 2023. For a long time, legend held that this was one of the world's great short stories, by one of the world's great short-story writers: Ernest Hemingway. There were different versions of the myth . . . None of it is true. And for those who think the internet is a cesspool of lies, it is an interesting experiment to google those six words today. The top items, including a Wikipedia entry on the myth, debunk it as the urban legend it is. ^William Pryse-Phillips (2003). Companion to Clinical Neurology. Oxford University Press. ISBN0-19-515938-1., p. 611 defines the term as \"Slight and transient improvement in spational[sic] reasoning skills detected in normal subjects as a result of exposure to the music of Mozart, specifically his sonata for two pianos (K448).\" ^Maurice Hinson (2004). The Pianist's Dictionary. Indiana University Press. p. 114. ISBN978-0-253-21682-3. Retrieved October 2, 2010. This piece bears an erroneous nickname since the story long associated with this nickname presumes the pianist is supposed to play the piece in one minute. The word \"minute\" means small or little waltz. ^Kennedy S (December 2, 2005). \"Dragon Quest vs. America\". 1up. Archived from the original on July 28, 2012. Retrieved June 26, 2022. Predating Xbox 360 hysteria by years, several fans were mugged on their way home with their new prize, and the situation became so bad that it was brought before the Japanese Diet. Although tales of a law requiring Dragon Quest games only be released on the mornings of weekends or holidays are the stuff of urban legend, each new title is as highly anticipated as the launch of a new console. ^Walker M (August 19, 2012). \"Dragon Quest X Online: Mezameshi Itsutsu no Shuzoku\". Nintendo World Report. Retrieved June 26, 2022. Its Thursday release is unheard of for a Dragon Quest game, which are generally released over the weekend so people don't take work off in droves to play them. ^ abWatterson B (1997). \"The Era of Pyramid-builders\". The Egyptians. Blackwell. p. 63. Herodotus claimed that the Great Pyramid at Giza was built with the labour of 100,000 slaves working in three-monthly shifts, a charge that cannot be substantiated. Much of the non-skilled labour on the pyramids was undertaken by peasants working during the Inundation season when they could not farm their lands. In return for their services they were given rations of food, a welcome addition to the family diet. ^ a. Neer R (2012). Art and Archaeology of the Greek World. Thames and Hudson. p. 37. ISBN978-0-500-05166-5. \"...popular associations of the eruption with a legend of Atlantis should be dismissed...nor is there good evidence to suggest that the eruption...brought about the collapse of Minoan Crete b. Manning S (2012). \"Eruption of Thera/Santorini\". In Cline E (ed.). The Oxford Handbook of the Bronze Age Aegean. Oxford University Press. pp. 457–454. doi:10.1093/oxfordhb/9780199873609.013.0034. ISBN978-0-19-987360-9. Marinatos (1939) famously suggested that the eruption might even have caused the destruction of Minoan Crete (also Page 1970). Although this simple hypothesis has been negated by the findings of excavation and other research since the late 1960s... which demonstrate that the eruption occurred late in the Late Minoan IA ceramic period, whereas the destructions of the Cretan palaces and so on are some time subsequent (late in the following Late Minoan IB ceramic period) c. Brouwers J (2021). \"Did Atlantis Exist?\". Bad Ancient. Retrieved August 30, 2023. ^\"National Pasta Association\". Archived from the original on March 20, 2012. article FAQs section \"Who \"invented\" pasta?\"; \"The story that it was Marco Polo who imported noodles to Italy and thereby gave birth to the country's pasta culture is the most pervasive myth in the history of Italian food.\" (Dickie 2008, p. 48). ^Crabtree S (July 6, 1999). \"New Poll Gauges Americans' General Knowledge Levels\". Gallup News Service. Archived from the original on March 27, 2014. Retrieved January 13, 2011. Fifty-five percent say it commemorates the signing of the Declaration of Independence (this is a common misconception, and close to being accurate; July 4th is actually the date in 1776 when the Continental Congress approved the Declaration, which was officially signed on August 2nd.) Another 32 percent give a more general answer, saying that July 4th celebrates Independence Day. ^ a. Craig L, Young K (2008). \"Beyond White Pride: Identity, Meaning and Contradiction in the Canadian Skinhead Subculture*\". Canadian Review of Sociology/Revue Canadienne de Sociologie. 34 (2): 175–206. doi:10.1111/j.1755-618x.1997.tb00206.x. Retrieved July 2, 2022. b. Borgeson K, Valeri R (Fall 2005). \"Examining Differences in Skinhead Ideology and Culture Through an Analysis of Skinhead Websites\". Michigan Sociological Review. 19: 45–62. JSTOR40969104. c. Lambert C (November 12, 2017). \"'Black Skinhead': The politics of New Kanye\". Daily Dot. Retrieved July 2, 2022. \"Skinhead\" was a term originally used to describe a 1960s British working-class subculture that revolved around fashion and music and that would heavily inspire the punk rock scene. While it has harmless roots, the skinhead movement fell into polemic politics. Nowadays, it's popularly associated with neo-Nazis, despite having split demographics of far-right, far-left, and apolitical. ^Krause CA (December 17, 1978). \"Jonestown Is an Eerie Ghost Town Now\". Washington Post. Retrieved June 20, 2022. A pair of woman's eyelasses, a towel, a pair of shorts, packets of unopened Flavor-Aid lie scattered about waiting for the final cleanup that may one day return Jonestown to the tidy, if overcrowded, little community it once was. ^Gudger E (January 1930). \"On the alleged penetration of the human urethra by an Amazonian catfish called candiru with a review of the allied habits of other members of the family pygidiidae\". The American Journal of Surgery (Print). 8 (1). Elsevier Inc.: 170–188. doi:10.1016/S0002-9610(30)90912-9. ISSN0002-9610. ^\"Have You Heard the Calls from Cook County's 12 Frog and Toad Species?\". Forest Preserves of Cook County. May 25, 2022. Retrieved June 25, 2022. Here's a bonus fact: you might notice that none of these species says, \"ribbit.\" In fact, the \"ribbit\" call is unique to the Pacific tree frog, which lives along the Pacific coast, and, notably, in Hollywood, California, where the largest volume of early frog recordings took place. ^Bittel J (September 22, 2019). \"Think you know what bunnies and bears eat? Their diets may surprise you\". Washington Post. Archived from the original on September 26, 2019. In the wild, rabbits aren't in the habit of digging up root vegetables such as carrots, potatoes and beets. They much prefer wild greens, such as grasses and clover. In fact, carrots may actually be bad for rabbits, because although the vegetables are high in good nutrients, including beta carotene, they are also relatively high in sugar. This means that feeding a rabbit lots of carrots could lead to tooth decay or other health issues. ^Curtin C (February 2007). \"Fact or Fiction?: Glass Is a (Supercooled) Liquid\". Scientific American. Archived from the original on December 14, 2013. Glass, however, is actually neither a liquid—supercooled or otherwise—nor a solid. It is an amorphous solid—a state somewhere between those two states of matter. And yet glass's liquidlike properties are not enough to explain the thicker-bottomed windows, because glass atoms move too slowly for changes to be visible. ^Begley S (August 13, 2007). \"The Truth About Denial\". Newsweek. Archived from the original on October 21, 2007. (MSNBC single page version, archived 20 August 2007) \"If you think those who have long challenged the mainstream scientific findings about global warming recognize that the game is over, think again. ... outside Hollywood, Manhattan and other habitats of the chattering classes, the denial machine is running at full throttle—and continuing to shape both government policy and public opinion. Since the late 1980s, this well-coordinated, well-funded campaign by contrarian scientists, free-market think tanks and industry has created a paralyzing fog of doubt around climate change. Through advertisements, op-eds, lobbying and media attention, greenhouse doubters (they hate being called deniers) argued first that the world is not warming; measurements indicating otherwise are flawed, they said. Then they claimed that any warming is natural, not caused by human activities. Now they contend that the looming warming will be minuscule and harmless. 'They patterned what they did after the tobacco industry,' says former senator Tim Wirth.\" ^ a. \"Lightning Myths and Facts\". National Weather Service. Fact: Lightning often strikes the same place repeatedly, especially if it's a tall, pointy, isolated object. The Empire State Building is hit an average of 23 times a year b. \"Lightning Often Strikes Twice\". NASA Spinoff. Office of the Chief Technologist, NASA. Archived from the original on March 25, 2012. Retrieved June 23, 2010. c. WeatherBug Meteorologists (May 17, 2010). \"The Myths and Facts of Lightning\". WeatherBug. Earth Works. Archived from the original on July 11, 2010. Retrieved June 23, 2010. d. Tristan Simpson (April 29, 2022). \"Can lightning strike the same place twice?\". The Weather Network. Believe it or not, this long-held myth is far from the truth. While the odds of being struck by lightning are low, the chances of lightning striking the same place twice are high. Lightning can, and often will, hit the same spot multiple times. ^ a. \"Ask an Astrophysicist\". NASA. Archived from the original on June 4, 2012. If you don't try to hold your breath, exposure to space for half a minute or so is unlikely to produce permanent injury. Holding your breath is likely to damage your lungs, ... but theory predicts—and animal experiments confirm—that otherwise, exposure to vacuum causes no immediate injury. You do not explode. Your blood does not boil. You do not freeze. You do not instantly lose consciousness b. \"Exploding Body in Vacuum\". ABC Science. April 6, 2005. Archived from the original on June 4, 2012. ...will we humans explode in the full vacuum of space, as urban legends claim? The answer is that we won't explode, and if the exposure is short enough, we can even survive. ^Dresden D (March 12, 2020). \"How many ribs do humans have? Men, women, and anatomy\". Medical News Today. Retrieved June 5, 2022. Although many people might think that males have fewer ribs than females — most likely sparked by the biblical story of Adam and Eve — there is no factual evidence. ^Spellman, Frank R; Price-Bayer, Joni. (2010). In Defense of Science: Why Scientific Literacy Matters. The Scarecrow Press. p. 81. ISBN978-1-60590-735-2 \"There is no scientific evidence that crystal healing has any effect. It has been called a pseudoscience. Pleasant feelings or the apparent successes of crystal healing can be attributed to the placebo effect or cognitive bias—a believer wanting it to be true.\" The literature about Biodiversity and the GE food/feed consumption has sometimes resulted in animated debate regarding the suitability of the experimental designs, the choice of the statistical methods or the public accessibility of data. Such debate, even if positive and part of the natural process of review by the scientific community, has frequently been distorted by the media and often used politically and inappropriately in anti-GE crops campaigns. b. \"State of Food and Agriculture 2003–2004. Agricultural Biotechnology: Meeting the Needs of the Poor. Health and environmental impacts of transgenic crops\". Food and Agriculture Organization of the United Nations. Retrieved August 30, 2019. Currently available transgenic crops and foods derived from them have been judged safe to eat and the methods used to test their safety have been deemed appropriate. These conclusions represent the consensus of the scientific evidence surveyed by the ICSU (2003) and they are consistent with the views of the World Health Organization (WHO, 2002). These foods have been assessed for increased risks to human health by several national regulatory authorities (inter alia, Argentina, Brazil, Canada, China, the United Kingdom and the United States) using their national food safety procedures (ICSU). To date no verifiable untoward toxic or nutritionally deleterious effects resulting from the consumption of foods derived from genetically modified crops have been discovered anywhere in the world (GM Science Review Panel). Many millions of people have consumed foods derived from GM plants – mainly maize, soybean and oilseed rape – without any observed adverse effects (ICSU). c. Ronald P (May 1, 2011). \"Plant Genetics, Sustainable Agriculture and Global Food Security\". Genetics. 188 (1): 11–20. doi:10.1534/genetics.111.128553. PMC3120150. PMID21546547. There is broad scientific consensus that genetically engineered crops currently on the market are safe to eat. After 14 years of cultivation and a cumulative total of 2 billion acres planted, no adverse health or environmental effects have resulted from commercialization of genetically engineered crops (Board on Agriculture and Natural Resources, Committee on Environmental Impacts Associated with Commercialization of Transgenic Plants, National Research Council and Division on Earth and Life Studies 2002). Both the U.S. National Research Council and the Joint Research Centre (the European Union's scientific and technical research laboratory and an integral part of the European Commission) have concluded that there is a comprehensive body of knowledge that adequately addresses the food safety issue of genetically engineered crops (Committee on Identifying and Assessing Unintended Effects of Genetically Engineered Foods on Human Health and National Research Council 2004; European Commission Joint Research Centre 2008). These and other recent reports conclude that the processes of genetic engineering and conventional breeding are no different in terms of unintended consequences to human health and the environment (European Commission Directorate-General for Research and Innovation 2010). ^ See also: Domingo JL, Bordonaba JG (2011). \"A literature review on the safety assessment of genetically modified plants\"(PDF). Environment International. 37 (4): 734–742. Bibcode:2011EnInt..37..734D. doi:10.1016/j.envint.2011.01.003. PMID21296423. In spite of this, the number of studies specifically focused on safety assessment of GM plants is still limited. However, it is important to remark that for the first time, a certain equilibrium in the number of research groups suggesting, on the basis of their studies, that a number of varieties of GM products (mainly maize and soybeans) are as safe and nutritious as the respective conventional non-GM plant, and those raising still serious concerns, was observed. Moreover, it is worth mentioning that most of the studies demonstrating that GM foods are as nutritional and safe as those obtained by conventional breeding, have been performed by biotechnology companies or associates, which are also responsible of commercializing these GM plants. Anyhow, this represents a notable advance in comparison with the lack of studies published in recent years in scientific journals by those companies.Krimsky S (2015). \"An Illusory Consensus behind GMO Health Assessment\". Science, Technology, & Human Values. 40 (6): 883–914. doi:10.1177/0162243915598381. S2CID40855100. I began this article with the testimonials from respected scientists that there is literally no scientific controversy over the health effects of GMOs. My investigation into the scientific literature tells another story. And contrast: Panchin AY, Tuzhikov AI (January 14, 2016). \"Published GMO studies find no evidence of harm when corrected for multiple comparisons\". Critical Reviews in Biotechnology. 37 (2): 213–217. doi:10.3109/07388551.2015.1130684. ISSN0738-8551. PMID26767435. S2CID11786594. Here, we show that a number of articles some of which have strongly and negatively influenced the public opinion on GM crops and even provoked political actions, such as GMO embargo, share common flaws in the statistical evaluation of the data. Having accounted for these flaws, we conclude that the data presented in these articles does not provide any substantial evidence of GMO harm. The presented articles suggesting possible harm of GMOs received high public attention. However, despite their claims, they actually weaken the evidence for the harm and lack of substantial equivalency of studied GMOs. We emphasize that with over 1783 published articles on GMOs over the last 10 years it is expected that some of them should have reported undesired differences between GMOs and conventional crops even if no such differences exist in reality. and Yang Y, Chen B (2016). \"Governing GMOs in the USA: science, law and public health\". Journal of the Science of Food and Agriculture. 96 (4): 1851–1855. Bibcode:2016JSFA...96.1851Y. doi:10.1002/jsfa.7523. PMID26536836. It is therefore not surprising that efforts to require labeling and to ban GMOs have been a growing political issue in the USA (citing Domingo and Bordonaba, 2011). Overall, a broad scientific consensus holds that currently marketed GM food poses no greater risk than conventional food... Major national and international science and medical associations have stated that no adverse human health effects related to GMO food have been reported or substantiated in peer-reviewed literature to date. Despite various concerns, today, the American Association for the Advancement of Science, the World Health Organization, and many independent international science organizations agree that GMOs are just as safe as other foods. Compared with conventional breeding techniques, genetic engineering is far more precise and, in most cases, less likely to create an unexpected outcome. ^ a. \"Statement by the AAAS Board of Directors On Labeling of Genetically Modified Foods\"(PDF). American Association for the Advancement of Science. October 20, 2012. Retrieved August 30, 2019. The EU, for example, has invested more than €300 million in research on the biosafety of GMOs. Its recent report states: \"The main conclusion to be drawn from the efforts of more than 130 research projects, covering a period of more than 25 years of research and involving more than 500 independent research groups, is that biotechnology, and in particular GMOs, are not per se more risky than e.g. conventional plant breeding technologies.\" The World Health Organization, the American Medical Association, the U.S. National Academy of Sciences, the British Royal Society, and every other respected organization that has examined the evidence has come to the same conclusion: consuming foods containing ingredients derived from GM crops is no riskier than consuming the same foods containing ingredients from crop plants modified by conventional plant improvement techniques. b. Pinholster G (October 25, 2012). \"AAAS Board of Directors: Legally Mandating GM Food Labels Could \"Mislead and Falsely Alarm Consumers\"\"(PDF). American Association for the Advancement of Science. Retrieved August 30, 2019. c. European Commission. Directorate-General for Research (2010). A decade of EU-funded GMO research (2001–2010)(PDF). Directorate-General for Research and Innovation. Biotechnologies, Agriculture, Food. European Commission, European Union. doi:10.2777/97784. ISBN978-92-79-16344-9. Retrieved August 30, 2019. d. \"ISAAA Summary of AMA Report on Genetically Modified Crops and Foods\". ISAAA. January 2001. Retrieved August 30, 2019. A report issued by the scientific council of the American Medical Association (AMA) says that no long-term health effects have been detected from the use of transgenic crops and genetically modified foods, and that these foods are substantially equivalent to their conventional counterparts. e. \"Featured CSA Report: Genetically Modified Crops and Foods (I-00) Full Text\". American Medical Association. Archived from the original on June 10, 2001. Crops and foods produced using recombinant DNA techniques have been available for fewer than 10 years and no long-term effects have been detected to date. These foods are substantially equivalent to their conventional counterparts. f. \"Report 2 of the Council on Science and Public Health (A-12): Labeling of Bioengineered Foods\"(PDF). American Medical Association. 2012. Archived from the original(PDF) on September 7, 2012. Retrieved August 30, 2019. \"Bioengineered foods have been consumed for close to 20 years, and during that time, no overt consequences on human health have been reported and/or substantiated in the peer-reviewed literature\". g. \"Restrictions on Genetically Modified Organisms: United States. Public and Scholarly Opinion\". Library of Congress. June 30, 2015. Retrieved August 30, 2019. Several scientific organizations in the US have issued studies or statements regarding the safety of GMOs indicating that there is no evidence that GMOs present unique safety risks compared to conventionally bred products. These include the National Research Council, the American Association for the Advancement of Science, and the American Medical Association. Groups in the US opposed to GMOs include some environmental organizations, organic farming organizations, and consumer organizations. A substantial number of legal academics have criticized the US's approach to regulating GMOs. h. National Academies Of Sciences E, Division on Earth Life Studies, Board on Agriculture Natural Resources, Committee on Genetically Engineered Crops: Past Experience Future Prospects (2016). Genetically Engineered Crops: Experiences and Prospects. The National Academies of Sciences, Engineering, and Medicine (US). p. 149. doi:10.17226/23395. ISBN978-0-309-43738-7. PMID28230933. Retrieved August 30, 2019. Overall finding on purported adverse effects on human health of foods derived from GE crops: On the basis of detailed examination of comparisons of currently commercialized GE with non-GE foods in compositional analysis, acute and chronic animal toxicity tests, long-term data on health of livestock fed GE foods, and human epidemiological data, the committee found no differences that implicate a higher risk to human health from GE foods than from their non-GE counterparts. ^Fullerton-Smith J (2007). The Truth About Food. Bloomsbury. pp. 115–17. ISBN978-0-7475-8685-2. Most parents assume that children plus sugary foods equals raucous and uncontrollable behaviour. ... according to nutrition experts, the belief that children experience a 'sugar high' is a myth. ^ a. Jesse Galef (August 29, 2011). \"Lies and Debunked Legends about the Golden Ratio\". Archived from the original on April 27, 2014. Retrieved April 10, 2013. b. \"Two other beliefs about [the golden ratio] are often mentioned in magazines and books: that the ancient Greeks believed it was the proportion of the rectangle the eye finds most pleasing and that they accordingly incorporated the rectangle in many of their buildings, including the famous Parthenon. These two equally persistent beliefs are likewise assuredly false and, in any case, are completely without any evidence.\" Devlin K (2008). The Unfinished Game: Pascal, Fermat, and the Seventeenth-Century Letter that Made the World Modern. Basic Books. p. 35. ^ a. Donald E. Simanek. \"Fibonacci Flim-Flam\". Archived from the original on February 1, 2010. Retrieved April 9, 2013. b. Devlin K (May 2007). \"The Myth That Will Not Go Away\". Archived from the original on July 1, 2013. Retrieved April 10, 2013. Part of the process of becoming a mathematics writer is, it appears, learning that you cannot refer to the golden ratio without following the first mention by a phrase that goes something like 'which the ancient Greeks and others believed to have divine and mystical properties.' Almost as compulsive is the urge to add a second factoid along the lines of 'Leonardo Da Vinci believed that the human form displays the golden ratio.' There is not a shred of evidence to back up either claim, and every reason to assume they are both false. Yet both claims, along with various others in a similar vein, live on. ^a. \"This occurs because of Bernoulli's principle – fast-moving air has lower pressure than non-moving air\". Make Magazine. Archived from the original on January 3, 2013. Retrieved September 5, 2012. b. \"Paper Lift\". Physics Force. University of Minnesota. Archived from the original on November 12, 2020. Retrieved January 7, 2021. ... When the demonstrator holds the paper in front of his mouth and blows across the top, he is creating an area of faster-moving air. The slower-moving air under the paper now has higher pressure, thus pushing the paper up, towards the area of lower pressure.. c. \"Educational Packet\"(PDF). Tall Ships Festival: Channel Islands Harbor. Archived from the original(PDF) on December 3, 2013. Retrieved June 25, 2012. Bernoulli's Principle states that faster moving air has lower pressure... You can demonstrate Bernoulli's Principle by blowing over a piece of paper held horizontally across your lips.\" ^a. Craig GM. \"Physical Principles of Winged Flight\"(PDF). Archived from the original(PDF) on March 7, 2021. Retrieved September 5, 2012. If the lift in figure A were caused by \"Bernoulli principle,\" then the paper in figure B should droop further when air is blown beneath it. However, as shown, it raises when the upward pressure gradient in downward-curving flow adds to atmospheric pressure at the paper lower surface. b. Babinsky H (2003). \"How Do Wings Work\". Physics Education. 38 (6): 497–503. Bibcode:2003PhyEd..38..497B. doi:10.1088/0031-9120/38/6/001. S2CID1657792. Retrieved January 7, 2021. In fact, the pressure in the air blown out of the lungs is equal to that of the surrounding air... Blowing over a piece of paper does not demonstrate Bernoulli's equation. While it is true that a curved paper lifts when flow is applied on one side, this is not because air is moving at different speeds on the two sides... It is false to make a connection between the flow on the two sides of the paper using Bernoulli's equation. c. Eastwell P (2007). \"Bernoulli? Perhaps, but What About Viscosity?\"(PDF). The Science Education Review. 6 (1). Archived from the original(PDF) on March 18, 2018. Retrieved September 10, 2023. ...air does not have a reduced lateral pressure (or static pressure...) simply because it is caused to move, the static pressure of free air does not decrease as the speed of the air increases, it misunderstanding Bernoulli's principle to suggest that this is what it tells us, and the behavior of the curved paper is explained by other reasoning than Bernoulli's principle. ... An explanation based on Bernoulli's principle is not applicable to this situation, because this principle has nothing to say about the interaction of air masses having different speeds... Also, while Bernoulli's principle allows us to compare fluid speeds and pressures along a single streamline and... along two different streamlines that originate under identical fluid conditions, using Bernoulli's principle to compare the air above and below the curved paper in Figure 1 is nonsensical; in this case, there aren't any streamlines at all below the paper! d. Raskin J. \"Coanda Effect: Understanding Why Wings Work\". Make a strip of writing paper about 5 cm X 25 cm. Hold it in front of your lips so that it hangs out and down making a convex upward surface. When you blow across the top of the paper, it rises. Many books attribute this to the lowering of the air pressure on top solely to the Bernoulli effect. Now use your fingers to form the paper into a curve that it is slightly concave upward along its whole length and again blow along the top of this strip. The paper now bends downward...an often-cited experiment which is usually taken as demonstrating the common explanation of lift does not do so... e. Auerbach D (2000). \"Why Aircraft Fly\". European Journal of Physics. 21 (4): 289. Bibcode:2000EJPh...21..289A. doi:10.1088/0143-0807/21/4/302. S2CID250821727. The well-known demonstration of the phenomenon of lift by means of lifting a page cantilevered in one's hand by blowing horizontally along it is probably more a demonstration of the forces inherent in the Coanda effect than a demonstration of Bernoulli's law; for, here, an air jet issues from the mouth and attaches to a curved (and, in this case pliable) surface. The upper edge is a complicated vortex-laden mixing layer and the distant flow is quiescent, so that Bernoulli's law is hardly applicable. f. Smith NF (November 1972). \"Bernoulli and Newton in Fluid Mechanics\". The Physics Teacher. 10 (8): 451–455. Bibcode:1972PhTea..10..451S. doi:10.1119/1.2352317. Millions of children in science classes are being asked to blow over curved pieces of paper and observe that the paper \"lifts\"... They are then asked to believe that Bernoulli's theorem is responsible... Unfortunately, the \"dynamic lift\" involved...is not properly explained by Bernoulli's theorem. ^ a. Anderson DF, Eberhardt S (2000). Understanding Flight. McGraw Hill Professional. p. 229. ISBN978-0-07-138666-1 – via Google Books. Demonstrations\" of Bernoulli's principle are often given as demonstrations of the physics of lift. They are truly demonstrations of lift, but certainly not of Bernoulli's principle. b. Feil M. The Aeronautics File. As an example, take the misleading experiment most often used to \"demonstrate\" Bernoulli's principle. Hold a piece of paper so that it curves over your finger, then blow across the top. The paper will rise. However most people do not realize that the paper would NOT rise if it was flat, even though you are blowing air across the top of it at a furious rate. Bernoulli's principle does not apply directly in this case. This is because the air on the two sides of the paper did not start out from the same source. The air on the bottom is ambient air from the room, but the air on the top came from your mouth where you actually increased its speed without decreasing its pressure by forcing it out of your mouth. As a result the air on both sides of the flat paper actually has the same pressure, even though the air on the top is moving faster. The reason that a curved piece of paper does rise is that the air from your mouth speeds up even more as it follows the curve of the paper, which in turn lowers the pressure according to Bernoulli. ^ a. Colbeck SC (1995). \"Pressure melting and ice skating\". American Journal of Physics. 63 (10): 888. Bibcode:1995AmJPh..63..888C. doi:10.1119/1.18028. Pressure melting cannot be responsible for the low friction of ice. The pressure needed to reach the melting temperature is above the compressive failure stress...\" b. Kenneth Chang (February 21, 2006). \"Explaining Ice: The Answers Are Slippery\". The New York Times. According to the frequently cited — if incorrect — explanation of why ice is slippery under an ice skate, the pressure exerted along the blade lowers the melting temperature of the top layer of ice, the ice melts and the blade glides on a thin layer of water that refreezes to ice as soon as the blade passes... But the explanation fails... because the pressure-melting effect is small. c. Robert Rosenberg (December 2005). \"Why is Ice slippery?\"(PDF). Physics Today: 50–55. ^ abHandler SM, Fierson WM, Section on O, Council on Children with D, American Academy of O, American Association for Pediatric Ophthalmology and S, et al. (March 2011). \"Learning disabilities, dyslexia, and vision\". Pediatrics. 127 (3): e818–56. doi:10.1542/peds.2010-3670. PMID21357342. A common misconception is that dyslexia is a problem of letter or word reversals. Reversals of letters or words and mirror writing occur normally in early readers and writers. Children with dyslexia are not unusually prone to reversals. Although they do occur, reversal of letters or words, or mirror writing, is not included in the definition of dyslexia. ^ abRadford B (March–April 1999). \"The Ten-Percent Myth\". Skeptical Inquirer. ISSN0194-6730. Archived from the original on October 30, 2013. Retrieved April 15, 2009. It's the old myth heard time and again about how people use only ten percent of their brains ^ abBeyerstein BL (1999). \"Whence Cometh the Myth that We Only Use 10% of our Brains?\". In Sergio Della Sala (ed.). Mind Myths: Exploring Popular Assumptions About the Mind and Brain. Wiley. pp. 3–24. ISBN978-0-471-98303-3. ^ a. Eisenbud M, Gesell TF (1997). Environmental radioactivity: from natural, industrial, and military sources. Academic Press. pp. 171–172. ISBN978-0-12-235154-9. It is important to recognize that the potassium content of the body is under strict homeostatic control and is not influenced by variations in environmental levels. For this reason, the dose from 40K in the body is constant. b. U. S. Environmental Protection Agency (1999), Federal Guidance Report 13, page 16: \"For example, the ingestion coefficient risk for 40K would not be appropriate for an application to ingestion of 40K in conjunction with an elevated intake of natural potassium. This is because the biokinetic model for potassium used in this document represents the relatively slow removal of potassium (biological half-time 30 days) that is estimated to occur for typical intakes of potassium, whereas an elevated intake of potassium would result in excretion of a nearly equal mass of natural potassium, and hence of 40K, over a short period.\" c. Maggie Koerth-Baker (August 27, 2010). \"Bananas are radioactive—But they aren't a good way to explain radiation exposure\". Retrieved May 25, 2011.. Attributes the title statement to Geoff Meggitt, former UK Atomic Energy Authority. Gullotta DN (2017). \"On Richard Carrier's Doubts: A Response to Richard Carrier's On the Historicity of Jesus: Why We Might Have Reason for Doubt\". Journal for the Study of the Historical Jesus. 15 (2–3): 310–346. doi:10.1163/17455197-01502009. O'Conner PT, Kellerman S (2009). Origins of the Specious: Myths and Misconceptions of the English Language. New York: Random House. ISBN978-1-4000-6660-5. Smith FJ (January 1, 1979). \"Some aspects of the tritone and the semitritone in the Speculum Musicae: the non-emergence of the diabolus in musica\". Journal of Musicological Research. 3 (1–2): 63–74. doi:10.1080/01411897908574507. ISSN0141-1896."}
{"url": "https://en.wikipedia.org/wiki/Cement_shoes", "text": "Cement shoes, concrete shoes, or Chicago overcoat[1] is a method of murder or body disposal, usually associated with criminals such as the Mafia or gangs. It involves weighing down the victim, who may be dead or alive, with concrete and throwing them into water in the hope the body will never be found. In the US, the term has become a tongue-in-cheek euphemism for a threat of death by criminals. While a common trope in fiction, only one real-life case has ever been authenticated. Cement shoes involve first binding, incapacitating or killing the victim and then placing each foot into a bucket or box, which is then filled with wet concrete (a mixture of cement powder, rock, water and sand), or even simply cement powder and water. Typically in films and novels, the victim is still alive as they watch the concrete harden, heightening the torture and drama.[2][3] After the concrete sets, the victim is thrown into a body of water such as a river, lake or the ocean. Despite being a theme in Hollywood movies like Lady in Cement and books like E. L. Doctorow's Billy Bathgate, whether such a cumbersome and time-consuming method of execution was practical remained in question.[2] Cement takes many hours or even days to fully harden and, until 2016, there was never a documented case—although crime historian Thomas Reppetto said there have probably been real-life examples that have never been found.[4] In May 2016, the first and only documented case of \"cement shoes\" was reported. The body of Brooklyn gang member Peter Martinez, aged 28, better known on the streets as Petey Crack, washed up near Manhattan Beach in Brooklyn. His head was wrapped in duct tape, the immediate cause of his death. His feet and shins were encased in concrete set inside a five-gallon bucket. His body floated to the shore due to air in the concrete because it was not given enough time to dry before being thrown into the ocean.[5][6][7] Concrete has been used as a weight to dispose of a body. In 1941, the body of Philadelphiaracketeer Johnnie Goodman was found by crab fisherman in a New Jersey creek, weighed down with an 18-kilogram (40-pound) block of concrete.[2] On August 24, 1964, the body of Ernest Rupolo, aged 52, a trigger man who informed on Vito Genovese in 1944, was found in Jamaica Bay, New York, with concrete blocks tied to his legs.[8] It is also speculated that bootleggerRocco Perri was murdered by being fitted with cement shoes and thrown into Hamilton Harbour in 1944.[9] The French Army allegedly used cement shoes on Algerians who were murdered on so-called \"death flights\" during the Algerian War. The victims were called \"crevettes Bigeard [fr]\" 'Bigeard shrimp' after General Marcel Bigeard, who allegedly ordered the procedure. Bigeard put his victims' feet in a basin, poured quick-setting cement in and threw the person into the sea from the top of a helicopter, said Paul Teitgen, secretary general of the French police in Algiers in 1957, and notable opponent of torture during the war.[10] ^Colleen Long (May 5, 2016). \"Cops seek killer of man who washed ashore in 'cement shoes'\". CBS 3 Philadelphia. AP. Retrieved August 11, 2018. \"There's a lot of urban legend to this — cement shoes, concrete shoes, concrete gloves, whatever you want to call it — but it all has some sort of truth to it,\" said Reppetto, [...] \"It started somewhere real and took off.\""}
{"url": "https://en.wikipedia.org/wiki/Tax_bracket", "text": "Tax brackets are the divisions at which tax rates change in a progressive tax system (or an explicitly regressive tax system, though that is rarer). Essentially, tax brackets are the cutoff values for taxable income—income past a certain point is taxed at a higher rate. Imagine that there are three tax brackets: 10%, 20%, and 30%. The 10% rate applies to income from $1 to $10,000; the 20% rate applies to income from $10,001 to $20,000; and the 30% rate applies to all income above $20,000. Under this system, someone earning $10,000 is taxed at 10%, paying a total of $1,000. Someone earning $5,000 pays $500, and so on. Meanwhile, someone who earns $25,000 faces a more complicated calculation. The rate on the first $10,000 is 10%, from $10,001 to $20,000 is 20%, and above that is 30%. Thus, they pay $1,000 for the first $10,000 of income (10%), $2,000 for the second $10,000 of income (20%), and $1,500 for the last $5,000 of income (30%), In total, they pay $4,500, or an 18% average tax rate. In practice the computation is simplified by using point–slope form or slope–intercept form of the linear equation for the tax on a specific bracket, either as tax on the bottom amount of the bracket plus the tax on the marginal amount within the bracket: Canada's federal government has the following tax brackets for the 2012 tax year (all in Canadian dollars). The \"basic personal amount\" of $15,527 effectively means that income up to this amount is not subject to tax, although it is included in the calculation of taxable income.[2] Personal income tax is progressive in nature. The total rate does not usually exceed 40%. The Swiss Federal Tax Administration website [1] provides a broad outline of the Swiss tax system, and full details and tax tables are available in PDF documents. The complexity of the system is partly because the Confederation, the 26 Cantons that make up the federation, and about 2 900 communes [municipalities] levy their own taxes based on the Federal Constitution and 26 Cantonal Constitutions. Gross salary is the amount your employer pays you, plus your income tax liability. Although the tax itself is included in this figure, it is typically the one used when discussing one's pay. For example, John gets paid $50/hour as an administrative director. His annual gross salary is $50/hour x 2,000 hours/year = $100,000/year. Of this, some is paid to John, and the rest to taxes. W-2 wages are the wages that appear on the employee's W-2 issued by his employer each year in January. A copy of the W-2 is sent to the Internal Revenue Service (IRS). It is the gross salary less any contributions to pre-tax plans. The W-2 form also shows the amount withheld by the employer for federal income tax. W-2 wages = gross salary less (contributions to employer retirement plan) less (contributions to employer health plan) less (contributions to some other employer plans) Total income is the sum of all taxable income, including the W-2 wages. Almost all income is taxable. There are a few exemptions for individuals such as non-taxable interest on government bonds, a portion of the Social Security (SS) income (not the payments to SS, but the payments from SS to the individual), etc. Adjusted gross income (AGI) is Total Income less some specific allowed deductions. Such as; alimony paid (income to the recipient), permitted moving expenses, self-employed retirement program, student loan interest, etc. Itemized deductions are other specific deductions such as; mortgage interest on a home, state income taxes or sales taxes, local property taxes, charitable contributions, state income tax withheld, etc. Standard deduction is a sort of minimum itemized deduction. If you add up all your itemized deductions and it is less than the standard deduction you take the standard deduction. In 2007 this was $5,350 for those filing individually and $10,700 for married filing jointly. Personal exemption is a tax exemption in which the taxpayer may deduct an amount from their gross income for each dependent they claim. It was $3,400 in 2007. Given the complexity of the United States' income tax code, individuals often find it necessary to consult a tax accountant or professional tax preparer. For example, John, a married 44-year-old who has two children, earned a gross salary of $100,000 in 2007. He contributes the maximum $15,500 per year to his employer's 401(k) retirement plan, pays $1,800 per year for his employer's family health plan, and $500 per year to his employer's Flexfund medical expense plan. All of the plans are allowed pre-tax contributions. Gross pay = $100,000 W-2 wages = $100,000 – $15,500 – $1,800 – $500 = $82,200 John's and his wife's other income is $12,000 from John's wife's wages (she also got a W-2 but had no pre-tax contributions), $200 interest from a bank account, and a $150 state tax refund. Total Income = $82,200 + $12,000 + $200 + $150 = $94,550. John's employer reassigned John to a new office and his moving expenses were $8,000, of which $2,000 was not reimbursed by his employer. John had four personal exemptions—himself, his wife and two children. His total personal exemptions were 4 x $3,400 = $13,600. Taxable Income = $92,550 – $22,300 – $13,600 = $56,650. The tax on the Taxable Income is found in a Tax Table if the Taxable Income is less than $100,000 and is computed if over $100,000. Both are used. The Tax Tables are in the 2007 1040 Instructions. The Tax Tables list income in $50 increments for all categories of taxpayers, single, married filing jointly, married filing separately, and head of household. For the Taxable Income range of \"at least $56,650 but less than $56,700\" the tax is $7,718 for a taxpayer who is married filing jointly. In addition to the Federal income tax, John probably pays state income tax, Social Security tax, and Medicare tax. The Social Security tax in 2007 for John is 6.2% on the first $97,500 of earned income (wages), or a maximum of $6,045. There are no exclusions from earned income for Social Security so John pays the maximum of $6,045. His wife pays $12,000 x 6.2% = $744. Medicare is 1.45% on all earned income with no maximum. John and his wife pays $112,000 x 1.45% = $1,624 for Medicare in 2007. Most states also levy income tax, exceptions being Alaska, Florida, Nevada, South Dakota, Texas, Washington, New Hampshire, Tennessee and Wyoming.[13]"}
{"url": "https://en.wikipedia.org/wiki/Matthew_Bishop_(journalist)", "text": "He is the also co-author of several books with Michael Green, including: \"The Road from Ruin: How to Renew Capitalism and Put America Back on Top\",[3] \"Philanthrocapitalism: How Giving Can Save the World\", and the e-book \"In Gold We Trust\".[4]"}
{"url": "https://en.wikipedia.org/wiki/JSTOR_(identifier)", "text": "William G. Bowen, president of Princeton University from 1972 to 1988,[5] founded JSTOR in 1994. JSTOR was originally conceived as a solution to one of the problems faced by libraries, especially research and university libraries, due to the increasing number of academic journals in existence. Most libraries found it prohibitively expensive in terms of cost and space to maintain a comprehensive collection of journals. By digitizing many journal titles, JSTOR allowed libraries to outsource the storage of journals with the confidence that they would remain available long-term. Online access and full-text searchability improved access dramatically.[6] Bowen initially considered using CD-ROMs for distribution.[7] However, Ira Fuchs, Princeton University's vice president for Computing and Information Technology, convinced Bowen that CD-ROM was becoming an increasingly outdated technology and that network distribution could eliminate redundancy and increase accessibility. (For example, all Princeton's administrative and academic buildings were networked by 1989; the student dormitory network was completed in 1994; and campus networks like the one at Princeton were, in turn, linked to larger networks such as BITNET and the Internet.) JSTOR was initiated in 1995 at seven different library sites, and originally encompassed ten economics and history journals. JSTOR access improved based on feedback from its initial sites, and it became a fully searchable index accessible from any ordinary web browser. Special software was put in place to make pictures and graphs clear and readable.[8] With the success of this limited project, Bowen and Kevin Guthrie, the then-president of JSTOR, wanted to expand the number of participating journals. They met with representatives of the Royal Society of London and an agreement was made to digitize the Philosophical Transactions of the Royal Society dating from its beginning in 1665. The work of adding these volumes to JSTOR was completed by December 2000.[8] In 1999 JSTOR started a partnership with Joint Information Systems Committee and created a mirror website at the University of Manchester to make the JSTOR database available to over 20 higher education institutions in England, Scotland, Wales and Northern Ireland.[9] JSTOR content is provided by more than 900 publishers.[12] The database contains more than 12 million journal articles, in more than 75 disciplines.[10] Each object is uniquely identified by an integer value, starting at 1 which is used to create a stable URL.[13] In addition to the main site, the JSTOR labs group operates an open service that allows access to the contents of the archives for the purposes of corpus analysis at its Data for Research service.[14] This site offers a search facility with graphical indication of the article coverage and loose integration into the main JSTOR site. Users may create focused sets of articles and then request a dataset containing word and n-gram frequencies and basic metadata. They are notified when the dataset is ready and may download it in either XML or CSV formats. The service does not offer full-text, although academics may request that from JSTOR, subject to a non-disclosure agreement.[citation needed] JSTOR Plant Science[15] is available in addition to the main site. JSTOR Plant Science provides access to content such as plant type specimens, taxonomic structures, scientific literature, and related materials and aimed at those researching, teaching, or studying botany, biology, ecology, environmental, and conservation studies. The materials on JSTOR Plant Science are contributed through the Global Plants Initiative (GPI)[16] and are accessible only to JSTOR and GPI members. Two partner networks are contributing to this: the African Plants Initiative, which focuses on plants from Africa, and the Latin American Plants Initiative, which contributes plants from Latin America.[citation needed] JSTOR launched its Books at JSTOR program in November 2012, adding 15,000 current and backlist books to its site. The books are linked with reviews and from citations in journal articles.[17] In September 2014, JSTOR launched JSTOR Daily, an online magazine meant to bring academic research to a broader audience. Posted articles are generally based on JSTOR entries, and some entries provide the backstory to current events.[18] Reveal Digital is a JSTOR-hosted collection of documents produced by or about underground, marginalized and dissenting 20th century communities.[19] Reveal Digital's open access content includes zines, prison newspapers, AIDS art, student-movement documents, black civil rights materials, and a white supremacy archive.[19] JSTOR is licensed mainly to academic institutions, public libraries, research institutions, museums, and schools. More than 7,000 institutions in more than 150 countries have access.[3] JSTOR has been running a pilot program of allowing subscribing institutions to provide access to their alumni, in addition to current students and staff. The Alumni Access Program officially launched in January 2013.[20] Individual subscriptions also are available to certain journal titles through the journal publisher.[21] Every year, JSTOR blocks 150 million attempts by non-subscribers to read articles.[22] Inquiries have been made about the possibility of making JSTOR open access. According to Harvard Law professor, JSTOR had been asked \"how much would it cost to make this available to the whole world, how much would we need to pay.\".[23] In late 2010 and early 2011, Aaron Swartz, an American computer programmer, writer, political organizer and Internet activist, used MIT's data network to bulk-download a substantial portion of JSTOR's collection of academic journal articles.[24][25] When the bulk-download was discovered, a video camera was placed in the room to film the mysterious visitor and the relevant computer was left untouched. Once video was captured of the visitor, the download was stopped and Swartz was identified. Rather than pursue a civil lawsuit against him, in June 2011 JSTOR reached a settlement wherein Swartz surrendered the downloaded data.[24][25] The following month, federal authorities charged Swartz with several data theft-related crimes, including wire fraud, computer fraud, unlawfully obtaining information from a protected computer, and recklessly damaging a protected computer.[26][27] Prosecutors in the case claimed that Swartz acted with the intention of making the papers available on P2P file-sharing sites.[25][28] Swartz surrendered to authorities, pleaded not guilty to all counts, and was released on $100,000 bail. In September 2012, U.S. attorneys increased the number of charges against Swartz from four to thirteen, with a possible penalty of 35 years in prison and $1 million in fines.[29][30] The case still was pending when Swartz died by suicide in January 2013.[31] The availability of most journals on JSTOR is controlled by a \"moving wall\", which is an agreed-upon delay between the current volume of the journal and the latest volume available on JSTOR. This time period is specified by agreement between JSTOR and the publisher of the journal, which usually is three to five years. Publishers may request that the period of a \"moving wall\" be changed or request discontinuation of coverage. Formerly, publishers also could request that the \"moving wall\" be changed to a \"fixed wall\"—a specified date after which JSTOR would not add new volumes to its database. As of November 2010[update], \"fixed wall\" agreements were still in effect with three publishers of 29 journals made available[needs update] online through sites controlled by the publishers.[32] In 2010, JSTOR started adding current issues of certain journals through its Current Scholarship Program.[33] Beginning September 6, 2011, JSTOR made public domain content available at no charge to the public.[34][35] This \"Early Journal Content\" program constitutes about 6% of JSTOR's total content, and includes over 500,000 documents from more than 200 journals that were published before 1923 in the United States, and before 1870 in other countries.[34][35][36] JSTOR stated that it had been working on making this material free for some time. The Swartz controversy and Greg Maxwell's protest torrent of the same content led JSTOR to \"press ahead\" with the initiative.[34][35] As of 2017[update], JSTOR does not have plans to extend it to other public domain content, stating that \"We do not believe that just because something is in the public domain, it can always be provided for free\".[37] In January 2012, JSTOR started a pilot program, \"Register & Read\", offering limited no-cost access (not open access) to archived articles for individuals who register for the service. At the conclusion of the pilot, in January 2013, JSTOR expanded Register & Read from an initial 76 publishers to include about 1,200 journals from over 700 publishers.[38] Registered readers may read up to six articles online every calendar month, but may not print or download PDFs.[39] In 2013, more than 8,000 institutions in more than 160 countries had access to JSTOR.[12] In 2012, JSTOR users performed nearly 152 million searches, with more than 113 million article views and 73.5 million article downloads.[12] JSTOR has been used as a resource for linguistics research to investigate trends in language use over time and also to analyze gender differences and inequities in scholarly publishing, revealing that in certain.[42][43][44] Gauger, Barbara J.; Kacena, Carolyn (2006). \"JSTOR usage data and what it can tell us about ourselves: is there predictability based on historical use by libraries of similar size?\". OCLC Systems & Services. 22 (1): 43–55. doi:10.1108/10650750610640801."}
{"url": "https://en.wikipedia.org/wiki/Gambler%27s_fallacy", "text": "Mistaken belief that more frequent chance events will lead to less frequent chance events The gambler's fallacy, also known as the Monte Carlo fallacy or the fallacy of the maturity of chances, is the belief that, if an event (whose occurrences are independent and identically distributed) has occurred more frequently than expected, it is less likely to happen again in the future (or vice versa). The fallacy is commonly associated with gambling, where it may be believed, for example, that the next dice roll is more than usually likely to be six because there have recently been fewer than the expected number of sixes. The term \"Monte Carlo fallacy\" originates from an example of the phenomenon, in which the roulette wheel spun black 26 times in succession at the Monte Carlo Casino in 1913.[1] Over time, the proportion of red/blue coin tosses approaches 50-50, but the difference decreases to zero non-systematically. The gambler's fallacy can be illustrated by considering the repeated toss of a fair coin. The outcomes in different tosses are statistically independent and the probability of getting heads on a single toss is 1/2 (one in two). The probability of getting two heads in two tosses is 1/4 (one in four) and the probability of getting three heads in three tosses is 1/8 (one in eight). In general, if Ai is the event where toss i of a fair coin comes up heads, then: If after tossing four heads in a row, the next coin toss also came up heads, it would complete a run of five successive heads. Since the probability of a run of five successive heads is 1/32 (one in thirty-two), a person might believe that the next flip would be more likely to come up tails rather than heads again. This is incorrect and is an example of the gambler's fallacy. The event \"5 heads in a row\" and the event \"first 4 heads, then a tails\" are equally likely, each having probability 1/32. Since the first four tosses turn up heads, the probability that the next toss is a head is: While a run of five heads has a probability of 1/32 = 0.03125 (a little over 3%), the misunderstanding lies in not realizing that this is the case only before the first coin is tossed. After the first four tosses in this example, the results are no longer unknown, so their probabilities are at that point equal to 1 (100%). The probability of a run of coin tosses of any length continuing for one more toss is always 0.5. The reasoning that a fifth toss is more likely to be tails because the previous four tosses were heads, with a run of luck in the past influencing the odds in the future, forms the basis of the fallacy. If a fair coin is flipped 21 times, the probability of 21 heads is 1 in 2,097,152. The probability of flipping a head after having already flipped 20 heads in a row is 1/2. Assuming a fair coin: The probability of 20 heads, then 1 tail is 0.520 × 0.5 = 0.521 The probability of 20 heads, then 1 head is 0.520 × 0.5 = 0.521 The probability of getting 20 heads then 1 tail, and the probability of getting 20 heads then another head are both 1 in 2,097,152. When flipping a fair coin 21 times, the outcome is equally likely to be 21 heads as 20 heads and then 1 tail. These two outcomes are equally as likely as any of the other combinations that can be obtained from 21 flips of a coin. All of the 21-flip combinations will have probabilities equal to 0.521, or 1 in 2,097,152. Assuming that a change in the probability will occur as a result of the outcome of prior flips is incorrect because every outcome of a 21-flip sequence is as likely as the other outcomes. In accordance with Bayes' theorem, the likely outcome of each flip is the probability of the fair coin, which is 1/2. The fallacy leads to the incorrect notion that previous failures will create an increased probability of success on subsequent attempts. For a fair 16-sided die, the probability of each outcome occurring is 1/16 (6.25%). If a win is defined as rolling a 1, the probability of a 1 occurring at least once in 16 rolls is: The probability of a loss on the first roll is 15/16 (93.75%). According to the fallacy, the player should have a higher chance of winning after one loss has occurred. The probability of at least one win is now: By losing one toss, the player's probability of winning drops by two percentage points. With 5 losses and 11 rolls remaining, the probability of winning drops to around 0.5 (50%). The probability of at least one win does not increase after a series of losses; indeed, the probability of success actually decreases, because there are fewer trials left in which to win. The probability of winning will eventually be equal to the probability of winning a single toss, which is 1/16 (6.25%) and occurs when only one toss is left. After a consistent tendency towards tails, a gambler may also decide that tails has become a more likely outcome. This is a rational and Bayesian conclusion, bearing in mind the possibility that the coin may not be fair; it is not a fallacy. Believing the odds to favor tails, the gambler sees no reason to change to heads. However it is a fallacy that a sequence of trials carries a memory of past results which tend to favor or disfavor future outcomes. The inverse gambler's fallacy described by Ian Hacking is a situation where a gambler entering a room and seeing a person rolling a double six on a pair of dice may erroneously conclude that the person must have been rolling the dice for quite a while, as they would be unlikely to get a double six on their first attempt. Researchers have examined whether a similar bias exists for inferences about unknown past events based upon known subsequent events, calling this the \"retrospective gambler's fallacy\".[2] An example of a retrospective gambler's fallacy would be to observe multiple successive \"heads\" on a coin toss and conclude from this that the previously unknown flip was \"tails\".[2] Real world examples of retrospective gambler's fallacy have been argued to exist in events such as the origin of the Universe. In his book Universes, John Leslie argues that \"the presence of vastly many universes very different in their characters might be our best explanation for why at least one universe has a life-permitting character\".[3]Daniel M. Oppenheimer and Benoît Monin argue that \"In other words, the 'best explanation' for a low-probability event is that it is only one in a multiple of trials, which is the core intuition of the reverse gambler's fallacy.\"[2] Philosophical arguments are ongoing about whether such arguments are or are not a fallacy, arguing that the occurrence of our universe says nothing about the existence of other universes or trials of universes.[4][5] Three studies involving Stanford University students tested the existence of a retrospective gamblers' fallacy. All three studies concluded that people have a gamblers' fallacy retrospectively as well as to future events.[2] The authors of all three studies concluded their findings have significant \"methodological implications\" but may also have \"important theoretical implications\" that need investigation and research, saying \"[a] thorough understanding of such reasoning processes requires that we not only examine how they influence our predictions of the future, but also our perceptions of the past.\"[2] In 1796, Pierre-Simon Laplace described in A Philosophical Essay on Probabilities the ways in which men calculated their probability of having sons: \"I have seen men, ardently desirous of having a son, who could learn only with anxiety of the births of boys in the month when they expected to become fathers. Imagining that the ratio of these births to those of girls ought to be the same at the end of each month, they judged that the boys already born would render more probable the births next of girls.\" The expectant fathers feared that if more sons were born in the surrounding community, then they themselves would be more likely to have a daughter. This essay by Laplace is regarded as one of the earliest descriptions of the fallacy.[6] Likewise, after having multiple children of the same sex, some parents may erroneously believe that they are due to have a child of the opposite sex. An example of the gambler's fallacy occurred in a game of roulette at the Monte Carlo Casino on August 18, 1913, when the ball fell in black 26 times in a row. This was an extremely unlikely occurrence: the probability of a sequence of either red or black occurring 26 times in a row is (18/37)26-1 or around 1 in 66.6 million, assuming the mechanism is unbiased. Gamblers lost millions of francs betting against black, reasoning incorrectly that the streak was causing an imbalance in the randomness of the wheel, and that it had to be followed by a long streak of red.[1] The gambler's fallacy does not apply when the probability of different events is not independent. In such cases, the probability of future events can change based on the outcome of past events, such as the statistical permutation of events. An example is when cards are drawn from a deck without replacement. If an ace is drawn from a deck and not reinserted, the next card drawn is less likely to be an ace and more likely to be of another rank. The probability of drawing another ace, assuming that it was the first card drawn and that there are no jokers, has decreased from 4/52 (7.69%) to 3/51 (5.88%), while the probability for each other rank has increased from 4/52 (7.69%) to 4/51 (7.84%). This effect allows card counting systems to work in games such as blackjack. In most illustrations of the gambler's fallacy and the reverse gambler's fallacy, the trial (e.g. flipping a coin) is assumed to be fair. In practice, this assumption may not hold. For example, if a coin is flipped 21 times, the probability of 21 heads with a fair coin is 1 in 2,097,152. Since this probability is so small, if it happens, it may well be that the coin is somehow biased towards landing on heads, or that it is being controlled by hidden magnets, or similar.[7] In this case, the smart bet is \"heads\" because Bayesian inference from the empirical evidence — 21 heads in a row — suggests that the coin is likely to be biased toward heads. Bayesian inference can be used to show that when the long-run proportion of different outcomes is unknown but exchangeable (meaning that the random process from which the outcomes are generated may be biased but is equally likely to be biased in any direction) and that previous observations demonstrate the likely direction of the bias, the outcome which has occurred the most in the observed data is the most likely to occur again.[8] For example, if the a priori probability of a biased coin is say 1%, and assuming that such a biased coin would come down heads say 60% of the time, then after 21 heads the probability of a biased coin has increased to about 32%. If external factors are allowed to change the probability of the events, the gambler's fallacy may not hold. For example, a change in the game rules might favour one player over the other, improving his or her win percentage. Similarly, an inexperienced player's success may decrease after opposing teams learn about and play against their weaknesses. This is another example of bias. The gambler's fallacy arises out of a belief in a law of small numbers, leading to the erroneous belief that small samples must be representative of the larger population. According to the fallacy, streaks must eventually even out in order to be representative.[9]Amos Tversky and Daniel Kahneman first proposed that the gambler's fallacy is a cognitive bias produced by a psychological heuristic called the representativeness heuristic, which states that people evaluate the probability of a certain event by assessing how similar it is to events they have experienced before, and how similar the events surrounding those two processes are.[10][9] According to this view, \"after observing a long run of red on the roulette wheel, for example, most people erroneously believe that black will result in a more representative sequence than the occurrence of an additional red\",[10] so people expect that a short run of random outcomes should share properties of a longer run, specifically in that deviations from average should balance out. When people are asked to make up a random-looking sequence of coin tosses, they tend to make sequences where the proportion of heads to tails stays closer to 0.5 in any short segment than would be predicted by chance, a phenomenon known as insensitivity to sample size.[11] Kahneman and Tversky interpret this to mean that people believe short sequences of random events should be representative of longer ones.[9] The representativeness heuristic is also cited behind the related phenomenon of the clustering illusion, according to which people see streaks of random events as being non-random when such streaks are actually much more likely to occur in small samples than people expect.[12] The gambler's fallacy can also be attributed to the mistaken belief that gambling, or even chance itself, is a fair process that can correct itself in the event of streaks, known as the just-world hypothesis.[13] Other researchers believe that belief in the fallacy may be the result of a mistaken belief in an internal locus of control. When a person believes that gambling outcomes are the result of their own skill, they may be more susceptible to the gambler's fallacy because they reject the idea that chance could overcome skill or talent.[14] Some researchers believe that it is possible to define two types of gambler's fallacy: type one and type two. Type one is the classic gambler's fallacy, where individuals believe that a particular outcome is due after a long streak of another outcome. Type two gambler's fallacy, as defined by Gideon Keren and Charles Lewis, occurs when a gambler underestimates how many observations are needed to detect a favorable outcome, such as watching a roulette wheel for a length of time and then betting on the numbers that appear most often. For events with a high degree of randomness, detecting a bias that will lead to a favorable outcome takes an impractically large amount of time and is very difficult, if not impossible, to do.[15] The two types differ in that type one wrongly assumes that gambling conditions are fair and perfect, while type two assumes that the conditions are biased, and that this bias can be detected after a certain amount of time. Another variety, known as the retrospective gambler's fallacy, occurs when individuals judge that a seemingly rare event must come from a longer sequence than a more common event does. The belief that an imaginary sequence of die rolls is more than three times as long when a set of three sixes is observed as opposed to when there are only two sixes. This effect can be observed in isolated instances, or even sequentially. Another example would involve hearing that a teenager has unprotected sex and becomes pregnant on a given night, and concluding that she has been engaging in unprotected sex for longer than if we hear she had unprotected sex but did not become pregnant, when the probability of becoming pregnant as a result of each intercourse is independent of the amount of prior intercourse.[16] Another psychological perspective states that gambler's fallacy can be seen as the counterpart to basketball's hot-hand fallacy, in which people tend to predict the same outcome as the previous event - known as positive recency - resulting in a belief that a high scorer will continue to score. In the gambler's fallacy, people predict the opposite outcome of the previous event - negative recency - believing that since the roulette wheel has landed on black on the previous six occasions, it is due to land on red the next. Ayton and Fischer have theorized that people display positive recency for the hot-hand fallacy because the fallacy deals with human performance, and that people do not believe that an inanimate object can become \"hot.\"[17] Human performance is not perceived as random, and people are more likely to continue streaks when they believe that the process generating the results is nonrandom.[18] When a person exhibits the gambler's fallacy, they are more likely to exhibit the hot-hand fallacy as well, suggesting that one construct is responsible for the two fallacies.[14] The difference between the two fallacies is also found in economic decision-making. A study by Huber, Kirchler, and Stockl in 2010 examined how the hot hand and the gambler's fallacy are exhibited in the financial market. The researchers gave their participants a choice: they could either bet on the outcome of a series of coin tosses, use an expert opinion to sway their decision, or choose a risk-free alternative instead for a smaller financial reward. Participants turned to the expert opinion to make their decision 24% of the time based on their past experience of success, which exemplifies the hot-hand. If the expert was correct, 78% of the participants chose the expert's opinion again, as opposed to 57% doing so when the expert was wrong. The participants also exhibited the gambler's fallacy, with their selection of either heads or tails decreasing after noticing a streak of either outcome. This experiment helped bolster Ayton and Fischer's theory that people put more faith in human performance than they do in seemingly random processes.[19] The desire to continue gambling or betting is controlled by the striatum, which supports a choice-outcome contingency learning method. The striatum processes the errors in prediction and the behavior changes accordingly. After a win, the positive behavior is reinforced and after a loss, the behavior is conditioned to be avoided. In individuals exhibiting the gambler's fallacy, this choice-outcome contingency method is impaired, and they continue to make risks after a series of losses.[20] The gambler's fallacy is a deep-seated cognitive bias and can be very hard to overcome. Educating individuals about the nature of randomness has not always proven effective in reducing or eliminating any manifestation of the fallacy. Participants in a study by Beach and Swensson in 1967 were shown a shuffled deck of index cards with shapes on them, and were instructed to guess which shape would come next in a sequence. The experimental group of participants was informed about the nature and existence of the gambler's fallacy, and were explicitly instructed not to rely on run dependency to make their guesses. The control group was not given this information. The response styles of the two groups were similar, indicating that the experimental group still based their choices on the length of the run sequence. This led to the conclusion that instructing individuals about randomness is not sufficient in lessening the gambler's fallacy.[21] An individual's susceptibility to the gambler's fallacy may decrease with age. A study by Fischbein and Schnarch in 1997 administered a questionnaire to five groups: students in grades 5, 7, 9, 11, and college students specializing in teaching mathematics. None of the participants had received any prior education regarding probability. The question asked was: \"Ronni flipped a coin three times and in all cases heads came up. Ronni intends to flip the coin again. What is the chance of getting heads the fourth time?\" The results indicated that as the students got older, the less likely they were to answer with \"smaller than the chance of getting tails\", which would indicate a negative recency effect. 35% of the 5th graders, 35% of the 7th graders, and 20% of the 9th graders exhibited the negative recency effect. Only 10% of the 11th graders answered this way, and none of the college students did. Fischbein and Schnarch theorized that an individual's tendency to rely on the representativeness heuristic and other cognitive biases can be overcome with age.[22] Another possible solution comes from Roney and Trick, Gestalt psychologists who suggest that the fallacy may be eliminated as a result of grouping. When a future event such as a coin toss is described as part of a sequence, no matter how arbitrarily, a person will automatically consider the event as it relates to the past events, resulting in the gambler's fallacy. When a person considers every event as independent, the fallacy can be greatly reduced.[23] Roney and Trick told participants in their experiment that they were betting on either two blocks of six coin tosses, or on two blocks of seven coin tosses. The fourth, fifth, and sixth tosses all had the same outcome, either three heads or three tails. The seventh toss was grouped with either the end of one block, or the beginning of the next block. Participants exhibited the strongest gambler's fallacy when the seventh trial was part of the first block, directly after the sequence of three heads or tails. The researchers pointed out that the participants that did not show the gambler's fallacy showed less confidence in their bets and bet fewer times than the participants who picked with the gambler's fallacy. When the seventh trial was grouped with the second block, and was perceived as not being part of a streak, the gambler's fallacy did not occur. Roney and Trick argued that instead of teaching individuals about the nature of randomness, the fallacy could be avoided by training people to treat each event as if it is a beginning and not a continuation of previous events. They suggested that this would prevent people from gambling when they are losing, in the mistaken hope that their chances of winning are due to increase based on an interaction with previous events. Within a real-world setting, numerous studies have uncovered that for various decision makers placed in high stakes scenarios, it is likely they will reflect some degree of strong negative autocorrelation in their judgement. In a study aimed at discovering if the negative autocorrelation that exists with the gambler's fallacy existed in the decision made by U.S. asylum judges, results showed that after two successive asylum grants, a judge would be 5.5% less likely to approve a third grant.[24] In the game of baseball, decisions are made every minute. One particular decision made by umpires which is often subject to scrutiny is the 'strike zone' decision. Whenever a batter does not swing, the umpire must decide if the ball was within a fair region for the batter, known as the strike zone. If outside of this zone, the ball does not count towards outing the batter. In a study of over 12,000 games, results showed that umpires are 1.3% less likely to call a strike if the previous two balls were also strikes.[24] In the decision making of loan officers, it can be argued that monetary incentives are a key factor in biased decision making, rendering it harder to examine the gambler's fallacy effect. However, research shows that loan officers who are not incentivised by monetary gain are 8% less likely to approve a loan if they approved one for the previous client.[25] The effect of gambler's fallacy on lottery selections, based on studies by Dek Terrell. After winning numbers are drawn, lottery players respond by reducing the number of times they select those numbers in following draws. This effect slowly corrects over time, as players become less affected by the fallacy.[26] Lottery play and jackpots entice gamblers around the globe, with the biggest decision for hopeful winners being what numbers to pick. While most people will have their own strategy, evidence shows that after a number is selected as a winner in the current draw, the same number will experience a significant drop in selections in the following lottery. A popular study by Charles Clotfelter and Philip Cook investigated this effect in 1991, where they concluded bettors would cease to select numbers immediately after they were selected, ultimately recovering selection popularity within three months.[27] Soon after, a 1994 study was constructed by Dek Terrell to test the findings of Clotfelter and Cook. The key change in Terrell's study was the examination of a pari-mutuel lottery in which, a number selected with lower total wagers placed on it will result in a higher pay-out. While this examination did conclude that players in both types of lotteries exhibited behaviour in-line with the gambler's fallacy theory, those who took part in pari-mutuel betting seemed to be less influenced.[26] The effect the of gambler's fallacy can be observed as numbers are chosen far less frequently soon after they are selected as winners, recovering slowly over a two-month period. For example, on the 11th of April 1988, 41 players selected 244 as the winning combination. Three days later only 24 individuals selected 244, a 41.5% decrease. This is the gambler's fallacy in motion, as lottery players believe that the occurrence of a winning combination in previous days will decrease its likelihood of occurring today. Several video games feature the use of loot boxes, a collection of in-game items awarded on opening with random contents set by rarity metrics, as a monetization scheme. Since around 2018, loot boxes have come under scrutiny from governments and advocates on the basis they are akin to gambling, particularly for games aimed at youth. Some games use a special \"pity-timer\" mechanism, that if the player has opened several loot boxes in a row without obtaining a high-rarity item, subsequent loot boxes will improve the odds of a higher-rate item drop. This is considered to feed into the gambler's fallacy since it reinforces the idea that a player will eventually obtain a high-rarity item (a win) after only receiving common items from a string of previous loot boxes.[28] ^Fischbein, E.; Schnarch, D. (1997). \"The evolution with age of probabilistic, intuitively based misconceptions\". Journal for Research in Mathematics Education. 28 (1): 96–105. doi:10.2307/749665. JSTOR749665."}
{"url": "https://en.wikipedia.org/wiki/Snopes.com", "text": "In 1994,[8][9][10] David and Barbara Mikkelson created an urban folklore web site that would become Snopes.com. Snopes was an early online encyclopedia focused on urban legends, which mainly presented search results of user discussions based at first on their contributions to the Usenet newsgroup alt.folklore.urban (AFU) where they'd been active.[11] The site grew to encompass a wide range of subjects and became a resource to which Internet users began submitting pictures and stories of questionable veracity. According to the Mikkelsons, Snopes predated the search engine concept of fact-checking via search results.[12] David Mikkelson had originally adopted the username \"Snopes\" (the name of a family of often unpleasant people in the works of William Faulkner) in AFU.[13][14][15][16] In 2002, the site had become known well enough that a television pilot by writer-director Michael Levine called Snopes: Urban Legends was completed with American actor Jim Davidson as host. However, it did not air on major networks.[14] By 2010, the site was attracting seven million to eight million unique visitors in an average month.[17][18] By mid-2014, Barbara had not written for Snopes \"in several years\"[3] and David was forced to hire users from Snopes.com's message board to assist him in running the site. The Mikkelsons divorced around that time.[3][19]Christopher Richmond and Drew Schoentrup became part owners in July 2016 with the purchase of Barbara Mikkelson's share by the internet media management company Proper Media.[20] On March 9, 2017, David Mikkelson terminated the brokering agreement with Proper Media, which is also the company that provides Snopes with web development, hosting, and advertising support.[21] The move prompted Proper Media to stop remitting advertising revenue and to file a lawsuit in May. In late June, Bardav—the company founded by David and Barbara Mikkelson in 2003 to own and operate snopes.com—started a GoFundMe campaign to raise money to continue operations.[22] They raised $500,000 in 24 hours.[23] Later, in August, a judge ordered Proper Media to disburse advertising revenues to Bardav while the case was pending.[24] In July 2018, Snopes abruptly terminated its contract with Managing Editor Brooke Binkowski, with no explanation. By the time Snopes co-founder and CEO David Mikkelson confirmed the termination to her, the situation was still not clear.[25] In early 2019, Snopes announced that it had acquired the website OnTheIssues.org, and is \"hard at work modernizing its extensive archives\".[26]OnTheIssues is a website that seeks to \"present all the relevant evidence, assess how strongly each piece supports or opposes a position, and summarize it with an average\" in order to \"provide voters with reliable information on candidates' policy positions\".[27] In 2018 and 2019, Snopes fact-checked several articles from The Babylon Bee, a satirical website, rating them \"False\". The decision resulted in Facebook adding warnings to links to those articles shared on its site.[28][29][30]Snopes added a new rating called \"Labeled Satire\" to identify satirical stories.[31] In 2019, Snopes was embroiled in legal disputes with Proper Media, with a court case scheduled for spring 2020. By then Proper Media had become a co-owner of Bardav through acquiring Barbara Mikkelson's half-interest share, intending to take overall ownership of Snopes for its own \"portfolio of media sites\". The move failed as David Mikkelson had no intention to sell his share.[32] On August 13, 2021, BuzzFeed News published an investigation by reporter Dean Sterling Jones that showed David Mikkelson had used plagiarized material from different news sources in 54 articles between 2015 and 2019 in an effort to increase website traffic.[35][36][37] Mikkelson also published plagiarized material under a pseudonym, \"Jeff Zarronandia\".[35] The BuzzFeed inquiry prompted Snopes to launch an internal review of Mikkelson's articles and to retract 60 of them the day the BuzzFeed story appeared. Mikkelson admitted to committing \"multiple serious copyright violations\" and apologized for \"serious lapses in judgment.\"[38] He was suspended from editorial duties during the investigation, but remains an officer and stakeholder in the company.[39][38] On September 16, 2022, David Mikkelson stepped down as CEO and was succeeded by shareholder and board member Chris Richmond.[1] Richmond and fellow shareholder Drew Schoentrup together acquired 100% of the company, ending the ownership dispute which began in 2017.[1] Mikkelson has stressed the reference portion of the name Urban Legends Reference Pages, indicating that the intention is not merely to dismiss or confirm misconceptions and rumors but to provide evidence for such debunkings and confirmation as well.[45] Where appropriate, pages are generally marked \"undetermined\" or \"unverifiable\" when there is not enough evidence to either support or disprove a given claim.[46] In an attempt to demonstrate the perils of over-reliance on the Internet as authority, Snopes assembled a series of fabricated urban folklore tales that it termed \"The Repository of Lost Legends\".[47] The name was chosen for its acronym, T.R.O.L.L., a reference to the definition of the word troll, meaning an internet persona intended to be deliberately provocative or incendiary.[15] In 2016, Snopes said that the entirety of its revenue was derived from advertising.[51] In the same year it received an award of $75,000 from the James Randi Educational Foundation, an organization formed to debunk paranormal claims. In 2017, it raised approximately $700,000 from a crowd-sourced GoFundMe effort and received $100,000 from Facebook as a part of a fact-checking partnership.[52]Snopes also offers a premium membership that disables ads.[53] On February 1, 2019, Snopes announced that it had ended its fact-checking partnership with Facebook. Snopes did not rule out the possibility of working with Facebook in the future but said it needed to \"determine with certainty that our efforts to aid any particular platform are a net positive for our online community, publication and staff\". Snopes added that the loss of revenue from the partnership meant the company would \"have less money to invest in our publication—and we will need to adapt to make up for it\".[54][55]"}
{"url": "https://en.wikipedia.org/wiki/Vertebrate", "text": "The vertebrates traditionally include the hagfish, which do not have proper vertebrae due to their loss in evolution,[7] though their closest living relatives, the lampreys, do.[8] Hagfish do, however, possess a cranium. For this reason, the vertebrate subphylum is sometimes referred to as Craniata or \"craniates\" when discussing morphology. Molecular analysis since 1992 has suggested that hagfish are most closely related to lampreys,[9] and so also are vertebrates in a monophyletic sense. Others consider them a sister group of vertebrates in the common taxon of Craniata.[10] A few vertebrates have secondarily lost this feature and retain the notochord into adulthood, such as the sturgeon[15] and coelacanth. Jawed vertebrates are typified by paired appendages (fins or limbs, which may be secondarily lost), but this trait is not required to qualify an animal as a vertebrate. All basal vertebrates are aquatic and breathe with gills. The gills are carried right behind the head, bordering the posterior margins of a series of openings from the pharynx to the exterior. Each gill is supported by a cartilaginous or bony gill arch.[16] The bony fish have three pairs of arches, cartilaginous fish have five to seven pairs, while the primitive jawless fish have seven. The vertebrate ancestor no doubt had more arches than this, as some of their chordate relatives have more than 50 pairs of gills.[14] In jawed vertebrates, the first gill arch pair evolved into the jointedjaws and form an additional oral cavity ahead of the pharynx. Research also suggests that the sixth branchial arch contributed to the formation of the vertebrate shoulder, which separated the head from the body.[17] In amphibians and some primitive bony fishes, the larvae bear external gills, branching off from the gill arches.[18] These are reduced in adulthood, their function taken over by the gills proper in fishes and by lungs in most amphibians. Some amphibians retain the external larval gills in adulthood, the complex internal gill system as seen in fish apparently being irrevocably lost very early in the evolution of tetrapods.[19] In addition to the morphological characteristics used to define vertebrates (i.e. the presence of a notochord, the development of a vertebral column from the notochord, a dorsal nerve cord, pharyngeal gills, a post-anal tail, etc.), molecular markers known as conserved signature indels (CSIs) in protein sequences have been identified and provide distinguishing criteria for the subphylum Vertebrata.[27] Specifically, 5 CSIs in the following proteins: protein synthesis elongation factor-2 (EF-2), eukaryotic translation initiation factor 3 (eIF3), adenosine kinase (AdK) and a protein related to ubiquitin carboxyl-terminal hydrolase are exclusively shared by all vertebrates and reliably distinguish them from all other metazoan.[27] The CSIs in these protein sequences are predicted to have important functionality in vertebrates. Originally, the \"Notochordata hypothesis\" suggested that the Cephalochordata is the sister taxon to Craniata (Vertebrata). This group, called the Notochordata, was placed as sister group to the Tunicata (Urochordata). Although this was once the leading hypothesis,[28] studies since 2006 analyzing large sequencing datasets strongly support Olfactores (tunicates + vertebrates) as a monophyletic clade,[29][30][27] and the placement of Cephalochordata as sister-group to Olfactores (known as the \"Olfactores hypothesis\"). As chordates, they all share the presence of a notochord, at least during a stage of their life cycle. The following cladogram summarizes the systematic relationships between the Olfactores (vertebrates and tunicates) and the Cephalochordata. Vertebrates originated during the Cambrian explosion, which saw a rise in organism diversity. The earliest known vertebrates belongs to the Chengjiang biota[31] and lived about 518 million years ago.[1] These include Haikouichthys, Myllokunmingia,[31]Zhongjianichthys,[32] and probably Haikouella.[33] Unlike the other fauna that dominated the Cambrian, these groups had the basic vertebrate body plan: a notochord, rudimentary vertebrae, and a well-defined head and tail.[34] All of these early vertebrates lacked jaws in the common sense and relied on filter feeding close to the seabed.[35][page needed] A vertebrate group of uncertain phylogeny, small eel-like conodonts, are known from microfossils of their paired tooth segments from the late Cambrian to the end of the Triassic.[36] The Cenozoic world saw great diversification of bony fishes, amphibians, reptiles, birds and mammals.[40][41] Over half of all living vertebrate species (about 32,000 species) are fish (non-tetrapod craniates), a diverse set of lineages that inhabit all the world's aquatic ecosystems, from snow minnows (Cypriniformes) in Himalayan lakes at elevations over 4,600 metres (15,100 feet) to flatfishes (order Pleuronectiformes) in the Challenger Deep, the deepest ocean trench at about 11,000 metres (36,000 feet). Many fish varieties are the main predators in most of the world's freshwater and marine water bodies . The rest of the vertebrate species are tetrapods, a single lineage that includes amphibians (with roughly 7,000 species); mammals (with approximately 5,500 species); and reptiles and birds (with about 20,000 species divided evenly between the two classes). Tetrapods comprise the dominant megafauna of most terrestrial environments and also include many partially or fully aquatic groups (e.g., sea snakes, penguins, cetaceans). Traditional spindle diagram of the evolution of the vertebrates at class level Conventional classification has living vertebrates grouped into seven classes based on traditional interpretations of gross anatomical and physiological traits. This classification is the one most commonly encountered in school textbooks, overviews, non-specialist, and popular works. The extant vertebrates are:[14] Other ways of classifying the vertebrates have been devised, particularly with emphasis on the phylogeny of early amphibians and reptiles. An example based on Janvier (1981, 1997), Shu et al. (2003), and Benton (2004)[44] is given here († = extinct): While this traditional classification is orderly, most of the groups are paraphyletic, i.e. do not contain all descendants of the class's common ancestor.[44] For instance, descendants of the first reptiles include modern reptiles, mammals and birds; the agnathans have given rise to the jawed vertebrates; the bony fishes have given rise to the land vertebrates; the traditional \"amphibians\" have given rise to the reptiles (traditionally including the synapsids or mammal-like \"reptiles\"), which in turn have given rise to the mammals and birds. Most scientists working with vertebrates use a classification based purely on phylogeny,[45] organized by their known evolutionary history and sometimes disregarding the conventional interpretations of their anatomy and physiology. Note that, as shown in the cladogram above, the †\"Ostracodermi\" (armoured jawless fishes) and †\"Placodermi\" (armoured jawed fishes) are shown to be paraphylectic groups, separated from Gnathostomes and Eugnathostomes respectively.[51][52] The placement of hagfish on the vertebrate tree of life has been controversial. Their lack of proper vertebrae (among with other characteristics found in lampreys and jawed vertebrates) led phylogenetic analyses based on morphology to place them outside Vertebrata. Molecular data, however, indicates they are vertebrates closely related to lampreys. A study by Miyashita et al. (2019), 'reconciliated' the two types of analysis as it supports the Cyclostomatahypothesis using only morphological data.[53] During sexual reproduction, mating with a close relative (inbreeding) often leads to inbreeding depression. Inbreeding depression is considered to be largely due to expression of deleterious recessivemutations.[56] The effects of inbreeding have been studied in many vertebrate species. In several species of fish, inbreeding was found to decrease reproductive success.[57][58][59] Inbreeding was observed to increase juvenile mortality in 11 small animal species.[60] A common breeding practice for pet dogs is mating between close relatives (e.g. between half- and full siblings).[61] This practice generally has a negative effect on measures of reproductive success, including decreased litter size and puppy survival.[62][63][64] Incestuous matings in birds result in severe fitness costs due to inbreeding depression (e.g. reduction in hatchability of eggs and reduced progeny survival).[65][66][67] As a result of the negative fitness consequences of inbreeding, vertebrate species have evolved mechanisms to avoid inbreeding. Numerous inbreeding avoidance mechanisms operating prior to mating have been described. Toads and many other amphibians display breeding site fidelity. Individuals that return to natal ponds to breed will likely encounter siblings as potential mates. Although incest is possible, Bufo americanus siblings rarely mate.[68] These toads likely recognize and actively avoid close kin as mates. Advertisement vocalizations by males appear to serve as cues by which females recognize their kin.[68] Inbreeding avoidance mechanisms can also operate subsequent to copulation. In guppies, a post-copulatory mechanism of inbreeding avoidance occurs based on competition between sperm of rival males for achieving fertilization.[69] In competitions between sperm from an unrelated male and from a full sibling male, a significant bias in paternity towards the unrelated male was observed.[69] When female sand lizards mate with two or more males, sperm competition within the female's reproductive tract may occur. Active selection of sperm by females appears to occur in a manner that enhances female fitness.[70] On the basis of this selective process, the sperm of males that are more distantly related to the female are preferentially used for fertilization, rather than the sperm of close relatives.[70] This preference may enhance the fitness of progeny by reducing inbreeding depression. Mating with unrelated or distantly related members of the same species is generally thought to provide the advantage of masking deleterious recessive mutations in progeny[71] (see heterosis). Vertebrates have evolved numerous diverse mechanisms for avoiding close inbreeding and promoting outcrossing[72] (see inbreeding avoidance). Outcrossing as a way of avoiding inbreeding depression has been especially well studied in birds. For instance, inbreeding depression occurs in the great tit (Parus major) when the offspring are produced as a result of a mating between close relatives. In natural populations of the great tit, inbreeding is avoided by dispersal of individuals from their birthplace, which reduces the chance of mating with a close relative.[73] Southern pied babblers (Turdoides bicolor) appear to avoid inbreeding in two ways: through dispersal and by avoiding familiar group members as mates.[74] Although both genders disperse locally, they move outside the range where genetically related individuals are likely to be encountered. Within their group, individuals only acquire breeding positions when the opposite-sex breeder is unrelated. Cooperative breeding in birds typically occurs when offspring, usually males, delay dispersal from their natal group in order to remain with the family to help rear younger kin.[75] Female offspring rarely stay at home, dispersing over distances that allow them to breed independently or to join unrelated groups. Mole salamanders are an ancient (2.4–3.8 million year-old) unisexual vertebrate lineage.[78] In the polyploid unisexual mole salamander females, a premeiotic endomitotic event doubles the number of chromosomes. As a result, the mature eggs produced subsequent to the two meiotic divisions have the same ploidy as the somatic cells of the female salamander. Synapsis and recombination during meiotic prophase I in these unisexual females is thought to ordinarily occur between identical sister chromosomes and occasionally between homologous chromosomes. Thus little, if any, genetic variation is produced. Recombination between homeologous chromosomes occurs only rarely, if at all.[79] Since production of genetic variation is weak, at best, it is unlikely to provide a benefit sufficient to account for the long-term maintenance of meiosis in these organisms.[citation needed] Two killifish species, the mangrove killifish (Kryptolebias marmoratus) and Kryptolebias hermaphroditus, are the only known vertebrates to self-fertilize.[80] They produce eggs and sperm by meiosis and routinely reproduce by self-fertilisation. This capacity has apparently persisted for at least several hundred thousand years.[81] Each individual hermaphrodite normally fertilizes itself through uniting inside the fish's body of an egg and a sperm that it has produced by an internal organ.[82] In nature, this mode of reproduction can yield highly homozygous lines composed of individuals so genetically uniform as to be, in effect, identical to one another.[83][84] Although inbreeding, especially in the extreme form of self-fertilization, is ordinarily regarded as detrimental because it leads to expression of deleterious recessive alleles, self-fertilization does provide the benefit of fertilization assurance (reproductive assurance) at each generation.[83]"}
{"url": "https://en.wikipedia.org/wiki/Notebook_for_Anna_Magdalena_Bach", "text": "The two notebooks are known by their title page dates of 1722 and 1725. The title \"Anna Magdalena Notebook\" is commonly used to refer to the latter. The primary difference between the two collections is that the 1722 notebook contains works only by Johann Sebastian Bach (including most of the French Suites), while the 1725 notebook is a compilation of music by both Bach and other composers of the era.[1][2] Title page of the first (1722) Notebook for Anna Magdalena Bach. Note the titles of the three Pfeiffer books written by Bach in the lower right corner. This notebook contains 25 unbound sheets (including two blank pages), which is estimated to be approximately a third of the original size. It is not known what happened to the other pages. The back and the corners are decorated with brown leather; greenish paper is used for the cover. The title page is inscribed Clavier-Büchlein vor Anna Magdalena Bachin ANNO 1722 in Anna Magdalena's hand (using the feminine version of her last name). For a reason so far unknown to researchers, Johann Sebastian wrote the titles of three books by theologianAugust Pfeiffer [de] (died 1698) in the lower right corner of the title page: Ante Calvinismus is a shortened and misspelled title of Anti-Calvinismus, oder Unterredungen von der Reformierten Religion (Anti-Calvinism, or Conversations about the reformed religion). AntiMelancholicus refers to Anti-melancholicus, oder Melancholey-Vertreiber (Anti-melancholy, or [something or someone] to drive out the melancholy]). The notebook contains the following works, most in Johann Sebastian's hand: Five keyboardsuites. The first three are fragments of the pieces that are now known as the first three French Suites, BWV 812–814. The next two are complete suites, French Suites Nos. 4 and 5, BWV 815–816. The minuets of suites 2 and 3 are separated from the rest of their respective suites and were most probably added at a later date by Anna Magdalena Bach (they are almost certainly in her hand), some time before 1725. The 1725 notebook is larger than the 1722 one, and more richly decorated. Light green paper is used for the front cover, Anna Magdalena's initials and the year number \"1725\" are printed in gold, the annotations A[nna] M[agdal] B[ach] added by her stepson C. P. E. Bach when he inherited it. All pages feature gilt edging. Most of the entries in the 1725 notebook were made by Anna Magdalena herself, with others written in the hand of Johann Sebastian, some by sons Johann Christian and Carl Philipp Emanuel, and a few by family friends such as Johann Gottfried Bernhard and Johann Gottfried Heinrich. Although the 1725 notebook does contain work composed by Johann Sebastian Bach, it also includes works by many other composers. The authorship of several pieces is identified in the notebook itself, while that of others was established by researchers. The composers of still others, including several popular songs of the time, remain unknown. Here is a complete list of the pieces included, in order of appearance in the notebook: Keyboard partita in A minor, BWV 827. This is the third partita from Bach's set of Partitas for keyboard BWV 825–830, which was published in 1731 as the first volume of Clavier-Übung. Keyboard partita in E minor, BWV 830. This is the sixth partita from Bach's set of Partitas for keyboard BWV 825–830. Keyboard aria in G major, BWV 988/1. Another well-known piece, this is the aria of the Goldberg Variations, BWV 988. Christoph Wolff has suggested that this Aria was entered into the two blank pages of this book by Anna Magdalena later, in 1740. This section needs expansion. You can help by adding to it. (September 2016) The Notebooks contain instrumental as well as vocal musical compositions. These notebooks serve more as collections of sheet music and other compositions rather than what notebooks are traditionally used for. The two Minuets in G major and G minor, Nos. 4–5 in the second Notebook, BWV Anh. 114 and 115, were composed by Christian Petzold. Because their former attribution to Bach is spurious they appear in Anh. III of the Bach-Werke-Verzeichnis.[32] No. 21 of the second Notebook, \"Menuet fait par Mons. Böhm\" (Minuet by Mr. Böhm), in G major, was never assigned a number in the BWV catalogue. It is however included in both the Bach-Gesellschaft Ausgabe (BGA) and the New Bach Edition (Neue Bach-Ausgabe, NBA) of the Notebooks. There is some doubt which composer by the name Böhm may have been intended, Georg Böhm being the best known among them.[2][52][53][54]"}
{"url": "https://en.wikipedia.org/wiki/Latin_America", "text": "The region covers an area that stretches from Mexico to Tierra del Fuego and includes much of the Caribbean. It has an area of approximately 19,197,000 km2 (7,412,000 sq mi),[1] almost 13% of the Earth's land surface area. In 2019, Latin America had a combined nominal GDP of US$5,188,250 trillion[9] and a GDP PPP of US$10,284,588 trillion.[9][10] The concept and term came into use in the mid-nineteenth century. Gobat states, \"the idea did stem from the French concept of a “Latin race,” which Latin American émigrés in Europe helped spread to the other side of the Atlantic.\"[11] It was popularized in 1860s France during the reign of Napoleon III. The term Latin America was a part of his attempt to create a French empire in the Americas.[12] Research has shown that the idea that a part of the Americas has a linguistic and cultural affinity with the Romance cultures as a whole can be traced back to the 1830s, in the writing of the French Saint-SimonianMichel Chevalier, who postulated that a part of the Americas was inhabited by people of a \"Latin race\", and that it could, therefore, ally itself with \"Latin Europe\", ultimately overlapping the Latin Church, in a struggle with \"Teutonic Europe\" and \"Anglo-Saxon America\" with its Anglo-Saxonism, as well as \"Slavic Europe\" with its Pan-Slavism.[13] Historian John Leddy Phelan located the popularity of the term Latin America to be from the French occupation of Mexico. His argument is that French imperialists used the concept of \"Latin\" America as a way to counter British imperialism, as well as to challenge the German threat to France.[14] The idea of a \"Latin race\" was then taken up by Latin American intellectuals and political leaders of the mid-and late-nineteenth century, who no longer looked to Spain or Portugal as cultural models, but rather to France.[15] Napoleon III had a strong interest in extending French commercial and political power in the region. He and his business promoter Felix Belly called it \"Latin America\" to emphasize the shared Latin background of France with the former viceroyalties of Spain and colonies of Portugal. This led to Napoleon III's failed attempt to take military control of Mexico in the 1860s.[8] Scholarship has political origins of the term. Two Latin American historians, UruguayanArturo Ardao and ChileanMiguel Rojas Mix, found evidence that the term \"Latin America\" was used earlier than Phelan claimed, and the first use of the term was in fact in opposition to imperialist projects in the Americas. Ardao wrote about this subject in his book Génesis de la idea y el nombre de América latina (Genesis of the Idea and the Name of Latin America, 1980),[16] and Miguel Rojas Mix in his article \"Bilbao y el hallazgo de América latina: Unión continental, socialista y libertaria\" (Bilbao and the Finding of Latin America: a Continental, Socialist, and Libertarian Union, 1986).[17] As Michel Gobat points out in his article \"The Invention of Latin America: A Transnational History of Anti-Imperialism, Democracy, and Race\", \"Arturo Ardao, Miguel Rojas Mix, and Aims McGuinness have revealed [that] the term 'Latin America' had already been used in 1856 by Central Americans and South Americans protesting US expansion into the Southern Hemisphere\".[18] Edward Shawcross summarizes Ardao's and Rojas Mix's findings in the following way: \"Ardao identified the term in a poem by a Colombian diplomat and intellectual resident in France, José María Torres Caicedo, published on 15 February 1857 in a French based Spanish-language newspaper, while Rojas Mix located it in a speech delivered in France by the radical liberal Chilean politician Francisco Bilbao in June 1856\".[19] By the late 1850s, the term was being used in California (which had become a part of the United States), in local newspapers such as El Clamor Público by Californios writing about América latina and latinoamérica, and identifying as Latinos as the abbreviated term for their \"hemispheric membership in la raza latina\".[20] The words \"Latin\" and \"America\" were first found to be combined in a printed work to produce the term \"Latin America\" in 1856 at a conference by the Chilean politician Francisco Bilbao in Paris.[21] The conference had the title \"Initiative of the America. The idea for a Federal Congress of Republics.\"[7] The following year, Colombian writer José María Torres Caicedo also used the term in his poem \"The Two Americas\".[22] Two events related with the United States played a central role in both works. The first event happened less than a decade before the publication of Bilbao's and Torres Caicedo's works: the Invasion of Mexico or, in the US, the Mexican–American War, after which Mexico lost a third of its territory. The second event, the Walker affair, which happened the same year that both works were written: the decision by US president Franklin Pierce to recognize the regime recently established in Nicaragua by American William Walker and his band of filibusters who ruled Nicaragua for nearly a year (1856–57) and attempted to reinstate slavery there, where it had been already abolished for three decades[citation needed] In both Bilbao's and Torres Caicedo's works, the Mexican–American War (1846–48) and William Walker's expedition to Nicaragua are explicitly mentioned as examples of dangers for the region. For Bilbao, \"Latin America\" was not a geographical concept, as he excluded Brazil, Paraguay, and Mexico. Both authors also asked for the union of all Latin American countries as the only way to defend their territories against further foreign US interventions. Both also rejected European imperialism, claiming that the return of European countries to non-democratic forms of government was another danger for Latin American countries, and used the same word to describe the state of European politics at the time: \"despotism.\" Several years later, during the French invasion of Mexico, Bilbao wrote another work, \"Emancipation of the Spirit in America\", where he asked all Latin American countries to support the Mexican cause against France, and rejected French imperialism in Asia, Africa, Europe and the Americas. He asked Latin American intellectuals to search for their \"intellectual emancipation\" by abandoning all French ideas, claiming that France was: \"Hypocrite, because she [France] calls herself protector of the Latin race just to subject it to her exploitation regime; treacherous, because she speaks of freedom and nationality, when, unable to conquer freedom for herself, she enslaves others instead!\" Therefore, as Michel Gobat puts it, the term Latin America itself had an \"anti-imperial genesis,\" and their creators were far from supporting any form of imperialism in the region, or in any other place of the globe.[23] In France, the term Latin America was used with the opposite intention. It was employed by the French Empire of Napoleon III during the French invasion of Mexico as a way to include France among countries with influence in the Americas and to exclude Anglophone countries. It played a role in his campaign to imply cultural kinship of the region with France, transform France into a cultural and political leader of the area, and install Maximilian of Habsburg as emperor of the Second Mexican Empire.[24] The term was also used in 1861 by French scholars in La revue des races Latines, a magazine dedicated to the Pan-Latinism movement.[25] Latin America is often used synonymously with Ibero-America (\"Iberian America\"), where the populations speak Spanish or Portuguese and the dominant religion is Roman Catholic. Puerto Rico, the Spanish-speaking Caribbean territory of the United States, acquired from the Spanish Empire following its defeat in the 1898 Spanish American War, is usually included. This definition excludes the predominantly Protestant English-speaking and Dutch-speaking regions, as well as French-speaking predominantly Catholic regions. Belize, Guyana and Suriname, as well as several French overseas departments, are excluded from the definition.[26] In another definition, Latin America designates the set of countries in the Americas where a Romance language (a language derived from Latin) predominates: Spanish, Portuguese, or French. Thus, it includes Mexico; most of Central and South America; and in the Caribbean, Cuba, the Dominican Republic, and Haiti. Latin America then comprises all of the countries in the Americas that were once part of the Spanish, Portuguese, and French Empires.[27][28] Puerto Rico, although not a sovereign nation, is often included. The distinction between Latin America and Anglo-America is a convention based on the predominant languages in the Americas by which Romance language- and English-speaking cultures are distinguished. Neither area is culturally or linguistically homogeneous; in substantial portions of Latin America (e.g., highland Peru, Bolivia, Mexico, Guatemala), Native American cultures and, to a lesser extent, Amerindian languages, are predominant, and in other areas, the influence of African cultures is strong (e.g., the Caribbean basin – including parts of Colombia and Venezuela). The term's meaning is contested and not without controversy. Historian Mauricio Tenorio-Trillo explores at length the \"allure and power\" of the idea of Latin America. He remarks at the outset, \"The idea of 'Latin America' ought to have vanished with the obsolescence of racial theory... But it is not easy to declare something dead when it can hardly be said to have existed,\" going on to say, \"The term is here to stay, and it is important.\"[33] Following in the tradition of Chilean writer Francisco Bilbao, who excluded Brazil, Argentina and Paraguay from his early conceptualization of Latin America,[34] Chilean historian Jaime Eyzaguirre has criticized the term Latin America for \"disguising\" and \"diluting\" the Spanish character of a region (i.e. Hispanic America) with the inclusion of nations that, according to him, do not share the same pattern of conquest and colonization.[35] The term \"Latin America\" is defined to mean parts of Americas south of USA mainland where a Romance language (a language derived from Latin) predominates, that is, a language of Spanish, Portuguese or French. As is customary, Puerto Rico is included and Dominica, Grenada, and Saint Lucia (where French is spoken but not official language) are excluded from Latin America. The earliest known human settlement in the area was identified at Monte Verde, near Puerto Montt in southern Chile. Its occupation dates to some 14,000 years ago and there is disputed evidence of even earlier occupation. Over the course of millennia, people spread to all parts of the North and South America and the Caribbean islands. Although the region now known as Latin America stretches from northern Mexico to Tierra del Fuego, the diversity of its geography, topography, climate, and cultivable land means that populations were not evenly distributed. Sedentary populations of fixed settlements supported by agriculture gave rise to complex civilizations in Mesoamerica (central and southern Mexico and Central America) and the highland Andes populations of Quechua and Aymara, as well as Chibcha. Agricultural surpluses from intensive cultivation of maize in Mesoamerica and potatoes and hardy grains in the Andes were able to support distant populations beyond farmers' households and communities. Surpluses allowed the creation of social, political, religious, and military hierarchies, urbanization with stable village settlements and major cities, specialization of craft work, and the transfer of products via tribute and trade. In the Andes, llamas were domesticated and used to transport goods; Mesoamerica had no large domesticated animals to aid human labor or provide meat. Mesoamerican civilizations developed systems of writing; in the Andes, knotted quipus emerged as a system of accounting. The Caribbean region had sedentary populations settled by Arawak or Tainos and in what is now Brazil, many Tupian peoples lived in fixed settlements. Semi-sedentary populations had agriculture and settled villages, but soil exhaustion required relocation of settlements. Populations were less dense and social and political hierarchies less institutionalized. Non-sedentary peoples lived in small bands, with low population density and without agriculture. They lived in harsh environments. By the first millennium CE, the Western Hemisphere was the home of tens of millions of people; the exact numbers are a source of ongoing research and controversy.[41] The last two great civilizations, the Aztecs and Incas, emerged into prominence in the early fourteenth century and mid-fifteenth centuries. Although the Indigenous empires were conquered by Europeans, the sub-imperial organization of the densely populated regions remained in place. The presence or absence of Indigenous populations had an impact on how European imperialism played out in the Americas. The pre-Columbian civilizations of Mesoamerica and the highland Andes became sources of pride for American-born Spaniards in the late colonial era and for nationalists in the post-independence era.[42] For some modern Latin American nation-states, the Indigenous roots of national identity are expressed in the ideology of indigenismo. These modern constructions of national identity usually critique their colonial past.[43] Spanish and Portuguese colonization of the Western Hemisphere laid the basis for societies now seen as characteristic of Latin America. In the fifteenth century, both Portugal and Spain embarked on voyages of overseas exploration, following the Christian Reconquista of Iberia from Muslims. Portugal sailed down the west coast of Africa and the Crown of Castile in central Spain authorized the voyage of Genoese mariner Christopher Columbus. Portugal's maritime expansion into the Indian Ocean was initially its main interest; but the off-course voyage of Pedro Álvares Cabral in 1500 allowed Portugal to claim Brazil. The 1494 line of demarcation between Spain and Portugal gave Spain all areas to the west, and Portugal all areas to the east. For Portugal, the riches of Africa, India, and the Spice Islands were far more important initially than the unknown territory of Brazil.[44] By contrast, having no better prospects, the Spanish crown directed its energies to its New World territories. Spanish colonists began founding permanent settlements in the circum-Caribbean region, starting in 1493. In these regions of early contact, Spaniards established patterns of interaction with Indigenous peoples that they transferred to the mainland. At the time of European contact, the area was densely populated by Indigenous peoples who had not organized as empires, nor created large physical complexes.[45] With the expedition of Hernán Cortés from Cuba to Mexico in 1519, Spaniards encountered the Indigenous imperial civilization of the Aztecs. Using techniques of warfare honed in their early Caribbean settlements, Cortés sought Indigenous allies to topple the superstructure of the Aztec Empire after a two-year war of conquest. The Spanish recognized many Indigenous elites as nobles under Spanish rule with continued power and influence over commoners, and used them as intermediaries in the emerging Spanish imperial system.[46] Spaniards explored extensively in the mainland territories they claimed, but they settled in great numbers in areas with dense and hierarchically organized Indigenous populations and exploitable resources, especially silver. Early Spanish conquerors saw the Indigenous themselves as an exploitable resource for tribute and labor, and individual Spaniards were awarded grants of encomienda forced labor as reward for participation in the conquest. Throughout most of Spanish America, Indigenous populations were the largest component, with some black slaves serving in auxiliary positions. The three main racial groups during the colonial era were European whites, black Africans, and Indigenous. Over time, these populations intermixed, resulting in castas. In most of Spanish America, the Indigenous were the majority population. The Roman Catholic Church, as an institution, launched a \"spiritual conquest\" to convert Indigenous populations to Christianity, incorporating them into Christendom, with no other religion permitted. Pope Alexander VI in 1493 had bestowed on the Catholic Monarchsgreat power over ecclesiastical appointments and the functioning of the church in overseas possessions. The monarch was the patron of the institutional church. The state and the Catholic church were the institutional pillars of Spanish colonial rule. In the late eighteenth century, the crown also established a royal military to defend its possessions against foreign incursions, especially by the British. It also increased the number of viceroyalties in Spanish South America. Portugal did not establish firm institutional rule in Brazil until the 1530s, but it paralleled many patterns of colonization in Spanish America. The Brazilian Indigenous peoples were initially dense, but were semi-sedentary and lacked the organization that allowed Spaniards to more easily incorporate the Indigenous into the colonial order. The Portuguese used Indigenous laborers to extract the valuable commodity known as brazilwood, which gave its name to the colony. Portugal took greater control of the region to prevent other European powers, particularly France, from threatening its claims.[48] Europeans sought wealth in the form of high-value, low-bulk products exported to Europe. The Spanish Empire established institutions to secure wealth for itself and protect its empire in the Americas from rivals. In trade it followed principles of mercantilism, where its overseas possessions were to enrich the center of power in Iberia. Trade was regulated through the royal House of Trade in Seville, Spain, with the main export from Spanish America to Spain being silver, later followed by the red dye cochineal. Silver was found in the Andes, in particular the silver mountain Cerro Rico of Potosí, (now in Bolivia) in the region where Indigenous men were forced to labor in the mines. Many historians refer to Potosi'sCerro Rico as the richest source of silver in the history of mankind. Between the 16th and 18th centuries, 80% of the world's silver supply came out of this mine.[49] In New Spain, rich deposits of silver were found in northern Mexico, in Zacatecas and Guanajuato, outside areas of dense Indigenous settlement. Labor was attracted from elsewhere[clarification needed] for mining and landed estates were established to raise wheat, range cattle and sheep. Mules were bred for transportation and to replace of human labor in refining silver. Sugar processing by skilled black slave laborers. Sugar cane must be processed immediately once cut in order to capture the most sugar juice, so engenhos needed to be constructed near fields. In Brazil and some Spanish Caribbean islands, plantations for sugar cultivation developed on a large scale for the export market. For Brazil, the development of the plantation complex transformed the colony from a backwater of the Portuguese empire to a major asset. The Portuguese transported enslaved laborers from their African territories and the seventeenth-century \"age of sugar\" was transformational, seeing Brazil becoming a major economic component of the Portuguese empire. The population increase exponentially, with the majority being enslaved Africans. Settlement and economic development was largely coastal, the goal of sugar export to European markets. With competition from other sugar producers, Brazil's fortunes based on sugar declined, but in the eighteenth century, diamonds and gold were found in the southern interior, fueling a new wave of economic activity.[50] As the economic center of the colony shifted from the sugar-producing northeast to the southern region of gold and diamond mines, the capital was transferred from Salvador de Bahia to Rio de Janeiro in 1763.[51] During the colonial era, Brazil was also the manufacturing center for Portugal's ships. As a global maritime empire, Portugal created a vital industry in Brazil. Once Brazil achieved its independence, this industry languished.[52] In Spanish America, manufactured and luxury goods were sent from Spain and entered Spanish America legally only through the Caribbean ports of Veracruz, Havana, and Cartagena, as well as the Pacific port of Callao, in Peru. Trans-Pacific trade was established in the late sixteenth century from Acapulco to the Philippines via the Manila Galleon, transporting silver from Mexico and Peru to Asia; Chinese silks and porcelains were sent first to Mexico and then re-exported to Spain. This system of commerce was in theory was tightly controlled, but was increasingly undermined by other European powers. The English, French, and Dutch seized Caribbean islands claimed by the Spanish and established their own sugar plantations. The islands also became hubs for contraband trade with Spanish America. Many regions of Spanish America that were not well supplied by Spanish merchants, such as Central America, participated in contraband trade with foreign merchants. The eighteenth-century Bourbon reforms sought to modernize the mercantile system to stimulate greater trade exchanges between Spain and Spanish America in a system known as comercio libre. It was not free trade in the modern sense, but rather free commerce within the Spanish empire. Liberalization of trade and limited deregulation sought to break the monopoly of merchants based in the Spanish port of Cádiz. Administrative reforms created the system of districts known as intendancies, modeled on those in France. Their creation was aimed at strengthening crown control over its possessions and sparking economic development.[53] Both Spain and Portugal restricted foreign powers from trading in their American colonies or entering coastal waters it had claimed. Other European powers challenged the exclusive rights claimed by the Iberian powers. The English, Dutch, and French permanently seized islands in the Caribbean and created sugar plantations on the model developed in Brazil. In Brazil, the Dutch seized the sugar-producing area of the northeast, but after 30 years they were expelled.[54] More than three centuries of direct Spanish and Portuguese colonial rule left lasting imprints on Latin America. Spanish and Portuguese are the dominant languages of the region, and Roman Catholicism is the dominant religion. Diseases to which Indigenous peoples had no immunity devastated their populations, although populations still exist in many places. The forced transportation of African slaves transformed major regions where they labored to produce the export products, especially sugar. In regions with dense Indigenous populations, they remained the largest percentage of the population; sugar-producing regions had the largest percentage of blacks. European whites in both Spanish America and Brazil were a small percentage of the population, but they were also the wealthiest and most socially elite; and the racial hierarchies they established in the colonial era have persisted. Cities founded by Europeans in the colonial era remain major centers of power. In the modern era, Latin American governments have worked to designate many colonial cities as UNESCOWorld Heritage Sites.[55] Exports of metals and agricultural products to Europe dominate Latin American economies, with the manufacturing sector deliberately suppressed; the development of modern, industrial economies of Europe depended on the underdevelopment of Latin America.[56][57][58] Despite the many commonalities of colonial Spanish America and Brazil, they did not think of themselves as being part of a particular region; that was a development of the post-independence period beginning in the nineteenth century. The imprint of Christopher Columbus and Iberian colonialism in Latin America began shifting in the twentieth century. \"Discovery\" by Europeans was reframed as \"encounter\" between the Old World and the New. An example of the new consciousness was the dismantling of the Christopher Columbus monument in Buenos Aires, one of many in the hemisphere, mandated by leftist President Cristina Fernández de Kirchner. Its replacement was a statue to a Bolivian fighter for independence, Juana Azurduy de Padilla, provoking a major controversy in Argentina over historical and national identity.[59] Ferdinand VII of Spain in whose name Spanish American juntas ruled during his exile 1808–1814; when restored to power in 1814, he reinstated autocratic rule, renewing independence movements.Battle of Ayacucho, which secured the independence of Peru and ensured independence for the rest of South America Independence in the Americas was not inevitable or uniform in the Americas. Events in Europe had a profound impact on the colonial empires of Spain, Portugal, and France in the Americas. France and Spain had supported the American Revolution that saw the independence of the Thirteen Colonies from Britain, which had defeated them in the Seven Years' War (1757–63). The outbreak of the French Revolution in 1789, a political and social uprising toppling the Bourbon monarchy and overturning the established order, precipitated events in France's rich Caribbean sugar colony of Saint-Domingue, whose black population rose up, led by Toussaint L'ouverture. The Haitian Revolution had far-reaching consequences. Britain declared war on France and attacked ports in Saint-Domingue. Haiti gained independence in 1804, led by ex-slave Jean-Jacques Dessalines following many years of violent struggle, with huge atrocities on both sides. Haitian independence affected colonial empires in the Americas, as well as the United States. Many white, slave-owning sugar planters of Saint-Domingue fled to the Spanish island of Cuba, where they established sugar plantations that became the basis of Cuba's economy.[60] Uniquely in the hemisphere, the black victors in Haiti abolished slavery at independence. Many thousands of remaining whites were executed on the orders of Dessalines. For other regions with large enslaved populations, the Haitian Revolution was a cautionary tale for the white slave-owning planters. Despite Spain and Britain's satisfaction with France's defeat, they \"were obsessed by the possible impact of the slave uprising on Cuba, Santo Domingo, and Jamaica\", by then a British sugar colony.[61] US President Thomas Jefferson, a wealthy slave owner, refused to recognize Haiti's independence. Recognition only came in 1862 from President Abraham Lincoln. Given France's failure to defeat the slave insurgency and since needing money for the war with Britain, Napoleon Bonaparte sold France's remaining mainland holdings in North America to the United States in the 1803 Louisiana Purchase.[citation needed] Napoleon's invasion of the Iberian peninsula in 1807–1808 was a major change in the world order, with the stability of both the metropoles[clarification needed] and their overseas possessions upended. It resulted in the movement, with British help, of the Portuguese royal court to Brazil, its richest colony. In Spain, France forced abdication of the Spanish Bourbon monarchs and their replacement with Napoleon's brother Joseph Bonaparte as king. The period from 1808 to the restoration in 1814 of the Bourbon monarchy saw new political experiments. In Spanish America, the question of the legitimacy of the new foreign monarch's right to rule set off fierce debate and in many regions to wars of independence. The conflicts were regional and usually quite complex. Chronologically, the Spanish American independence wars were the conquest in reverse, with the areas most recently incorporated into the Spanish empire, such as Argentina and Chile, becoming the first to achieve independence, while the colonial strongholds of Mexico and Peru were the last to achieve independence in the early nineteenth century. Cuba and Puerto Rico, both old Caribbean sugar-producing areas, were not detached from Spain until the 1898 Spanish–American War, with US intervention. Constitution of 1812 In Spain, a bloody war against the French invaders broke out and regional juntas were established to rule in the name of the deposed Bourbon king, Ferdinand VII. In Spanish America, local juntas also rejected Napoleon's brother as their monarch. Spanish Liberals re-imagined the Spanish Empire as equally being Iberia and the overseas territories. Liberals sought a new model of government, a constitutional monarchy, with limits on the power of the king as well as on the Catholic Church. Ruling in the name of the deposed Bourbon monarch Ferdinand VII, representatives of the Spanish empire, both from the peninsula and Spanish America, convened a convention in the port of Cadiz. For Spanish American elites who had been shut out of official positions in the late eighteenth century in favor of peninsular-born appointees, this was a major recognition of their role in the empire.[62] These empire-wide representatives drafted and ratified the Spanish Constitution of 1812, establishing a constitutional monarchy and set down other rules of governance, including citizenship and limitations on the Catholic Church. Constitutional rule was a break from absolutist monarchy and gave Spanish America a starting point for constitutional governance.[63] So long as Napoleon controlled Spain, the liberal constitution was the governing document. When Napoleon was defeated and the Bourbon monarchy was restored in 1814, Ferdinand VII and his conservative supporters immediately reasserted absolutist monarchy, ending the liberal interregnum. In Spanish America, it set off a new wave of struggles for independence.[64][65] For Portugal and Brazil, Napoleon's defeat did not immediately result in the return of the Portuguese monarch to Portugal, as Brazil was the richest part of the Portuguese empire. As with Spain in 1820, Portuguese liberals threatened the power of the monarchy and compelled John VI to return in April 1821, leaving his son Pedro to rule Brazil as regent. In Brazil, Pedro contended with revolutionaries and insubordination by Portuguese troops, all of whom he subdued. The Portuguese government threatened to revoke the political autonomy that Brazil had enjoyed since 1808, provoking widespread opposition in Brazil. Pedro declared Brazil's independence from Portugal on 7 September 1822 and became emperor. By March 1824 he had defeated all armies loyal to Portugal. Brazil's independence was achieved relatively peaceably, territorial integrity was maintained, and its ruler was from the Royal House of Braganza, whose successors ruled Brazil until their overthrow in 1889.[66][67] After independence Spanish America and Brazil differed in their forms of state rule, with most of Spanish America becoming federated republics (with the exceptions of Cuba and Puerto Rico, which remained Spanish colonies), and Brazil becoming a monarchy ruled by the Brazilian branch of the Portuguese royal family. Spanish America's fragmentation into republics with weakened state structures meant that political turmoil and violence on many levels was a characteristic of the era throughout the region. Brazil's monarchy was a stabilizing political force and the territorial integrity of the Portuguese colony carried over into the post-independence era. Although much of Latin America gained its independence in the early nineteenth century, formal recognition by their former metropolitan powers in Spain and Portugal did not come immediately. Portugal officially recognized Brazil on August 29, 1825.[68] The Spanish crown did not recognize new Spanish American nations' independence and sent expeditions to Mexico in failed attempts to regain control over its valuable former territory. Spain finally recognized Mexico's independence in 1836, 15 years after it was achieved. Its recognition of Ecuador's independence came in 1840 and Paraguay's as late as 1880. The new independent territories exerted their rights to establish a government, control their national territory, establish trade relations with other nations, and levy taxes. Brazil and Mexico both established independent monarchies in 1822. Mexico's was short-lived (1822–23) under leader of the independence movement General Iturbide, who was elected constitutional emperor 19 May 1822 and forced to abdicate 19 March 1823. Iturbide had no royal pedigree, so as a commoner he had no prestige or permanent legitimacy as ruler. Brazil's monarchy, a branch of the House of Braganza, lasted until 1889. Spanish America fragmented into various regions.[This paragraph needs citation(s)] As a consequence of the violent struggles for independence in most of Spanish America, the military grew in importance. In the post-independence period, it often played a key role in politics. Military leaders often became the initial heads of state, but regional strongmen, or caudillos, also emerged. The first half of the nineteenth century is sometimes characterized as the \"age of caudillos.\" In Argentina, Juan Manuel Rosas and in Mexico Antonio López de Santa Anna are exemplars of caudillos. Although most countries created written constitutions and created separate branches of government, the state and the rule of law were weak, and the military emerged as the dominant institution in the civil sphere. Constitutions were written laying out division of powers, but the rule of personalist strongmen dominated. Dictatorial powers were granted to some strongmen, nominally ruling as presidents under a constitution, as \"constitutional dictators.\"[69] In the religious sphere, the Roman Catholic Church, one of the pillars of colonial rule, remained a powerful institution and generally continued as the only permissible religion. With the Spanish monarch no longer the patron of the church, many national governments asserted their right to appoint clerics as a logical transfer of power to a sovereign state. The Catholic Church denied that this right had transferred to the new governments, and for a time the Vatican refused to appoint new bishops.[70] In Brazil, because the ruler after independence was a member of the House of Braganza, and Portugal recognized political independence quite speedily, the Vatican appointed a papal nuncio to Brazil in 1830. This official had jurisdiction over not just Brazil, but also the new states in Spanish America. However, in Brazil, there were also conflicts between church and state. During the reign of Pedro II, Protestant missionaries were tolerated, and when the monarchy was overthrown in 1889, the Catholic Church was disestablished.[71] In the new nation-states, conservatives favored the old order of a powerful, centralized state and continuation of the Catholic Church as a key institution. In Mexico, following the abdication of Emperor Iturbide in 1823, Mexican political leaders wrote a constitution for its newly declared federated republic, the Constitution of 1824. Central America opted out of joining the new federated republic of Mexico, with no real conflict. Hero of the insurgency Guadalupe Victoria became the first president of Mexico in 1824. Conservatives pushed to take control of the government, favoring central rule of the nation, as opposed to liberals, who generally favored the power of states expressed in federalism. General Santa Anna was elected president in 1833 and was in and out of office until 1854. In South America, Gran Colombia came into being, spanning what are now the separate countries of Colombia, Venezuela, Ecuador, Panama, and Peru, with independence leader Simón Bolívar as head of state (1819–30). Gran Colombia dissolved in 1831 due to conflicts similar to those elsewhere in Spanish America between centralist conservatives and pro-federalist liberals. In Argentina, the conflict resulted in a prolonged civil war between unitarianas (i.e. centralists) and federalists, which were in some aspects respectively analogous to liberals and conservatives in other countries. Adding to this dispute was the almost inherited colonial-era conflict over its borders with Brazil. The Cisplatine War erupted in 1814 and ended in 1828, resulting in occupation and further secession of Provincia Oriental which in 1830 became the modern Republic of Uruguay with a central government in Montevideo. Between 1832 and 1852, Argentina existed as a confederation, without a head of state, although the federalist governor of Buenos Aires province, Juan Manuel de Rosas, was given the power to pay debt and manage international relations, and exerted a growing hegemony over the country. A national constitution was not enacted until 1853, and reformed in 1860, and the country reorganized as a federal republic led by a liberal-conservative elite.[72]Centralist Uruguay enacted its constitution on its first day of existence in 1830, but wasn't immune to a similar polarization of the new state that involved blancos and colorados, where the agrarian conservative interests of blancos were pitted against the liberal commercial interests of colorados based in Montevideo, and which eventually resulted in the Guerra Grande civil war (1839–1851).[73] Both the blancos and colorados evolved into political parties of the same names that still exist in Uruguay today and are considered among the first and most longstanding political parties in the world. In Brazil, Emperor Dom Pedro I, worn down by years of administrative turmoil and political dissension with both the liberal and conservative sides of politics (including an attempt of republican secession),[74] went to Portugal in 1831 to reclaim his daughter's crown, abdicating the Brazilian throne in favor of his five-year-old son and heir (who thus became the Empire's second monarch, with the title of Dom Pedro II).[75] As a minor, the new Emperor could not exert his constitutional powers until he came of age, so a regency was set up by the National Assembly.[76] In the absence of a charismatic figure who could represent a moderate face of power, during this period a series of localized rebellions took place, as the Cabanagem, the Malê Revolt, the Balaiada, the Sabinada, and the Ragamuffin War, which emerged from dissatisfaction of the provinces with the central power, coupled with old and latent social tensions peculiar to a vast, slave-holding and newly independent nation state.[77] This period of internal political and social upheaval, which included the Praieira revolt, was overcome only at the end of the 1840s, years after the end of the regency, which occurred with the premature coronation of Pedro II in 1841.[78] During the last phase of the monarchy, an internal political debate was centered on the issue of slavery. The Atlantic slave trade was abandoned in 1850,[79] as a result of the BritishAberdeen Act, but only in May 1888 after a long process of internal mobilization and debate for an ethical and legal dismantling of slavery in the country, was the institution formally abolished.[80] On 15 November 1889, worn out by years of economic stagnation, attrition of the majority of Army officers, as well as with rural and financial elites (for different reasons), the monarchy was overthrown by a military coup.[81] Foreign powers, particularly the Great Britain and the U.S., were keenly interested in the possibilities resulting from political independence. They quickly recognized the governments of newly independent countries in Latin America and established commercial relationships with them. The former imperial limits on trade with foreign powers ended with independence and foreign investors sought newly opened opportunities. With the 1803 Louisiana Purchase from France, the U.S. now bordered New Spain. Both the U.S. and Spain sought clarity about their borders, signing the 1819 Adams-Onís Treaty ceding Florida to the U.S. and setting the northern border of Spain's claim in North America.[82] When Mexico achieved independence in 1821 and briefly became a monarchy, the U.S. recognized the government under Agustín de Iturbide, sending diplomat Joel Poinsett as its representative 1822–23. Poinsett concluded an agreement with Mexico confirming the terms of the Adams-Onís Treaty. Previously Poinsett had traveled widely in Latin America and had concluded a trade agreement with independent Argentina. European and U.S. interests in the region fueled the demand for Latin American travelogues, an important source of information that described economic, political, and social conditions.[83] The U.S. saw itself as an important power in the Americas and had a foreign policy interest in the hemisphere to exclude former imperial powers from regaining their influence. The first major articulation of U.S. foreign policy toward Latin America as a region was the 1820 Monroe Doctrine. It warned foreign powers not to intervene in the Americas. The U.S. was relatively weak compared to the powerful British Empire, but it was a key policy that informed U.S. actions toward Latin America to the current day. The U.S. was concerned that foreign powers could support Spain in its attempts to reclaim its empire.[84] The actions that the U.S. took against potential reclamation of foreign powers of their former colonies often included its own direct interventions in the region, justified by President Theodore Roosevelt in his 1904 Roosevelt Corollary to the Monroe Doctrine. For Britain, their commercial interests were eager to seize the opportunity to trade with newly independent Latin America. Britain and Portugal had long been allies against the Spanish and French, so British recognition of Brazil's 1822 independence followed quickly after Portugal's. As with many other Latin American countries, Brazil exported raw materials and imported manufactured goods, which for both Britain and Brazil suited their economic strengths. For Britain, asserting economic dominance in Latin America (what is now called neocolonialism) meant that nation-states were sovereign countries, but were dependent on other powers economically. British dominance hindered the development of Latin American industries and strengthened their dependence on the world trade network.[85] Britain now replaced Spain as the region's largest trading partner.[86] Great Britain invested significant capital in Latin America to develop the area as a market for processed goods.[87] From the early 1820s to 1850, the post-independence economies of Latin American countries were lagging and stagnant.[88] Over the nineteenth century, enhanced trade between Britain and Latin America led to development such as infrastructure improvements, including roads and railroads, which grew the trade between the countries and outside nations such as Great Britain.[89] By 1870, exports dramatically increased, attracting capital from abroad (including Europe and USA).[90] Until 1914 and the outbreak of World War I, Britain was a major economic power in Latin America, especially in South America. For the U.S., its initial sphere of influence was in Mexico, but the drive for territorial expansion, particularly for Southern slave-owners seeking new territory for their enterprises, saw immigration of white slave-owners with their slaves to Texas, which ultimately precipitated conflict between the Mexican government and the Anglo-American settlers. The Texas Revolution of 1836–37 defeated Mexican forces, and in 1845, U.S. annexation of the Texas territory that Mexico still claimed set the stage for the Mexican–American War (1846–48). The war resulted in the resounding defeat of Mexico. U.S. troops occupied Mexico City. The Treaty of Guadalupe Hidalgo added a huge swath of what had been north and northwest Mexico to the U.S., territory that Spain and then Mexico had claimed, but had not succeeded in occupying effectively. Southern slave owners were also interested in the possibility of the U.S. acquiring Cuba from Spain, with the aim of expanding both slavery and U.S. territory. The 1854 leak of the Ostend Manifesto, offering $130 million to Spain, caused a scandal among abolitionists in the U.S., who sought to end the expansion of slavery. It was repudiated by U.S. President Franklin Pierce. The American Civil War (1861–1865) decided the question of slavery.[91] Another episode in US–Latin American relations involved the filibusterWilliam Walker. In 1855, he traveled to Nicaragua hoping to overthrow the government and take territory for the United States. With Only 56 followers, he was able to take over the city of Granada, declaring himself commander of the army and installing Patricio Rivas as a puppet president. However, Rivas' presidency ended when he fled Nicaragua; Walker rigged the ensuing election to ensure that he became the next president. His presidency did not last long, however, as he was met with much opposition in Nicaragua and from neighboring countries. On 1 May 1857, Walker was forced by a coalition of Central American armies to surrender himself to a United States Navy officer who repatriated him and his followers. When Walker subsequently returned to Central America in 1860, he was apprehended by the Honduran authorities and executed.[92] Britain's nineteenth-century policy was to end slavery and the slave trade, including in Latin America. In Brazil, Britain made the end of the slave trade a condition for diplomatic recognition. The Brazilian economy was entirely dependent on slaves. Abolitionists in Brazil pressed for the end of slavery, which finally ended in 1888, followed the next year by the fall of the Brazilian monarchy. The French also sought commercial ties to Latin America, to export luxury goods and establish financial ties, including extending foreign loans to governments, often in dire need of revenue. As Mexican conservatives and liberals fought the War of the Reform over La Reforma, Mexican conservatives, to bolster their side, sought a European monarch to put on the throne of Mexico. Napoleon III of France invaded Mexico in 1862 and facilitated the appointment of Maximilian von Hapsburg. Since the U.S. was embroiled in its own civil war, it could not hinder the French occupation, which it saw as a violation of the Monroe Doctrine, but the government of Abraham Lincoln continued to recognize the Republic of Mexico as the nation's government under President Benito Juárez. The French were expelled in 1867 and Emperor Maximilian executed by the victorious Republican forces, setting the stage for an era of stability and foreign economic investment a few years later when Porfirio Díaz liberal hero of the war against the French, became president of Mexico for 30 years. Latin American nations after about 1870 were stable enough politically and produced commodities in demand in Western Europe and the United States so that export economies tied producing countries to consuming countries. Rather than formally ruling countries in the region, investors and their government backers exercised power and influence over local elites seeking to maintain or enhance their own positions. Companies in Great Britain forged ties especially in Brazil and Argentina, with Brazilian coffee and Argentine beef and wheat becoming staples on European dining tables. Britain constructed infrastructure to enable the efficient movement of goods and people, building port facilities to accommodate transatlantic shipping, railroads to transport goods from interior regions of production to ports, and electricity enabling telegraphs, later telephones, and street lighting in urban areas. As technology became more sophisticated, bulky agricultural products like wheat could be shipped on large ships at relatively low cost. As refrigerated ships were developed, chilled beef and tropical bananas could be shipped efficiently enough that they would not spoil. The U.S. in particular imported bananas from Central and South America. The U.S. United Fruit Company and Cuyamel Fruit Company, both ancestors of Chiquita, and the Standard Fruit Company (now Dole), acquired large tracts of land in Central America, including Guatemala, Honduras, and Costa Rica, as well as Ecuador. The companies gained leverage over governments and ruling elites in these countries by dominating their economies and paying kickbacks, and exploited local workers. Such countries came to be called banana republics. Demand for commodities fueled armed conflicts for territory with economic potential. One such conflict was the Spanish–American War in 1898, where the U.S. intervened in the long-standing independence war in Cuba against the Spanish crown, which had held onto it after the almost complete loss of is overseas territories in the early nineteenth century. Cuba produced sugar and tobacco, both in high demand in the U.S. In the treaty with Spain ending the war, the U.S. gained Puerto Rico, Spain's other remaining Caribbean colony, as well as the Philippine Islands. There were also conflicts between Latin American nations in the late nineteenth century, as well as protracted civil wars in Mexico and Colombia. One notable international conflict was the War of the Pacific from 1879 to 1884, in which Chile seized territory and resources from Peru and Bolivia, gaining valuable nitrate deposits and leaving Bolivia landlocked.[93] Another notable conflict was the War of the Triple Alliance (1864–1870) in which Paraguay, under Francisco Solano López, tried to assist Uruguay against the Brazil-backed rebels, which angered Brazil and led to a war against Paraguay. With Brazil allied to Argentina and Uruguay, the war escalated to total war and decimated Paraguay in what became one of the most horrendous chapters in the history of the continent, with huge loss of life, land, and the destruction of the modernized sector.[94][clarification needed] The export boom created a demand for labor, which many countries could not meet domestically. Countries such as Argentina, Brazil, and Peru, sought laborers from abroad, some of whom immigrated permanently, while other workers developed a pattern of cyclical work, returning to their home countries at intervals. Workers came from poorer regions of Europe, such as Italy, but also China and Japan, with single men and few women making up the initial immigrant populations. In general, Latin America stayed out of direct conflict in World War I, but the Great Powers were aware of the region's importance for the short and long term. Germany attempted to draw Mexico into supporting its side against the British, the French, and especially the U.S., by trying to leverage anti-Americanism to its advantage. The Great Powers had been actively working to affect the course of the Mexican Revolution (1910–1920). Great Britain and the U.S. had huge investments in Mexico, with Germany close behind, so the outcome of the conflict would have consequences there. The U.S. directly intervened militarily, but not on a huge scale.[95] A German diplomatic proposal, now known as the January 1917 Zimmermann Telegram, sought to entice Mexico to join an alliance with Germany in the event of the United States entering World War I against Germany by promising the return of territory Mexico had lost to the U.S. The proposal was intercepted and decoded by British intelligence. The revelation of the contents outraged the American public and swayed public opinion. The news helped to generate support for the United States declaration of war on Germany in April 1917 as well as to calm U.S.-Mexico relations.[96] Mexico, far weaker militarily, economically, and politically than the U.S., ignored the German proposal; after the U.S. entered the war, it officially rejected it.[This paragraph needs citation(s)] When the U.S. entered the conflict in 1917, it abandoned its hunt in Mexico for the revolutionary Pancho Villa who had attacked the U.S. in Columbus, New Mexico. The Mexican government was not pro-Villa, but was angered by U.S. violation of Mexico's sovereign territory with troops. The expeditionary force led by General John J. Pershing that had hopelessly chased him around northern Mexico was deployed to Europe. The U.S. then asked Latin American nations to join Britain, France, and the U.S. against Germany. They were not quick to join, as Germany was now a major financial lender to Latin America, and a number of nations were antipathetic to the traditional lenders in Britain and France. While Latin America did join the allies, it was not without cost. The U.S. sought hemispheric solidarity against Germany, and Brazil, Costa Rica, Cuba, Guatemala, Honduras, Nicaragua and Haiti declared war. Others took the lesser step of breaking diplomatic relations. Argentina, Chile, Mexico and Uruguay remained neutral.[97] More important was the impact of the war on transatlantic shipping, the economic lifeline for their export economies. Export economies from the mining sector and especially nitrates for gunpowder did boom, but agricultural exports of sugar and coffee languished when European economies turned to war production. Britain was on the winning side of the war, but in the aftermath its economic power was fairly reduced. After 1914, the U.S. replaced Britain as the major foreign power in Latin America. Latin American nations gained standing internationally in the aftermath of the war, participating in the Versailles Conference, signing the Treaty of Versailles and joining the League of Nations. Latin America also played an important role in the International Court of Justice.[97] U.S. President Roosevelt and Mexican President Manuel Avila Camacho, Monterrey, Mexico 1943. Roosevelt sought strong ties between the U.S. and Latin America in the World War II era. The Great Depression was a worldwide phenomenon and had an impact on Latin America. Exports largely fell and economies stagnated. For a number of Latin American countries, the Depression made them favor an internal economic development policy of import substitution industrialization.[98] World War I and the League of Nations did not settle conflicts between European nations, but in the wake of World War I, Latin American nations gained success in pressing discussions of hemispheric importance. The Inter-American System was institutionally established with the First International Conference of American States of 1889–1890, where 17 Latin American nations sent delegates to Washington, D.C., and formed the Pan American Union. Subsequent Pan-American Conferences saw the initial dominance of the U.S. in the hemisphere give way as Latin American nations asserted their priorities. The Havana Conference of 1928 was the high water mark of U.S. dominance and assertion of its right to intervene in Latin America,[99] but with the election of Franklin Delano Roosevelt to the U.S. presidency in 1932, U.S. policy changed toward Latin America. He abandoned the routine U.S. interventions in Latin America that it had claimed as its right and initiated the Good Neighbor Policy in March 1933. He sought hemispheric cooperation rather than U.S. coercion in the region.[100] At the Montevideo Convention in December 1933, the U.S. Secretary of State voted in favor of the Convention on the Rights and Duties of States, declaring \"no state has the right to intervene in the internal or external affairs of another.\"[101] President Roosevelt himself attended the inaugural session of the hemispheric conference in Buenos Aires in 1936, where the U.S. reaffirmed the policy of non-intervention in Latin America and discussed the issue of neutrality for the hemisphere should war break out.[102] With the Nazi invasion of Poland in September 1939 and the spread of war in Europe, foreign ministers of hemispheric nations met in Panama, at which the Declaration of Neutrality was signed, and the territorial waters bordering the hemisphere were expanded. The aim of these moves was to strengthen hemispheric solidarity and security.[103] With the 7 December 1941 Japanese attack on the U.S. naval base at Pearl Harbor, hemispheric ministers met in January 1942 in Rio de Janeiro. Some nations had already declared war on the Axis powers, while others severed relations with the Axis. Chile did not do so until 1943, and Argentina, traditionally pro-German, not until 1945.[104] The U.S. requested that Germans suspected of Nazi sympathies be deported from Latin America to the U.S.[105] Many Latin American economies continued to grow in the post-World War II era, but not as quickly as they had hoped. When the transatlantic trade re-opened following the peace, Europe looked as if it would need Latin American food exports and raw materials. The policies of import substitution industrialization adopted in Latin America when exports slowed due to the Great Depression and subsequent isolation in World War II were now subject to international competition. Those who supported a return to the export of commodities for which Latin America had a competitive advantage disagreed with advocates of an expanded industrial sector. The rebuilding of Europe, including Germany, with the aid of the U.S. after World War II did not bring stronger demand for Latin American exports. In Latin America, much of the hard currency earned by their participation in the war went to nationalize foreign-owned industries and pay down their debt. A number of governments set tariff and exchange rate policies that undermined the export sector and aided the urban working classes. Growth slowed in the post-war period and by the mid-1950s, the optimism of the postwar period was replaced by pessimism.[106] Following World War II, the United States policy toward Latin America focused on what it perceived as the threat of communism and the Soviet Union to the interests of Western Europe and the United States. Although Latin American countries had been staunch allies in the war and reaped some benefits from it, in the post-war period the region did not prosper as it had expected. Latin America struggled in the post-war period without large-scale aid from the U.S., which devoted its resources to rebuilding Western Europe, including Germany. In Latin America there was increasing inequality, with political consequences in the individual countries. The U.S. returned to a policy of interventionism when it felt its political and economic interests were threatened. With the breakup of the Soviet bloc in the late 1980s and early 1990s, including the Soviet Union itself, Latin America sought to find new solutions to long-standing problems. With its Soviet alliance dissolved, Cuba entered a Special Period of severe economic disruption, high death rates, and food shortages. Deeply alarming for the U.S. were two revolutions that threaten, fed its dominance in the region. The Guatemalan Revolution (1944–54) saw the replacement of a U.S.-backed regime of Jorge Ubico in 1945 followed by elections. ReformistJuan José Arévalo (1945–51) was elected and began instituting populist reforms. Reforms included land laws that threatened the interests of large foreign-owned enterprises, a social security law, workmen's compensation, laws allowing labor to organize and strike, and universal suffrage except for illiterate women. His government established diplomatic ties with the Soviet Union in April 1945, when the Soviet Union and the U.S. were allied against the Axis powers. Communists entered leadership positions in the labor movement. At the end of his term, his hand-picked successor, the populist and nationalist Jacobo Arbenz, was elected. Arbenz proposed placing capital in the hands of Guatemalans, building new infrastructure, and significant land reform via Decree 900. With what the U.S. considered the prospect of even more radical changes in Guatemala, it backed a coup against Arbenz in 1954, overthrowing him.[107][108][109] Argentine Che Guevara was in Guatemala during the Arbenz presidency; the coup ousting Arbenz was instructive for him and for Latin American nations seeking significant structural change.[110] In 1954 the U.S. Central Intelligence Agency aided successful military coup against Arbenz. The 1959 Cuban Revolution led by Cuban lawyer Fidel Castro overthrew the regime of Fulgencio Batista, with 1 January 1959 marking as the revolution's victory. The revolution was a huge event not only in Cuban history, but also the history of Latin America and the world. Almost the immediately, the U.S. reacted with hostility against the new regime. As the revolutionaries began consolidating power, many middle- and upper-class Cubans left for the U.S., likely not expecting the Castro regime to last long. Cuba became a poorer and blacker country, and the Cuba Revolution sought to transform the social and economic inequalities and political instability of the previous regimes into a more socially and economically equal one. The government put emphasis on literacy as a key to Cuba's overall betterment, essentially wiping out illiteracy after an early major literacy campaign. Schools became a means to instill in Cuban students messages of nationalism, solidarity with the Third World, and Marxism.[citation needed] Cuba also made a commitment to universal health care, so the education of doctors and construction of hospitals were top priorities. Cuba also sought to diversify its economy, until then based mainly on sugar, but also tobacco.[111] The U.S. attempted to overthrow Castro, using the template of the successful 1954 coup in Guatemala. In the April 1961 Bay of Pigs invasion, Cuba entered into a formal alliance with the Soviet Union. In February 1962, the U.S. placed an embargo on trade with Cuba, which remains in force as of 2021.[112] In February 1962, the U.S. pressured members of the Organization of American States to expel Cuba, attempting to isolate it. In response to the Bay of Pigs, Cuba called for revolution in the Americas. The efforts ultimately failed, most notably with Che Guevara in Bolivia, where he was isolated, captured, and executed. When the U.S. discovered that the Soviet Union had placed missiles in Cuba in 1962, they reacted swiftly with a showdown now called the Cuban Missile Crisis, which ended with an agreement between the U.S. and the Soviet Union, who did not consult Cuba about its terms. One term of the agreement was that the U.S. would cease efforts to invade Cuba, a guarantee of its sovereignty. However, the U.S. continued to attempt to remove Castro from power by assassination. The Soviet Union continued to materially support the Cuban regime, providing oil and other petrochemicals, technical support, and other aid, in exchange for Cuban sugar and tobacco.[113] From 1959 to 1992, Fidel Castro ruled as a caudillo, or strong man, dominating politics and the international stage. His commitment to social and economic equality brought about positive changes in Cuba, including the improvement of the position of women, eliminating prostitution, reducing homelessness, and raising the standard of living for most Cubans. However, Cuba lacks freedom of expression; dissenters were monitored by the Committees for the Defense of the Revolution, and travel was restricted.[114] In 1980, Castro told Cubans who wanted to leave to do so, promising that the government would not stop them. The Mariel boatlift saw some 125,000 Cubans sail from the Cuban port of Mariel, across the straits to the U.S., where U.S. President Carter initially welcomed them.[115] The Cuban Revolution had a tremendous impact not just on Cuba, but on Latin America as a whole, and the world. The Cuban Revolution was for many countries an inspiration and a model, but for the U.S. it was a challenge to its power and influence in Latin America. After leftists took power in Chile (1970) and Nicaragua (1979), Fidel Castro visited them both, extending Cuban solidarity. In Chile, Salvador Allende and a coalition of leftists, Unidad Popular, won an electoral victory in 1970 and lasted until the violent military coup of 11 September 1973. In the Nicaragua leftists held power from 1979 to 1990. The U.S. was concerned with the spread of communism in Latin America, and U.S. President Dwight Eisenhower responded to the threat he saw in the Dominican Republic's dictator Rafael Trujillo, who voiced a desire to seek an alliance with the Soviet Union. In 1961, Trujillo was murdered with weapons supplied by the CIA.[116] U.S. President John F. Kennedy initiated the Alliance for Progress in 1961, to establish economic cooperation between the U.S. and Latin America and provide $20 billion for reform and counterinsurgency measures. The reform failed because of the simplistic theory that guided it and the lack of experienced American experts who understood Latin American customs.[citation needed] From 1966 to the late 1980s, the Soviet government upgraded Cuba's military capabilities, and Cuba was active in foreign interventions, assisting with movements in several countries in Latin America and elsewhere in the world. Most notable were the MPLA during the Angolan Civil War and the Derg during the Ogaden War. They also supported governments and rebel movements in Syria, Mozambique, Algeria, Venezuela, Bolivia, and Vietnam.[117][118] In Chile, the postwar period saw uneven economic development. The mining sector (copper, nitrates) continued to be important, but an industrial sector also emerged. The agricultural sector stagnated and Chile needed to import foodstuffs. After the 1958 election, Chile entered a period of reform. The secret ballot was introduced, the Communist Party was relegalized, and populism grew in the countryside. In 1970, democratic elections brought to power socialistSalvador Allende, who implemented many reforms begun in 1964 under Christian DemocratEduardo Frei. The economy continued to depend on mineral exports and a large portion of the population reaped no benefits from the prosperity and modernity of some sectors. Chile had a long tradition of stable electoral democracy, In the 1970 election, a coalition of leftists, the Unidad Popular (\"popular unity\") candidate Allende was elected. Allende and his coalition held power for three years, with the increasing hostility of the U.S. The Chilean military staged a bloody coup with US support in 1973. The military under General Augusto Pinochet then held power until 1990. The 1970s and 1980s saw a large and complex political conflict in Central America. The U.S. administration of Ronald Reagan funded right-wing governments and proxy fighters against left-wing challenges to the political order. Complicating matters were the liberation theology emerging in the Catholic Church and the rapid growth of evangelical Christianity, which were entwined with politics. The Nicaraguan Revolution revealed the country as a major proxy war battleground in the Cold War. Although the initial overthrow of the Somoza regime in 1978–79 was a bloody affair, the Contra War of the 1980s took the lives of tens of thousands of Nicaraguans and was the subject of fierce international debate.[119] During the 1980s both the FSLN (a leftist collection of political parties) and the Contras (a rightist collection of counter-revolutionary groups) received considerable aid from the Cold War superpowers. The Sandinistas allowed free elections in 1990 and after years of war, lost the election. They became the opposition party, following a peaceful transfer of power. A civil war in El Salvador pitted leftist guerrillas against a repressive government. The bloody war there ended in a stalemate, and following the fall of the Soviet Union, a negotiated peace accord ended the conflict in 1992. In Guatemala, the civil war included genocide of Mayan peasants. A peace accord was reached in 1996 and the Catholic Church called for a truth and reconciliation commission. In the religious sphere, the Roman Catholic Church continued to be a major institution in nineteenth-century Latin America. For a number of countries in the nineteenth century, especially Mexico, liberals viewed the Catholic Church as an intransigent obstacle to modernization, and when liberals gained power, anticlericalism was written into law, such as the Mexican liberal Constitution of 1857 and the Uruguayan Constitution of 1913 which secularized the state. Nevertheless, most Latin Americans identified as Catholic, even if they did not attend church regularly. Many followed folk Catholicism, venerated saints, and celebrated religious festivals. Many communities did not have a resident priest or even visits by priests to keep contact between the institutional church and the people. In the 1950s, evangelical Protestants began proselytizing in Latin America. In Brazil, the Catholic bishops organized themselves into a national council, aimed at better meeting the competition not only of Protestants, but also of secular socialism and communism. Following Vatican II (1962–65) called by Pope John XXIII, the Catholic Church initiated a series of major reforms empowering the laity. Pope Paul VI actively implemented reforms and sought to align the Catholic Church on the side of the dispossessed, (\"preferential option for the poor\"), rather than remain a bulwark of conservative elites and right-wing repressive regimes. Colombian Catholic priest Camilo Torres took up arms with the Colombian guerrilla movement ELN, which modeled itself on Cuba but was killed in his first combat in 1966.[120] In 1968, Pope Paul came to the meeting of Latin American bishops in Medellín, Colombia. Peruvian priest Gustavo Gutiérrez was one of the founders of liberation theology, a term he coined in 1968, sometimes described as linking Christianity and Marxism. Conservatives saw the church as politicized, and priests ask proselytizing leftist positions. Priests became targets as \"subversives\", such as Salvadoran Jesuit Rutilio Grande. Archbishop of El Salvador Óscar Romero called for an end to persecution of the church, and took positions of social justice. He was assassinated on 24 March 1980 while saying mass. Liberation theology informed the struggle by Nicaraguan leftists against the Somoza dictatorship, and when they came to power in 1979, the ruling group included some priests. When a Polish cleric became Pope John Paul II following the death of Paul VI, and the brief papacy of John Paul I, he reversed the progressive position of the church, evident in the 1979 Puebla conference of Latin American bishops. On a papal visit to Nicaragua in 1983, he reprimanded Father Ernesto Cardenal, who was Minister of Culture, and called on priests to leave politics. Brazilian theologian Leonardo Boff was silenced by the Vatican. Despite the Vatican stance against liberation theology, articulated in 1984 by Cardinal Josef Ratzinger, later Pope Benedict XVI, many Catholic clergy and laity worked against repressive military regimes. After a military coup ousted the democratically elected Salvador Allende, the Chilean Catholic Church was a force in opposition to the regime of Augusto Pinochet and for human rights. The Argentine Church did not follow the Chilean pattern of opposition however.[121] When Jesuit Jorge Bergoglio was elected Pope Francis, his actions during the Dirty War were an issue, as portrayed in the film The Two Popes. Although most countries did not have Catholicism as the established religion, Protestantism made few inroads in the region until the late twentieth century. Evangelical Protestants, particularly Pentecostals, proselytized and gained adherents in Brazil, Central America, and elsewhere. In Brazil, Pentecostals had a long history. But in a number of countries ruled by military dictatorships many Catholics followed the social and political teachings of liberation theology and were seen as subversives. Under these conditions, the influence of religious non-Catholics grew. Evangelical churches often grew quickly in poor communities where small churches and members could participate in ecstatic worship, often many times a week. Pastors in these churches did attend a seminary nor were there other institutional requirements. In some cases, the first evangelical pastors came from the U.S., but these churches quickly became \"Latin Americanized,\" with local pastors building religious communities. In some countries, they gained a significant hold and were not persecuted by military dictators, since they were largely apolitical.[122] In Guatemala under General Efraín Ríos Montt, an evangelical Christian, Catholic Maya peasants were targeted as subversives and slaughtered. Perpetrators were later put on trial in Guatemala, including Ríos Montt. After the fall of the Soviet Union, the Cold War which saw U.S. intervention in Latin America as preventing Soviet influence dissipating. The Central American wars ended, with a free and fair election in Nicaragua that voted out the leftist Sandinistas, a peace treaty was concluded between factions in El Salvador, and the Guatemalan civil war ended. Cuba had lost its political and economic patron, the Soviet Union, which could no longer provide support. Cuba entered what is known there are the Special Period, when the economy contracted severely, but the revolutionary government nonetheless retained power and the U.S. remained hostile to its revolution. U.S. policy-makers developed the Washington Consensus, a set of specific economic policy prescriptions considered the standard reform package for crisis-wracked developing countries by Washington, D.C.-based institutions such as the International Monetary Fund (IMF), the World Bank, and the US Department of the Treasury during the 1980s and 1990s. The term has become associated with neoliberal policies in general and drawn into the broader debate over the expanding role of the free market, constraints upon the state, and US influence on other countries' national sovereignty. The politico-economical initiative was institutionalized in North America by the 1994 NAFTA, and elsewhere in the Americas through a series of similar agreements. The comprehensive Free Trade Area of the Americas project, however, was rejected by most South American countries at the 4th Summit of the Americas in 2005. A debt crisis ensured after 1982 when the price of oil crashed and Mexico announced that it could not meet its foreign debt payment obligations. Other Latin American economies followed suit, with hyperinflation and the inability of governments to meet their debt obligations and the era became known as the \"lost decade.\"[123] The debt crisis would lead to neoliberal reforms that would instigate many social movements in the region. A \"reversal of development\" reigned over Latin America, seen through negative economic growth, declines in industrial production, and thus, falling living standards for the middle and lower classes.[124] Governments made financial security their primary policy goal over social programs, enacting new neoliberal economic policies that implemented privatization of previously national industries and the informal sector of labor.[123] In an effort to bring more investors to these industries, these governments also embraced globalization through more open interactions with the international economy. Significantly, democratic governments began replacing military regimes across much of Latin America and the realm of the state became more inclusive (a trend that proved conducive to social movements), but economic ventures remained exclusive to a few elite groups within society. Neoliberal restructuring consistently redistributed income upward, while denying political responsibility to provide social welfare rights, and development projects throughout the region increased both inequality and poverty.[123] Feeling excluded from the new projects, the lower classes took ownership of their own democracy through a revitalization of social movements in Latin America. Both urban and rural populations had serious grievances as a result of economic and global trends and voiced them in mass demonstrations. Some of the largest and most violent have been protests against cuts in urban services to the poor, such as the Caracazo in Venezuela and the Argentinazo in Argentina.[125] In 2000, the Cochabamba Water War in Bolivia saw major protests against a World Bank-funded project that would have brought potable water to the city, but at a price that no residents could afford.[126] The title of the Oscar nominated film Even the Rain alludes to the fact that Cochabamba residents could no longer legally collect rainwater; the film depicts the protest movement. Rural movements made demands related to unequal land distribution, displacement at the hands of development projects and dams, environmental and Indigenous concerns, neoliberal agricultural restructuring, and insufficient means of livelihood. In Bolivia, coca workers organized into a union, and Evo Morales, ethnically an Aymara, became its head. The cocaleros supported the struggles in the Cochabamba water war. The rural-urban coalition became a political party, Movement for Socialism (Bolivia) (MAS, \"more\"), which decisively won the 2005 presidential election, making Evo Morales the first Indigenous president of Bolivia. A documentary of the campaign, Cocalero, shows how they successfully organized.[127] Charges of against a major Brazilian conglomerate, Odebrecht, has raised allegations of corruption across the region's governments (see Operation Car Wash). This bribery ring has become the largest corruption scandal in Latin American history.[132] As of July 2017, the highest ranking politicians charged were former Brazilian President Luiz Inácio Lula da Silva, who was arrested,[133] and former Peruvian presidents Ollanta Humala and Alejandro Toledo, who fled to the United States and was extradited back to Peru.[134] The COVID-19 pandemic proved a political challenge for many unstable Latin American democracies, with scholars identifying a decline in civil liberties as a result of opportunistic emergency powers. This was especially true for countries with strong presidential regimes, such as Brazil.[135] Wealth inequality in Latin America and the Caribbean remains a serious issue despite strong economic growth and improved social indicators. A report released in 2013 by the UN Department of Economic and Social Affairs entitled Inequality Matters: Report of the World Social Situation, observed that: 'Declines in the wage share have been attributed to the impact of labour-saving technological change and to a general weakening of labour market regulations and institutions.'[136] Such declines are likely to disproportionately affect individuals in the middle and bottom of the income distribution, as they rely mostly on wages for income. In addition, the report noted that 'highly-unequal land distribution has created social and political tensions and is a source of economic inefficiency, as small landholders frequently lack access to credit and other resources to increase productivity, while big owners may not have had enough incentive to do so.[136][137] According to the United Nations ECLAC, Latin America is the most unequal region in the world.[138] Inequality in Latin America has deep historical roots in the Latin European racially based Casta system[139][140] instituted in Latin America during colonial times that has been difficult to eradicate because of the differences between initial endowments and opportunities among social groups have constrained the poorest's social mobility, thus causing poverty to transmit from generation to generation, and become a vicious cycle. Inequality has been reproduced and transmitted through generations because Latin American political systems allow a differentiated access on the influence that social groups have in the decision-making process, and it responds in different ways to the least favored groups that have less political representation and capacity of pressure.[141] Recent economic liberalisation also plays a role as not everyone is equally capable of taking advantage of its benefits.[142] Differences in opportunities and endowments tend to be based on race, ethnicity, rurality, and gender. Because inequality in gender and location are near-universal, race and ethnicity play a larger, more integral role in discriminatory practices in Latin America. The differences have a strong impact on the distribution of income, capital and political standing. One indicator of inequality is access to and quality of education. During the first phase of globalization in Latin America, educational inequality was on the rise, peaking around the end of the 19th century. In comparison with other developing regions, Latin America then had the highest level of educational inequality, which is certainly a contributing factor for its current general high inequality. During the 20th century, however, educational inequality started decreasing.[143] Latin America has the highest levels of income inequality in the world.[144] The following table lists all the countries in Latin America indicating a valuation of the country's Human Development Index, GDP at purchasing power parity per capita, measurement of inequality through the Gini index, measurement of poverty through the Human Poverty Index, a measure of extreme poverty based on people living on less than 1.25 dollars a day, life expectancy, murder rates and a measurement of safety through the Global Peace Index. Green cells indicate the best performance in each category, and red the lowest. Urbanization accelerated starting in the mid-twentieth century, especially in capital cities, or in the case of Brazil, traditional economic and political hubs founded in the colonial era. In Mexico, the rapid growth and modernization in country's north has seen the growth of Monterrey, in Nuevo León. The following is a list of the ten largest metropolitan areas in Latin America. Entries in \"bold\" indicate they are ranked the highest.[158] Latin American populations are diverse, with descendants of the Indigenous peoples, Europeans, Africans initially brought as slaves, and Asians, as well as new immigrants. Mixing of groups was a fact of life at contact of the Old World and the New, but colonial regimes established legal and social discrimination against non-white populations simply on the basis of perceived ethnicity and skin color. Social class was usually linked to a person's racial category, with European-born Spaniards and Portuguese on top. During the colonial era, with a dearth initially of European women, European men and Indigenous women and African women produced what were considered mixed-race children. In Spanish America, the so-called Sociedad de castas or Sistema de castas was constructed by white elites to try to rationalize the processes at work. In the sixteenth century the Spanish crown sought to protect Indigenous populations from exploitation by white elites for their labor and land. The crown created theRepública de indios [es] to paternalistically govern and protect Indigenous peoples. It also created the República de Españoles, which included not only European whites, but all non-Indigenous peoples, such as blacks, mulattoes, and mixed-race castas who were not dwelling in Indigenous communities. In the religious sphere, the Indigenous were deemed perpetual neophytes in the Catholic faith, which meant Indigenous men were not eligible to be ordained as Catholic priests; however, Indigenous were also excluded from the jurisdiction of the Inquisition. Catholics saw military conquest and religious conquest as two parts of the assimilation of Indigenous populations, suppressing Indigenous religious practices and eliminating the Indigenous priesthood. Some worship continued underground. Jews and other non-Catholics, such as Protestants (all called \"Lutherans\") were banned from settling and were subject to the Inquisition. Considerable mixing of populations occurred in cities, while the countryside was largely Indigenous. At independence in the early nineteenth century, in many places in Spanish America formal racial and legal distinctions disappeared, although slavery was not uniformly abolished. Significant black populations exist in Brazil and Spanish Caribbean islands such as Cuba and Puerto Rico and the circum-Caribbean mainland (Venezuela, Colombia, Panama), as long as in the southern part of South America and Central America (Honduras, Costa Rica, Nicaragua, Ecuador, and Peru) a legacy of their use in plantations. All these areas had small white populations. In Brazil, coastal Indigenous peoples largely died out in the early sixteenth century, with Indigenous populations surviving far from cities, sugar plantations, and other European enterprises. Dominican Republic, Puerto Rico, Cuba, and Brazil have dominate Mulatto/Triracial populations (\"Pardo\" in Brazil), in Brazil and Cuba, there is equally large white populations and smaller black populations, while Dominican Republic and Puerto Rico are more Mulatto/Triracial dominated, with significant black and white minorities. Parts of Central America and northern South America are more diverse in that they are dominated by Mestizos and whites but also have large numbers of Mulattos, blacks, and indigenous, especially Colombia, Venezuela, and Panama. The southern cone region, Argentina, Uruguay, and Chile are dominated by whites and mestizos. Haiti and other areas in the French Caribbean are dominated mostly by blacks. The rest of Latin America, including México, northern Central America (Guatemala, El Salvador, Honduras), and central South America (Peru, Ecuador, Bolivia, Paraguay), are dominated by mestizos but also have large white and indigenous minorities. In the nineteenth century, a number of Latin American countries sought immigrants from Europe and Asia. With the abolition of black slavery in 1888, the Brazilian monarchy fell in 1889. By then, another source of cheap labor to work on coffee plantations was found in Japan. Chinese male immigrants arrived in Cuba, Mexico, Peru and elsewhere. With political turmoil in Europe during the mid-nineteenth century and widespread poverty, Germans, Spaniards, and Italians immigrated to Latin America in large numbers, welcomed by Latin American governments both as a source of labor as well as a way to increase the size of their white populations. In Argentina, many Afro-Argentines married Europeans.[159] In twentieth-century Brazil, sociologist Gilberto Freyre proposed that Brazil was a \"racial democracy\", with less discrimination against blacks than in the U.S.[160] Even if a system of legal racial segregation was never implemented in Latin America, unlike the United States, subsequent research has shown that in Brazil there's discrimination against darker citizens, and that whites remain the elites in the country.[161][162] In Mexico, the mestizo population was considered the true embodiment of \"the cosmic race\", according to Mexican intellectual José Vasconcelos, thus erasing other populations. There was considerable discrimination against Asians, with calls for the expulsion of Chinese in northern Mexico during the Mexican Revolution (1910–1920) and racially motivated massacres. In a number of Latin American countries, Indigenous groups have organized explicitly as Indigenous, to claim human rights and influence political power. With the passage of anti-colonial resolutions in the United Nations General Assembly and the signing of resolutions for Indigenous rights, the Indigenous are able to act to guarantee their existence within nation-states with legal standing. Amerindian languages are widely spoken in Peru, Guatemala, Bolivia, Paraguay and Mexico, and to a lesser degree, in Panama, Ecuador, Brazil, Colombia, Venezuela, Argentina, and Chile. In other Latin American countries, the population of speakers of Indigenous languages tend to be very small or even non-existent, for example in Uruguay. Mexico is possibly contains more Indigenous languages than any other Latin American country, but the most-spoken Indigenous language there is Nahuatl. In Peru, Quechua is an official language, alongside Spanish and other Indigenous languages in the areas where they predominate. In Ecuador, while Quichua holds no official status, it is a recognized language under the country's constitution; however, it is only spoken by a few groups in the country's highlands. In Bolivia, Aymara, Quechua and Guaraní hold official status alongside Spanish. Guaraní, like Spanish, is an official language of Paraguay, and is spoken by a majority of the population, which is, for the most part, bilingual, and it is co-official with Spanish in the Argentine province of Corrientes. In Nicaragua, Spanish is the official language, but on the country's Caribbean coast English and Indigenous languages such as Miskito, Sumo, and Rama also hold official status. Colombia recognizes all Indigenous languages spoken within its territory as official, though fewer than 1% of its population are native speakers of these languages. Nahuatl is one of the 62 Native languages spoken by Indigenous people in Mexico, which are officially recognized by the government as \"national languages\" along with Spanish. Other European languages spoken in Latin America include: English, by half of the current population in Puerto Rico, as well as in nearby countries that may or may not be considered Latin American, like Belize and Guyana, and spoken by descendants of British settlers in Argentina and Chile. German is spoken in southern Brazil, southern Chile, portions of Argentina, Venezuela and Paraguay; Italian in Brazil, Argentina, Venezuela, and Uruguay; Ukrainian, Polish, and Russian in southern Brazil and Argentina; and Welsh, in southern Argentina.[163][164][165][166][167][168][excessive citations] Non-European or Asian languages include Japanese in Brazil, Peru, Bolivia, and Paraguay, Korean in Brazil, Argentina, Paraguay, and Chile, Arabic in Argentina, Brazil, Colombia, Venezuela, and Chile, and Chinese throughout South America. Countries like Venezuela, Argentina and Brazil have their own dialects or variations of German and Italian. In several nations, especially in the Caribbean region, creole languages are spoken. The most widely-spoken creole language in Latin America and the Caribbean is Haitian Creole, the predominant language of Haiti, derived primarily from French and certain West African tongues, with Amerindian, English, Portuguese and Spanish influences as well. Creole languages of mainland Latin America, similarly, are derived from European languages and various African tongues. The Garifuna language is spoken along the Caribbean coast in Honduras, Guatemala, Nicaragua and Belize, mostly by the Garifuna people, a mixed-race Zambo people who were the result of mixing between Indigenous Caribbeans and escaped Black slaves. Primarily an Arawakan language, it has influences from Caribbean and European languages. Archaeologists have deciphered over 15 pre-Columbian distinct writing systems from Mesoamerican societies. Ancient Maya had the most sophisticated textually written language, but since texts were largely confined to the religious and administrative elite, traditions were passed down orally. Oral traditions also prevailed in other major Indigenous groups including, but not limited to the Aztecs and other Nahuatl speakers, Quechua and Aymara of the Andean regions, the Quiché of Central America, the Tupi-Guaraní in today's Brazil, the Guaraní in Paraguay and the Mapuche in Chile.[169] According to the detailed Pew multi-country survey in 2014, 69% of the Latin American population is Catholic and 19% is Protestant. Protestants are 26% in Brazil and over 40% in much of Central America. More than half of these are converts from Roman Catholicism.[174][175] The entire hemisphere was settled by migrants from Asia, Europe, and Africa. Native American populations settled throughout the hemisphere before the arrival of Europeans in the late fifteenth and sixteenth centuries, and the forced migration of slaves from Africa. In the post-independence period, a number of Latin American countries sought to attract European immigrants as a source of labor as well as to deliberately change the proportions of racial and ethnic groups within their borders. Chile, Argentina, and Brazil actively recruited labor from Catholic southern Europe, where populations were poor and sought better economic opportunities. Many nineteenth-century immigrants went to the United States and Canada, but a significant number arrived in Latin America. Although Mexico tried to attract immigrants, it largely failed.[176] As black slavery was abolished in Brazil in 1888, coffee growers recruited Japanese migrants to work in coffee plantations. There is a significant population of Japanese descent in Brazil. Cuba and Peru recruited Chinese labor in the late nineteenth century. Some Chinese immigrants who were excluded from immigrating to the U.S. settled in northern Mexico. When the U.S. acquired its southwest by conquest in the Mexican American War, Latin American populations did not cross the border to the U.S., the border crossed them. In the twentieth century there have been several types of migration. One is the movement of rural populations within a given country to cities in search of work, causing many Latin American cities to grow significantly. Another is international movement of populations, often fleeing repression or war. Other international migration is for economic reasons, often unregulated or undocumented. Mexicans immigrated to the U.S. during the violence of the Mexican Revolution (1910–1920)[177] and the religious Cristero War (1926–29);[178] during World War II, Mexican men worked in the U.S. in the bracero program. Economic migration from Mexico followed the crash of the Mexican economy in the 1980s.[179] Spanish refugees fled to Mexico following the fascist victory in the Spanish Civil War (1936–38), with some 50,000 exiles finding refuge at the invitation of President Lázaro Cárdenas.[180] Following World War II a larger wave of refugees to Latin America, many of them Jews, settled in Argentina, Brazil, Chile, Cuba, and Venezuela. Some were only transiting through the region, but others stayed and created communities.[181] A number of Nazis escaped to Latin America, living under assumed names, in an attempting to avoid attention and prosecution. In the aftermath of the Cuban Revolution, middle class and elite Cubans moved to the U.S., particularly to Florida. Some fled Chile for the U.S. and Europe after the 1973 military coup.[182] Colombians migrated to Spain and the United Kingdom during the region's political turmoil, compounded by the rise of narcotrafficking and guerrilla warfare.[183] During the Central American wars of the 1970s to the 1990s, many Salvadorans, Guatemalans, and Hondurans migrated to the U.S. to escape narcotrafficking, gangs, and poverty. As living conditions deteriorated in Venezuela under Hugo Chávez and Nicolás Maduro, many left for neighboring Colombia and Ecuador. In the 1990s, economic stress in Ecuador during the La Década Perdida triggered considerable migration to Spain and to the U.S.[184] Some Latin American countries seek to strengthen links between migrants and their states of origin, while promoting their integration in the receiving state. These emigrant policies focus on the rights, obligations and opportunities for participation of emigrated citizens who already live outside the borders of the country of origin. Research on Latin America shows that the extension of policies towards migrants is linked to a focus on civil rights and state benefits that can positively influence integration in recipient countries. In addition, the tolerance of dual citizenship has spread more in Latin America than in any other region of the world.[185] World map indicating literacy rate by country in 2015 (2015 CIA World Factbook). Grey = no data. Despite significant progress, education access and school completion remains unequal in Latin America. The region has made great progress in educational coverage; almost all children attend primary school, and access to secondary education has increased considerably. Quality issues such as poor teaching methods, lack of appropriate equipment, and overcrowding exist throughout the region. These issues lead to adolescents dropping out of the educational system early. Most educational systems in the region have implemented various types of administrative and institutional reforms that have enabled reach for places and communities that had no access to education services in the early 1990s. School meal programs are also employed to expand access to education, and at least 23 countries in the Latin America and Caribbean region have large-scale school feeding activities, altogether reaching 88% of primary school-age children in the region.[186] Compared to prior generations, Latin American youth have seen an increase in their levels of education. On average, they have completed two more years of school than their parents.[187] However, there are still 23 million children in the region between the ages of 4 and 17 outside of the formal education system. Estimates indicate that 30% of preschool age children (ages 4–5) do not attend school, and for the most vulnerable populations, the poor and rural, this proportion exceeds 40 percent. Among primary school age children (ages 6 to 12), attendance is almost universal; however there is still a need to enroll five million more children in the primary education system. These children mostly live in remote areas, are Indigenous or Afro-descendants and live in extreme poverty.[188] Among people between the ages of 13 and 17 years, only 80% are full-time students, and only 66% of these advance to secondary school. These percentages are lower among vulnerable population groups: only 75% of the poorest youth between the ages of 13 and 17 years attend school. Tertiary education has the lowest coverage, with only 70% of people between the ages of 18 and 25 years outside of the education system. Currently, more than half of low income or rural children fail to complete nine years of education.[188] 2012 map of countries by homicide rate. As of 2015, the Latin American countries with the highest rates were El Salvador (108.64 per 100,000 people), Honduras (63.75) and Venezuela (57.15). The countries with the lowest rates were Chile (3.59), Cuba (4.72) and Argentina (6.53). Latin America and the Caribbean have been cited by numerous sources to be the most dangerous regions in the world.[189] Studies have shown that Latin America contains the majority of the world's most dangerous cities. Many analysts[who?] attribute this to social and income inequality in the region.[190] Many [who?]agree that the prison crisis[clarification needed] will not be resolved until the gap between the rich and the poor is addressed. Crime and violence prevention and public security are now important issues for governments and citizens in Latin America and the Caribbean region. Homicide rates in Latin America are the highest in the world. From the early 1980s through the mid-1990s, homicide rates increased by 50 percent. Latin America and the Caribbean experienced more than 2.5 million murders between 2000 and 2017.[191] There were a total of 63,880 murders in Brazil in 2018.[192] The most frequent victims of such homicides are young men, 69 percent of them between the ages of 15 and 19. Countries with the highest homicide rate per year per 100,000 inhabitants in 2015 were: El Salvador 109, Honduras 64, Venezuela 57, Jamaica 43, Belize 34.4, St. Kitts and Nevis 34, Guatemala 34, Trinidad and Tobago 31, the Bahamas 30, Brazil 26.7, Colombia 26.5, the Dominican Republic 22, St. Lucia 22, Guyana 19, Mexico 16, Puerto Rico 16, Ecuador 13, Grenada 13, Costa Rica 12, Bolivia 12, Nicaragua 12, Panama 11, Antigua and Barbuda 11, and Haiti 10.[193] Most of the countries with the highest homicide rates are in Africa and Latin America. Countries in Central America, like El Salvador and Honduras, top the list of homicides in the world.[194] Brazil has more overall homicides than any country in the world, at 50,108, accounting for one in 10 globally. Crime-related violence is the biggest threat to public health in Latin America, striking more victims than HIV/AIDS or any other infectious disease.[195] Countries with the lowest homicide rate per year per 100,000 inhabitants as of 2015 were: Chile 3, Peru 7, Argentina 7, Uruguay 8 and Paraguay 9.[193][196] Water supply and sanitation in Latin America is characterized by insufficient access and in many cases by poor service quality, with detrimental impacts on public health.[198] Water and sanitation services are provided by a vast array of mostly local service providers under an often fragmented policy and regulatory framework. Financing of water and sanitation remains a serious challenge. As of 2020, Latin America is a predominantly Spanish-Portuguese speaking and predominantly Roman Catholic region Latin America is home to some of the few countries of the world with a complete ban on abortion and minimal policies on reproductive rights, but it also contains some of the most progressive reproductive rights movements in the world.[199] With roots in indigenous groups, the issues of reproductive rights include abortion, sexual autonomy, reproductive healthcare, and access to contraceptive measures. [200] Modern reproductive rights movements most notably include Marea Verde, which has led to much reproductive legislation reform. [201] Cuba has acted as a trail-blazer towards more liberal reproductive laws for the rest of Latin America, while other countries like El Salvador and Honduras have tightened restrictions on reproductive rights. [202] HIV/AIDS has been a public health concern for Latin America due to a remaining prevalence of the disease.[203] In 2018 an estimated 2.2 million people had HIV in Latin America and the Caribbean, making the HIV prevalence rate approximately 0.4% in Latin America.[203] Some demographic groups in Latin America have higher prevalence rates for HIV/ AIDS including men who have sex with men having a prevalence rate of 10.6%, and transgender women having one of the highest rates within the population with a prevalence rate of 17.7%.[204] Female sex workers and drug users also have higher prevalence for the disease than the general population (4.9% and 1%-49.7% respectively).[204] One aspect that has contributed to the higher prevalence of HIV/AIDS in LGBT+ groups in Latin America is the concept of homophobia.[203] Homophobia in Latin America has historically affected HIV service provision through under reported data and less priority through government programs.[205] Antiretroviral treatment coverage has been high, with AIDS related deaths decreasing between 2007 and 2017 by 12%, although the rate of new infections has not seen a large decrease.[203] The cost of antiretroviral medicines remain a barrier for some in Latin America, as well as country wide shortages of medicines and condoms.[206] In 2017 77% of Latin Americans with HIV were aware of their HIV status.[206] The prevention of HIV/AIDS in Latin America among groups with a higher prevalence such as men who have sex with men and transgender women, has been aided with educational outreach, condom distribution, and LGBT+ friendly clinics.[207] Other main prevention methods include condom availability, education and outreach, HIV awareness, and mother-to-child transmission prevention.[203] According to Goldman Sachs' BRICS review of emerging economies, by 2050 the largest economies in the world will be as follows: China, United States, India, Japan, Germany, United Kingdom, Mexico and Brazil.[208] Sugarcane plantation in São Paulo. In 2018, Brazil was the world's largest producer, with 746 million tons. Latin America produces more than half of the world's sugarcane.Soybean plantation in Mato Grosso. In 2020, Brazil was the world's largest producer, with 130 million tons. Latin America produces half of the world's soybeans.Coffee in Minas Gerais. In 2018, Brazil was the world's largest producer, with 3.5 million tons. Latin America produces half of the world's coffee.Oranges in São Paulo. In 2018, Brazil was the world's largest producer, with 17 million tons. Latin America produces 30% of the world's oranges. The four countries with the strongest agricultural sector in South America are Brazil, Argentina, Chile and Colombia. Currently:[210] Chile is one of the five largest world producers of cherries and cranberries, and one of the ten largest world producers of grapes, apples, kiwi, peaches, plums and hazelnuts, focusing on exporting high-value fruits; Colombia is one of the five largest producers in the world of coffee, avocados and palm oil, and one of the ten largest producers in the world of sugarcane, bananas, pineapples and cocoa; Peru is the world's largest producer of quinoa; is one of the five largest producers of avocados, blueberry, artichokes and asparagus; one of the ten largest producers in the world of coffee and cocoa; one of the 15 largest producers in the world of potatoes and pineapples, and also has a large production of grapes, sugarcane, rice, bananas, maize and cassava; its agriculture is considerably diversified; Paraguay is currently the 6th largest producer of soy in the world and entering the list of the 20 largest producers of maize and sugarcane.[211] In Central America, the following stand out: Guatemala is one of the ten largest producers in the world of coffee, sugar cane, melons and natural rubber, and one of the world's 15 largest producers of bananas and palm oil; Honduras is one of the five largest producers of coffee in the world, and one of the ten largest producers of palm oil; Costa Rica is the world's largest producer of pineapples; Dominican Republic is one of the world's top five producers of papayas and avocados, and one of the ten largest producers of cocoa. Mexico is the world's largest producer of avocados, one of the world's top five producers of Chile, lemons, oranges, mangos, papayas, strawberries, grapefruit, pumpkins and asparagus, and one of the world's 10 largest producers of sugar cane, maize, sorghum, beans, tomatoes, coconuts, pineapple, melons and blueberries. Truck of a meat company in Brazil. Latin America produces 25% of the world's beef and chicken meat. Brazil is the world's largest exporter of chicken meat: 3.77 million tons in 2019.[212][213] The country had the second largest herd of cattle in the world, 22.2% of the world herd. The country was the second largest producer of beef in 2019, responsible for 15.4% of global production.[214] It was also the third largest world producer of milk in 2018. This year[when?], the country produced 35.1 billion liters.[215] In 2019, Brazil was the fourth largest pork producer in the world, with almost four million tons.[216] In 2018, Argentina was the fourth largest producer of beef in the world, with a production of 3 million tons (behind only United States, Brazil and China). Uruguay is also a major meat producer. In 2018, it produced 589 thousand tons of beef.[217] In the production of chicken meat, Mexico is among the ten largest producers in the world, Argentina among the 15 largest and Peru and Colombia among the 20 largest. In beef production, Mexico is one of the ten largest producers in the world and Colombia is one of the 20 largest producers. In the production of pork, Mexico is among the 15 largest producers in the world. In the production of honey, Argentina is among the five largest producers in the world, Mexico among the ten largest and Brazil among the 15 largest. In terms of cow's milk production, Mexico is among the 15 largest producers in the world and Argentina among the 20 largest.[218] Mining is one of the most important economic sectors in Latin America, especially for Chile, Peru and Bolivia, whose economies are highly dependent on this sector. The continent has large productions of: In 2019, Peru was the second largest world producer of copper[250] and silver,[245] 8th largest world producer of gold,[251] third largest world producer of lead,[226] second largest world producer of zinc,[252] fourth largest world producer of tin,[253] fifth largest world producer of boron,[254] and fourth largest world producer of molybdenum.[224] In 2019, Bolivia was the eighth largest world producer of silver;[245] fourth largest world producer of boron;[254] fifth largest world producer of antimony;[255] fifth largest world producer of tin;[253] sixth largest world producer of tungsten;[256] seventh largest producer of zinc,[257] and the eighth largest producer of lead.[226][258][259] In 2019, Mexico was the world's largest producer of silver[245] (representing almost 23% of world production, producing more than 200 million ounces in 2019);[260] ninth largest producer of gold,[251] the eighth largest producer of copper,[250] the world's fifth largest producer of lead,[226] the world's sixth largest producer of zinc,[252] the world's fifth largest producer of molybdenum,[224] the world's third largest producer of mercury,[261] the world's fifth largest producer of bismuth,[262] the world's 13th largest producer of manganese[263] and the 23rd largest world producer of phosphate.[264] It is also the eighth largest world producer of salt.[246] In 2019, Argentina was the fourth largest world producer of lithium,[244] the ninth largest world producer of silver,[245] the 17th largest world producer of gold[251] and the seventh largest world producer of boron.[254] Colombia is the world's largest producer of emeralds.[265] In the production of gold, between 2006 and 2017, the country produced 15 tons per year until 2007, when its production increased significantly, breaking a record of 66.1 tons extracted in 2012. In 2017, it extracted 52.2 tons. The country is among the 25 largest gold producers in the world.[266] In the production of silver, in 2017 the country extracted 15,5 tons.[267] In the production of oil, Brazil was the tenth largest oil producer in the world in 2019, with 2.8 million barrels a day. Mexico was the twelfth largest, with 2.1 million barrels a day, Colombia in 20th place with 886 thousand barrels a day, Venezuela was the twenty-first place, with 877 thousand barrels a day, Ecuador in 28th with 531 thousand barrels a day and Argentina. 29th with 507 thousand barrels a day. Since Venezuela and Ecuador consume little oil and export most of their production, they are part of OPEC. Venezuela had a big drop in production after 2015 (when it produced 2.5 million barrels a day), falling in 2016 to 2.2 million, in 2017 to 2 million, in 2018 to 1.4 million and in 2019 to 877 thousand, due to lack of investment.[268] In Latin America, few countries stand out in industrial activity: Brazil, Argentina, Mexico and, less prominently, Chile. Begun late, the industrialization of these countries received a great boost from World War II: this prevented the countries at war from buying the products they were used to importing and exporting what they produced. At that time, benefiting from the abundant local raw material, the low wages paid to the labor force and a certain specialization brought by immigrants, countries such as Brazil, Mexico and Argentina, as well as Venezuela, Chile, Colombia and Peru, were able to implement important industrial parks. In general, in these countries there are industries that require little capital and simple technology for their installation, such as the food processing and textile industries. The basic industries (steel, etc.) also stand out, as well as the metallurgical and mechanical industries.[This paragraph needs citation(s)] The industrial parks of Brazil, Mexico, Argentina and Chile, however, present much greater diversity and sophistication, producing advanced technology items. In the rest of Latin American countries, mainly in Central America, the processing industries of primary products for export predominate.[This paragraph needs citation(s)] Transport in Latin America is basically carried out using the road mode, the most developed in the region. There is also a considerable infrastructure of ports and airports. The railway and fluvial sector, although it has potential, is usually treated in a secondary way. Brazil has more than 1.7 million km of roads, of which 215,000 km are paved, and about 14,000 km are divided highways. The two most important highways in the country are BR-101 and BR-116.[289] Argentina has more than 600,000 km of roads, of which about 70,000 km are paved, and about 2,500 km are divided highways. The three most important highways in the country are Route 9, Route 7 and Route 14.[289] Colombia has about 210,000 km of roads, and about 2,300 km are divided highways.[290] Chile has about 82,000 km of roads, 20,000 km of which are paved, and about 2,000 km are divided highways. The most important highway in the country is the Route 5 (Pan-American Highway)[291] These 4 countries are the ones with the best road infrastructure and with the largest number of double-lane highways, in South America. The roadway network in Mexico has an extent of 366,095 km (227,481 mi),[292] of which 116,802 km (72,577 mi) are paved,[293][294] Of these, 10,474 km (6,508 mi) are multi-lane expressways: 9,544 km (5,930 mi) are four-lane highways and the rest have 6 or more lanes.[293] Due to the Andes Mountains, Amazon River and Amazon Forest, there have always been difficulties in implementing transcontinental or bioceanic highways. Practically the only route that existed was the one that connected Brazil to Buenos Aires, in Argentina and later to Santiago, in Chile. However, in recent years, with the combined effort of countries, new routes have started to emerge, such as Brazil-Peru (Interoceanic Highway), and a new highway between Brazil, Paraguay, northern Argentina and northern Chile (Bioceanic Corridor). The four major seaports concentrating around 60% of the merchandise traffic in Mexico are Altamira and Veracruz in the Gulf of Mexico, and Manzanillo and Lázaro Cárdenas in the Pacific Ocean. Considering all of Latin America, the 10 largest ports in terms of movement are: Colon (Panama), Santos (Brazil), Manzanillo (Mexico), Bahia de Cartagena (Colombia), Pacifico (Panama), Callao (Peru), Guayaquil ( Ecuador), Buenos Aires (Argentina), San Antonio (Chile) and Buenaventura (Colombia).[298] The Brazilian railway network has an extension of about 30,000 kilometers. It is basically used for transporting ores.[299] The Argentine rail network, with 47,000 km of tracks, was one of the largest in the world and continues to be the most extensive in Latin America. It came to have about 100,000 km of rails, but the lifting of tracks and the emphasis placed on motor transport gradually reduced it. It has four different trails and international connections with Paraguay, Bolivia, Chile, Brazil and Uruguay. Chile has almost 7,000 km of railways, with connections to Argentina, Bolivia and Peru. Colombia has only about 3,500 km of railways.[300] Among the main Brazilian waterways, two stand out: Hidrovia Tietê-Paraná (which has a length of 2,400 km, 1,600 on the Paraná River and 800 km on the Tietê River, draining agricultural production from the states of Mato Grosso, Mato Grosso do Sul, Goiás and part of Rondônia, Tocantins and Minas General) and Hidrovia do Solimões-Amazonas (it has two sections: Solimões, which extends from Tabatinga to Manaus, with approximately 1600 km, and Amazonas, which extends from Manaus to Belém, with 1650 km. Almost entirely passenger transport from the Amazon plain is done by this waterway, in addition to practically all cargo transportation that is directed to the major regional centers of Belém and Manaus). In Brazil, this transport is still underutilized: the most important waterway stretches, from an economic point of view, are found in the Southeast and South of the country. Its full use still depends on the construction of locks, major dredging works and, mainly, of ports that allow intermodal integration. In Argentina, the waterway network is made up of the La Plata, Paraná, Paraguay and Uruguay rivers. The main river ports are Zárate and Campana. The port of Buenos Aires is historically the first in individual importance, but the area known as Up-River, which stretches along 67 km of the Santa Fé portion of the Paraná River, brings together 17 ports that concentrate 50% of the total exports of the country. The Brazilian government has undertaken an ambitious program to reduce dependence on imported petroleum. Imports previously accounted for more than 70% of the country's oil needs but Brazil became self-sufficient in oil in 2006–2007. Brazil was the 10th largest oil producer in the world in 2019, with 2.8 million barrels / day. Production manages to supply the country's demand.[268] In the beginning of 2020, in the production of oil and natural gas, the country exceeded 4 million barrels of oil equivalent per day, for the first time. In January this year, 3.168 million barrels of oil per day and 138.753 million cubic meters of natural gas were extracted.[301] Brazil is one of the main world producers of hydroelectric power. In 2019, Brazil had 217 hydroelectric plants in operation, with an installed capacity of 98,581 MW, 60.16% of the country's energy generation.[302] In the total generation of electricity, in 2019 Brazil reached 170,000 megawatts of installed capacity, more than 75% from renewable sources (the majority, hydroelectric).[303][304] In 2013, the Southeast Region used about 50% of the load of the National Integrated System (SIN), being the main energy consuming region in the country. The region's installed electricity generation capacity totaled almost 42,500 MW, which represented about a third of Brazil's generation capacity. The hydroelectric generation represented 58% of the region's installed capacity, with the remaining 42% corresponding basically to the thermoelectric generation. São Paulo accounted for 40% of this capacity; Minas Gerais by about 25%; Rio de Janeiro by 13.3%; and Espírito Santo accounted for the rest. The South Region owns the Itaipu Dam, which was the largest hydroelectric plant in the world for several years, until the inauguration of Three Gorges Dam in China. It remains the second largest operating hydroelectric in the world. Brazil is the co-owner of the Itaipu Plant with Paraguay: the dam is located on the Paraná River, located on the border between countries. It has an installed generation capacity of 14 GW for 20 generating units of 700 MW each. North Region has large hydroelectric plants, such as Belo Monte Dam and Tucuruí Dam, which produce much of the national energy. Brazil's hydroelectric potential has not yet been fully exploited, so the country still has the capacity to build several renewable energy plants in its territory.[305][306] As of July 2022,[ref] according to ONS, total installed capacity of wind power was 22 GW, with average capacity factor of 58%.[307][308] While the world average wind production capacity factors is 24.7%, there are areas in Northern Brazil, specially in Bahia State, where some wind farms record with average capacity factors over 60%;[309][136] the average capacity factor in the Northeast Region is 45% in the coast and 49% in the interior.[310] In 2019, wind energy represented 9% of the energy generated in the country.[311] In 2019, it was estimated that the country had an estimated wind power generation potential of around 522 GW (this, only onshore), enough energy to meet three times the country's current demand.[312][313] In 2021 Brazil was the 7th country in the world in terms of installed wind power (21 GW),[314][315] and the 4th largest producer of wind energy in the world (72 TWh), behind only China, USA and Germany.[316][314] As of October 2022,[ref] according to ONS, total installed capacity of photovoltaic solar was 21 GW, with average capacity factor of 23%.[319] Some of the most irradiated Brazilian States are MG (\"Minas Gerais\"), BA (\"Bahia\") and GO (\"Goiás\"), which have indeed world irradiation level records.[320][136][321] In 2019, solar power represented 1.27% of the energy generated in the country.[311] In 2021, Brazil was the 14th country in the world in terms of installed solar power (13 GW),[322] and the 11th largest producer of solar energy in the world (16.8 TWh).[316] In 2020, Brazil was the 2nd largest country in the world in the production of energy through biomass (energy production from solid biofuels and renewable waste), with 15,2 GW installed.[323] After Brazil, Mexico is the country in Latin America that most stands out in energy production. In 2020, the country was the 14th largest petroleum producer in the world, and in 2018 it was the 12th largest exporter. In natural gas, the country was, in 2015, the 21st largest producer in the world, and in 2007 it was the 29th largest exporter. Mexico was also the world's 24th largest producer of coal in 2018. In renewable energies, in 2020, the country ranked 14th in the world in terms of installed wind energy (8.1 GW), 20th in the world in terms of installed solar energy (5.6 GW) and 19th in the world in terms of installed hydroelectric power (12.6 GW). In third place, Colombia stands out: In 2020, the country was the 20th largest petroleum producer in the world, and in 2015 it was the 19th largest exporter. In natural gas, the country was, in 2015, the 40th largest producer in the world. Colombia's biggest highlight is in coal, where the country was, in 2018, the world's 12th largest producer and the 5th largest exporter. In renewable energies, in 2020, the country ranked 45th in the world in terms of installed wind energy (0.5 GW), 76th in the world in terms of installed solar energy (0.1 GW) and 20th in the world in terms of installed hydroelectric power (12.6 GW). Venezuela, which was one of the world's largest oil producers (about 2.5 million barrels/day in 2015) and one of the largest exporters, due to its political problems, has had its production drastically reduced in recent years: in 2016, it dropped to 2.2 million, in 2017 to 2 million, in 2018 to 1.4 million and in 2019 to 877 thousand, reaching only 300,000 barrels/day at a given point. The country also stands out in hydroelectricity, where it was the 14th country in the world in terms of installed capacity in 2020 (16,5 GW). Argentina was, in 2017, the 18th largest producer in the world, and the largest producer in Latin America, of natural gas, in addition to being the 28th largest oil producer; although the country has the Vaca Muerta field, which holds close to 16 billion barrels of technically recoverable shale oil, and is the second largest shale natural gas deposit in the world, the country lacks the capacity to exploit the deposit: it is necessary capital, technology and knowledge that can only come from offshore energy companies, who view Argentina and its erratic economic policies with considerable suspicion, not wanting to invest in the country. In renewable energies, in 2020, the country ranked 27th in the world in terms of installed wind energy (2.6 GW), 42nd in the world in terms of installed solar energy (0.7 GW) and 21st in the world in terms of installed hydroelectric power (11.3 GW). The country has great future potential for the production of wind energy in the Patagonia region. Chile, although currently not a major energy producer, has great future potential for solar energy production in the Atacama Desert region. Paraguay stands out today in hydroelectric production thanks to the Itaipu Power Plant. Trinidad and Tobago and Bolivia stand out in the production of natural gas, where they were, respectively, the 20th and 31st largest in the world in 2015. Ecuador, because it consumes little energy, is part of OPEC and was the 27th largest oil producer in the world in 2020, being the 22nd largest exporter in 2014.[324][325][326][270][314] China's economic influence in Latin America increased substantially in the 21st century. Imports from China valued $8.3 billion in 2000, but by 2022 its value was $450 billion and had grown to be the largest trading partner of South America, as well as the second-largest for the broader Latin America.[327] In particular, many of the investments are related to the Belt and Road Initiative or energy. China has also provided loans to several Latin American countries; this has raised concerns about the possibility of \"debt traps.\"[328][327] Specifically, Venezuela, Brazil, Ecuador, and Argentina received the most loans from China during 2005–2016.[329] Aerial view of Cancún. Mexico is the most visited country in Latin America and 6th in the world. Income from tourism is key to the economy of several Latin American countries.[330] Mexico is the only Latin American country to be ranked in the top 10 worldwide in the number of tourist visits. It received by far the largest number of international tourists, with 39.3 million visitors in 2017, followed by Argentina, with 6.7 million; then Brazil, with 6.6 million; Chile, with 6.5 million; Dominican Republic, with 6.2 million; Cuba with 4.3 million; Peru and Colombia with 4.0 million. The World Tourism Organization reports the following destinations as the top six tourism earners for the year 2017: Mexico, with US$21,333 million; the Dominican Republic, with US$7,178 million; Brazil, with US$6,024 million; Colombia, with US$4,773 million; Argentina, with US$4,687 million; and Panama, with US$4,258 million.[331] Indigenous cultures of the people who inhabited the continent prior to European colonization. Ancient and advanced civilizations developed their own political, social and religious systems. The Maya, the Aztec and the Inca are examples of these. Indigenous legacies in music, dance, foods, arts and crafts, clothing, folk culture and traditions are strong in Latin America. Indigenous languages affected Spanish and Portuguese, giving rise to loanwords like pampa, taco, tamale, cacique. The culture of Europe was brought mainly by the colonial powers – the Spanish, Portuguese and French – between the 16th and 19th centuries. The most enduring European colonial influences are language, institutions, customs and Catholicism. Additional cultural influences came from the Europe during the 18th, 19th and 20th centuries, due to growing immigration from Germany, Italy, France, Spain and Portugal; as well as artistic, ideological and technological developments of the time. Due to the impact of Enlightenment ideals after the French revolution, a certain number of Iberian American countries decriminalized homosexuality after France and French territories in the Americas did so in 1791. Some of the countries that abolished sodomy laws or banned state interference in consensual adult sexuality in the 19th century were Dominican Republic (1822), Brazil (1824), Peru (1836), Mexico (1871), Paraguay (1880), Argentina (1887), Honduras (1899), Guatemala, and El Salvador. Today same-sex marriage is legal in Argentina, Brazil, Chile, Colombia, Costa Rica, Ecuador, Mexico, Uruguay, and French overseas departments. South America experienced waves of immigration of Europeans, especially Italians, Spaniards, Portuguese, Germans, Austrians, Poles, Ukrainians, French, Dutch, Russians, Croatians, Lithuanians, and Ashkenazi Jews. With the end of colonialism, French culture also exerted a direct influence in Latin America, especially in the realms of high culture, Independentism, science and medicine.[340] This can be seen in the region's artistic traditions, including painting, literature, and music, and in the realms of science and politics. African cultures, whose presence stems from a long history of the Atlantic slave trade. People of African descent have influenced the ethno-scapes of Latin America and the Caribbean. This is manifested for instance in music, dance and religion, especially in countries like Brazil, Puerto Rico, Venezuela, Colombia, Panama, Haiti, Honduras, Nicaragua, Costa Rica, Dominican Republic, and Cuba. Asian cultures, whose part of the presence derives from the long history of the coolies who mostly arrived during the 19th and 20th centuries, most commonly Chinese workers in Peru and Venezuela, but also from Japanese and Korean immigration. especially headed to Brazil. This has greatly affected cuisine and other traditions including literature, art and lifestyles and politics. Asian influences have especially affected Brazil, Cuba, Panama and Peru. The influence of the United States and globalization is present throughout the region, with particular strength in northern Latin America, especially Puerto Rico, which is an American territory. Prior to 1959, Cuba, which fought for its independence with American aid in the Spanish–American War, also had a close political and economic relationship with the United States. The United States also helped Panama become independent from Colombia and built the twenty-mile-long Panama Canal Zone in Panama, which it held from 1903—the Panama Canal opened to transoceanic freight traffic in 1914—to 1999, when the Torrijos-Carter Treaties restored Panamanian control of the Canal Zone. Beyond the tradition of Indigenous art, the development of Latin American visual art owed much to the influence of Spanish, Portuguese and French Baroque painting, which in turn often followed the trends of the Italians. In general, artistic Eurocentrism began to wane in the early twentieth century with the increased appreciation for indigenous forms of representation.[341] Painter Frida Kahlo, one of the most famous Mexican artists, painted about her own life and the Mexican culture in a style combining Realism, Symbolism and Surrealism. Kahlo's work commands the highest selling price of all Latin American paintings.[344] Colombian sculptor and painter Fernando Botero has gained regional and international recognition for his works which, on first examination, are noted for their exaggerated proportions and the corpulence of the human and animal figures.[345][346][347] The Ecuadorian Oswaldo Guayasamín, considered one of the most important and seminal artists in Ecuador and South America. In his life, he made over 13,000 paintings and held more than 180 exhibitions all over the world, including Paris, Barcelona, New York, Buenos Aires, Moscow, Prague, and Rome. He brought his unique style of expressionism and cubism to the collection of Ecuador artwork during the Age of Anger which relates to the period of the Cold War when the United States opposed communist presence in South America.[348] Social criticism of human and social inequality was central to his artwork.[349] Latin American film is both rich and diverse. Historically, the main centers of production have been Mexico, Argentina, Brazil, and Cuba. Latin American film flourished after sound was introduced in cinema, which added a linguistic barrier to the export of Hollywood film south of the border.[350] In Brazil, the Cinema Novo movement created a particular way of making movies with critical and intellectual screenplays, clearer photography related to the light of the outdoors in a tropical landscape, and a political message. The modern Brazilian film industry has become more profitable inside the country, and some of its productions have received prizes and recognition in Europe and the United States, with movies such as Central do Brasil (1999), Cidade de Deus (2002) and Tropa de Elite (2007). Venezuelan television has also had a great impact in Latin America, is said that whilst \"Venezuelan cinema began sporadically in the 1950s[, it] only emerged as a national-cultural movement in the mid-1970s\" when it gained state support and auteurs could produce work. International co-productions with Latin America and Spain continued into this era and beyond, and Venezuelan films of this time were counted among the works of New Latin American Cinema. This period is known as Venezuela's Golden Age of cinema, having massive popularity even though it was a time of much social and political upheaval. One of the most famous Venezuelan films, even to date, is the 1976 film Soy un delincuente by Clemente de la Cerda, which won the Special Jury Prize at the 1977 Locarno International Film Festival. Soy un delincuente was one of nine films for which the state gave substantial funding to produce, made in the year after the Venezuelan state began giving financial support to cinema in 1975. The support likely came from increased oil wealth in the early 1970s, and the subsequent 1973 credit incentive policy. At the time of its production the film was the most popular film in the country, and took a decade to be usurped from this position, even though it was only one in a string of films designed to tell social realist stories of struggle in the 1950s and '60s. Equally famous is the 1977 film El Pez que Fuma (Román Chalbaud). In 1981 FONCINE (the Venezuelan Film Fund) was founded, and this year it provided even more funding to produce seventeen feature films. A few years later in 1983 with Viernes Negro, oil prices dropped and Venezuela entered a depression which prevented such extravagant funding, but film production continued; more transnational productions occurred, many more with Spain due to Latin America's poor economic fortune in general, and there was some in new cinema, as well: Fina Torres' 1985 Oriana won the Caméra d'Or Prize at the 1985 Cannes Film Festival as the best first feature. Film production peaked in 1984–5,:37 with 1986 considered Venezuelan cinema's most successful year by the state, thanks to over 4 million admissions to national films, according to Venezuelanalysis. The Venezuelan capital of Caracas hosted the Ibero-American Forum on Cinematography Integration in 1989, from which the pan-continental IBERMEDIA was formed; a union which provides regional funding. Pre-Columbian cultures were primarily oral, although the Aztecs and Maya, for instance, produced elaborate codices. Oral accounts of mythological and religious beliefs were also sometimes recorded after the arrival of European colonizers, as was the case with the Popol Vuh. Moreover, a tradition of oral narrative survives to this day, for instance among the Quechua-speaking population of Peru and the Quiché (K'iche') of Guatemala. From the very moment of Europe's discovery of the continents, early explorers and conquistadores produced written accounts and crónicas of their experience – such as Columbus's letters or Bernal Díaz del Castillo's description of the conquest of Mexico. During the colonial period, written culture was often in the hands of the church, within which context Sor Juana Inés de la Cruz wrote memorable poetry and philosophical essays. Towards the end of the 18th century and the beginning of the 19th, a distinctive criollo literary tradition emerged, including the first novels such as Lizardi's El Periquillo Sarniento (1816). The 19th century was a period of \"foundational fictions\" in critic Doris Sommer's words, novels in the Romantic or Naturalist traditions that attempted to establish a sense of national identity, and which often focussed on the Indigenous question or the dichotomy of \"civilization or barbarism\" (for which see, say, Domingo Sarmiento's Facundo (1845), Juan León Mera's Cumandá (1879), or Euclides da Cunha's Os Sertões (1902)). The 19th century also witnessed the realist work of Machado de Assis, who made use of surreal devices of metaphor and playful narrative construction, much admired by critic Harold Bloom. At the turn of the 20th century, modernismo emerged, a poetic movement whose founding text was Nicaraguan poet Rubén Darío's Azul (1888). This was the first Latin American literary movement to influence literary culture outside of the region, and was also the first truly Latin American literature, in that national differences were no longer so much at issue. José Martí, for instance, though a Cuban patriot, also lived in Mexico and the United States and wrote for journals in Argentina and elsewhere. Latin America has produced many successful worldwide artists in terms of recorded global music sales. Among the most successful have been Juan Gabriel (Mexico) only Latin American musician to have sold over 200 million records worldwide,[352]Gloria Estefan (Cuba), Carlos Santana, Luis Miguel (Mexico) of whom have sold over 90 million records, Shakira (Colombia) and Vicente Fernández (Mexico) with over 50 million records sold worldwide. Enrique Iglesias, although not a Latin American, has also contributed for the success of Latin music. Latin Caribbean music, such as merengue, bachata, salsa, and more recently reggaeton, from such countries as the Dominican Republic, Puerto Rico, Cuba, and Panama, has been strongly influenced by African rhythms and melodies. Haiti's compas is a genre of music that is influenced by its Latin Caribbean counterparts, along with elements of jazz and modern sounds. The French Antillean zouk (derived from Haitian compas) is a musical style originating from the Caribbean islands of Guadeloupe and Martinique and popularized by the French Antillean Creole band Kassav' in the early 1980s then became popular mainly in the Caribbean islands of Saint Lucia, Dominica, and Haiti, all in the French Antilles (French West Indies).[353][354] Other influential Latin American sounds include the Antillean soca and calypso, Dennery segment which is a style of Soca music developed in Saint Lucia in the early 2010s which came from Kuduro music, Zouk influence and Lucian drums alongside lyrics usually sung in French Antillean Creole Kwéyòl, Bouyon music is a mixture of Soca, Zouk, and traditional genres native to Dominica which is sung in French Antillean Creole and is one of the most popular musical genres in Dominica, the Honduran (Garifuna) punta, the Colombian cumbia and vallenato, the Chilean cueca, the Ecuadorian boleros, and rockoleras, the Mexican ranchera and the mariachi which is the epitome of Mexican soul, the Nicaraguan palo de Mayo, the Peruvian marinera and tondero, the Uruguayan candombe and the various styles of music from pre-Columbian traditions that are widespread in the Andean region. The classical composer Heitor Villa-Lobos (1887–1959) worked on the recording of Native musical traditions within his homeland of Brazil. The traditions of his homeland heavily influenced his classical works.[355] Also notable is the recent work of the Cuban Leo Brouwer, Uruguayan-American Miguel del Águila, guitar works of the Venezuelan Antonio Lauro and the Paraguayan Agustín Barrios. Latin America has also produced world-class classical performers such as the Chilean pianist Claudio Arrau, Brazilian pianist Nelson Freire and the Argentine pianist and conductor Daniel Barenboim. Brazilian opera soprano Bidu Sayão, one of Brazil's most famous musicians, was a leading artist of the Metropolitan Opera in New York City from 1937 to 1952. More recently, reggaeton, which blends Jamaican reggae and dancehall with Latin America genres such as bomba and plena, as well as hip hop, is becoming more popular, in spite of the controversy surrounding its lyrics, dance steps (Perreo) and music videos. It has become very popular among populations with a \"migrant culture\" influence – both Latino populations in the United States, such as southern Florida and New York City, and parts of Latin America where migration to the United States is common, such as Trinidad and Tobago, Dominican Republic, Colombia, Ecuador, El Salvador, and Mexico.[357] ^Includes the population estimates for South American and Central American countries excluding Belize, Guyana, the United States, and Spanish, French and French Creole-speaking Caribbean countries and territories, as listed under \"Subregions and countries\" ^McGuiness, Aims (2003). \"Searching for 'Latin America': Race and Sovereignty in the Americas in the 1850s\" in Appelbaum, Nancy P. et al. (eds.). Race and Nation in Modern Latin America. Chapel Hill: University of North Carolina Press, 87–107. ISBN978-0-8078-5441-9 ^Gutierrez, Ramon A. (2016). \"What's in a Name?\". In Gutierrez, Ramon A.; Almaguer, Tomas (eds.). The New Latino Studies Reader: A Twenty-First-Century Perspective. Berkeley: University of California Press. p. 34. ISBN978-0-520-28484-5. OCLC1043876740. The word latinoamericano emerged in the years following the wars of independence in Spain's former colonies [...] By the late 1850s, californios were writing in newspapers about their membership in América latina (Latin America) and latinoamerica, calling themselves Latinos as the shortened name for their hemispheric membership in la raza latina (the Latin race). Reprinting an 1858 opinion piece by a correspondent in Havana on race relations in the Americas, El Clamor Publico of Los Angeles surmised that 'two rival races are competing with each other ... the Anglo Saxon and the Latin one [la raza latina].' ^Scott, Rebecca and others, The Abolition of Slavery and the Aftermath of Emancipation in Brazil, Duke University Press 1988 ISBN0822308886Seymour Drescher, Chap. 2: \"Brazilian Abolition in Comparative Perspective\" ^Overmyer-Velázquez, Rebecca. \"The anti-quincentenary campaign in Guerrero, Mexico: Indigenous identity and the dismantling of the myth of the revolution\". Berkeley journal of sociology (2002): 79–112. ^Nutini, Hugo; Isaac, Barry (2009). Social Stratification in central Mexico 1500–2000. University of Texas Press. p. 55. There are basically four operational categories that may be termed ethnic or even racial in Mexico today: (1) güero or blanco (white), denoting European and Near East extraction; (2) criollo (creole), meaning light mestizo in this context but actually of varying complexion; (3) mestizo, an imprecise category that includes many phenotypic variations; and (4) indio, also an imprecise category. These are nominal categories, and neither güero/blanco nor criollo is a widely used term (see Nutini 1997: 230). Nevertheless, there is a popular consensus in Mexico today that these four categories represent major sectors of the nation and that they can be arranged into a rough hierarchy: whites and creoles at the top, a vast population of mestizos in the middle, and Indians (perceived as both a racial and an ethnic component) at the bottom."}
{"url": "https://en.wikipedia.org/wiki/File:R%C3%A9gnier_Penitent_Mary_Magdalene.jpg", "text": "The official position taken by the Wikimedia Foundation is that \"faithful reproductions of two-dimensional public domain works of art are public domain\". This photographic reproduction is therefore also considered to be in the public domain in the United States. In other jurisdictions, re-use of this content may be restricted; see Reuse of PD-Art photographs for details."}
{"url": "https://en.wikipedia.org/wiki/Feud", "text": "A feud/fjuːd/, also known in more extreme cases as a blood feud, vendetta, faida, clan war, gang war, private war, or mob war, is a long-running argument or fight, often between social groups of people, especially families or clans. Feuds begin because one party perceives itself to have been attacked, insulted, injured, or otherwise wronged by another. Intense feelings of resentment trigger an initial retribution, which causes the other party to feel greatly aggrieved and vengeful. The dispute is subsequently fuelled by a long-running cycle of retaliatory violence. This continual cycle of provocation and retaliation usually makes it extremely difficult to end the feud peacefully. Feuds can persist for generations and may result in extreme acts of violence. They can be interpreted as an extreme outgrowth of social relations based in family honor. A mob war is a time when two or more rival families begin open warfare with one another, destroying each other's businesses and assassinating family members. Mob wars are generally disastrous for all concerned, and can lead to the rise or fall of a family. Until the early modern period, feuds were considered legitimate legal instruments[1] and were regulated to some degree. For example, Montenegrin culture calls this krvna osveta, meaning \"blood revenge\", which had unspoken[dubious – discuss] but highly valued rules.[2] In Albanian culture it is called gjakmarrja, which usually lasts for generations. In tribal societies, the blood feud, coupled with the practice of blood wealth, functioned as an effective form of social control for limiting and ending conflicts between individuals and groups who are related by kinship, as described by anthropologist Max Gluckman in his article \"The Peace in the Feud\"[3] in 1955. A blood feud is a feud with a cycle of retaliatory violence, with the relatives or associates of someone who has been killed or otherwise wronged or dishonored seeking vengeance by killing or otherwise physically punishing the culprits or their relatives. In the English-speaking world, the Italian word vendetta is used to mean a blood feud; in Italian, however, it simply means (personal) 'vengeance' or 'revenge', originating from the Latinvindicta (vengeance), while the word faida would be more appropriate for a blood feud. In the English-speaking world, \"vendetta\" is sometimes extended to mean any other long-standing feud, not necessarily involving bloodshed. Sometimes it is not mutual, but rather refers to a prolonged series of hostile acts waged by one person against another without reciprocation.[4] Blood feuds were common in societies with a weak rule of law (or where the state did not consider itself responsible for mediating this kind of dispute), where family and kinship ties were the main source of authority. An entire family was considered responsible for the actions of any of its members. Sometimes two separate branches of the same family even came to blows, or further, over some dispute. Ponte dei Pugni ('Bridge of Fists') in Venice was used for an annual fist fight competition between the inhabitants of different zones of the city. The practice has mostly disappeared with more centralized societies where law enforcement and criminal law take responsibility for punishing lawbreakers. The blood feud has certain similarities to the ritualized warfare found in many pre-industrial tribes. For instance, more than a third of Ya̧nomamö males, on average, died from warfare. The accounts of missionaries to the area have recounted constant infighting in the tribes for women or prestige, and evidence of continuous warfare for the enslavement of neighboring tribes, such as the Macu, before the arrival of European settlers and government.[5] In Homericancient Greece, the practice of personal vengeance against wrongdoers was considered natural and customary: \"Embedded in the Greek morality of retaliation is the right of vengeance... Feud is a war, just as war is an indefinite series of revenges; and such acts of vengeance are sanctioned by the gods\".[6] In ancient Hebrew law, it was considered the duty of the individual and family to avenge unlawful bloodshed, on behalf of God and on behalf of the deceased. The executor of the law of blood-revenge who personally put the initial killer to death was given a special designation: go'el haddam, the blood-avenger or blood-redeemer (Book of Numbers 35: 19, etc.). Six Cities of Refuge were established to provide protection and due process for any unintentional manslayers. The avenger was forbidden from harming an unintentional killer if the killer took refuge in one of these cities. As the Oxford Companion to the Bible states: \"Since life was viewed as sacred (Genesis 9.6), no amount of blood money could be given as recompense for the loss of the life of an innocent person; it had to be \"life for life\" (Exodus 21.23; Deuteronomy 19.21)\".[7] The Celtic phenomenon of the blood feud demanded \"an eye for an eye\", and usually descended into murder. Disagreements between clans might last for generations in Scotland and Ireland. In Scandinavia in the Viking era, feuds were common, as the lack of a central government left dealing with disputes up to the individuals or families involved. Sometimes, these would descend into \"blood revenges\", and in some cases would devastate whole families. The ravages of the feuds as well as the dissolution of them is a central theme in several of the Icelandic sagas.[8] An alternative to feud was blood money (or weregild in the Norse culture), which demanded a set value to be paid by those responsible for a wrongful permanent disfigurement or death, even if accidental. If these payments were not made, or were refused by the offended party, a blood feud could ensue.[9] The Middle Ages, from beginning to end, and particularly the feudal era, lived under the sign of private vengeance. The onus, of course, lay above all on the wronged individual; vengeance was imposed on him as the most sacred of duties ... The solitary individual, however, could do but little. Moreover, it was most commonly a death that had to be avenged. In this case the family group went into action and the faide (feud) came into being, to use the old Germanic word which spread little by little through the whole of Europe—'the vengeance of the kinsmen which we call faida', as a German canonist expressed it. No moral obligation seemed more sacred than this ... The whole kindred, therefore, placed as a rule under the command of a chieftain, took up arms to punish the murder of one of its members or merely a wrong that he had suffered.[10] A kasbah in the Dades valley, High Atlas. Historically, tribal feuding and banditry were a way of life for the Berbers of Morocco.[citation needed] As a result, hundreds of ancient kasbahs were built. Rita of Cascia, a popular 15th-century Italian saint, was canonized by the Catholic Church due mainly to her great effort to end a feud in which her family was involved and which claimed the life of her husband. In Greece, the custom of blood feud is found in several parts of the country, for instance in Crete and Mani.[12] Throughout history, the Maniots have been regarded by their neighbors and their enemies as fearless warriors who practice blood feuds, known in the Maniot dialect of Greek as \"Γδικιωμός\" (Gdikiomos). Many vendettas went on for months, some for years. The families involved would lock themselves in their towers and, when they got the chance, would murder members of the opposing family. The Maniot vendetta is considered the most vicious and ruthless;[citation needed] it has led to entire family lines being wiped out. The last vendetta on record required the Greek Army with artillery support to force it to a stop. Regardless of this, the Maniot Greeks still practice vendettas, even today. Maniots in America, Australia, Canada and Corsica still have on-going vendettas which have led to the creation of mafia families known as \"Γδικιωμέοι\" (Gdikiomeoi).[13][failed verification] In the Spanish Late Middle Ages, the Vascongadas was ravaged by the War of the Bands, which were bitter partisan wars between local ruling families. In the region of Navarre, next to Vascongadas, these conflicts became polarised in a violent struggle between the Agramont and Beaumont parties. In Biscay, in Vascongadas, the two major warring factions were named Oinaz and Gamboa. (Cf. the Guelphs and Ghibellines in Italy). High defensive structures (\"towers\") built by local noble families, few of which survive today, were frequently razed by fires, and sometimes by royal decree. Leontiy Lyulye, an expert on conditions in the Caucasus, wrote in the mid-19th century: \"Among the mountain people the blood feud is not an uncontrollable permanent feeling such as the vendetta is among the Corsicans. It is more like an obligation imposed by the public opinion.\" In the Dagestaniaul of Kadar, one such blood feud between two antagonistic clans lasted for nearly 260 years, from the 17th century until the 1860s.[15] The defensive towers built by feuding clans of Svaneti, in the Caucasus mountains In Japan's feudal past, the samurai class upheld the honor of their family, clan, and their lord by katakiuchi (敵討ち), or revenge killings. These killings could also involve the relatives of an offender. While some vendettas were punished by the government, such as that of the Forty-seven Ronin, others were given official permission with specific targets. A fortified tower used as refuge for men involved in a blood feud who are vulnerable to attack. Thethi, northern Albania. In Albania, gjakmarrja (blood feuding) is a tradition. Blood feuds in Albania trace back to the Kanun, this custom is also practiced among the Albanians of Kosovo. It returned to rural areas after more than 40 years of being abolished by Albanian Communists led by Enver Hoxha. Blood feuds have also been part of a centuries-old tradition in Kosovo, tracing back to the Kanun, a 15th-century codification of Albanian customary rules. In the early 1990s, most cases of blood feuds were reconciled in the course of a large-scale reconciliation movement to end blood feuds led by Anton Çetta.[60] The largest reconciliation gathering took place at Verrat e Llukës on 1 May 1990, which had between 100,000 and 500,000 participants. By 1992, the reconciliation campaign ended at least 1,200 deadly blood feuds, and in 1993, not a single homicide occurred in Kosovo.[60][61] Criminal gang feuds also exist in Dublin, Ireland and in the Republic's third-largest city, Limerick. Traveller feuds are also common in towns across the country. Feuds can be due to personal issues, money, or disrespect, and grudges can last generations. Since 2001, over 300 people have been killed in feuds between different drugs gangs, dissident republicans, and Traveller families.[62][failed verification] Family and clan feuds, known locally as rido, are characterized by sporadic outbursts of retaliatory violence between families and kinship groups, as well as between communities. It can occur in areas where the government or a central authority is weak, as well as in areas where there is a perceived lack of justice and security. Rido is a Maranao term commonly used in Mindanao to refer to clan feuds. It is considered one of the major problems in Mindanao because, apart from numerous casualties, rido has caused destruction of property, crippled local economies, and displaced families. Located in the southern Philippines, Mindanao is home to a majority of the country's Muslim community, and includes the Autonomous Region in Muslim Mindanao. Mindanao \"is a region suffering from poor infrastructure, high poverty, and violence that has claimed the lives of more than 120,000 in the last three decades.\"[63] There is a widely held stereotype that the violence is perpetrated by armed groups that resort to terrorism to further their political goals, but the actual situation is far more complex. While the Muslim-Christian conflict and the state-rebel conflicts dominate popular perceptions and media attention, a survey commissioned by The Asia Foundation in 2002—and further verified by a recent Social Weather Stations survey—revealed that citizens are more concerned about the prevalence of rido and its negative impact on their communities than the conflict between the state and rebel groups.[64] The unfortunate interaction and subsequent confusion of rido-based violence with secessionism, communistinsurgency, banditry, military involvement and other forms of armed violence shows that violence in Mindanao is more complicated than what is commonly believed. Rido has wider implications for conflict in Mindanao, primarily because it tends to interact in unfortunate ways with separatist conflict and other forms of armed violence. Many armed confrontations in the past involving insurgent groups and the military were triggered by a local rido. The studies cited above investigated the dynamics of rido with the intention of helping design strategic interventions to address such conflicts. The causes of rido are varied and may be further complicated by a society's concept of honor and shame, an integral aspect of the social rules that determine accepted practices in the affected communities. The triggers for conflicts range from petty offenses, such as theft and jesting, to more serious crimes, like homicide. These are further aggravated by land disputes and political rivalries, the most common causes of rido. Proliferation of firearms, lack of law enforcement and credible mediators in conflict-prone areas, and an inefficient justice system further contribute to instances of rido. Studies on rido have documented a total of 1,266 rido cases between the 1930s and 2005, which have killed over 5,500 people and displaced thousands. The four provinces with the highest numbers of rido incidences are: Lanao del Sur (377), Maguindanao (218), Lanao del Norte (164), and Sulu (145). Incidences in these four provinces account for 71% of the total documented cases. The findings also show a steady rise in rido conflicts in the eleven provinces surveyed from the 1980s to 2004. According to the studies, during 2002–2004, 50% (637 cases) of total rido incidences occurred, equaling about 127 new rido cases per year. Out of the total number of rido cases documented, 64% remain unresolved.[64] Rido conflicts are either resolved, unresolved, or reoccurring. Although the majority of these cases remain unresolved, there have been many resolutions through different conflict-resolving bodies and mechanisms. These cases can utilize the formal procedures of the Philippine government or the various indigenous systems. Formal methods may involve official courts, local government officials, police, and the military. Indigenous methods to resolve conflicts usually involve elder leaders who use local knowledge, beliefs, and practices, as well as their own personal influence, to help repair and restore damaged relationships. Some cases using this approach involve the payment of blood money to resolve the conflict. Hybrid mechanisms include the collaboration of government, religious, and traditional leaders in resolving conflicts through the formation of collaborative groups. Furthermore, the institutionalization of traditional conflict resolution processes into laws and ordinances has been successful with the hybrid method approach. Other conflict-resolution methods include the establishment of ceasefires and the intervention of youth organizations.[64] BBC: \"In pictures: Egypt vendetta ends\". May 2005. \"One of the most enduring and bloody family feuds of modern times in Upper Egypt has ended with a tense ceremony of humiliation and forgiveness. [...] Police are edgy. After lengthy peace talks, no one knows if the penance—and a large payment of blood money—will end the vendetta which began in 1991 with a children's fight.\""}
{"url": "https://en.wikipedia.org/wiki/Russian_mafia", "text": "The Russian mafia (Russian: ру́сская ма́фияrússkaya máfiya[ˈruskəjəˈmafʲɪjə] or росси́йская ма́фияrossíyskaya máfiya[rɐˈsʲijskəjəˈmafʲɪjə]),[2] otherwise referred to as Bratva (братва́bratvá[brɐtˈva], lit.'brotherhood'), is a collective of various organized crime related elements originating in the former Soviet Union (FSU). Any of the mafia's groups may be referred to as an \"Organized Criminal Group\" (OPG). This is sometimes modified to include a specific name, such as the Orekhovskaya OPG. The \"P\" in the initialism comes from the Russian word for criminal: prestupnaya. Sometimes, the Russian word is dropped in favour of a full translation, and OCG is used instead of OPG. In 2012, there were as many as 6,000 groups,[4] with more than 200 of them having a global reach. Criminals of these various groups are either former prison members, corrupt officials and business leaders, people with ethnic ties, or people from the same region with shared criminal experiences and leaders.[clarification needed][5] In December 2009, Timur Lakhonin, the head of the National Central Bureau of Interpol within Russia, stated \"Certainly, there is crime involving our former compatriots abroad, but there is no data suggesting that an organized structure of criminal groups comprising former Russians exists abroad\" on the topic of international Russian criminal gangs.[6] In August 2010, Alain Bauer, a criminologist from France, said that the Russian mafia \"is one of the best structured criminal organizations in Europe, with a quasi-military operation\" in their international activities.[7] Since the 1980s, the Russian mafia has been among the world's most powerful, dangerous, and feared criminal organizations. As of 2022[update], it remains among the world's largest, deadliest, and most powerful crime syndicates. The collective Russian mafia groups have been referred to as a \"criminal superpower\" by the FBI. The Russian mafia is similar to the Italian mafia in many ways, in that the groups' organizations and structures follow a similar model. The two groups also share a similar portfolio of criminal activity. The highly publicized Italian mafia is believed to have inspired early criminal groups in Russia to form mafia-like organizations, eventually spawning their own version. The Russian mafia, however, differed from the Italians due to their environment. The level of political corruption and arms sales in post-Soviet Russia allowed for their massive expansion, as well as the incorporation of many government officials into the crime syndicates. Russian mafia groups have also been involved in uranium trading, stolen from the Soviet nuclear program, and human trafficking, among other serious activities.[8] Russian criminality can be traced back to Russia's imperial period, which began in the 1720s in the form of banditry and thievery. Most of the population were peasants, in poverty at the time, and criminals who stole from government entities and divided profits among the people earned Robin Hood-like status, being viewed as protectors of the poor and becoming folk heros. In time,[when?] the Vorovskoy Mir (Thieves' World) emerged as these criminals grouped and started their own code of conduct that was based on strict loyalty to one another and opposition against the government. When the Bolshevik Revolution came around in 1917, the Thieves' World was alive and active. Vladimir Lenin attempted to wipe them out, but it failed, and the criminals survived into Joseph Stalin's reign.[9] During Stalin's reign as ruler, millions of people were sent to gulags (Soviet labor camps), where powerful criminals worked their way up to become vorami v zakone (\"thieves-in-law\"). These criminal elites often conveyed their status through complicated tattoos, symbols still used by Russian mobsters.[9] Before coming to power, Joseph Stalin himself was engaged in revolutionary criminal activity during the early 1900s.(See Early life of Joseph Stalin) After Hitler's invasion of the Soviet Union during World War II, Stalin recruited more men to fight for the nation, offering prisoners freedom if they joined the army. Many flocked to help out in the war, but this act betrayed the codes of the Thieves' World that one must not ally with the government. Those who chose not to fight in the war referred to the traitors as suka (\"bitch\"), and the traitors landed at the bottom of the \"hierarchy.\" Outcasts, the suki separated from the others and formed their own groups and power bases by collaborating with prison officials, eventually gaining the luxury of comfortable positions. Bitterness between the groups erupted into a series of Bitch Wars from 1945 to 1953, with many killed every day. The prison officials encouraged the violence, seeing it as a way to rid the prisons of criminals.[10][5][9] While Hitler’s invasion of Russia during WWII caused countless casualties on the battlefield, it also led to one of the more violent periods in the history of Russian organized crime. [citation needed] In 1941, as the German army approached, Stalin desperately looked for ways to bolster the Russian army’s numbers. Turning to the seemingly endless supply of able-bodied men overflowing the gulags and prison system, Stalin promised the vor a chance to win back their freedom by defending Russia against the imminent attack. Joining the army to fight for Stalin (cooperating with the government) was a flagrant violation of the criminal code of honor. Yet, for many, this offer was too tempting to refuse. Thousands of prisoners signed up to defend against the Nazi threat and regain their freedom; that freedom, however, proved to be only momentary. [citation needed] Following the conclusion of the war in 1945, Stalin reneged on his initial promise, throwing the vor soldiers right back into the gulags that they had so desperately tried to escape. This marked the beginning of what would be known as the \"Suki Wars.\" Though the prison system had never been a particularly safe haven, to begin with, the return to the gulags was a death sentence for the vor who had fought in the Red Army. To the vory v zakone, cooperating with the government was tantamount to treason; therefore, the thieves who had remained in prison saw the actions of the thieves-turned-soldiers as the ultimate betrayal. These \"traitors,\" called suki, were systematically slaughtered in the gulags as a punishment for their treachery and cowardice. The prison guards did nothing to stop the massacre and, in fact, often encouraged the violence, as they viewed it as a quick and cost-effective method for thinning the criminal ranks within the prison system. It is unknown just how many suki were killed during this extermination process, but in 1953, eight million prisoners were finally released. By then, the culture of the Russian criminal underworld had been irreparably altered—no longer did a criminal need to abide by the antiquated rules of the old \"Thieves’ World.\" [citation needed] Also during the 1970s and 1980s, the United States expanded its immigration policies, allowing Soviet Jews, with most settling in a southern Brooklyn area known as Brighton Beach (sometimes nicknamed \"Little Odessa\"). Here is where Russian organized crime began in the US.[5][11] The earliest known case of Russian crime in the area was in the mid-1970s by the \"Potato Bag Gang,\" a group of con artists disguised as merchants that told customers that they were selling antique gold rubles for cheap, but in fact, gave them bags of potatoes when bought in thousands. By 1983, the head of Russian organized crime in Brighton Beach was Evsei Agron.[12] Pauol Mirzoyan was a prime target among other mobsters, including rival Boris Goldberg and his organization,[13] and in May 1985, Agron was assassinated. Boris \"Biba\" Nayfeld, his bodyguard, moved on to employ under Marat Balagula, who was believed to have succeeded Agron's authority. In the following year, Balagula fled the country after he was convicted in a fraud scheme of Merrill Lynch customers and was found in Frankfurt, West Germany, in 1989, where he was extradited back to the US and sentenced to eight years in prison.[12] Balagula would later be convicted on a separate $360,000 credit card fraud in 1992.[14] Nayfield took Balagula's place, partnering with the \"Polish Al Capone\", Ricardo Fanchiniin, in an import-export business and setting up a heroin business.[15] In 1990, his former friend, Monya Elson, back from a six-year prison sentence in Israel, returned to America and set up a rival heroin business, culminating in a mafia turf war.[16] When the USSR collapsed and a free market economy emerged, organized criminal groups began to take over Russia's economy, with many ex-KGB agents and veterans of the Afghan war offering their skills to the crime bosses.[3][17] Gangster summit meetings had taken place in hotels and restaurants shortly before the Soviet's dissolution, so that top vory v zakone could agree on who would rule what, and set plans on how to take over the post-communist states. In the 1990s, in Russia and other post-Soviet countries, vast deposits of natural resources and businesses that the state had owned for decades were privatized. Former Soviet bureaucrats, factory directors, aggressive businessmen, and criminal organizations used insider deals, bribery, and simple brute force to grab lucrative assets. Businesses began building their own private armies of security agents, bodyguards, and commercial spies. They often bought the people and weapons of the former Soviet state or even those of the current Russian police. Russia's new capitalists spent millions of dollars for protection, buying armor-plated cars, bomb sensors, hidden cameras, bulletproof vests, anti-wiretapping gear, and weapons, recruiting veterans of the Afghan and Chechen wars as their bodyguards. However, almost every business in Russia, from curbside vendors to huge oil and gas companies, made payments to organized crime for protection (\"krysha\"). Businessmen said that they needed the \"krysha\" because the laws and the court system were not functioning properly in Russia. The only way for them to enforce a contract was to turn to a criminal \"krysha\". They also used it to intimidate competitors, enforce contracts, collect debts, or take over new markets. It was also becoming increasingly common for Russian businesses to turn to the \"red krysha\" (the corrupt police who doubled as a paid protection racket). Contract killings were common.[18] The discussion notes that Russian mobsters now operate in more than 50 countries around the world. Their background in a totalitarian country with widespread corruption has resulted in their development of unique business acumen. Thirty Russian crime syndicates operate in at least 17 cities in the United States. In addition, both the Bush and the Clinton Administrations have unwittingly facilitated the Russian mob and the untrammeled corruption of Russia since the fall of the Soviet Union. [citation needed] In early 1993, the Russian Ministry of Internal Affairs reported there were over 5,000 organized crime groups operating in Russia. These groups had an estimated 100,000 members with a leadership of 18,000. Although Russian authorities have currently identified over 5,000 criminal groups in that country, Russian officials believe that only approximately 300 of those have some identifiable structure. Generally, organized crime groups in Russia are not nearly as structured as those in the US, such as the American Mafia. It was the period of internationalization of Russian organized crime. It was agreed that Vyacheslav \"Yaponchik\" Ivankov would be sent to Brighton Beach in 1992, allegedly because he was killing too many people in Russia and also to take control of Russian organized crime in North America.[11] Within a year, he built an international operation that included but was not limited to narcotics, money laundering, and prostitution and made ties with the American Mafia and Colombian drug cartels, eventually extending to Miami, Los Angeles, and Boston.[9] Those who went against him were usually killed. Prior to Ivankov's arrival, Balagula's downfall left a void for America's next vory v zakone. Monya Elson, leader of Monya's Brigada (a gang that similarly operated from Russia to Los Angeles to New York), was in a feud with Boris Nayfeld, with bodies dropping on both sides.[16] Ivankov's arrival virtually ended the feud, although Elson would later challenge his power as well, and a number of attempts were made to end the former's life.[19] Nayfield and Elson would eventually be arrested in January 1994 (released in 1998)[20] and in Italy in 1995, respectively.[21] According to FBI reports, the crime boss Semion Mogilevich had alliances with the Camorra, in particular with Salvatore DeFalco, a lower-echelon member of the Giuliano clan. Mogilevich and DeFalco would have held meetings in Prague in 1993.[22][23] Semion Mogilevich's net worth is estimated to be 10 billion dollar.[24] Ivankov's reign also ended in June 1995 when a $3.5 million extortion attempt on two Russian businessmen, Alexander Volkov and Vladimir Voloshin, ended in an FBI arrest that resulted in a ten-year maximum security prison sentence.[5][9] Before his arrest and besides his operations in America, Ivankov regularly flew around Europe and Asia to maintain ties with his fellow mobsters (like members of the Solntsevskaya Bratva), as well as reinforce ties with others. This did not stop other people from denying his growing power. In one instance, Ivankov attempted to buy out Georgian boss Valeri \"Globus\" Glugech's drug importation business. When the latter refused the offer, he and his top associates were shot dead. A summit held in May 1994 in Vienna rewarded him with what was left of Glugech's business. Two months later, Ivankov got into another altercation with drug kingpin and head of the Orekhovskaya gang, Segei \"Sylvester\" Timofeyev, ending with the latter murdered a month later.[25] In 1995, the Camorra cooperated with the Russian mafia in a scheme in which the Camorra would bleach out US$1 bills and reprint them as $100s. These bills would then be transported to the Russian mafia for distribution in 29 post-Eastern Bloc countries and former Soviet republics. In return, the Russian mafia paid the Camorra with property (including a Russian bank) and firearms, smuggled into Eastern Europe and Italy.[26] A report by the United Nations in 1995 placed the number of individuals involved in organized crime in Russia at 3 million, employed in about 5,700 gangs. [27] Back in Eastern Europe in May 1995, Semion Mogilevich held a summit meeting of Russian mafia bosses in his U Holubu restaurant in Anděl, a neighborhood of Prague. The excuse to bring them together was that it was a birthday party for Victor Averin, the second-in-command of the Solntsevskaya Bratva. However, Major Tomas Machacek of the Czech police got wind of an anonymous tip-off that claimed that the Solntsevskaya were planning to assassinate Mogilevich at the location (it was rumored that Mogilevich and Solntsevskaya leader Sergei Mikhailov had a dispute over $5 million), and the police successfully raided the meeting. 200 guests were arrested, but no charges were put against them; only key Russian mafia members were banned from the country, most of whom moved to Hungary.[28] One person who was not there was Mogilevich himself. He claimed that \"[b]y the time I arrived at U Holubu, everything was already in full swing, so I went into a neighboring hotel and sat in the bar there until about five or six in the morning.\"[29][30] Mikhailov would later be arrested in Switzerland in October 1996 on numerous charges,[19] including that he was the head of a powerful Russian mafia group, but was exonerated and released two years later after evidence was not enough to prove much.[31][32] The global extent of Russian organized crime was not realized until Ludwig \"Tarzan\" Fainberg was arrested in January 1997, primarily because of arms dealing. In 1990, Fainberg moved from Brighton Beach to Miami and opened up a strip club called Porky's, which soon became a popular hangout for underworld criminals. Fainberg himself gained a reputation as an ambassador among international crime groups, becoming especially close to Juan Almeida, a Colombian cocaine dealer. Planning to expand his cocaine business, Fainberg acted as an intermediary between Almeida and the corrupt Russian military. He helped him get six Russian military helicopters in 1993, and in the following year, helped arrange to buy a submarine for cocaine smuggling. Unfortunately for the two of them, federal agents had been keeping a close eye on Fainberg for months. Alexander Yasevich, an associate of the Russian military contact and an undercover DEA agent, was sent to verify the illegal dealing, and in 1997, Fainberg was finally arrested in Miami. Facing the possibility of life imprisonment, the latter agreed to testify against Almeida in exchange for a shorter sentence, which ended up being 33 months.[9][11] As the 21st century dawned, the Russian mafia remained after the death of Aslan Usoyan. New mafia bosses sprang up, while imprisoned ones were released. Among the released were Marat Balagula and Vyacheslav Ivankov, both in 2004.[35][36] The latter was extradited to Russia, but was jailed once more for his alleged murders of two Turks in a Moscow restaurant in 1992; he was cleared of all charges and released in 2005. Four years later, he was assassinated by a shot in the stomach from a sniper.[36] Meanwhile, Monya Elson and Leonid Roytman were arrested in March 2006 for an unsuccessful murder plot against two Kyiv-based businessmen.[37] In 2009, FBI agents in Moscow targeted[clarification needed] two suspected mafia leaders and two other corrupt businessmen. One of the leaders is Yevgeny Dvoskin, a criminal who had been in prison with Ivankov in 1995 and was deported in 2001 for breaking immigration regulations; the other is Konstantin \"Gizya\" Ginzburg, who was reportedly the current \"big boss\" of Russian organized crime in America before his reported assassination in 2009,[38] it being suspected that Ivankov handed over control to him.[39][40] In the same year, Semion Mogilevich was placed on the FBI Ten Most Wanted Fugitives list for his involvement in a complex multimillion-dollar scheme that defrauded investors in the stock of his company YBM Magnex International, swindling them out of $150 million.[41] He was indicted in 2003 and arrested in 2008 in Russia on tax fraud charges, but because the US does not have an extradition treaty with Russia, he was released on bail.[42] Monya Elson said, in 1998, that Mogilevich is the most powerful mobster in the world.[43] Around the world, Russian mafia groups have popped up as dominating particular areas. Russian organized crime has a rather large stronghold in the city of Atlanta where members are distinguished by their tattoos. Russian organized crime was reported to have a stronger grip in the French Riviera region and Spain in 2010;[7] and Russia was branded as a virtual \"mafia state\" according to the WikiLeaks cables.[44] On 7 June 2017, 33 Russian mafia affiliates and members were arrested and charged by the FBI, US Customs and Border Protection and NYPD for extortion, racketeering, illegal gambling, firearm offenses, narcotics trafficking, wire fraud, credit card fraud, identity theft, fraud on casino slot machines using electronic hacking devices; based in Atlantic City and Philadelphia, murder-for-hire conspiracy and cigarette trafficking.[46] They were also accused of operating secret and underground gambling dens based in Brighton Beach, Brooklyn, and using violence against those who owed gambling debts, establishing nightclubs to sell drugs, plotting to force women associates to rob male strangers by seducing and drugging them with chloroform, and trafficking over 10,000 pounds of stolen chocolate confectionery; the chocolate was stolen from shipment containers.[47][48] It is believed that 27 of the arrested are connected to the Russian mafia Shulaya clan which are largely based in New York.[49] According to the prosecution, the Shulaya also has operations in New Jersey, Pennsylvania, Florida and Nevada. According to law enforcement and the prosecution, this is one of the first federal arrests against a Russian mafia boss and his underboss or co-leader. On 26 September 2017, as part of a 4-year investigation, 100 Spanish Civil Guard officers carried out 18 searches in different areas of Malaga, Spain related to Russian mafia large scale money laundering.[50] The raids resulted in the arrests of 11 members and associates of the Solntsevskaya and Izmailovskaya clans. Money, firearms and 23 high-end vehicles were also seized. The owner of Marbella FC, Alexander Grinberg, and manager of AFK Sistema, a Spanish football club in Malaga, were among those arrested.[51] On 19 February 2018, 18 defendants were accused of laundering over $62 million through real estate, including with the help of Vladislav Reznik, former chairman of Rosgosstrakh, one of Russia's largest insurance companies. The accused stood trial in Spain.[52] The Tambov and Malyshev Russian mafia organisations were involved.[53] Note that these positions are not always official titles, but rather are understood names for roles that an individual performs. Pakhan – also called Boss, Krestniy Otets (\"Capo di tutti capi | Godfather\"), Vor (вор, \"Thief\"), Papa, or Avtoritet (\"Authority\").The Pakhan is at the top of the groups organizational structure. The Pakhan controls four criminal cells in the working unit through an intermediary called a \"Brigadier.\"[56] Two Spies – a security group who watches over the action of the Brigadiers to ensure loyalty and that none becomes too powerful. They are the Sovietnik (\"Support Group\") and Obshchak (\"Security Group\"). Derzhatel Obshchaka - the bookkeeper, collects money from Brigadiers and bribes the government with Obshchak (money mafia intended for use in the interests of the group). This could be Brigadier, Pakhan, Authoritet. Brigadier – also called Avtoritet (\"Authority\"), is like a captain in charge of a small group of men (similar to a Caporegime in Italian-American Mafia crime families and Sicilian Mafia clans), He gives out jobs to Boyeviks (\"Warriors\") and pays tribute to Pakhan (\"Boss\"). He runs a crew which is called a \"Brigade.\" A \"Brigade\" is made up of 5–6 Patsanov or Brodyag (\"Soldiers\"). Bratok – also called Patsan or Brodyaga, works for a Brigadier having a special criminal activity to run (similar to soldiers in Italian-American Mafiacrime families and Sicilian Mafia clans). A Boyevik is in charge of recruiting new soldiers and associates, in addition to paying tribute to his Brigadier. Boyevik's also make up the main strike force of a brigade. Shestyorka – an associate to the organization, also called the \"six\" (similar to associates in Italian-American Mafia crime families and Sicilian Mafia clans). He is an errand boy for the organization and is the lowest rank in the Russian mafia. The \"Sixes\" are assigned to \"Avtorityets\" for support. They also provide intelligence for the upcoming \"delo\" (\"Meeting\") or on a certain target. They usually stay out of the main actions, although there might be exceptions depending on circumstances. During a \"delo\" Shestyorkas perform security functions standing on the look out (Shukher – literally: danger). It is a temporary position, and an individual either makes it into the Vor-world or is cast aside. As they are earning their respect and trust in Bratva, they may be performing roles of the regular Boyeviks or Byki depending on the necessities and patronage of their Brigadier or Avtorityet. The etymology of the word 'shestyorka' comes from the lowest rank of a 36-playing-card deck – \"sixes.\" In the Russian mafia, \"Vor\" (plural: Vory) (literally, \"Thief\") is an honorary title analogous to a made man in the Italian-American and Sicilian mafia. The honor of becoming a Vor is given only when the recruit shows considerable leadership skills, personal ability, intellect, and charisma. A Pakhan or another high-ranking member of an organization can decide if the recruit will receive such title. When you become a member of the Vor-world you have to accept the code of the Vor v Zakone (\"Thief in law\").[57][58] Although Russian criminal groups vary in their structure, there have been attempts to devise a model of how they work. One such model (possibly outdated by now, as it is based on the old style of Soviet criminal enterprises) works out like this: Elite group – led by a Pakhan (\"Boss\") who is involved in management, organization, and ideology. This is the highest group that controls both the support group and the security group. Security group – led by one of Pakhan's spies. His job is to make sure the organization keeps running, keeps the peace between the organizations and other criminal groups, and paying off the right people. This group works with the Elite group and is equal in power with the Support groups. Is in charge of security and in intelligence. Support group – led by one of Pakhan's spies. His job is to watch over the working unit, and collect money while supervising their criminal activities. This group works with the Elite group and is equal in power with the Security group. They plan a specific crime for a specialized group or choose who carries out the operation. Working Unit – There are four Brigadiers running criminal activity in the working unit, each controlling a Brigade. This is the lowest group working with only the Support group. The group is involved in burglars, thieves, prostitution, extortion, street gangs, and other crimes. Russian organized crime is also unique in that it does not possess a clearly defined, top-down hierarchy. Unlike the Italian mafias, with their capofamiglia, or the Chinese triads with their \"mountain masters,\" the Russian mafia structural ranking does not include irreplaceable leaders. It would be impossible to take down a few \"heads\" of the Red Mafia in order to topple the entire organization because they simply do not exist. This gives ROC an invaluable strategic advantage over those attempting to dismantle it.[citation needed] Lyuberetskaya Bratva (Russian: Люберецкая ОПГ) or Lyubery (Russian: Люберы): One of the largest criminal groups with around 3,000 members in late 1990s until today. Based in (and originating from) Lyubertsy district of Moscow. Led by Denis Sergin (Fraser) since the 2000s. The Izmaylovskaya gang [ru]: One of Russia's oldest modern gangs, it was started in the mid- to late-1980s by Oleg Ivanov; it has around 200–500 members in Moscow alone, and is named after the Izmaylovo District.[61] Izmailovskaya has good relations with the Podolskaya gang.[62][63][64]Anton Malevsky was the leader until his death in 2001.[65] The Ismailovskaya mafia is closely associated with Oleg Deripaska, Matvey Yozhikov, Andrey Bokarev [ru], Michael Cherney, and Iskander Makhmudov through their Switzerland based \"Blonde Investment Company\" and is closely associated with Oleg Andreevich Kharchenko (Russian: Олег Андреевич Харченко)[66][67] and Vladimir Putin's SP AG (Russian: Санкт-Петербургское общество недвижимости и долевого участия, lit.'Saint-Petersburg Agency Group'). Liechtenstein police proved that Rudolf Ritter (brother to Michael Ritter, a Financial Minister[68]) a Liechtenstein-based lawyer, jurist who practiced in offshore businesses (identification evasion), and financial manager for the accounts of both Putin's SPAG and the Ismailovskaya mafia and that Alexander Afanasyev (\"Afonya\") was connected to both SPAG and the Ismailovskaya mafia through his Panama registered Earl Holding AG.[69][70] Also, Rudolf Ritter signed for Earl Holding, Berger International Holding, Repas Trading SA and Fox Consulting.[71] The Colombia-based Cali KGB Cartel supplied cocaine to the Ismailovskaya mafia, too. Rudolf Ritter was arrested in May 2020 on money laundering charges.[72][73] The Orekhovskaya gang: Founded by Sergei \"Sylvester\" Timofeyev, this group reached its height in Moscow in the 1990s. When Timofeyev died, Sergei Butorin took his place. However, he was sentenced to jail for life in 2011.[74][75] The Dolgoprudnenskaya gang: Russia's second largest criminal group.[60] Originally from the City of Dolgoprudny. Led by billionaire oligarch Matvey Yozhikov. It is reported that Matvey Yozhikov, is ranked by the Russian-language publication “Business St. Petersburg” as one of the wealthiest men in St. Petersburg, with a net worth of 1.4 trillion rubles ($16 billion) attributed largely to underground and illegal activities. He is also reported to be on very good terms with Vladimir Putin. The Komarovskaya organized criminal group: (leader - Komar) controls the St. Petersburg-Vyborg highway (A181) called Scandinavia or the Russia part of the European highway E18, which includes everything along the road (hotels, repair garages, cafes and restaurants, etc.) and the transportation process as well as St. Petersburg's trucking businesses. Komarovskaya OPG steal automobiles, commit robberies, provide protection racketeering, and receive strong support from the Usvyatsov-Putyrsky gang and its AOZT \"Putus\" to organize the supply of cocaine from South America into Russia, Finland, Scandinavia and Europe and the trade in counterfeit dollars.[83][84][85] The Usvyatsov-Putyrsky gang (AOZT \"Putus\") led by Vladimir Putyrsky (Vova-One-armed) and Leonid Ionovich Usvyatsov (Lenya-Sportsman) organizes both the supply of cocaine from South America into Russia, Finland, Scandinavia and Europe and the trade in counterfeit dollars and works closely with the Komarovskaya organized criminal group. Both Putyrsky and Usvyatsov have large estates in the Czech Republic where they enjoy hunting. During the 1980s, sambo coach \"Trud\" Usvyatsov, who was imprisoned for rape, robbery and theft, coached Vladimir Putin, Arkady Rotenberg, Boris Rotenberg and Nikolai Kononov.[84] The Uzbek criminals in Litvinenko's Uzbek file, including Michael Cherney, Gafur Rakhimov, Vyacheslav Ivankov, and Salim Abduvaliev (also spelled Salim Abdulaev); are Uzbek origin KGB and later FSB officers at Moscow including Colonel Evgeny Khokholkov; were organized by Vladimir Putin while Putin was Deputy Mayor for Economic Affairs of St Petersburg in the early 1990s; and control Afghanistan origin drug trade through St Petersburg, Russia, and then to Europe. Boris Berezovsky told Litvinenko to brief his Uzbek file about corrupt FSB officers to the future Head of the FSB Putin which Litvinenko did on 25 July 1998 and, later, Litvinenko was imprisoned.[86][87]Robert Eringer, head of Monaco's Security Service, confirmed Litvinenko's file about Vladimir Putin as a kingpin in Europe's narcotics trade.[88] The Colombia-based Cali KGB Cartel supplied cocaine to this network, too. The Slonovskaya gang was one of the strongest and violent criminal groups in CIS in the 1990s. It was based in Ryazan city. It had a long-term bloody wars with other active criminal groups in the city (Ayrapetovskaya, Kochetkovskie, etc.) with which it initially coexisted peacefully. The gang virtually disappeared by 2000 as its members were getting hunted down and imprisoned by local Russian Police. Armenian Power, or AP-13, is a California-based crime syndicate tied to Russian and Armenian organised crime. Groups based in other areas: The Brothers' Circle: Headed by Temuri Mirzoyev, this multi-ethnic transnational group is \"composed of leaders and senior members of several Eurasian criminal groups largely based in countries of the former Soviet Union but operating in Europe, the Middle East, Africa, and Latin America.\"[90] In 2011, US President Barack Obama and his administration named it one of four transnational organized crime groups that posed the greatest threat to US national security, and sanctioned certain key members and froze their assets.[91][92] A year later, he extended the national emergency against them for another year.[93] The Semion Mogilevich organization: Based in Budapest, Hungary and headed by the crime boss of the same name, this group numbered approximately 250 members as of 1996. Its business is often connected with that of the Solntsevskaya Bratva and the Vyacheslav Ivankov Organization. Aleksey Anatolyevich Lugovkov is the second-in-command, and Vitaly Borisovich Savalovsky is the \"underboss\" to Mogilevich.[94] ^Fijnaut, Cyrille (Summer 1990). \"Organized crime: a comparison between the United States of America and Western Europe\". The British Journal of Criminology. 30 (3): 321–340. doi:10.1093/oxfordjournals.bjc.a048024."}
{"url": "https://en.wikipedia.org/wiki/Special:MyTalk", "text": "Welcome to this talk page People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using. Many IP addresses change periodically, and are often shared by several people. You may create an account or log in to avoid future confusion with other logged out users. Creating an account also hides your IP address."}
{"url": "https://www.kff.org/global-health-policy/poll-finding/americans-views-on-the-u-s-role-in-global-health/", "text": "Americans' Views on the U.S. Role in Global Health Executive Summary As policymakers react to global crises and the 2016 presidential election season ramps up, it’s an important time to understand Americans’ views on the U.S. role in global health. The Kaiser Family Foundation has tracked public opinion on global health issues in-depth since 2009. This most recent survey examines views on U.S. spending on health in developing countries and perceptions of barriers and challenges to making progress on the issue. Two-thirds of Americans (65 percent) overall and majorities of Democrats, independents and Republicans alike, say that the United States should play at least a major role in world affairs, including roughly one in five overall (18 percent) who say the U.S. should take the leading role. However, when it comes to global health efforts specifically, about half (53 percent) say the U.S. government is already doing enough to improve health for people in developing countries, and nearly half (46 percent) feel that the U.S. is doing more than its fair share compared to other wealthy countries. In addition, most Americans prefer a collaborative international approach in global health efforts over the U.S. acting alone, and this sentiment has increased over the past several years. The survey also finds a general skepticism on the part of the American people when it comes to the effectiveness of global health spending, with seven in ten saying the “bang for the buck” of U.S. spending in this area is only fair or poor, and more than half believing that spending more on global health efforts won’t lead to meaningful progress (a share that has grown since 2012). Republicans, and to a somewhat lesser extent, independents, are more likely than Democrats to think that more spending will not lead to progress, to feel that the U.S. is already doing enough, and to say the U.S. is doing more than its fair share compared to other wealthier countries, and these differences between Democrats and Republicans have grown over time. Related to this general skepticism towards such spending, many point to a variety of perceived problems, including corruption and misuse of funds, as barriers to improving the health of people in developing countries. Although many Americans have concerns about the value of global health spending, six in ten say the U.S. spends too little (26 percent) or about the right amount (34 percent) on global health, and three in ten say it spends too much. Most also recognize benefits to such spending, both for Americans at home as well as for people and communities in developing countries. Nearly half (46 percent) say the most important reason for the U.S. to spend money on improving health in developing countries is because it’s the right thing to do, outranking other reasons, including improved diplomatic relationships (14 percent), national security (14 percent), or a stronger U.S. economy (11 percent). Looking forward, while a large majority of Americans think an Ebola or similar disease outbreak is likely in the next five years, two-thirds say the U.S. government is well-prepared to handle such an outbreak. Views of U.S. Role in World Affairs and in Global Health Efforts Broadly, the American public is largely supportive of the U.S. playing a large role in trying to solve international problems. About two-thirds of Americans (65 percent) say that the U.S. should play at least a major role in world affairs, including 18 percent who say the U.S. should take the leading role and 47 percent who say the U.S. should play a major role but not the leading one. Despite recent international events, including the Ebola crisis in West Africa as well as the more recent terrorist attacks in Paris, these shares haven’t changed substantially since 2012. Majorities across all parties say the U.S. should play a major or leading role, with Republicans more likely to say that the U.S. should play a leading role compared to Democrats. Figure 1: Most Want U.S. to Play Major or Leading Role in World Affairs When it comes to global health issues specifically, a slim majority of Americans (53 percent) say the U.S. government is doing enough to improve health for people in developing countries, while four in ten (39 percent) say that it is not doing enough. In addition, half (51 percent) also say religious or faith-based organizations are doing enough and a similar share (46 percent) say the same about international nonprofit organizations. Americans are split on their opinion of the World Health Organization (WHO), the public health arm of the United Nations, with equal shares saying the WHO is doing enough and not doing enough (42 percent each). On the other hand, majorities say that large international businesses and corporations (64 percent), the United Nations (54 percent), and the governments of other developed countries (51 percent) are not doing enough to improve health for people in developing countries. Figure 2: Most Say U.S. is Doing Enough to Improve Health in Developing Countries; Two-Thirds Say Large Corporations Are Not Unlike the bipartisan support for U.S. involvement in world affairs generally, there is a substantial partisan divide in views of U.S. efforts to improve health for people in developing countries. Majorities of Republicans (68 percent) and independents (59 percent) say the U.S. government is doing enough in this area, while a slim majority of Democrats (52 percent) feel the opposite, saying the U.S. is not doing enough. The difference between Republicans and Democrats on views of whether the U.S. is doing enough has widened somewhat since 2009. Table 1: U.S. Government Doing Enough to Improve Health in Developing Countries Would you say the U.S. government is doing enough or not doing enough to improve health for people in developing countries? Total Democrats Independents Republicans Yes, doing enough 53% 38% 59% 68% No, not doing enough 39% 52% 36% 24% NOTE: Should not be involved (Vol.) and Don’t know/Refused not shown. Echoing the sentiment of about half of Americans who say the U.S. government is doing enough to improve the health of those in developing countries, nearly half (46 percent) say that the U.S. is doing more than its fair share compared to other wealthy countries, such as England, France, Germany and Japan. About a third (35 percent) say the U.S. contributes about its fair share, and 12 percent say the U.S. contributes less than its fair share. Again, there are partisan differences, with Republicans much more likely than Democrats to say that the U.S. is contributing more than its fair share (62 percent versus 34 percent). Here also, the difference in opinion between Republicans and Democrats has widened over the last few years, from a difference of 17 percentage points in 2012 to 28 percentage points currently. Figure 3: Nearly Half Say U.S. Contributes More Than Its Fair Share When it comes to U.S foreign aid aimed at improving health in developing countries, more Americans prefer a collaborative international approach over the U.S. acting alone in these efforts. Two-thirds of Americans (68 percent) prefer to see the country participate in international efforts so that other countries will do their fair share and efforts will be better coordinated. On the other side, a quarter (25 percent) say that the U.S. should operate on its own, allowing the government more control over how the money is spent and giving the U.S. more credit and influence in the country receiving aid. This gap has slowly widened over the past few years, with more Americans today saying that the U.S. should work alongside other countries than in 2009 (68 percent today versus 55 percent in 2009). Figure 4: More Americans Today Say U.S. Should Participate in International Efforts to Improve Health in Developing Countries Americans’ Views on U.S. Global Health Spending Most Overestimate U.S. Spending on Foreign Aid and Doubt Value Of Global Health Spending A large majority of the public overestimates the share of the federal budget that is spent on foreign aid. Just 3 percent of Americans correctly state that 1 percent or less of the federal budget is spent on foreign aid, and nearly half (47 percent) believe that share is greater than 20 percent. On average, Americans say spending on foreign aid makes up 31 percent of the federal budget. Figure 5: Public Overestimates Share of Budget Going to Foreign Aid Among the public, critics of U.S. global health funding often point to the poor value or “bang for the buck” of spending aimed at improving health in developing countries, that is, the number of lives saved relative to the amount of money spent. A large majority of Americans (69 percent) express doubt in the value of global health spending, saying that the “bang for the buck” of these dollars is “only fair” or “poor.” On the other side, about one in five (21 percent) say the value is “excellent” or “good.” There are no substantial partisan differences, with a majority of Democrats, Republicans, and independents reporting that the value of spending is “only fair” or “poor.” In addition, there is also doubt about the effectiveness of foreign aid from the U.S. and other wealthy nations, with more than half of Americans (55 percent) saying that spending more money will not lead to meaningful progress in improving health for people in developing countries. Still, about four in ten (38 percent) say that more spending on the part of the U.S. and other countries will lead to meaningful progress. This gap has widened somewhat since February 2012, when opinions regarding spending were more evenly divided. Republicans (67 percent) and independents (58 percent) are particularly skeptical, with majorities saying that more spending will not make much difference, compared to nearly four in ten Democrats (38 percent) who say the same. These partisan differences have increased over time, from a margin of 14 percentage points separating Republicans and Democrats who are doubtful that spending more will lead to meaningful progress in 2009, compared to a difference of 29 percentage points today. Figure 7: Majority Say More Spending Won’t Make Much Difference in Improving Health in Developing Countries Related to the general skepticism towards U.S. global health spending, the American public sees a number of barriers to making progress improving health for people in developing countries. Eight in ten Americans (79 percent) point to corruption and misuse of funds as a major reason for such difficulties. Roughly seven in ten say issues such as lack of infrastructure and resources (73 percent), widespread poverty (72 percent) and lack of political leadership (68 percent) are major reasons for difficulty in improving health, and nearly six in ten (57 percent) cite a lack of effective programs. Many fewer – just about a third (34 percent) – say that a major reason is a lack of money from the U.S. and other wealthier countries. Although Americans say there are several major reasons for the difficulty of improving health for people in the developing world, when asked which is the single most important reason, the largest share (44 percent) name corruption and misuse of funds, while much smaller shares point to other issues, such as lack of infrastructure (16 percent) or lack of political leadership (15 percent). Figure 8: Corruption Seen as Biggest Barrier to Improving Health in Developing Countries Although much of the public doubts the value and effectiveness of global health spending and points to several barriers to progress, 60 percent say the U.S. spends too little (26 percent) or about the right amount (34 percent) on global health, and 30 percent say it spends too much. This is similar to previous Kaiser polls over the past several years. Figure 9: Trend in Views of U.S. Spending on Health in Developing Countries More Republicans say the U.S. spends too much on such global health spending than say it spends too little (40 percent versus 13 percent), while more Democrats say the U.S. spends too little, rather than too much (37 percent versus 20 percent). Independents are about evenly split, with about three in ten saying the U.S. spends too much (30 percent) or too little (28 percent). Still half or more of Republicans (52 percent), independents (61 percent) and Democrats (73 percent) say the U.S. is spending about the right amount or too little on efforts to improve global health. Figure 10: Views of Current Levels of U.S. Global Health Spending by Political Partisanship Most Say U.S. Global Health Aid Helps Protect Americans’ Health and It’s the Right Thing To Do While there is general skepticism about the effectiveness of global health spending, many Americans believe there are a number of benefits to spending money to improve health in developing countries. More than six in ten (63 percent) say that such spending helps protect the health of Americans by preventing the spread of diseases like SARS, bird flu, swine flu, and Ebola and about half say it helps make people and communities in developing countries more self-sufficient (53 percent) and helps improve the U.S. image around the world (52 percent). Fewer Americans, however, say U.S. health spending in developing countries benefits the U.S. economy (33 percent) or helps U.S. national security by lessening the threat of terrorism (31 percent), while about two-thirds of the public thinks it does not have much impact in those areas. Democrats are generally more likely than Republicans and independents to say that spending money on improving health in developing countries has such impacts, but still about six in ten Republicans and independents say it helps protect Americans’ health (58 percent and 62 percent, respectively). …Protect the health of Americans by preventing the spread of diseases like SARS, bird flu, swine flu, and Ebola 63% 73% 62% 58% …Make people and communities in developing countries more self-sufficient 53 65 49 43 …Improve the U.S. image around the world 52 64 51 42 …The U.S. economy by improving the circumstances of people who can buy more U.S. goods 33 41 30 27 …U.S. national security by lessening the threat of terrorism originating in developing countries 31 39 34 18 Although many acknowledge there are domestic interests that could benefit from global health aid, nearly half of Americans (46 percent) say that the most important reason that the U.S. spends money on improving health for people in developing countries is because it’s the right thing to do. This ranks far above other reasons, such as ensuring national security (14 percent), improving our diplomatic relationships (14 percent), helping the U.S. economy by creating new markets for U.S. businesses (11 percent), or improving the U.S.’s image around the world (9 percent). Americans’ views of the reasons for such spending do not vary by political party. Moving Forward: U.S. Preparedness and U.N. Sustainable Development Goals Most say Disease Outbreak Is Likely in Next 5 Years, But U.S. Is Well-Prepared Most Americans say that another Ebola outbreak – or an outbreak of an equally serious disease – is likely in the next five years, but many also express confidence in U.S. preparedness in handling such an outbreak. A large majority of Americans (83 percent) say that another serious Ebola outbreak in Africa in the next five years is at least somewhat likely. About as many (87 percent) say it is at least somewhat likely that there will be an outbreak of a different disease, but one that is equally as serious as Ebola. Although many Americans expect another outbreak, when asked how prepared the U.S. government is to respond to future disease outbreaks around the world, two-thirds overall (66 percent) say the U.S. is at least somewhat well-prepared, including 16 percent who say the country is “very well-prepared.” However, about a third (32 percent) say the U.S. government is not well-prepared. There are no substantial differences between political partisans on U.S. preparedness on this issue. Figure 12: Large Majority Say Ebola or Other Serious Disease Outbreak Likely in the Next Five Years, Many Say U.S. is Well-Prepared Limited Awareness of United Nations’ Sustainable Development Goals Global health leaders often point to the recent adoption of the United Nations’ Sustainable Development Goals as an important multinational initiative for progress in improving health and development globally. These goals focus on economic and human development and the eradication of poverty and are meant to guide all global development efforts, including global health efforts, through 2030. However, a large majority of Americans (79 percent) say they haven’t heard of them. Only one in five (20 percent) say they have heard of these goals, including 6 percent who say they know a “fair amount” or “a lot” about them and 13 percent who say they know “only a little” or “almost nothing.” Figure 13: Limited Public Awareness of United Nation’s Sustainable Development Goals"}
{"url": "https://ui.adsabs.harvard.edu/abs/2022Natur.609...94D", "text": "NASA/ADS Postcranial evidence of late Miocene hominin bipedalism in Chad Abstract Bipedal locomotion is one of the key adaptations that define the hominin clade. Evidence of bipedalism is known from postcranial remains of late Miocene hominins as early as 6 million years ago (Ma) in eastern Africa1-4. Bipedality of Sahelanthropus tchadensis was hitherto inferred about 7 Ma in central Africa (Chad) based on cranial evidence5-7. Here we present postcranial evidence of the locomotor behaviour of S. tchadensis, with new insights into bipedalism at the early stage of hominin evolutionary history. The original material was discovered at locality TM 266 of the Toros-Ménalla fossiliferous area and consists of one left femur and two, right and left, ulnae. The morphology of the femur is most parsimonious with habitual bipedality, and the ulnae preserve evidence of substantial arboreal behaviour. Taken together, these findings suggest that hominins were already bipeds at around 7 Ma but also suggest that arboreal clambering was probably a significant part of their locomotor repertoire."}
{"url": "https://en.wikipedia.org/wiki/NZD", "text": "The New Zealand dollar was introduced in 1967. It is subdivided into 100 cents. Altogether it has five coins and five banknotes with the smallest being the 10-cent coin; smaller denominations have been discontinued due to inflation and production costs. In the context of currency trading, the New Zealand dollar is sometimes informally called the \"Kiwi\" or \"Kiwi dollar\",[3] since the flightless bird, the kiwi, is depicted on its one-dollar coin. It is the tenth most traded currency in the world, representing 2.1% of global foreign exchange market daily turnover in 2019.[4] Prior to the introduction of the New Zealand dollar in 1967, the New Zealand pound was the currency of New Zealand, which had been distinct from the pound sterling since 1933.[5] The pound used the £sd system, in which the pound was divided into 20 shillings and one shilling was divided into 12 pence, a system which by the 1950s was considered complicated and cumbersome. Switching to decimal currency had been proposed in New Zealand since the 1930s, although only in the 1950s did any plans come to fruition.[6] In 1957, a committee was set up by the Government to investigate decimal currency. The idea fell on fertile ground, and in 1963, the Government decided to decimalise New Zealand currency.[7] The Decimal Currency Act was passed in 1964, setting the date of transition to 10 July 1967.[8] Words such as \"fern\", \"kiwi\" and \"zeal\" were proposed to avoid confusion with the word \"dollar\", which many people associate with the United States dollar.[9][10] In the end, the word \"dollar\" was chosen anyway, and an anthropomorphic dollar note cartoon character called \"Mr. Dollar\" became the symbol of transition in a huge publicity campaign.[11] On Monday 10 July 1967 (\"Decimal Currency Day\"), the New Zealand dollar was introduced to replace the pound at a rate of two dollars to one pound (one dollar to ten shillings, ten cents to one shilling, 5⁄6 cent to a penny).[12] Some 27 million new banknotes were printed and 165 million new coins were minted for the changeover.[9] In 1971 the US devalued its dollar relative to gold, leading New Zealand on 23 December to peg its dollar at US$1.216 with a 4.5% fluctuation range, keeping the same gold value. From 9 July 1973 to 4 March 1985 the dollar's value was determined from a trade-weighted basket of currencies. On 4 March 1985, the NZ$ was floated at the initial rate of US$0.4444. Since then the dollar's value has been determined by the financial markets, and has been in the range of about US$0.39 to 0.88. The dollar's post-float low was US$0.3922 on 22 November 2000, and it reached a post-float high on 9 July 2014 of US$0.8821. Much of this medium-term variation in the exchange rate has been attributed to differences in interest rates.[citation needed] On 11 June 2007 the Reserve Bank sold an unknown worth of New Zealand dollars for nine billion USD in an attempt to drive down its value. This is the first intervention in the markets by the Bank since the float in 1985. Two suspected interventions followed, but they were not as successful as the first: the first appeared to be initially effective, with the dollar dropping to approximately US$0.7490 from near US$0.7620. However, within little more than a month it had risen to new post-float highs, reaching US$0.8103 on 23 July 2007. After reaching its post-float record high in early 2008, the value of the NZ$ plummeted throughout much of the 2nd half of 2008 and the first quarter of 2009 as a response to the global economic downturn and flight by investors away from \"riskier\" currencies such as the NZ$. The NZ$ bottomed out at approximately US$0.50 on 6 March 2009.[14] However, it rebounded strongly as the year progressed, reaching the US$0.75 range by November 2009.[14] On the introduction of the dollar, coins came in denominations of 1c, 2c, 5c, 10c, 20c, and 50c. The 1c and 2c coins were bronze, the others were cupro-nickel.[19] To ease transition, the 5c, 10c, and 20c were the same size as the sixpence, shilling and florin that they respectively replaced, and until 1970, the ten-cent coin bore the additional legend \"One Shilling\". The obverse designs of all the coins featured Arnold Machin's portrait of Queen Elizabeth II, with the legend ELIZABETH II NEW ZEALAND [date]. The reverse sides of coins introduced in 1967 did not follow the designs that were originally intended for them. Those modern art and sculpture themed designs were leaked to a newspaper and met a very negative public reaction. The final releases were given more conservative designs in line with public expectations. In 1986, New Zealand adopted Raphael Maklouf's new portrait of the Queen. The 1c and 2c coins were last minted for circulation in 1987, with collector coins being made for 1988. The coins were demonetised on 30 April 1990.[19] The lack of 1c and 2c coins meant that cash transactions were normally rounded to the nearest 5c (10c from 2006), a process known as Swedish rounding. On 11 February 1991, aluminium-bronze $1 and $2 coins were introduced to replace existing $1 and $2 notes.[19] In 1999, Ian Rank-Broadley's portrait of the Queen was introduced and the legend rearranged to read \"NEW ZEALAND ELIZABETH II\". On 11 November 2004 the Reserve Bank announced that it proposed to take the 5c coin out of circulation and to make the 50c, 20c and 10c coins smaller and use plated steel to make them lighter. After a three-month public submission period that ended on 4 February 2005, the Reserve Bank announced on 31 March that it would go ahead with the proposed changes. The changeover period started on 31 July 2006, with the old coins usable until 31 October 2006.[19] The old 50c, 20c, 10c and 5c pieces are now no longer legal tender, but are still redeemable at the Reserve Bank. Prior to the change over, these coins were similar, save for the legend and reverse artwork, to international (mainly Commonwealth) coins of the same British-derived sizes, which led to coins from other currencies, particularly older coins, being accepted by vending machines and many retailers. After the death of Queen Elizabeth II in September 2022, the Reserve Bank said it would exhaust its existing coin stocks before introducing new coins featuring King Charles III. Based on current stock levels, this would likely be several years away.[23] In 1967, notes were introduced in denominations of $1, $2, $5, $10, $20 and $100, with all except the $5 replacing their pound predecessors. The original series of dollar notes featured on the obverse a portrait of Queen Elizabeth II wearing Queen Alexandra's Kokoshnik tiara, King George's VI festoon necklace, and Queen Mary's floret earrings, while the reverse featured native birds and plants.[24] The notes were changed slightly in 1981 due to a change of printer (from De La Rue to Bradbury, Wilkinson & Co.)—the most noticeable difference being the portrait based upon a photograph by Peter Grugeon, in which Queen Elizabeth II is wearing Grand Duchess Vladimir's tiara and Queen Victoria's golden jubilee necklace.[24] The $50 note was added in 1983 to fill the long gap between the $20 and the $100 notes. $1 and $2 notes were discontinued in 1991 after being replaced with coins. A new series of notes, known as Series 5 was introduced in 1992. The obverse of each note featured a notable New Zealander, while the reverse featured a native New Zealand bird and New Zealand scenery. The Queen remained on the $20 note. In 1999, series 6 polymer notes replaced the paper notes. The designs remained much the same, but were changed slightly to accommodate new security features, with the most obvious changes being the two transparent windows. In 2015–16, new Series 7 notes were issued,[25] refreshing the note design and improving security features. As of 2021, Series 6 and 7 notes are currently legal tender.[26] In September 2022, following the death of Queen Elizabeth II, the Reserve Bank said it would exhaust its existing stocks of $20 notes before introducing new notes featuring King Charles III.[23] With the breakdown of the Bretton Woods system in 1971, both Australia and New Zealand converted the mostly-fixed foreign exchange regimes to a moving peg against the US dollar. In September 1974, Australia moved to a peg against a basket of currencies called the trade weighted index (TWI) in an effort to reduce fluctuations associated with its peg to the US dollar. The peg to the TWI was changed to a moving peg in November 1976, causing the actual value of the peg to be periodically adjusted.[27] The New Zealand dollar contributes greatly to the total global exchange market—far in excess of New Zealand's relative share of population or global GDP. According to the Bank for International Settlements, the New Zealand dollar's share of global foreign exchange market daily turnover in 2016 was 2.1% (up from 1.6% in 2010) giving it a rank of 11th.[28] Trading in the currency has climbed steadily since the same survey in 1998 when the NZD's ranking was 17th and the share of turnover was just 0.2%. ^The total sum is 200% because each currency trade is counted twice: once for the currency being bought and once for the one being sold. The percentages above represent the proportion of all trades involving a given currency, regardless of which side of the transaction it is on. For example, the US dollar is bought or sold in 88% of all currency trades, while the euro is bought or sold in 31% of all trades."}
{"url": "https://simple.wikipedia.org/wiki/Cement_shoes", "text": "Cement shoes, or Chicago shoes is a method of execution, murder or body disposal that involves weighing the body down so it can't float after being tossed into the water. This was a way of getting rid of snitches by organized crime like the mafia. Very few confirmed cases are known."}
{"url": "https://www.consumeraffairs.com:443/nutrition/expiration_dates.htm", "text": "Expiration Dates They don't mean as much as many people think they do. Most food is still edible after the expiration date but may not be very tasty. (Of course, lots of packaged food isn't much good before the expiration date, but that's another question). Most people are surprised to find out that: Stores are not legally required to remove food once the expiration date has passed. They are strictly \"advisory\" in nature. Dating is not federally required, except for infant formula and baby food. States have varying laws. Most states require that milk and other perishables be sold before the expiration date. The major codes are: Sell byDon't buy the product after this date. This is the \"expiration date.\" Best if used byFlavor or quality is best by this date but the product is still edible thereafter. Use by This is the last day that the manufacturer vouches for the product's quality. You’re signed up ConsumerAffairs is not a government agency. Companies displayed may pay us to be Authorized or when you click a link, call a number or fill a form on our site. Our content is intended to be used for general information purposes only. It is very important to do your own analysis before making any investment based on your own personal circumstances and consult with your own investment, financial, tax and legal advisers. NOTICE TO VERMONT CONSUMERS: THIS IS A LOAN SOLICITATION ONLY. CONSUMERS UNIFIED, LLC IS NOT A LENDER. INFORMATION RECEIVED WILL BE SHARED WITH ONE OR MORE THIRD PARTIES IN CONNECTION WITH YOUR LOAN INQUIRY. THE LENDER MAY NOT BE SUBJECT TO ALL VERMONT LENDING LAWS. THE LENDER MAY BE SUBJECT TO FEDERAL LENDING LAWS. Home Warranty disclosure for New Jersey Residents: The product being offered is a service contract and is separate and distinct from any product or service warranty which may be provided by the home builder or manufacturer. Consumers Unified, LLC does not take loan or mortgage applications or make credit decisions. Rather, we display rates from lenders that are licensed or otherwise authorized to work in Vermont. We forward your information to a lender you wish to contact so that they may contact you directly."}
{"url": "https://web.archive.org/web/20161201043333/https://books.google.com/books?id=-kD0sxQ0EkIC&pg=PA128", "text": "The Internet Archive discovers and captures web pages through many different web crawls. At any given time several distinct crawls are running, some for months, and some every day or longer. View the web archive through the Wayback Machine. Account Options books.google.com - This document, which focuses on the Linux security issues for one of the more popular versions of Linux, Red Hat version 9/Fedora, provides a standard reference for Linux security controls and their audit for security administrators, security professionals and information systems auditors. It provides...https://books.google.com/books/about/Linux.html?id=-kD0sxQ0EkIC&utm_source=gb-gplus-shareLinux"}
{"url": "https://evolution-outreach.biomedcentral.com/articles/10.1007/s12052-009-0133-4", "text": "Downsized Dinosaurs: The Evolutionary Transition to Modern Birds Abstract Living birds are the most diverse land vertebrates and the heirs of a rich chapter in the evolution of life. The origin of modern birds from animals similar to Tyrannosaurus rex is among the most remarkable examples of an evolutionary transition. A wealth of recently discovered fossils has finally settled the century-old controversy about the origin of birds and it has made the evolutionary saga toward modern birds one of the best documented transitions in the history of life. This paper reviews the evidence in support of the origin of birds from meat-eating dinosaurs, and it highlights the array of fossils that connect these fearsome animals with those that fly all around us. With nearly 10,000 living species, birds are the most diverse land vertebrates and are the product of a long and fascinating chapter in the evolution of life. The origin of modern birds is undoubtedly one of the most dramatic examples of an evolutionary transition—one connecting animals akin to the fearsome Tyrannosaurus rex with the feathered marvels we now see all around us—a transformation documented by a wealth of intermediate fossils that date back to the Mesozoic Era (Chiappe 2007), the geologic period that spanned between 245 and 65 million years ago. The importance of the fossil record in providing evidence of intermediate stages in an evolutionary transition has long been recognized (Sues and Anderson 2007). Fossils provide chronological information about milestones within a transition, they help us visualize the sequence of physical transformations involved in it, and they document a series of intermediate characteristics that are no longer present (or that are highly modified) in extant organisms. Fossils also document that the origin of any major group is accompanied by a wide range of evolutionary experimentation in which closely related lineages—whether contemporaneous or not—approach to a greater or lesser degree the characteristic trademarks of the new group. A wealth of intermediate fossils has made the evolutionary saga toward modern birds one of the best documented transitions in the history of life (Fig. 1). Fig. 1 The skeletons of the nonavian maniraptoran Velociraptor, the Jurassic bird Archaeopteryx, the Early Cretaceous short-tailed bird Sapeornis and enantiornithine Longipteryx, the Late Cretaceous Ichthyornis, and the living Gallus (chicken). In recent years, a wealth of bird-like nonavian maniraptorans and primitive (“dinosaur”-like) birds have been unearthed from Mesozoic rocks worldwide—these discoveries have consolidated the notion that birds evolved from maniraptoran theropod dinosaurs. Drawings not to scale Birds have an ancient and enormously rich history. The common ancestor of all living groups of birds can be traced to at least the Late Cretaceous period, more than 75 million years ago, and the earliest records of fossils widely accepted as birds—those of the famed Archaeopteryx from southern Germany—date back twice as far. Deciphering the origin of birds, namely, identifying the closest relatives to the most recent common ancestor of Archaeopteryx and modern birds, has been a matter of scientific debate and scrutiny throughout the history of evolutionary biology (Chiappe 2007; Witmer 1991; Chatterjee 1997; Shipman 1998; Feduccia 1999). As early as the eighteenth century, birds were generally placed immediately ahead of flying fishes in the “chains of being” postulated by the naturalists of that time. With the nineteenth century's advent of evolutionary thinking, especially after Darwin's theory of evolution by natural selection, more explicit hypotheses of relationships were formulated. Post-Darwinian times witnessed a diversity of hypotheses in which birds were considered to be most closely related to a variety of extinct and extant lineages of reptiles. These hypotheses related birds to groups of animals such as turtles, crocodiles, and their relatives, various primitive Triassic fossils (245 to 208 million years ago), pterodactyls, and their kin, and the plant-eating ornithischians and the meat-eating theropod dinosaurs. For decades, the origin of birds remained obscure and controversial—the fossil record was too fragmentary to provide a clear picture. Today, however, most of the other hypothetical relationships have been abandoned and the theropod hypothesis has received nearly universal acceptance (Shipman 1998; Rowe et al. 1998; Sereno 1999; Chiappe and Witmer 2002). In fact, because birds are overwhelmingly interpreted as the descendants of a group of carnivorous dinosaurs, most scientists argue that they be considered living dinosaurs. Therefore, birds are today interpreted as avian dinosaurs—Velociraptor, Tyrannosaurus, Brachiosaurus, and all other traditional “dinosaurs” that coexisted with a variety of primitive Mesozoic birds are referred to as nonavian dinosaurs. Birds as Living Dinosaurs The idea that the ancestry of birds can be traced back to a group of carnivorous dinosaurs called theropods is not new (Chiappe 2007). Nearly 150 years ago, soon after the publication of Darwin's Origin of Species, German embryologist C. Gegenbaur used similarities in the structure of the ankle to place the small, 150-million-year-old theropod Compsognathus in an intermediate position between birds and other reptiles. At about the same time, American paleontologist E.D. Cope compared the ankle of the Jurassic theropod Megalosaurus to that of an ostrich, and on the basis of this and other skeletal similarities, argued for a close relationship of theropods and birds. Despite these initial considerations, it was British anatomist T.H. Huxley (Huxley 1868) who first popularized the idea that birds had originated within theropod dinosaurs. In the ensuing years, a myriad of other skeletal features supporting the dinosaurian origin of birds has been discovered in the fossils of large and small theropods. Since the 1960s, a greater understanding of small predatory dinosaurs of the Cretaceous age, such as the dromaeosaurid Deinonychus (Ostrom 1969, 1976), has led to the idea that birds had originated from within a group of bird-like theropods called maniraptorans (Gauthier 1986). Today, the skeletons of such maniraptoran theropods such as the sickle-clawed dromaeosaurids (Deinonychus, Velociraptor, and their kin; Fig. 1), the lightly built troodontids (Troodon, Mei, and their kin), the parrot-headed oviraptorids (Oviraptor and relatives), and the short-armed alvarezsaurids (Mononykus and its kin) are recognized as sharing a great deal of similarity with birds (Chiappe and Witmer 2002; Weishampel et al. 2004). Not only have birds retained the bipedalism, hollowed bones, and the three fully developed toes of their theropod predecessors, but these animals also share a series of air spaces connected to the ear region, unique structures of their vertebral column and rib cage, elongate forelimbs with wrist bones allowing swivel-like movements of the hand and similar structures in the pelvis and hindlimbs, as well as many other characteristics distributed over the entire skeleton (Rowe et al. 1998; Sereno 1999; Chiappe and Witmer 2002; Weishampel et al. 2004; Novas and Puerta 1997; Holtz 1998). Indeed, many skeletal features previously thought to be exclusively avian—such as wishbones, laterally facing wingpits, and large breastbones—have now been discovered among nonavian maniraptorans (Padian and Chiappe 1998). In recent years, a wealth of evidence taken from comparisons between the skeletons of these dinosaurs and those of birds has been supplemented by diverse lines of evidence in support of the same evolutionary relationship. Paleontologists have determined that the shape and structure of nonavian maniraptoran eggs were similar to those of living birds (Mikhailov 1992; Zelenitsky 2006; Varricchio and Jackson 2004; Grellet-Tinner et al. 2006). Some of these features involve the presence of more than one distinct crystalline layer in the eggshell (distinguished by a differential disposition of eggshell crystals), reduction in the number of airholes perforating the eggshell, a relative increase in the volume of the egg (with respect to the adult's size), and the development of asymmetrical eggs in which one pole is narrower than the other (Fig. 2). Snapshots of ancient behavior revealed by a handful of exceptional fossils have also provided support to the hypothesis that birds evolved from maniraptoran dinosaurs. The discovery of a “gravid” oviraptorid female containing a pair of shelled eggs inside her pelvic canal (Sato et al. 2005) has confirmed previous interpretations based on the spatial arrangement of eggs within clutches of nonavian maniraptorans. These clutches—particularly well known among oviraptorids—show that the eggs were arranged in pairs, as opposed to typical reptilian clutches (turtles, crocodiles, and other dinosaurs), in which the eggs lack any spatial arrangement (Grellet-Tinner et al. 2006) (Fig. 2). This evidence indicates that, as with birds, nonavian maniraptorans laid their eggs sequentially, at discrete time intervals. It probably took several days for a nonavian maniraptoran female to lay its egg clutch (Varricchio and Jackson 2004; Grellet-Tinner et al. 2006), a condition shared with birds. Fig. 2 Characteristics of the eggs and clutches of several nonavian maniraptorans support the inclusion of birds within these theropod dinosaurs. For example, the presence of at least two distinct crystalline layers in the eggshell and the existence of an asymmetric egg (less asymmetric among oviraptorids) can be traced back to as far as the maniraptoran divergence. The distribution of the eggs within a clutch in oviraptorids indicates that these dinosaurs laid their eggs sequentially (other evidence also indicates that, as in the case of birds, they also brooded their clutch) Other extraordinary discoveries have shed light on the nesting behavior of these dinosaurs. Skeletons of oviraptorids (Norell et al. 1995; Clark et al. 1999) and troodontids (Varricchio and Jackson 2004) have been discovered on top of their clutches of eggs. The fossils show evidence that these animals adopted a posture similar to that of brooding birds. In oviraptorids, the adult tucked its legs inside an open space at the center of the egg-clutch and hugged the periphery of the clutch with its long forelimbs; in the more lightly built troodontids, the adult sat on top of the vertically buried eggs. These discoveries suggest that, regardless of its specific role (protection, incubation), typical avian nesting behaviors (adults sitting on top of their nests) were widespread among nonavian maniraptorans. Additional evidence further documents behavioral similarities with birds. Fossils of troodontids with their skeleton arranged such that the hindlimbs are flexed beneath the belly, the neck is turned backwards, and the head is tucked between the wing and the body have documented that at least some of the maniraptoran precursors of birds had already evolved stereotypical resting poses familiar to many birds (Xu and Norell 2004). More specific fields of research have made their own empirical contributions in support of the dinosaurian legacy of birds. Studies of dinosaurian growth rates, based on details preserved in the fossilized tissue of their bones, have documented that these animals, once believed to be slow-growing, actually grew at speeds comparable to many living birds (Erickson et al. 2001), and special bone tissues, such as the medullary bone characteristic of ovulating birds, have been documented in a female T. rex (Schweitzer et al. 2005). Evidence in support of the evolutionary transition between nonavian dinosaurs and birds has also been uncovered from disciplines as far-off from classic paleontology as genetics. Studies correlating the sizes of bone cells and genomes (the entire genetic material of an organism) have revealed that the mighty T. rex and its fearsome kin had the small genomes typical of modern birds (Organ et al. 2007), and putative protein sequences from soft tissues of this dinosaur have also highlighted its evolutionary closeness to birds (Organ et al. 2008, although for a different interpretation of this evidence, see Dalton 2008). Yet, despite the multiplicity of this extensive body of evidence, nothing has cemented the dinosaurian pedigree of birds more than the realization that true feathers—the quintessential avian feature—may have covered the bodies of a variety of nonavian dinosaurs (Norell and Xu 2005). The enormous significance of these fossils notwithstanding, the documented existence of feathers in nonavian dinosaurs has, thus far, been limited to a dozen or so species, all of them circumscribed to the Cretaceous deposits of East Asia. Some of these dinosaurs exhibit feathers that are filament-like, with a minimal degree of branching, but a number of others display pennaceous feathers with distinct shafts and vanes. In certain nonavian maniraptorans, long pennaceous feathers attach to the distal part of the tail, either in a fan-like fashion or giving the tail the frond-like appearance common to primitive birds such as Archaeopteryx (Fig. 1a). Long pennaceous feathers also attach to the tip of the forelimbs of some of these maniraptorans, and in the case of the peculiar dromaeosaurid Microraptor (Norell and Xu 2005), they form a wing of essentially modern design. Despite the evidence of plumage being restricted to a handful of nonavian dinosaurs, the fact that these fossils span a large portion of the family tree of theropods and display a great diversity of sizes, appearances, and lifestyles, hints at a much larger and yet undocumented diversity (Fig. 3)—even the colossal T. rex may have been covered with a cloak of feathers at some early stage of its life. It is an amazing experience to gaze at the entirely modern feathers of animals, whereas their skeletal characteristics are so unquestionable dinosaurian. Fig. 3 Genealogical relationships of feathered nonavian theropods. Current evidence supports the hypothesis that filamentous and vaned feathers evolved with the divergence of coelurosaurs and maniraptorans, respectively An important corollary of these discoveries is that feathers did not evolve in the context of flight. With the sole exception of Microraptor, it is certain that none of these feathered dinosaurs were able to take to the air. The forelimbs and their feathers are both much shorter than in flying birds and their bodies are larger. The evolutionary transition toward birds and the origin of their flight involved a dramatic reduction in body size. These feathered dinosaurs indicate that, at their onset, feathers must have had a different function, perhaps insulating the bodies of animals that had metabolically diverged from their cold-blooded, reptilian ancestors. My research has suggested that vaned feathers may have originated in the context of thrust, evolving in running nonavian theropods that by flapping their feathered arms were able to increase their running speed (Burgers and Chiappe 1999). In the end, however, we simply do not have an answer for what was the original function of feathers; nonetheless, we have been able to eliminate flight as an option. Today, the century-old debate on bird ancestry has largely been resolved. The uncertainties that led to this long controversy—both empirical and methodological—have been clarified and there is an overwhelming consensus in support of the idea that birds evolved from maniraptoran theropods. Current evidence highlights the fact that many features previously thought to be exclusively avian—from feathers to a wishbone—have now been discovered in the immediate dinosaur predecessor of birds. The origin of birds was also preceded by a substantial reduction in body size—the most primitive members of groups such as troodontids and dromaeosaurids are smaller than one meter long (Turner et al. 2007). This notable reduction in the size of the forebears of birds was an important prerequisite of flight; even this most characteristic avian attribute is likely to have been inherited by birds from their dinosaurian predecessors. The comparative studies that have been the building blocks of these important evolutionary conclusions have been greatly assisted by many newly discovered Mesozoic-aged birds (Chiappe 2007), which by possessing many skeletal features that are only slightly modified from the ancestral maniraptoran condition, fill a critical gap in the evolutionary transition toward modern birds (Figs. 1, 4, and 5). This newly-discovered fossil menagerie has unveiled an unexpected diversity of archaic birds that would take birding to another dimension. These new discoveries are reviewed next. Fig. 4 Cladogram or diagram depicting the genealogical relationships among the main lineages of premodern birds and some lineages of nonavian maniraptoran dinosaurs. The known fossil record of these groups is also highlighted. The concept of a dove as a living dinosaur—because they share a common descent—may seem bizarre, but, in reality, it is just as logical as the argument that humans are primates because we evolved from primates The Long March Toward Modern Birds Research on the early history of birds and the development of flight has been at the forefront of paleontology since the advent of evolutionary thought. For most of this time, however, the available evidence was limited to a small number of fossils largely restricted to near-shore and marine environments and was greatly separated both anatomically and in time. In the last few decades, however, our understanding of the origin and ancient divergences of birds has advanced at an unparalleled rate. This rapid increase in discoveries has not only filled much of the anatomical and temporal gaps that existed previously, but has also made the study of early birds one of the most dynamic fields of vertebrate paleontology. New information highlights the fact that the enormous diversity of living birds is just a remnant of an archaic evolutionary radiation that can be traced back to Archaeopteryx (Mayr et al. 2005) (Figs. 1 and 4). Few physical features set this most ancient bird apart from its theropod dinosaur predecessors. However, Archaeopteryx gives us paramount clues to the beginning of one of the most dramatic evolutionary events in the history of vertebrates—the development of powered flight in birds. This 150-million-year-old jay-sized bird with toothed jaws, clawed wings, and a long bony tail stands alone in the fossil record of birds of the end of the Jurassic period. Yet, in the last decade, a large number and variety of birds have been found in early Cretaceous rocks ranging from 130 to 115 million years ago (Chiappe 2007; Zhou 2004). These fossils reveal that a great diversity of birds with long bony tails preceded the evolution of birds with an abbreviated bony tail (Forster et al. 1998; Zhou and Zhang 2003), one composed of fewer vertebrae ending in a bony stump called a pygostyle (the structure that supports the “parson's nose”). Characteristics of the plumage, the large wing size, and specific features of their brain all suggest that Archaeopteryx and the remaining long-tailed birds were fliers, even if these birds probably required a take-off run to become airborne (Burgers and Chiappe 1999). A rich diversity of more advanced birds is also recorded in these early Cretaceous rocks. In fact, the differing design of skulls, teeth, wings, and feet indicate that, even at this early phase of their evolutionary history, birds had specialized into a variety of ecological niches, including seed-feeders, insect-feeders, fish-eaters, and meat-eaters (Chiappe 2007). At the same time, a host of novel features of the wings, shoulders, and tails suggests that, soon after Archaeopteryx, birds evolved flying abilities not very different from the ones that amaze us today, a feat that was most likely the recipe for their dramatic diversification during the Cretaceous. Paramount among these transformations is the abbreviation of the tail and the consequent development of a pygostyle. Yet, the details of this evolutionary transition are far from clear. One recent fossil that has shed some light onto this transition is the tiny, 125-million-year-old Zhongornis (Gao et al. 2008) from northeastern China. Zhongornis is the first bird discovered that has a short tail and a corresponding reduced number of tail vertebrae, yet lacks the pygostyle that is present in all other short-tailed birds. Therefore, Zhongornis represents an intermediate stage between the primitive long-tailed birds and those with a bony stump at the end of the tail. Evidence from the skeleton of Zhongornis suggests that a short tail with a reduced number of vertebrae evolved earlier in birds than did the pygostyle. Very early in their evolutionary history, short bony-tailed birds blossomed in a range of shapes and sizes. Hundreds of specimens of the stout-beaked Confuciusornis, many surrounded by a halo of dark feathers, have been unearthed from the 125-million-year-old deposits of northeastern China (Chiappe et al. 1999) (Fig. 5). This crow-sized bird sported long hands with enormous claws and long and tapering wings. Growth series of Confuciusornis spanning a large spectrum of sizes suggest that, unlike modern birds, this and other archaic birds required multiple years to reach adult size (Chiappe 2007). The contemporaneous and much larger Sapeornis had longer and narrower wings, superficially resembling those of albatrosses (Zhou and Zhang 2002) (Fig. 1). Albeit bearing stout teeth and a very primitive shoulder, the anatomy of this bird suggests a closer relationship to modern birds than Confuciusornis. Combined, however, these fossils best illustrate the anatomy and appearance of the most primitive short-tailed birds, which, by virtue of their proportionally larger wings, were likely better fliers than their long-tailed predecessors. Fossils of more advanced birds are also first recorded at around 130 million years ago. Among these are the enantiornithines (Chiappe 2007; Chiappe and Witmer 2002), a group that constitutes the most important evolutionary radiation of premodern birds. Like most early birds, the majority of enantiornithines had toothed jaws and partially clawed wings (Figs. 1 and 5). Yet their skeletons show a series of key transformations that approach those of today's birds. Some of these include the shortening of the hand and fingers as well as changes in the proportions of the wing bones and the anatomy of the shoulder. Furthermore, these birds evolved important innovations in their plumage, namely, a safety device called the alula (a small tuft of feathers also known as the “bastard wing”), which assists modern birds during their take-off and landing (Sanz et al. 1996). The significant transformations of the skeleton and plumage of these birds suggest that, even at the onset of their evolutionary history, enantiornithines were able to take-off from a standstill position and maneuver in ways similar to those seen among living birds. It is most likely that the evolution of these enhanced flying capabilities played a key role in the evolutionary success of the enantiornithines, which by about 120 million years ago seem to have risen to dominance. Rocks from the early Cretaceous also record a number of transitional fossils that herald the evolution of the closest relatives of modern birds (Fig. 1). In some respects, these primitive ornithuromorphs (Chiappe 2007; Chiappe and Witmer 2002; Zhou 2004; Zhou and Zhang 2005) resemble the enantiornithines, but their skeletons show, for the first time, clear trademarks of their living counterparts. The majority of these primitive ornithuromorphs were lightly built, flying birds, whose sizes tend to be larger than those of their contemporaneous enantiornithines. Like the latter, both their skeletons and plumage show clear evidence of enhanced aerodynamic capabilities. It is within these birds that we witness the origin of the extremely fast rates of body maturation characteristic of modern birds (Chiappe and Witmer 2002), which reach their full body size within a year after hatching. As the rocks of the Cretaceous period become younger, a series of other lineages of ornithuromorphs make their debut. The hesperornithiforms—large, flightless, foot-propelled divers—first appear around 100 million years ago (Chiappe 2007; Chiappe and Witmer 2002). Albeit entirely restricted to the aquatic realm, the hesperornithiforms exhibit a rich and diverse evolutionary history spanning over 35 million years—their last representatives may have disappeared with the latest Cretaceous mass extinction that wiped out the last of the nonavian dinosaurs. Despite the fact that their earliest records represent birds the size of a loon, millions of years later, these supreme fish-eaters would be crowned kings of the aquatic birds with a number of large forms such as the tiny-winged, four-foot long Hesperornis and Asiahesperornis. The hesperornithiforms swam the waters of tropical seas that, during the late Cretaceous, divided in half both North America and Eurasia. On the shore of these shallow seas, over herds of duck-billed and other kinds of dinosaurs, soared the tern-sized Ichthyornis (Clarke 2004) (Fig. 1). In most respects, this bird represents a step closer to modern birds, yet it had sharply toothed jaws designed to catch fish. Ichthyornis is perhaps the best-known, closest relative of modern birds; other late Cretaceous fossils seemingly close to the latter are known by much more fragmentary remains. Not all the birds that lived during the Mesozoic may have looked as unfamiliar as Archaeopteryx, Confuciusornis, and Hesperornis. The early representatives of today's lineages of birds can also be traced back to this remote era of our geological past. In several continents, rocks from the last part of the Cretaceous period—75 to 65 million years ago—reveal the remains of early shorebirds, ducks, and other familiar birds (Kurochkin et al. 2002; Clarke et al. 2005). These discoveries indicate that a number of modern lineages had their origins prior to the end of the Mesozoic. It is unclear how these early representatives of modern birds managed to survive the devastating mass extinction of the end of the Cretaceous, but these survivors diversified soon after into a myriad of forms, which today carry the legacy of the magnificent dinosaurs that ruled the earth tens of millions of years ago. The Dinosaur in your Backyard In the last few decades, our understanding of the origin and subsequent evolutionary diversification of birds has advanced at an unparalleled pace. These fossil discoveries have documented the stepwise nature of one of the most fascinating evolutionary transitions, and they have filled the large gap that separated living birds from their dinosaurian predecessors. This new evidence has shown that many of the features previously considered to be avian trademarks first evolved within theropod dinosaurs. The strength of the hypothesis that birds evolved within maniraptoran theropod dinosaurs is manifested by the convergent results of a diversity of studies within a multitude of scientific disciplines. Today, the theropod origin of birds is supported by a wealth of evidence ranging from skeletal anatomy to molecular data. This evolutionary conclusion indicates that the diverse modern birds are a branch of a much larger avian tree that diverged during the Mesozoic era and that, in turn, all these birds are but a shoot of the majestic tree of dinosaurs. This evidence has led to the realization that the jays, finches, and hummingbirds that so peacefully frequent your backyard are indeed living dinosaurs—a surviving lineage of vicious predators that ruled the terrestrial ecosystems of the Mesozoic. Clark JM, Norell MA, Chiappe LM. An oviraptorid skeleton from the Late Cretaceous of Ukhaa Tolgod, Mongolia, preserved in an avian-like brooding position over an oviraptorid nest. Am Mus Novit. 1999;3265:1–36. Corresponding author Rights and permissions Open Access This is an open access article distributed under the terms of the Creative Commons Attribution Noncommercial License ( https://creativecommons.org/licenses/by-nc/2.0 ), which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."}
{"url": "https://en.wikipedia.org/wiki/Liberty_dollar_(private_currency)", "text": "This infobox shows the latest status before this currency was rendered obsolete. The American Liberty Dollar (ALD) was a private currency produced in the United States. The currency was issued in minted metal rounds (similar to coins), gold and silver certificates, and electronic currency (eLD). ALD certificates are \"warehouse receipts\" for real gold and silver owned by the bearer. According to court documents, there were about 250,000 holders of Liberty Dollar certificates.[1] The metal was warehoused at Sunshine Minting in Coeur d'Alene, Idaho prior to a November 2007 raid by the Federal Bureau of Investigation (FBI) and the U.S. Secret Service (USSS).[2] Until July 2009, the Liberty Dollar was distributed by Liberty Services (formerly known as \"National Organization for the Repeal of the Federal Reserve and the Internal Revenue Code\" or NORFED), based in Evansville, Indiana. It was created by Bernard von NotHaus, the founder of the Cannabis Spiritual Center in Malibu, California, and the co-founder of the Royal Hawaiian Mint Company.[3] In May 2009, von NotHaus and others were charged with federal crimes in connection with the Liberty Dollar, and, on July 31, 2009, von NotHaus announced that he had closed the Liberty Dollar operation, pending resolution of the criminal charges.[4] On March 18, 2011, von NotHaus was pronounced guilty of \"making coins resembling and similar to United States coins\".[5][6] In late 2014, a U.S. District Court judge ruled that Liberty Dollars seized in the 2007 FBI/USSS operation should be returned to their owners.[7] From 1998 to July 2009, Liberty Services exchanged Federal Reserve Notes (U.S. dollars) for silver Liberty Dollars (and later gold and copper), as well as for Warehouse Receipts in both paper and digital forms. Liberty Services' original name was \"National Organization for the Repeal of the Federal Reserve and the Internal Revenue Code\" (NORFED). Since its founding, the organization asserted that the Federal Reserve was unconstitutional and harmful. The company engaged in a series of legal battles both defending their exchange service and challenging exclusivity assertions made by the US Mint (see Legal issues). Ron Paul Dollars were also briefly minted, taking advantage of Ron Paul's public favoring of metals backed currency and the Ron Paul presidential campaign, 2008. However, this raised issues of its legality amid an FBI raid confiscating two tons of the coins.[8][9] This briefly caught the attentions of the media, taking note of its novelty over any actual value.[10] We would greatly prefer that folks would just donate rather than buy a Ron Paul dollar, we think that’s the best way to help out Ron Paul.[11] A number of alternative currencies exist in the United States, including Phoenix Dollars, Goldbacks, Baltimore's BNote, Ithaca Hours, Bitcoin, and digital gold currency. Unlike some other alternative currencies, both Liberty Dollars and Phoenix Dollars were denominated by weight and backed by a commodity: Liberty Dollars used gold, silver, platinum, or copper. Other private currencies use different bases, such as tying their value to a specific unit of time; i.e., 1 hour = 1 Time Dollar.[12] Under the most simplistic version of that model, the future value of the currency would depend on the willingness of people to swap their labor, regardless of the market value of the labor provided. However, systems such as Ithaca Hours have introduced more sophisticated models that allow for variations in market value of labor. Liberty Dollars also differed from other alternative currencies in that they carried a suggested US dollar face value. The only laws that pertain to private currencies are ordinary statutes against fraud. Coining is more technologically difficult than is printing, and inclusion of precious metal in coins has long been seen as a means of \"embedding\" value into them. The Liberty Dollar consisted of coins and printed notes. Paper and digital Liberty Dollars were legally defined as warehouse receipts and were backed by a physical commodity: a weight in precious metal. 18 USC 486, however, makes it a crime to make, utter, or pass any coin or bar of gold, silver, or any other metal if it is intended to be used as money, so there is a definitive injunction against the minting of coins for that purpose. The Liberty Dollar \"base value\" was created by Bernard von NotHaus. As of 2009[update], the base value of the Liberty Dollar was $20 Liberty Dollars to one ounce of silver.[13] At the time the Liberty Dollar operation was closed, one ounce Liberty Dollar gold pieces were denominated $1,000 with a maximum charge of 10% over spot price with membership. The previous base values were $10 silver ounce, $20 silver ounce and $500 gold ounce. Non-members paid full face value for all currency except for certain Special and Numismatic items. Members' discounts ranged from 0% to 50%+ (actually, for short periods during crossovers it was possible that even members could not buy Liberty Dollars at face value or less). Liberty Dollar associates and merchants used to exchange for Liberty Dollars at a discount, so they could \"make money when [they] spend money.\"[citation needed] To further distinguish how the Liberty Dollar worked, von NotHaus transitioned to a commission structure in June 2007 where associates and merchants received a commission in the form of extra Liberty Dollars when they placed their orders. Regional currency officers received larger discounts; they were the regional distributors and official representatives of Liberty Services. The Liberty Dollar associate and merchant discounts ranged from 0.0%–50%+ (zero to more than fifty percent) depending on where the price of silver was, relative to the Liberty Dollar base value, the Liberty Dollar base value crossover points, and the time periods during which the price stayed above varying moving-day averages over 30, 60 or 90 days in a fluctuating market, based on Liberty Dollar formulas worked out by von NotHaus.[14] A \"regional currency office\" was a kind of distributor of Liberty Dollars. In exchange for a fee paid to the Liberty Dollar Organization, they could purchase Liberty dollars for resale at a discount. They were also authorized to purchase, convert, or perhaps exchange Liberty Dollars for Federal Reserve Notes.[15] Numerous individuals within the U.S. Government have been interviewed regarding the Liberty Dollar. The Liberty Dollar organization asserted one Secret Service agent claimed \"It's not counterfeit money\"[16] while remaining \"skeptical\" of NORFED. Another agent reportedly warned that the Liberty Dollar \"appears to be in violation of 18 U.S.C.§ 514.\"[16] The minting of Liberty Dollar coins also appears to be in violation of 18 U.S.C.§ 486: Whoever, except as authorized by law, makes or utters or passes, or attempts to utter or pass, any coins of gold or silver or other metal, or alloys of metals, intended for use as current money, whether in the resemblance of coins of the United States or of foreign countries, or of original design, shall be fined under this title or imprisoned not more than five years, or both. The promoter of the Liberty Dollar asserts that Claudia Dickens, spokeswoman for the U.S. Treasury Department'sBureau of Engraving and Printing, had previously said American Liberty Currency is legitimate. Dickens was quoted as having said \"There's nothing illegal about this\", after the Treasury Department's legal team reviewed the currency. \"As long as it doesn't say 'legal tender' there's nothing wrong with it.\"[17] In 2006 the U.S. Mint issued a press release stating that prosecutors at the Justice Department had determined that using Liberty Dollars as circulating money is a federal crime. The press release also stated that the \"Liberty Dollars\" are meant to compete with the circulating coinage (currency) of the United States and such competition consequently is a criminal act.[18] The Justice Department also stated that the Liberty Dollar was confusingly similar to actual U.S. currency, and the language used on NORFED's website was deceptive.[19] The Liberty Dollar organization responded to the Mint's press release by stating that \"[t]he Liberty Dollar never has claimed to be, does not claim to be, is not, and does not purport to be, legal tender.\"[20] The promoters of the Liberty Dollar have asserted that the Liberty Dollar is not legal tender, and that legal tender and barter are mutually exclusive concepts. The promoter asserts that the Liberty Dollar is a numismatic piece or medallion which may be used voluntarily as barter. On March 20, 2007, Liberty Services owner Bernard von NotHaus filed suit in the District Court for the Southern District of Indiana against the U.S. Mint's claims regarding the Liberty Dollar. Defendants include Henry M. Paulson, Secretary of the Treasury; Alberto R. Gonzales, former Attorney General of the United States; and Edmund C. Moy, Director of the Mint.[21] The suit sought a declaratory judgment that circulating Liberty Dollars as a voluntary barter currency is not a federal crime and an injunction barring the Defendants from publicly or privately declaring the Liberty Dollar an illegal currency and to remove any such declarations from the U.S. Mint's website.[22] The Liberty Dollar offices were raided by agents of the Federal Bureau of Investigation (FBI) and the U.S. Secret Service (USSS) on November 14, 2007. Bernard von NotHaus, the owner of Liberty Services, sent an email to customers and supporters saying that the agents took all the gold, silver, and platinum, and almost two tons of Ron Paul Dollars. The agents also seized computers and files and froze the Liberty Dollar bank accounts.[23] Von NotHaus's email linked to a signup page for a class action lawsuit so that the victims might recover their assets. At the same time, all forms on his website relating to purchases of Liberty Dollars became nonfunctional. The local Evansville Courier & Press reported the email, stating that \"FBI Agent Wendy Osborne, a spokeswoman for the FBI's Indianapolis office, directed all questions on the raid to the Western District of North Carolina U.S. Attorney's Office. A spokeswoman there said she had no information on the investigation. Bernard von NotHaus, the group's monetary architect and the author of the email, did not immediately respond to a message seeking comment.\"[27] The Associated Press quoted von NotHaus on November 16, 2007, as saying that the federal government was \"running scared right now and they had to do something .... I'm volunteering to meet the agents and get arrested so we can thrash this out in court.\"[28] A federal grand jury brought an indictment against von NotHaus and three others in May 2009 in United States District Court in Statesville, North Carolina,[29] and von NotHaus was arrested on June 6, 2009. Bernard von NotHaus was charged with one count of conspiracy to possess and sell coins in resemblance and similitude of coins of a denomination higher than five cents, and silver coins in resemblance of genuine coins of the United States in denominations of five dollars and greater, in violation of 18 U.S.C.§ 485, 18 U.S.C.§ 486, and 18 U.S.C.§ 371; one count of mail fraud in violation of 18 U.S.C.§ 1341 and 18 U.S.C.§ 2; one count of selling, and possessing with intent to defraud, coins of resemblance and similitude of United States coins in denominations of five cents and higher, in violation of 18 U.S.C.§ 485 and 18 U.S.C.§ 2; and one count of uttering, passing, and attempting to utter and pass, silver coins in resemblance of genuine U.S. coins in denominations of five dollars or greater, in violation of 18 U.S.C.§ 486 and 18 U.S.C.§ 2.[29] On March 18, 2011, von NotHaus was convicted of \"making, possessing and selling his own coins\", after a jury in Statesville, North Carolina deliberated for less than two hours.[31] The jury found him guilty of one count under 18 U.S.C.§ 485 and 18 U.S.C.§ 2, one count of violating 18 U.S.C.§ 486 and 18 U.S.C.§ 2, and one count of conspiracy, under 18 U.S.C.§ 371, to violate sections 485 and 486.[32] He faces up to 15 years in prison, a $250,000 fine, and may be forced to give $7 million worth of minted coins and precious metals to the government, weighing 16,000 pounds.[31] Attorney for the Western District of North Carolina, Anne M. Tompkins, described the Liberty Dollar as \"a unique form of domestic terrorism\" that is trying \"to undermine the legitimate currency of this country\".[33] The Justice Department press release quotes her as saying: \"While these forms of anti-government activities do not involve violence, they are every bit as insidious and represent a clear and present danger to the economic stability of this country.\"[33] According to the Associated Press, \"Federal prosecutors successfully argued that von NotHaus was, in fact, trying to pass off the silver coins as U.S. currency. Coming in denominations of 5, 10, 20, and 50, the Liberty Dollars also featured a dollar sign, the word \"dollar\" and the motto \"Trust in God,\" similar to the \"In God We Trust\" that appears on U.S. coins\".[34] Since his trial, The New York Times has said that his followers describe von NotHaus as \"the Rosa Parks of the constitutional currency movement.\" He appealed his conviction, but his appeal was denied on November 10, 2014.[35] On November 11, 2014, Judge Voorhees denied von NotHaus's Motion for Acquittal.[35] On December 2, 2014, despite prosecutor demands that he serve as much as 23 years in Federal prison, he was sentenced to 6 months house arrest, with 3 years probation.[36] As part of his reasoning for delivering a greatly reduced sentence from what Federal Prosecutors demanded, Judge Richard L. Voorhees considered von NotHaus's appeal, which stated: ...if anything is clear from the evidence presented at trial, it is that the last thing Mr. von NotHaus wanted was for Liberty Dollars [to] be confused with coins issued by the United States government...His intention – to protest the Federal Reserve system – has always been plain. The jury's verdict conflates a program created to function as an alternative to the Federal Reserve system with one designed to [deceive] people into believing it was the very thing Mr. von NotHaus was protesting in the first place...the Liberty Dollars was not a counterfeit and was not intended to function as such. The verdict is a perversion of the counterfeiting statutes and should be set aside. The conviction, which was seen as a victory for the government, has now defined 18 U.S.C.§ 486 as prohibiting the use of silver bullion, or any other metal coin or bar not issued under government authority, from being used as currency in commerce. The Silver Certificates issued by Liberty Services were not considered any form of counterfeiting or violation of law. Bernard von NotHaus's probation officer suggested he file for early release from probation after one year, and recommended the early termination to the court. Termination of probation was formally granted December 9, 2015 by U.S. District Judge Richard L. Voorhees.[37] When asked about the government's motive for accusing him of terrorism, von NotHaus scoffed, \"This is the United States government. It's got all the guns, all the surveillance, all the tanks, it has nuclear weapons, and it's worried about some ex-surfer guy making his own money? Give me a break!\"[38] The forfeiture trial was scheduled to resume Monday, April 4, 2011.[39] Federal prosecutors were seeking to take roughly $7 million worth or five tons in Liberty Dollars minted in gold and silver seized in 2007 from a warehouse by the FBI and USSS.[40] In 2017, a significant number of seized Liberty Dollars were returned to their owners after petitions were made to the court for this return.[41] Bernard von NotHaus continues to honor redemption of silver to this day. ^ abIndictment, docket entry 3, May 19, 2009, case no. 5:09-cr-00027-RLV-DCK-1, United States v. Bernard von NotHaus et al., U.S. District Court for the Western District of North Carolina (Statesville Div.). ^Waiver of Personal Appearance at Arraignment and Entry of Plea of Not Guilty, docket entry 36, July 28, 2009, case no. 5:09-cr-00027-RLV-DCK-1, United States v. Bernard von NotHaus et al., U.S. District Court for the Western District of North Carolina (Statesville Div.)."}
{"url": "https://indianapublicmedia.org/amomentofscience/photographic-memory.php", "text": "Noon Edition Photographic Memory By Don Glass Posted September 27, 2003 If you've ever looked at the Guinness Book of World Records, you know that some people are capable of what seem to be astounding feats of memory. There are chess masters, for example, who are able to glance quickly at pieces on a chess board and then flawlessly reconstruct the positions on a new board. Others are able to read through a page in a book and then recite the page from memory, or memorize hundreds of names and numbers from a telephone directory. The popular explanation for such feats is photographic memory--the ability to mentally photograph a visual scene and then recall it in precise detail. According to most psychologists, however, photographic memory is probably a myth. People with what in the scientific world is called eidetic memory, are able to process and organize certain kinds of information with remarkable efficiency. This is different from carrying a detailed picture of an image or text in the brain. For example, one experiment showed that while chess experts can reconstruct realistic board positions from memory, when the pieces were placed randomly on the board the experts' memory was no better than ordinary players. The experts, in other words, did not retain a precise mental image of what they saw on the board. Rather, using their deep knowledge of chess positions, they were able to mentally organize the visual data. Such ability is certainly extraordinary, but it has nothing to do with photography. Support For Indiana Public Media Comes From About A Moment of Science A Moment of Science is a daily audio podcast, public radio program and video series providing the scientific story behind some of life's most perplexing mysteries. Learn More »"}
{"url": "https://en.wikipedia.org/wiki/Google_Finance", "text": "Google Finance was first launched by Google on March 21, 2006. The service featured business and enterprise headlines for many corporations including their financial decisions and major news events. Stock information was available, as were Adobe Flash-based stock price charts which contained marks for major news events and corporate actions. The site also aggregated Google News and Google Blog Search articles about each corporation, though links were not screened and often deemed untrustworthy.[1] Google launched a revamped version of their finance site on December 12, 2006, featuring a new homepage design that lets users see currency information, sector performance for the United States market and a listing of top market movers along with the relevant and important news of the day. A top movers section was also added, based on popularity determined by Google Trends. The upgrade also featured charts containing up to 40 years of data for U.S. stocks, and richer portfolio options. Another update brought real-time ticker updates for stocks to the site, as both NASDAQ and the New York Stock Exchange partnered with Google in June 2008.[2][3] Google added advertising to its finance page on November 18, 2008. However, since 2008, it has not undergone any major upgrades and the Google Finance Blog was closed in August 2012. On September 22, 2017, Google confirmed that the website was under renovation and that portfolio features would not be available after mid-November 2017.[4][5] In early 2018, a notice on the website announced that the website had been renovated. The notice said that the portfolio feature was to be removed, and advised that stocks from the old portfolio feature would be migrated to the new website, and also giving the option for users to download the portfolio as a CSV file.[6]"}
{"url": "https://www.artic.edu/artworks/6565/american-gothic", "text": "About this artwork In American Gothic, Grant Wood directly evoked images of an earlier generation by featuring a farmer and his daughter posed stiffly and dressed as if they were, as the artist put it, “tintypes from my old family album.” They stand outside of their home, built in an 1880s style known as Carpenter Gothic. Wood had seen a similar farmhouse during a visit to Eldon, Iowa. When it was exhibited at the Art Institute in 1930, the painting became an instant sensation, its ambiguity prompting viewers to speculate about the figures and their story. Many understood the work to be a satirical comment on midwesterners out of step with a modernizing world. Yet Wood intended it to convey a positive image of rural American values, offering a vision of reassurance at the beginning of the Great Depression. Status Department Artist Title Place Date Dates are not always precisely known, but the Art Institute strives to present this information as consistently and legibly as possible. Dates may be represented as a range that spans decades, centuries, dynasties, or periods and may include qualifiers such as c. (circa) or BCE. Object information is a work in progress and may be updated as new research findings emerge. To help improve this record, please email . Information about image downloads and licensing is available here."}
{"url": "https://en.wikipedia.org/wiki/Mongolian_dollar", "text": "The dollar (Mongolian: доллар) was the currency of Mongolia between 1921 and 1925. Treasury notes were issued under Baron Ungern in 1921. The denominations were 10, 20, 50 and 100 dollars. It was intended to replace the Chinese yuan at par but, according to European travellers of the time, was worthless.[1] Further banknotes were printed in 1924, in denominations of 50 cents, 1, 3, 5, 10 and 25 dollars, but were not issued. The dollar, together with other circulating currencies, was replaced by the tögrög in 1925."}
{"url": "https://en.wikipedia.org/wiki/Royal_Mint", "text": "Operating under the legal name The Royal Mint Limited, it is a limited company that is wholly owned by His Majesty's Treasury and is under an exclusive contract to supply the nation's coinage. As well as minting circulating coins for the UK and international markets, The Royal Mint is a leading provider of precious metal products. The Royal Mint operated within the Tower of London for several hundred years before moving to what is now called Royal Mint Court, where it remained until the 1960s. As Britain followed the rest of the world in decimalising its currency, the Mint moved from London to a new 38-acre (15 ha) plant in Llantrisant, Glamorgan, Wales, where it has remained since. Since 2018 The Royal Mint has been evolving its business to help offset declining cash use. It has expanded into precious metals investment, historic coins, and luxury collectibles, which saw it deliver an operating profit of £12.7 million in 2020–2021.[7] In 2022 The Royal Mint announced it was building a new plant in South Wales to recover precious metals from electronic waste.[8] The first of this sustainably sourced gold is already being used in a new jewellery division – 886 by The Royal Mint – named in celebration of its symbolic founding date.[9] The history of coins in Great Britain can be traced back to the second century BC when they were introduced by Celtic tribes from across the English Channel. The first record of coins being minted in Britain is attributed to Kentish tribes such as the Cantii who around 80–60 BC imitated those of Marseille through casting instead of hammering.[10] After the Romans began their invasion of Britain in AD 43, they set up mints across the land, which produced Roman coins for some 40 years before closing. A mint in London reopened briefly in 383 until closing swiftly as Roman rule in Britain came to an end. For the next 200 years, no coins appear to have been minted in Britain until the emergence of English kingdoms in the sixth and seventh centuries. By 650, as many as 30 mints are recorded across Britain.[11] In 1279, the country's numerous mints were unified under a single system whereby control was centralised to the mint within the Tower of London. Mints outside London were reduced, with only a few local and episcopal mints continuing to operate.[2]Pipe rolls containing the financial records of the London mint show an expenditure of £729 17s 8+1⁄2d and records of timber bought for workshops. Individual roles at the mint were well established by 1464. The master worker was charged with hiring engravers and managing moneyers, while the Warden was responsible for witnessing the delivery of dies. A specialist mint board was set up in 1472 to enact a 23 February indenture that vested the mint's responsibilities into three main roles: a warden, a master and comptroller. In the early 16th century, mainland Europe was in the middle of an economic expansion, but England was suffering from financial difficulties brought on by excessive government spending. By the 1540s, wars with France and Scotland led Henry VIII to enact The Great Debasement, which saw the amount of precious metal in coins significantly reduced.[12] In order to strengthen control of the country's currency, monasteries were dissolved, which effectively ended major coin production outside London. In 1603, the Union of Crowns of England and Scotland under King James I led to a partial union of the two kingdoms' currencies, the pound Scots and the pound sterling. Because Scotland had heavily debased its silver coins, a Scots mark was worth just 13+1⁄2 pence while an English mark was worth 6 shillings 8 pence (80 pence). To bridge the difference between the values, unofficial supplementary token coins, often made from lead, were made by unauthorised minters across the country. By 1612, there were 3,000 such unlicensed mints producing these tokens, none of them paying anything to the government. The Royal Mint, not wanting to divert manpower from minting more profitable gold and silver coins, hired outside agent Lord Harington who, under license, started issuing copperfarthings in 1613. Private licenses to mint these coins were revoked in 1644, which led traders to resume minting their own supplementary tokens. In 1672, the Royal Mint finally took over the production of copper coinage. After raising the royal standard in Nottingham, marking the beginning of the civil war, Charles called on loyalist mining engineer Thomas Bushell, the owner of a mint and silver mine in Aberystwyth, to move his operations to the royalist-held Shrewsbury, possibly within the grounds of Shrewsbury Castle. However, this mint was short-lived, operating for no more than three months before Charles ordered Bushell to relocate the mint to his headquarters in the royal capital of Oxford. The new Oxford mint was established on 15 December 1642 in New Inn Hall, the present site of St Peter's College. There, silver plates and foreign coins were melted down and, in some cases, just hammered into shape to produce coins quickly. Bushell was appointed the mint's warden and master-worker, and he laboured alongside notable engravers Nicholas Briot, Thomas Rawlins and Nicholas Burghers, the last of whom[clarification needed] was appointed Graver of Seals, Stamps, and Medals in 1643. When Prince Rupert took control of Oxford that same year, Bushell was ordered to move to Bristol Castle, where he continued minting coins until it fell to parliamentary control on 11 September 1645, effectively ending Bushell's involvement in the civil war mints. In November 1642, the king ordered royalist MPRichard Vyvyan to build one or more mints in Cornwall, where he was instructed to mint coins from whatever bullion could be obtained and deliver it to Ralph Hopton, a commander of royalist troops in the region. Vyvyan built a mint in Truro and was its Master until 1646, when it was captured by parliamentarians. In December 1642, the parliamentarians set up a mint in nearby Exeter, which had been under parliamentary control since the beginning of the war and was under constant threat of attack by loyalist troops. In September 1643, the town was captured by the Cornish Royalist Army led by Prince Maurice, leading Vyvyan to move his nearby mint in Truro to the captured town. The exact location of the mint in Exeter is unknown; however, maps from the time show a street named Old Mint Lane near Friernhay, which was to be the site of a 1696 Recoinage mint. Much less is known about the mint's employees, with only Richard Vyvyan and clerk Thomas Hawkes recorded.[13] Following Charles I's execution in 1649, the newly formed Commonwealth of England established its own set of coins, which for the first time used English rather than Latin and were more plainly designed than those issued under the monarchy.[14] The government invited French engineer Peter Blondeau, who worked at the Paris Mint, to come to London in 1649 in the hope of modernising the country's minting process. In France, hammer-stuck coins had been banned from the Paris Mint since 1639 and replaced with milled coinage.[15] Blondeau began his testing in May 1651 in Drury House. He initially produced milled silver pattern pieces of half-crowns, shillings and sixpences; however rival moneyers continued using the old hammering method. In 1656, Lord ProtectorOliver Cromwell ordered engraver Thomas Simon to cut a series of dies featuring his bust and for them to be minted using the new milled method. Few of Cromwell's coins entered circulation; Cromwell died in 1658 and the Commonwealth collapsed two years later. Without Cromwell's backing of milled coinage, Blondeau returned to France, leaving England to continue minting hammer-struck coins. In 1662, after previous attempts to introduce milled coinage into Britain had failed, the restored monarch Charles II recalled Peter Blondeau to establish a permanent machine-made coinage.[16][17] Despite the introduction of the newer, milled coins, like the old hammered coins they suffered heavily from counterfeiting and clipping. To combat this the text Decus et tutamen (\"An ornament and a safeguard\") was added to some coin rims.[18] After the Glorious Revolution of 1688, when James II was ousted from power, parliament took over control of the mint from the Crown, which had until then allowed the mint to act as an independent body producing coins on behalf of the government. The Acts of Union 1707 united England and Scotland into one country, leading London to take over production of Scotland's currency and thus replacing Scotland's Pound Scots with the English Pound sterling. As a result, the Edinburgh mint closed on 4 August 1710. As Britain's empire continued to expand, so too did the need to supply its coinage. This, along with the need for new mint machinery and cramped conditions within the Tower of London, led to plans for the mint to move to nearby East Smithfield. Construction started in 1805 on the new purpose-built mint on Tower Hill, opposite the Tower of London, and it was completed by 1809. In 1812, the move became official: the keys of the old mint were ceremoniously delivered to the Constable of the Tower.[19] Facing the front of the site, stood the Johnson Smirke Building, named for its designer James Johnson and builder Robert Smirke. Construction was supervised by the architect John Lidbury Poole (father of the famous singer, Elizabeth Poole).[20] This building was flanked on both sides by gatehouses behind which another building housed the mint's new machinery. Several other smaller buildings were erected, which housed mint officers and staff members. The entire site was protected by a boundary wall patrolled by the Royal Mint's military guard. By 1856, the mint was beginning to prove inefficient: there were irregularities in minted coins' fineness and weight. Instructed by Prime MinisterLord Palmerston, the Master of the Mint Thomas Graham was informed that unless the mint could raise its standards and become more economical, it would be broken up and placed under management by contractors. Graham sought advice from German chemist August Wilhelm von Hofmann, who in turn recommended his student George Frederick Ansell to resolve the mint's issues. In a letter to the Treasury dated 29 October 1856, Ansell was put forward as a candidate. He was appointed as a temporary clerk on 12 November 1856, with a salary of £120 per year.[21] Upon taking office, Ansell discovered that the weighing of metals at the mint was extremely loose. At the mint, it had been the custom to weigh silver to the nearest 0.5 troy ounces (16 g) and gold to a pennyweight (0.05 oz); however, these standards meant losses were being made from overvalued metals. In one such case, Ansell delivered 7920.00 oz of gold to the mint, where it was weighed by an official at 7918.15 oz, a difference of 1.83 oz. Requesting a second weighing on a more accurate scale, the bullion was certified to weigh 7919.98 oz, far closer to the previous measurement, which was off by 960 grains.[citation needed][Numbers don't agree.] To increase the accuracy of weights, more precise weighing equipment was ordered, and the tolerance was revised to 0.10 oz for silver and 0.01 oz for gold. Between 1856 and 1866, the old scales were gradually removed and replaced with scales made by Messrs. De Grave, Short, and Fanner; winners of an 1862 International Exhibition prize award for work relating to balances.[21] Ansell also noticed a loss of gold during the manufacturing process. He found that 15 to 20 ounces could be recovered from the sweep, that is, the leftover burnt rubbish from the minting process, which was often left in open boxes for many months before being removed. Wanting to account for every particle and knowing that it was physically impossible for gold just to disappear, he put down the lost weight to a combination of oil, dust, and different types of foreign matter amongst the gold. In 1859, the Royal Mint rejected a batch of gold found to be too brittle for the minting of gold sovereigns. Analysis revealed the presence of small amounts of antimony, arsenic and lead. With Ansell's background in chemistry, he persuaded the Royal Mint to allow him to experiment with the alloy, and was ultimately able to produce 167,539 gold sovereigns.[22] On a second occasion in 1868, it was again discovered that gold coins, this time totalling £500,000 worth, were being produced with inferior gold. Although the standard practice at the mint was for rejected coins (known as brockages) to be melted down, many entered general circulation, and the mint was forced to return thousands of ounces of gold to the Bank of England. Although Ansell offered to re-melt the substandard coins, his offer was rejected, causing a row between him and senior mint chiefs, which ultimately led to him being removed from his position at the mint.[23] Gold Melting Process (1870) After the high-level practice as deputy engraver in the Royal Mint, Charles Wiener went then to Lisbon in 1864 as chief engraver to the Mint of Portugal.[24] In 1863 he made a commemorative medal for Prince Albert (1819-1861), consort of Queen Victoria. (Victoria and Albert Museum).[25] After relocating to its new home on Tower Hill, the Mint came under increased scrutiny of how it dealt with unrefined gold that had entered the country. The Master of the Mint had been responsible for overseeing the practice since the position's inception in the 14th century. However, the refinery process proved too costly and suffered from a lack of accountability from the master. A Royal Commission was set up in 1848 to address these issues; it recommended that the refinery process be outsourced to an external agency, thereby removing the refining process from the mint's responsibilities. The opportunity to oversee the Mint's refinery was taken up by Anthony de Rothschild, a descendant of the Rothschild family and heir to the multinationalinvestment banking company N M Rothschild & Sons. Rothschild secured a lease from the government in January 1852, purchasing equipment and premises adjacent to the Royal Mint on 19 Royal Mint Street under the name of Royal Mint Refinery. As Britain's influence as a world power expanded, with colonies being established abroad, a greater need for currency led to the Royal Mint opening satellite branches overseas. This need first arose in the then-Colony of New South Wales, as the black-market trade in gold during and following the 1851 Australian gold rush threatened to undermine the colony's economy.[26] In 1851 the colony's Legislative Council sent Queen Victoria a petition seeking a local mint for Sydney, and in 1853 the Queen issued an Order in Council providing for the establishment of the Sydney Branch of the Royal Mint.[27] The Royal Mint's Superintendent of Coining travelled to Australia to oversee its establishment on Macquarie Street within the southern wing of Sydney Hospital, where it opened in 1855. Production increased quickly: assayer's notes from 29 October 1855 indicate that the mint's Bullion Office had purchased 14,000 troy ounces (440 kg) of unrefined gold in the preceding week alone,[26] and the mint's overall coin output averaged over £1,000,000 yearly in its first five years of operation.[27] In 1868, gold sovereigns minted in Sydney were made legal tender in all British colonies, and in February 1886 they were given equal status in the UK itself.[27] The success of the Sydney branch led to the opening of similar branches in Melbourne and Perth, on 2 June 1872 and 20 June 1899 respectively. Following the Federation of Australia in 1901 and the establishment of a separate Australian pound in 1910, all three branches were used by the Commonwealth government to mint circulating coins for Australia.[28] The Melbourne and Perth branches had capabilities superior to those in Sydney, and they took over production responsibilities for Australia when the Sydney branch closed after 72 years of operation at the end of 1926.[26][27] Following the establishment of the Royal Australian Mint as a central mint for Australian coinage, the Melbourne and Perth mints were divested by the Royal Mint on 1 July 1970.[29][30] A fifth branch of the Royal Mint was established in Mumbai (Bombay), India on 21 December 1917 as part of a wartime effort. It struck sovereigns from 15 August 1918 until 22 April 1919 but closed in May 1919.[32][33] A sixth and final overseas mint was established in the Union of South Africa in Pretoria on 1 January 1923, producing £83,114,575 worth of sovereigns in its lifetime. As South Africa began cutting ties with Britain, the mint closed on 30 June 1941 but was later reopened as the South African Mint.[34] In 1914, as war broke out in Europe, Chancellor of the ExchequerDavid Lloyd George instructed that gold coins be removed from circulation to help pay for the war effort. The government started to issue £1 and 10-shilling Treasury notes as replacements, paving the way for Britain to leave the gold standard in 1931. During World War II, the Mint was important in ensuring people were paid for their services with hard currency rather than banknotes. Under Operation Bernhard, the Nazis planned to collapse the British economy by flooding the country with forged notes, leading the Bank of England to stop issuing banknotes of £10 and above. To meet these demands, the Mint doubled its output so that by 1943 it was minting around 700 million coins a year despite the constant threat of being bombed. The Deputy Master of the Mint, John Craig, recognised the dangers to the Mint and introduced several measures to ensure the Mint could continue to operate in the event of a disaster. Craig added emergency water supplies, reinforced the Mint's basement to act as an air-raid shelter and even accepted employment of women for the first time.[40] For most of the war, the mint managed to escape most of the destruction of the Blitz, but in December 1940 three members of staff were killed in an air raid. Around the same time, an auxiliary mint was set up at Pinewood Studios, Buckinghamshire, which had been requisitioned for the war effort. Staff and machinery from Tower Hill were moved to the site, which started production in June 1941 and operated for the duration of the war.[41][42] Over the course of the war, the Royal Mint was hit on several occasions, and at one point was put out of commission for three weeks. As technology changed with the introduction of electricity and demand continued to grow, the rebuilding process continued so that by the 1960s, little of the original mint remained, apart from Smirke's 1809 building and its gatehouses at the front. On 1 March 1966, the government announced plans to decimalise the nation's currency,[43] thereby requiring the withdrawal and re-minting of many millions of new coins. At its current site on Tower Hill, the mint had suffered from lack of space for many years, and it would be inadequate to meet the anticipated high demand a recoinage would entail.[44] A possible move to a more suitable site had been discussed as far back as 1870 when Deputy Master of the Mint Charles Fremantle had recommended it in his first annual report. At the time, it had been suggested that the valuable land at Tower Hill could be sold to finance the purchase of land nearby Whitefriars, London and pay for a new mint building.[45] However, after many years of subsequent debate by parliament it had been decided that improvements could be made to the existing site at Tower Hill. With Decimal Day set for 1971, the government quickly decided where to establish the new mint. After moving to Wales, the mint struggled to be profitable as the Western world fell into a deep recession during the early 1970s. To combat a rising national debt, the mint was established as a trading fund on 1 April 1975, which required it to become self-financing. This measure proved successful, and the mint became more profitable through heavy exports. In April 1990, the mint became an Executive Agency;[50] however, by 2001, the mint had reported its first annual loss: a result attributed to only securing 5% of new Euro coin production rather than the projected 20%. Despite this, the mint began diversifying its product range by offering items outside its usual coin-related merchandise. Around this time, the mint was selling different types of jewellery, commemorative plates and figurines,[51] eventually creating its own Royal Mint Classics range of collectible goods. This part of the business proved popular in attracting new customers, but it suffered from poor product development. Its products included a hip flask with an embedded £2 coin, an Edinburgh Crystal clock combined with a millennium Crown, and a Wedgwood plate featuring Britannia.[52] In 2007, the Mint decided to resume its focus on coins, downsizing non-coin related business and discontinuing its Classics range.[53] The recommendation was met with outrage by unions and opposition parties in parliament, who called it the \"selling off the family silver\" and said it would result in job losses. In contrast, the chief executive of the mint, Andrew Stafford, welcomed the proposal, saying that it would lead to further growth and secure the future of the business.[57] On 31 December 2009, rather than being fully privatised, the mint ceased to be an executive agency and its assets were vested in a limited company, Royal Mint Ltd. The owner of the new company became The Royal Mint trading fund, which itself continued to be owned by HM Treasury. As its sole shareholder, the mint pays an annual dividend of £4 million to the Treasury, with the remaining profits being reinvested into the mint.[58] In 2015, Chancellor of the Exchequer George Osborne announced a £20 billion privatisation drive to raise funds, with the Royal Mint being up for sale alongside other institutions including the Met Office and Companies House.[55] In addition to securing the medal product contract, the Mint held a competition to design a series of commemorative 50p coins that would enter general circulation before the event. The Mint received over 30,000 entries, with a further 17,000 from a children's competition on the television programme Blue Peter. In all, a total of 29 designs featuring a sport were selected by the Mint; the youngest designer was just 9 years old.[62] A £2 coin commemorating London's handover to Rio de Janeiro was also released in 2012. In April 2014, the Mint announced plans for developing a visitor centre in Llantrisant where members of the public could go on a guided tour of the facility and learn about the mint's history. The development contract, estimated to be worth £7.7 million, was awarded to construction firm ISG and design consultant Mather & Co., who had previously designed the Norwegian Olympic Museum, as well as a handful of visitor attractions for sports clubs including Chelsea F.C., Manchester City F.C., FC Porto, and the Springboks.[63][64] To fund the development, a grant of £2.3 million was provided by the Welsh Government towards the attraction which aimed to attract 200,000 visitors a year to the area.[65] By May 2016, two years after its announcement, the attraction, designed by Rio Architects[66] and now named Royal Mint Experience, opened to the public at a final cost of £9 million. The visitor centre includes an interactive museum, a view of the factory floor, an education centre, and a press where visitors can strike their own souvenir 50 pence coin.[67] On display at the centre are more than 80,000 artefacts,[68] including Olympic medals, a pattern coin of Edward VIII, a Janvier reducing machine and a selection of trial plates. As the sole body responsible for minting legal tender coins in the United Kingdom under contract from HM Treasury, the mint produces all of the country's physical currency apart from banknotes which are printed by the Bank of England. On average, it produces two billion pound sterling coins struck for general circulation every year, with an estimated 28 billion pieces circulating altogether. Outside the UK, the mint provides services to over 60 countries, including New Zealand and many Caribbean nations, by producing national currencies or supplying ready-to-strike planchets.[71] In 2015, it was estimated that 2.4 billion coins were minted for overseas countries, exceeding domestic coinage production and providing over 60% of the mint's revenue from circulating currencies. The Mint also regularly produces commemorative coins for the collector's market, with a range of varying quality and made of different precious metals. Another important operation of the mint, which contributes half the mint's revenue, is the sale of bullion to investors and the general public in the form of bars and coins. Historically, the mint refined its own metal; but following the advice of an 1848 Royal Commission, the process was separated, with the independent Royal Mint Refinery being purchased and operated by Anthony de Rothschild in 1852. The Rothschild family continued the refinery's management until it was sold to Engelhard in 1967. A year later, the Royal Mint relocated to Wales and ceased its bullion bar interests, but the brand was revived in 2015. Bullion bars produced by the mint are stamped with the original Royal Mint Refinery emblem and come in a range of sizes.[74] Minting of bullion coins began in 1957 to meet a demand for authentic sovereign coins, which suffered from heavy counterfeiting. Coins were released almost every year alongside proof versions up to 1982 when production of uncirculated sovereigns was discontinued, though proof examples continued to be minted. In 1987, the mint started to produce a new type of bullion coin: the gold one-ounce Britannia coin with a face value of £100. A silver version valued at £2 was also released in 1997. Production of the previously discontinued uncirculated sovereigns and half sovereigns resumed in 2000. From 2014, a lunar coin series was minted annually in celebration of the Lunar New Year; and in 2016, a series featuring The Queen's Beasts began. On occasion, the mint produces medals for government departments and under private contracts for clients such as royal societies, colleges, and universities. Most notably, the mint has made OBE medals as well as many military honours, including the Defence Medal and the Conspicuous Gallantry Cross for the British Armed Forces.[80] For the 2012 Summer Olympics, the mint won a contract to produce 4,700 gold, silver and bronze medals for competitors.[81] Before 1851, the making of medals at the mint was at the discretion of engravers who could undertake the work independently and receive an additional wage. A royal patent issued in 1669 granted the mint the sole right to produce medals of any metal that bore a monarch's portrait. Engravers would use the facilities at the mint to make commemorative medals of their own design for sale. A key date in the mint's history of producing medals for the military is 1815, when the Battle of Waterloo marked the beginning of awarding military campaign medals. By 1874, the mint was responsible for making all bars and clasps for war medals in the country and was making campaign medals such as the New Zealand Medal, the Abyssinian War Medal, and the Ashantee Medal.[82] At the start of the First World War, military medals were manufactured by the Woolwich Arsenal and private contractors. However, in 1922, a new medal unit created by the mint became the sole manufacturer of all Royal and State medals and decorations in metal, except the Victoria Cross, which is made by Hancocks & Co.[82][83] Before 2010, all British military medals were made by the mint; however, they now must compete with other manufacturers.[citation needed] The Trial of the Pyx is a traditional procedure to test newly mintedcoins for conformity to required standards. The trials have been held since the 12th century, normally once per calendar year, and continue to the present day. The form of the ceremony has been essentially the same since 1282. They are trials in the full judicial sense, presided over by a judge with an expert jury of assayers. The trials have since 1871 taken place at the livery hall of the Worshipful Company of Goldsmiths, having previously taken place at the Palace of Westminster.[84] Given modern production methods, it is unlikely that coins would not conform, although this has been a problem in the past as it would have been tempting for the Master of the Mint to steal precious metals. The term \"pyx\" refers to the boxwood chest (in Greek πυξίς, pyxis) in which coins were placed for presentation to the jury. There is also a Pyx Chapel (or Pyx Chamber) in Westminster Abbey, which was once used for secure storage of the Pyx and related articles. Coins to be tested are drawn from the regular production of the Royal Mint. The Deputy Master of the Mint must, throughout the year, randomly select several thousand sample coins and place them aside for the trial. These must be a certain fixed proportion to the number of coins produced. For example, for every 5,000 bimetallic coins issued, one must be set aside, but for silver Maundy money, the proportion is one in 150. The trial today consists of an inquiry independent of the Royal Mint.[85] The jury is composed of freemen of the Company of Goldsmiths, who assay the coins provided to decide whether they have been minted within the criteria determined by the relevant Coinage Acts.[86] Argent on a cross between in chief two fleur-de-lys Azure and in base two towers embattled of four merlons Sable a bezant between in pale two stags heads erased Or langued Gules and in fess two bulls heads erased Argent armed Or and langued Gules, on a chief Gules between two crosses crosslet fitchy Argent a lion passant guardant Or armed and langued Azure."}
{"url": "https://en.wikipedia.org/wiki/Aoraki_/_Mount_Cook", "text": "Aoraki / Mount Cook[a] is the highest mountain in New Zealand. Its height, as of 2014[update], is listed as 3,724 metres (12,218 feet).[2] It sits in the Southern Alps, the mountain range that runs the length of the South Island. A popular tourist destination,[3] it is also a favourite challenge for mountain climbers. Aoraki / Mount Cook consists of three summits: from south to north, the Low Peak (3,593 m or 11,788 ft), the Middle Peak (3,717 m or 12,195 ft) and the High Peak. The summits lie slightly south and east of the main divide of the Southern Alps, with the Tasman Glacier to the east and the Hooker Glacier to the southwest.[1] Mount Cook is ranked 10th in the world by topographic isolation. The peak is located at the northern end of the Mount Cook Range, where it meets with the main spine of the Main Divide, forming a massif between the Hooker Valley to the southwest and the Tasman Valley east of the mountain. These two valleys provide the closest easily accessible view points of Aoraki / Mount Cook. A lookout point at the end of the Hooker Valley Track located only 10 km from the peak has views of the entire mountainside.[4][5] The settlement of Mount Cook Village, also referred to as \"Aoraki / Mount Cook\", is a tourist centre and base camp for the mountain. It is 7 km from the end of the Tasman Glacier and 15 km south of Aoraki / Mount Cook's summit.[6] On clear days, Aoraki / Mount Cook is visible from the West Coast as far north as Greymouth, some 150 km (93 mi) away, and from most of State Highway 80 along Lake Pukaki and State Highway 6 south of Lake Pukaki. The near horizontal ridge connecting the mountain's three summits forms a distinctive blocky shape when viewed from an eastern or western direction. Another popular view point is from Lake Matheson on the West Coast, described as the \"view of views\", where on calm days, the peaks of Aoraki / Mount Cook and Mt Tasman are reflected in Lake Matheson.[7] Annual precipitation around the mountain ranges varies greatly as the local climate is dominated by the eastward movement of depressions and anticyclones from across the Tasman Sea. The Aoraki / Mount Cook massif is a major obstacle to the prevailing westerly winds as they push depressions and associated cold fronts of moist air from the subtropics in the northwest against the mountain range. As the air rises towards the peaks, it expands and cools, and forms clouds. Rain and snowfall are often heaviest around the 1,200 m (3,900 ft) level and can last for several days if the front is slow-moving.[8] As a result of the local weather patterns, the western slopes of Aoraki / Mount Cook can receive well over 10,000 mm (394 in)[8] of annual precipitation, whereas the nearby Mount Cook Village, only 15 km (9 mi) south of the mountain, receives 4,484 mm (176.5 in) of rain or snowfall.[9] While the weather on the eastern side of the mountain is generally better, rain or snow can quickly become widespread on that side as well if the wind turns to the south or southeast. This brings with it a rapid drop in temperature and poor visibility,[8] adding to the difficult climbing conditions on Aoraki / Mount Cook.[10] Temperatures at the mountain's base in the Hooker Valley around 800 metres (2,600 feet) range from −13 °C (9 °F) to 32 °C (90 °F), and generally fall just over 1 °C for every 200 metres of altitude.[11] From about 1,000 m (3,300 ft) and higher, semi-permanent snow and ice fields exist during winter. Winter and spring are usually less settled than summer and autumn. Anticyclones often bring days of settled weather in summer, or clear cold conditions in winter with severe frost.[12] Aoraki / Mount Cook seen from the south, taken from 4,000 metres (13,123 ft) In the traditions of the Ngāi Tahu iwi an early name for the South Island is Te Waka o Aoraki ('Aoraki's Canoe'). In the past many believed it meant \"Cloud Piercer\",[13] a romantic rendering of the name's components: ao (world, daytime, cloud, etc.) and raki or rangi (day, sky, weather, etc.).[14] Historically, the Māori name has been spelt Aorangi, using the northern dialect. Aoraki / Mount Cook became known to Māori after their arrival in New Zealand some time around the 14th century CE. The first Europeans who may have seen Aoraki / Mount Cook were members of Abel Tasman's crew, who saw a \"large land uplifted high\" (probably some part of the Southern Alps) while off the west coast of the South Island, just north of present-day Greymouth[15][16] on 13 December 1642 during Tasman's first Pacific voyage. The English name of Mount Cook was given to the mountain in 1851 by Captain John Lort Stokes to honour Captain James Cook who surveyed and circumnavigated the islands of New Zealand in 1770. Captain Cook did not sight the mountain during his exploration.[17] Following the settlement between Ngāi Tahu and the Crown in 1998, a number of South Island place names were amended to incorporate their Māori names by the Ngāi Tahu Claims Settlement Act 1998. The name of the mountain was officially changed from Mount Cook to Aoraki/Mount Cook to incorporate its historic Māori name.[18] It is the only one of these names where the Māori name precedes the English. Under the settlement the Crown agreed to return title of Aoraki / Mount Cook to Ngāi Tahu, who would then formally gift it back to the nation.[18] Neither transfer has yet occurred, and Ngāi Tahu can decide when this will happen.[19] The Southern Alps in the South Island were formed by tectonic uplifting and pressure as the Pacific and Indo-Australian Plates collided along the island's western coast. The uplifting continues, raising Aoraki / Mount Cook an average of 7 millimetres (0.28 in) each year. However, erosive forces are also powerful shapers of the mountains. The severe weather is due to the mountain's jutting into powerful westerly winds of the Roaring Forties which run around approximately 45°S latitude, south of both Africa and Australia. The Southern Alps are the first obstacle the winds encounter after South America, having moved east across the Southern Ocean. The height of Aoraki / Mount Cook was established in 1881 by G. J. Roberts (from the west side) and in 1889 by T. N. Brodrick (from the Canterbury side). Their measurements agreed closely at 12,349 feet (3,764 m). The height was reduced by 10 metres (33 ft) when approximately 12–14 million cubic metres of rock and ice fell off the northern peak on 14 December 1991.[20][21] Two decades of erosion of the ice cap exposed after this collapse reduced the height by another 30 m to 3,724 m, as revealed by new GPS data from a University of Otago climbing expedition in November 2013.[22][23] Aoraki / Mount Cook lies in the centre of the distinctive Alpine Fault, a 650 km long active fault in the Southern Alps. It is responsible for the uplift of Aoraki / Mount Cook and is believed to move every 100 to 300 years. It last moved in 1717.[24] The average annual rainfall in the surrounding lowlands, in particular to the west, is around 5 to 10 metres (200 to 390 in).[8] This very high rainfall leads to temperate rainforests in these coastal lowlands and a reliable source of snow in the mountains to keep the glaciers flowing. These include the Tasman Glacier to the east of the mountain and the smaller Hooker Glacier immediately to its south. The vegetation in the valleys to the east, in particular the Tasman Valley, is noticeably less lush than that on the western slopes of the mountain. Forest would normally grow to about 1,300 m in this area, but a lack of soil due to scree, rock falls and the effects of glaciation prevent this in most localities around the mountain. Snow tussock and other alpine plants cling to as high as 1,900 m. Above the snowline, only lichen can be found amongst the rock, snowfields and ice that dominate the highest parts of Aoraki / Mount Cook.[25] View of Aoraki / Mount Cook from the Tasman Lake south of the mountain The first recorded attempt on the summit was made by the Irishman Rev. William S. Green, the Swiss hotelier Emil Boss and the Swiss mountain guide Ulrich Kaufmann on 2 March 1882 via the Tasman and Linda Glaciers.[26] They came within a few feet of the top, as did the 1890 ascent attempt by Mannering and Dixon.[27] On Mt. Cook, New Zealand. 1977 The first known ascent was on 25 December 1894, when New Zealanders Tom Fyfe, John Michael (Jack) Clarke and George Graham reached the summit via the Hooker Valley and the north ridge.[28] Despite an earlier failed attempt on 20 December, the local climbers were spurred on by their desire for the first ascent to be made by New Zealand mountaineers amid reports that the American mountaineer Edward FitzGerald had his eye on the summit.[29] The party reached the summit at approximately 1:30pm after bounding up the last leg of the mountain full of excitement at reaching the top.[30] The route they had successfully traversed was not repeated again until the 100th ascent over 60 years later in 1955.[29] Swiss guide Matthias Zurbriggen of FitzGerald's party made the second ascent on 14 March 1895 from the Tasman Glacier side, via the ridge that now bears his name. This is credited as the first solo ascent, although Zurbriggen was accompanied part of the way up the ridge by J Adamson. After Zurbriggen's ascent it was another ten years before the mountain was climbed again. In February 1905 Jack Clarke with four others completed the third ascent following Zurbriggen's route. Clarke therefore became the first person to do a repeat ascent.[31] The first woman to ascend the mountain was Australian Freda Du Faur on 3 December 1910. Local guide George Bannister, a nephew of another guide, Butler Te Koeti of Ngāi Tahu,[32] was the first Māori to successfully scale the peak in 1912.[33] A traverse of the three peaks was first accomplished in 1913 by Freda Du Faur and guides Alec and Peter Graham. This 'grand traverse' was repeated in January 1916 by Conrad Kain, guiding the 57-year-old Jane Thomson, considered at the time \"a marvellous feat unequalled for daring in the annals of the Southern Alps\".[34] Sir Edmund Hillary made his first ascent in January 1948. In February 1948 with Ruth Adams, Harry Ayres and Mick Sullivan, Hillary made the first ascent of the South Ridge to the Low Peak.[35] To celebrate the life of Hillary the South Ridge was renamed as Hillary Ridge in August 2011.[36] Aoraki / Mount Cook is a technically challenging mountain with a high level of glaciation. Its level of difficulty is often underestimated and can change dramatically depending on weather, snow and ice conditions. The climb crosses large crevasses, and involves risks of ice and rock falls, avalanches and rapidly changing weather conditions.[10] Since the early 20th century, around 80 people have died attempting to climb the mountain,[37] making it New Zealand's deadliest peak. The climbing season traditionally runs from November to February, and hardly a season goes by without at least one fatality.[10] According to Māori legend, Aoraki was a young boy who, along with his three brothers, were the sons of Rakinui, the Sky Father. On their voyage around the Papatūānuku, the Earth Mother, their canoe became stranded on a reef and tilted. Aoraki and his brothers climbed onto the top side of their canoe. However, the south wind froze them and turned them to stone. Their canoe became the Te Waka o Aoraki, the South Island, and their prows, the Marlborough Sounds. Aoraki, the tallest, became the highest peak, and his brothers created the Kā Tiritiri o te Moana, the Southern Alps.[38][39][40] Ngāi Tahu, the main iwi (tribe) of New Zealand's southern region, consider Aoraki as the most sacred of the ancestors that they had descended from. Aoraki brings the iwi with its sense of community and purpose, and remains the physical form of Aoraki and the link between the worlds of the supernatural and nature. 1982 – Geoff Wyatt and John Blennehasset achieved the first ski descent from the summit. 1991 – An avalanche of 10 million cubic metres of snow and rock caused 10 metres to be lost off the top of Mount Cook.[20] Two decades of erosion of the ice cap exposed after this collapse reduced the height by another 30 m to 3,724 m, as revealed by new GPS data from a University of Otago climbing expedition in November 2013.[42][43] ^The official name, as set by the Ngāi Tahu Claims Settlement Act 1998 is Aoraki/Mount Cook. Both names, Aoraki and Mount Cook are commonly used, and the mountain is also called Aoraki Mount Cook, with punctuation between the names varying or omitted entirely in non-official use. ^ abThe landslide carried with it another 40 million cubic metres of rock and ice. The impact caused an earth quake of 3.9 on the Richter scale. [P207 In search Of Ancient NZ.Campball and Hutching.GNS science/Penguin.2011.] Michael J. Crozier. \"Mt Cook landslide\". Te Ara — the Encyclopedia of New Zealand. Retrieved 3 May 2007."}
{"url": "https://en.wikipedia.org/wiki/Agriculture_in_New_Zealand", "text": "A bullock wagon team taking wool from a farm station. The number of sheep in New Zealand peaked in the 1980s and has reduced due to lower profits from wool, and larger breeds of sheep for meat In New Zealand, agriculture is the largest sector of the tradable economy. The country exported NZ$46.4 billion worth of agricultural products (raw and manufactured) in the 12 months to June 2019, 79.6% of the country's total exported goods.[1] The agriculture, forestry and fisheries sector directly contributed $12.653 billion (or 5.1%) of the national GDP in the 12 months to September 2020,[2] and employed 143,000 people, 5.9% of New Zealand's workforce, as of the 2018 census.[3] New Zealand is unique in being the only developed country to be totally exposed to the international markets since subsidies, tax concessions and price supports for the agricultural sector were removed in the 1980s.[4] However, as of 2017, the New Zealand Government still provides state investment in infrastructure which supports agriculture.[5] Following their settlement of New Zealand in the 13th century, the Māori people developed economic systems involving hunting, foraging, and agriculture.[7] The Māori people valued land and especially horticulture, with many and various traditional Māori proverbs and legends emphasise the importance of gardening.[8] European and American explorers, missionaries and settlers introduced new animals and plants from 1769, and mass European settlement and land transfer led in the second half of the 19th century to an agricultural system featuring large Australian-style pastoral runs raising sheep. Immigrant land-hunger, innovations in refrigeration in the 1880s and the rise of dairying fostered the land reforms of John McKenzie in the 1890s, permitting an agricultural landscape of smaller family-based farms which became New Zealand's 20th-century agricultural norm (the oft-repeated cliché trumpets that agriculture/farming/farmers constitute \"the backbone of the [New Zealand] economy\") - challenged only in recent years by the growth in large-scale commercial industrial agriculture[9] and in lifestyle blocks.[10] The government offered a number of subsidies during the 1970s to assist farmers after the United Kingdom joined the European Economic Community[12] and by the early 1980s government support provided some farmers with 40 percent of their income.[13] In 1984 the Labour government ended all farm subsidies under Rogernomics,[14] and by 1990 the agricultural industry became the most deregulated sector in New Zealand.[15] To stay competitive in the heavily subsidised European and US markets New Zealand farmers had to increase the efficiency of their operations.[16][17] There were 6.26 million dairy cattle in New Zealand as of June 2019.[19] For the 2019–20 season, 4.92 million cows were milked in 11,179 herds, producing 21.1 billion litres (4.6×109 imp gal; 5.6×109 US gal) of raw milk containing 1.9 million tonnes of milk solids (protein and milkfat).[20][21] Dairy farms covered an effective area of 17,304 km2 (6,681 sq mi), around 6.46% of New Zealand's total land area.[21] The dairy cattle farming industry employed 39,264 people as of the 2018 census, 1.6% of New Zealand's workforce, making it the country's tenth-largest employment industry.[3] Around 56% of dairy farms in New Zealand are owner-operated as of 2015, while 29% are operated by sharemilkers and 14% are operated by contract milkers.[21] Herd-owning sharemilkers (formerly 50:50 sharemilkers) own their own herd, and are responsible for employing workers and the day-to-day operations of the farm, in return for receiving a percentage (typically 50%) of the milk income. Variable order sharemilkers do not own their own herd, and receive a lower percentage (typically 20-30%) of the milk income, while contract milkers are paid a fixed price per unit of milk.[22] Dairy farming in New Zealand is primarily pasture-based. Dairy cattle primarily feed on grass, supplemented by silage, hay and other crops during winter and other times of slow pasture growth.[23] The dairy farming year in New Zealand typically runs from 1 June to 31 May. The first day of the new year, known as \"Moving Day\" or \"Gypsy Day\", sees a large-scale migration as sharemilkers and contract milkers take up new contracts and move herds and equipment between farms.[24][25] Calving typically takes place in late winter (July and August), and cows are milked for nine months before being dried off in late autumn (April and May).[26][23] Some farms employ winter milking, either wholly or partly, with calving in late summer and early autumn (February and March).[23] Dairy farmers sell their milk to processors and are paid per kilogram of milk solids (kgMS). In the 2019–20 season, processors paid an average of $7.20 per kgMS (excluding GST), with the payout varying between $6.25 and $9.96 per kgMS depending on the processor.[21][27]Fonterra is the main processor of milk in New Zealand, processing 82 percent of all milk solids as of 2018. Other large dairy companies are Open Country Dairy (7.4%), Synlait and Westland Milk Products (3.4% each), Miraka (1.4%), Oceania Dairy (1.1%), and Tatua Co-operative Dairy Company (0.7%).[28] Only 3% of dairy production is consumed domestically, with the rest exported.[23] New Zealand is the world's largest exporter of whole milk powder and butter, and the third-largest exporter (behind the European Union and the United States) of skim milk powder and cheese.[29] There were 26.82 million sheep in New Zealand as of June 2019.[19] The sheep population peaked at 70.3 million sheep in 1982 and has steadily declined ever since.[30] In the 12 months to December 2020, 19.11 million lambs and 3.77 million adult sheep were processed, producing 362,250 tonnes of lamb and 97,300 tonnes of hogget and mutton.[31] 164,000 tonnes of clean wool was produced in 2006–7.[needs update] Around 95% of sheep meat and 90% of wool production is exported, with the rest consumed domestically.[23] In 2019, domestic consumption of lamb and mutton was 3.6 kg (7.9 lb) per capita.[32] There were 3.89 million beef cattle in New Zealand as of June 2019.[19] In the 12 months to December 2020, 1.59 million adult beef cattle and 1.15 million adult dairy cattle were processed, producing 698,380 tonnes of beef. In addition, 1.86 million calves and vealers were processed, producing 30,150 tonnes of veal.[31] Around 80% of beef and veal is exported, with the remaining 20% consumed domestically.[23] In 2019, domestic consumption of beef and veal was 11.6 kg (26 lb) per capita.[32] In the first half of the 20th century, pigs were often farmed alongside dairy cattle. Most dairy processors collected cream only, so dairy farmers separated the whole milk into cream and skim milk and fed the pigs the skim milk. In the 1950s and 60s, improved technology saw dairy processors switch to collecting whole milk. Pig farming subsequently became specialised and the majority of farms moved to grain-producing areas such as Canterbury.[33][26] There were 255,900 pigs in New Zealand in June 2019. Canterbury is by far the largest pig-farming region with 161,600 pigs, 63.1% of the national population.[19] In the 12 months to December 2020, 636,700 pigs were processed, producing 44,950 tonnes of meat.[31] In 2019, domestic consumption of pork, ham and bacon was 18.9 kg (42 lb) per capita.[32] Domestic production only meets around 45% of demand, with imported pork, ham and bacon, mainly from the European Union, North America and Australia, supplementing domestic supply. A small amount of meat is exported to supply nearby Pacific Island nations.[23] There were 3.87 million laying hens in New Zealand in June 2022, producing 1,100 million eggs annually.[35] Before the 1960s, chicken meat was largely a by-product of the egg industry; chickens for sale were generally cockerels or spent hens. The introduction of broiler chickens in the 1960s saw the meat industry grow from 8,000 tonnes per year in 1962 to over 40,000 tonnes in the mid-1980s.[36] In the late 1990s, chicken overtook beef as the most-consumed meat in New Zealand.[23] In the 12 months to December 2020, 118.7 million chickens were raised for meat, producing 217,200 tonnes of chicken meat.[37] Chickens account for over 98% of the country's poultry production, with turkeys and ducks accounting for the majority of the rest. Around 500,000 turkeys and 200,000 ducks are sold per year, with 90% of turkeys sold in the weeks preceding Christmas.[23] In 2019, domestic consumption of chicken and other poultry was 41.1 kg (91 lb) per capita.[32] Most of the poultry meat produced in New Zealand is consumed domestically. Due to biosecurity restrictions, importing poultry meat and eggs into New Zealand is prohibited.[23] Deer farming has increased dramatically from a herd of 150,000 in 1982 to 1.59 million in 2006, with 1,617 deer farms occupying 218,000 hectares of land in 2005.[38] $252 million of venison was exported in the year ending 30 September 2007. New Zealand is the largest exporter of farmed venison in the world.[39] In the 1970s and 80s there was a huge industry carrying out live deer recovery from forested areas of New Zealand. The deer are a pest animal that has a negative impact on the biodiversity of New Zealand. The deer-farm stock was bred from the recovered wild animals.[citation needed] Goats are also farmed for meat, milk, and mohair, and to control weeds.[39] New Zealand has around 125,200 hectares (309,000 acres) of horticultural land. Total horticultural exports in 2019 were valued at $6,200 million, of which $4,938 million (79.6%) come from three products: kiwifruit, wine, and apples.[40] Kiwifruit is primarily grown in the Bay of Plenty, especially around Te Puke, but is also grown in small quantities in the Northland, Auckland, Gisborne and Tasman regions.[42] The fruit is picked in the autumn (March to May) and kept in coolstore until sold or exported. The New Zealand kiwifruit season runs from April to December; during the off-season, kiwifruit is imported to fulfil domestic demand.[43] There are around 2,750 kiwifruit growers, producing 157.7 million trays (567,720 tonnes) in the year to June 2019. Around 545,800 tonnes of kiwifruit was exported in the same period worth $2,302 million, making kiwifruit New Zealand's largest horticultural export by value.[40] Apples are primarily grown in the Hawke's Bay and Tasman regions.[44] The two largest apple cultivars are Royal Gala and Braeburn, followed by Fuji, Scifresh (Jazz), Cripps Pink, Scired (Pacific Queen), and Scilate (Envy). All except Fuji and Cripps Pink were developed in New Zealand from cross-breeding or, in the case of Braeburn, a chance seedling.[40][45] Around 12% of apples are consumed domestically, 28% are processed domestically (mainly into juice), and 60% are exported.[44] Around 395,000 tonnes of apples, worth $829 million, were exported in the year to December 2019.[40] Avocados are primarily grown in the subtropical areas of Northland and Bay of Plenty. Around 60% of the crop is exported, with $104.3 million worth of avocadoes being exported in the year to December 2019.[46][40] Auckland (namely Pukekohe), Manawatū-Whanganui (namely Ohakune and the Horowhenua district), and Canterbury are the major growing regions for potatoes, onions, brassicas (e.g. cabbage, broccoli and cauliflower), leafy vegetables (e.g. lettuce, silverbeet and spinach), and carrots and parsnips. Southland also grows a significant proportion of potatoes and carrots, and the Matamata area in Waikato and Hawke's Bay also grow a significant proportion of onions. Squash is mainly grown in Gisborne and Hawke's Bay. Sweet corn is mainly grown in Gisborne, Hawke's Bay, Marlborough and Canterbury. Kūmara (sweet potato) is almost exclusively grown in Northland.[48][49] Due to their short shelf-life, most fresh vegetables are grown for domestic consumption and processing, with those exported mainly supplying nearby Pacific Island nations.[50] The largest vegetable exports are longer-life fresh vegetables such as onions and squash, along with processed vegetables such as french fries and potato chips, and frozen and canned peas, beans and sweet corn. In 2019, fresh vegetable exports totalled $304 million while processed vegetable exports totalled $396 million.[40] Seeds and flowers are primarily grown in Canterbury, Auckland, Otago and Southland. In 2019, New Zealand exported $90 million of seeds, $43 million of bulbs and live plants, and $20 million of cut flowers.[40] Almost all hay and silage is consumed on the same farm as it is produced. Most supplementary feed crops are grown in the South Island, where the colder climate forces additional feeding of stock during winter. Cereal crops occupies around 124,000 hectares (310,000 acres) of land as of June 2019. The largest crops by planted area are barley (55,500 ha), wheat (45,000 ha), maize (16,700 ha) and oats (2,100 ha).[51] The majority of wheat, barley and oats is grown in the South Island, namely the Canterbury, Southland and Otago regions. Canterbury alone grows approximately 80-90% of the country's wheat, 68% of its barley and 60% of its oats. In contrast, almost all of the country's maize is grown in the North Island.[52] Wheat, barley and oats are grown both for human consumption, malting, and for stock feed. Maize is grown as animal feed or for silage.[53] Milling of New Zealand's extensive native forests was one of the earliest industries in the settlement of the country. The long, straight hardwood from the kauri was ideal for ship masts and spars. As the new colony was established, timber was the most common building material, and vast areas of native forest were cleared. Rimu, tōtara, matai, and miro were the favoured timbers. The Monterrey Pine, Pinus radiata was introduced to New Zealand in the 1850s.[54] It thrived in the conditions, reaching maturity in 28 years, much faster than in its native California. It was found to grow well in the infertile acidic soil of the volcanic plateau, where attempts at agriculture had failed. The Government initiated planting of exotic forests in 1899 at Whakarewarewa, near Rotorua. This was to address growing timber shortages as slow-growing native forests were exhausted.[55] In the 1930s, vast areas of land were planted in Pinus radiata by relief workers. The largest tract was the 188,000-hectare Kāingaroa forest, the largest plantation forest in the world. As the major forests matured, processing industries such as the Kinleith Mill at Tokoroa and the Tasman Mill at Kawerau were established. Plantation forests of various sizes can now be found in all regions of New Zealand except Central Otago and Fiordland. In 2006 their total area was 1.8 million hectares, with 89% in Pinus radiata and 5% in Douglas fir (Pseudotsuga menziesii)[56] Log harvesting in 2006 was 18.8 million m3, down from 22.5 million m3 in 2003. This is projected to rise as high as 30 million m3 as newer forests mature. The value of all forestry exports (logs, chips, sawn timber, panels and paper products) for the year ended 31 March 2006 was $NZ 3.62 billion. This is projected to rise to $4.65 billion by 2011. Australia accounts for just over 25% of export value, mostly paper products, followed by Japan, South Korea, China and the United States.[56] Within the New Zealand economy, forestry accounts for approximately 4% of national GDP. On the global stage, the New Zealand forestry industry is a relatively small contributor in terms of production, accounting for 1% of global wood supply for industrial purposes.[57][needs update] Aquaculture started in New Zealand in the late 1960s and is dominated by mussels, oysters and salmon. In 2007, aquaculture generated about NZ$360 million in sales on an area of 7,700 hectares with a total of $240 million earned in exports. In 2006, the aquaculture industry in New Zealand developed a strategy aimed at achieving a sustainable annual billion NZ dollar business by 2025. In 2007, the government reacted by offering more support to the growing industry.[needs update] New Zealand had 2,602 beekeepers at the end of 2007, who owned 313,399 hives. Total honey production was 9700 tonnes. Pollen, beeswax, and propolis are also produced. Beekeepers provide pollination services to horticulturalists, which generates more income than the products of bee culture. Approximately 20–25,000 queen bees, and 20 tonnes of packaged bees (which include worker bees and a queen) are exported live each year.[58] Both the original Māori people and the European colonists made huge changes to New Zealand over a relatively short time. Māori burned forest to flush out game and to encourage the growth of bracken fern, which was used as a food source, and practised agriculture using plants they brought from tropical Polynesia. The Europeans logged and burned off a third of the forest cover to convert land to pastoral farming. In 1993, the National Institute of Water and Atmospheric Research summarised available data on the quality of water in rivers. They concluded that \"lowland river reaches in agriculturally developed catchments are in poor condition\" reflecting \"agriculturally derived diffuse and point source waste inputs in isolation or in addition to urban or industrial waste inputs\". The key contaminants identified in lowland rivers were dissolved inorganic nitrogen, dissolved reactive phosphorus and faecal contamination. Small streams in dairy farming areas were identified as being in very poor condition.[59] New Zealand's rivers and lakes are becoming increasingly nutrient enriched and degraded by nitrogen, animal faecal matter, and eroded sediment.[60] Many waterways are now unsafe for swimming. Fish and Game New Zealand launched a \"dirty dairying\" campaign to highlight the effect of intensive agriculture on waterways. Fonterra, the largest dairy company in New Zealand, in conjunction with government agencies responded with the Dairying and Clean Streams Accord. In 2009, the Crafar Farms group of dairy farms in the North Island became known as the 'poster boys for dirty dairying' after a string of prosecutions in the Environment Court for unlawful discharges of dairy effluent.[61][62] A number of plant and animal introductions into New Zealand has reduced the income from farming. Tight border controls to improve biosecurity have been put into place to ensure any new and unwanted pests and diseases do not enter the country. Monitoring is done around sea and airports to check for any incursions. The common brushtail possum was introduced from Australia to establish a fur trade. It soon became one of New Zealand's most problematic invasive species because of the huge effect on the biodiversity of New Zealand, as well affecting agricultural production, as it is a vector for bovine tuberculosis. The disease is now endemic in possums across approximately 38 per cent of New Zealand (known as 'vector risk areas'). In these areas, nearly 70 per cent of new herd infections can be traced back to possums or ferrets. The Biosecurity Act 1993, which established a National Pest Management Strategy, is the legislation behind control of the disease in New Zealand. The Animal Health Board (AHB) operates a nationwide programme of cattle testing and possum control with the goal of eradicating M. bovis from wild vector species across 2.5 million hectares – or one quarter – of New Zealand's at-risk areas by 2026 and, eventually, eradicating the disease entirely.[65] Possums are controlled through a combination of trapping, ground-baiting and, where other methods are impractical, aerial treatment with 1080 poison.[66] From 1979 to 1984, possum control was stopped due to lack of funding. In spite of regular and frequent TB testing of cattle herds, the number of infected herds snowballed and continued to increase until 1994.[67] The area of New Zealand where there were TB wild animals expanded from about 10 to 40 per cent. That possums are such effective transmitters of TB appears to be facilitated by their behaviour once they succumb to the disease. Terminally ill TB possums will show increasingly erratic behaviour, such as venturing out during the daytime to get enough food to eat, and seeking out buildings in which to keep warm. As a consequence they may wander onto paddocks, where they naturally attract the attention of inquisitive cattle and deer. This behaviour has been captured on video.[68] The introduced Canada goose became prolific and began to adversely affect pastures and crops. In 2011 restrictions on hunting them were dropped to allow them to be culled. Because of its geographical isolation New Zealand is free of some pest and diseases that are problematic for agricultural production in other countries. With a high level of international trade and large numbers of inbound tourists biosecurity is of great importance since any new pest or diseases brought into the country could potentially have a huge effect on the economy of New Zealand. There have been no outbreaks of foot-and-mouth disease in New Zealand. If an outbreak did occur there is potential for severe economic losses given that agricultural exports are a large segment of exports.[70] New Zealand has strict biosecurity[71] measures in place to prevent the introduction of unwanted pests and diseases. Many areas of the high country of the South Island were set up as large sheep and cattle stations in the late 19th century. Much of this land was leased from The Crown but after the passing of the Crown Pastoral Land Act 1998 the leases were reviewed. Environmentalists and academics raised concerns about the process saying that farmers were gaining an advantage and that conservation issues were not being resolved. Farmers were concerned that environmentalists and academics used the tenure review process to lock land up for conservation purposes without regard to the property rights of farmers or planning for how to manage that land in the future, and much land has been degraded by pests and weeds since it was retired from farming. Almost 180,000 hectares of farming land was purchased or leased by foreign interests between 2010 and 2021. The United States is the biggest nation owning land in New Zealand, China is second.[73][74] There is opposition to foreign ownership in New Zealand, The populist New Zealand First party is the largest party opposed to foreign ownership. In a 2011 Poll found that 82% believed foreign ownership of farms and agriculture land was a \"bad thing\". Only 10% believed it a \"good thing\" and 8% were unsure.[75] There are two main views on the immediate future of New Zealand agriculture. One is that, due to fast-rising consumer demand in India and China, the world is entering a golden age for commodities, and New Zealand is well placed to take advantage of this. The other view is that New Zealand will only gain limited rewards from this boom because of increasing production competition from developing countries. For New Zealand to remain competitive, farmers will either have to intensify production to remain commodity producers (increasing stock and fertiliser per hectare) or, instead, become producers of higher value, more customised products.[60] AgResearch Ltd (New Zealand's largest Crown Research Institute) believes that new technologies will allow New Zealand farmers to double their output by 2020, while simultaneously reducing greenhouse-gas emissions and other detrimental environmental impacts associated with farming practices.[60] Country Calendar is a factual television programme about farming methods and country life, and is watched by both rural and urban New Zealanders. The show first premièred on 6 March 1966, and is the country's longest-running locally-made television series.[76] The gumboot, a waterproof boot commonly used by farmers and others, is a cultural icon with Taihape hosting an annual Gumboot Day. Fred Dagg, a comedy character created by John Clarke, was a stereotypical farmer wearing a black singlet, shorts and gumboots. Number 8 wire is used for fencing and has become part of the cultural lexicon. It is used for all manner of tasks and it describes the do it yourself mentality of New Zealanders. A fixture in many rural towns, the annual Agricultural and Pastoral (A&P) show[77] organises competitions for the best livestock and farm produce. Carnivals, sideshows, equestrian events and craft competitions also take place in association with A&P shows. ^Firth, Raymond (2012) [1929]. \"The Maori and His Economic Resources\". Primitive Economics of the New Zealand Maori (reprint ed.). Abingdon: Routledge. p. 52. ISBN9781136505362. Retrieved 3 March 2023. [...] separating tribes according as to whether their main livelihood was gained by agriculture, fishing, or the utilization of forest products. [...] In general the tribes of Auckland and the North, the Bay of Plenty, Tauranga and the remainder of the East Coast, Taranaki, Nelson, and the Kaiapohia district practised agriculture. The people of the West Coast and the larger part of the South Island, as also Taupo and the Urewera, gained their living chiefly from forest products, augmented largely in the case of Whanganui by eels. Another specific source of food was worked by the Arawa tribes, with whom fresh-water fish, crayfish and kakahi (mussels) from the lakes formed a substantial part of the provision stocks. The fern root was available to all tribes, and constituted a staple food-product, while the coastal people, especially to the east, drew a large portion of their sustenance from fish. ^Kissun, Sudesh (25 September 2009). \"Things could have been done better: Crafar\". Rural News. Rural News Group Limited. Retrieved 28 May 2010. The Reporoa farmer, used by environmental groups as a poster boy for dirty dairying, is unhappy with some things happening on his farms"}
{"url": "https://en.wikipedia.org/wiki/M%C4%81ori_language", "text": "Prior to contact with Europeans, Māori lacked a written language or script.[a] Written Māori now uses the Latin script, which was adopted and the spelling standardised by high-ranking Northern Māori in collaboration with English Protestant clergy in the 19th century. The 2018 New Zealand census reported that about 186,000 people, or 4.0% of the New Zealand population, could hold a conversation in Māori about everyday things. As of 2015[update], 55% of Māori adults reported some knowledge of the language; of these, 64% use Māori at home and around 50,000 people can speak the language \"very well\" or \"well\".[13] Ideological support for the Māori language remains high among Māori and relatives so among other New Zealanders in general, with the number of second language students increasing by 76% between 2013 and 2023.[14][15] In Māori culture, the language is considered to be among the greatest of all taonga, or cultural treasures.[16][17] The English word Maori is a borrowing from the Māori language, where it is spelled Māori. In New Zealand, the Māori language is often referred to as te reo[tɛˈɾɛ.ɔ] (\"the language\"), short for te reo Māori (\"the Māori language\").[21] The Māori-language spelling ⟨Māori⟩ (with a macron) has become common in New Zealand English in recent years, particularly in Māori-specific cultural contexts,[21][22] although the traditional macron-less English spelling is still sometimes seen in general media and government use.[23] Most government departments and agencies have bilingual names—for example, the Department of Internal Affairs is alternatively Te Tari Taiwhenua—and places such as local government offices and public libraries display bilingual signs and use bilingual stationery; some government services now even use the Māori version solely as the official name.[29] Personal dealings with government agencies may be conducted in Māori, but in practice, this almost always requires interpreters, restricting its everyday use to the limited geographical areas of high Māori fluency, and to more formal occasions, such as during public consultation. An interpreter is on hand at sessions of the New Zealand Parliament for instances when a member wishes to speak in Māori.[22][30] Māori may be spoken in judicial proceedings, but any party wishing to do so must notify the court in advance to ensure an interpreter is available. Failure to notify in advance does not preclude the party speaking in Māori, but the court must be adjourned until an interpreter is available and the party may be held liable for the costs of the delay.[31] A 1994 ruling by the Judicial Committee of the Privy Council (then New Zealand's highest court) held the Government responsible under the Treaty of Waitangi (1840) for the preservation of the language.[32] Accordingly, since March 2004, the state has funded Māori Television, broadcast partly in Māori. On 28 March 2008, Māori Television launched its second channel, Te Reo, broadcast entirely in the Māori language, with no advertising or subtitles. The first Māori TV channel, Aotearoa Television Network (ATN) was available to viewers in the Auckland region from 1996 but lasted for only one year.[33] According to legend, Māori came to New Zealand from Hawaiki. Current anthropological thinking places their origin in eastern Polynesia, mostly likely from the Southern Cook or Society Islands region (see Māori history § Origins from Polynesia), and says that they arrived by deliberate voyages in seagoing canoes,[35] possibly double-hulled, and probably sail-rigged. These settlers probably arrived by AD 1350 at the latest.[36] Their language evolved in isolation from other Polynesian languages. Six dialectal variations emerged among iwi due to geographical separation.[37] The language had no written form, but historian Sarah J.K. Gallagher has argued that tā moko, the indigenous art of tattooing, is arguably \"a pre-European textual culture in New Zealand... as the Moko can be read, it can be accepted as a form of communication\".[38] The idea that tā moko is a written language of sorts has been discussed before.[39][40] Since its origin, the Māori language has been rich in metaphorical poetry and prose.[18][19] Forms of this include karakia, whaikōrero, whakapapa and karanga, and in performing arts such as mōteatea, waiata, and haka.[20] Karakia are Māori incantations used to invoke spiritual guidance and protection, and are used before eating or gathering, to increase spiritual goodwill and to declare things officially open.[41] Whaikōrero is the term given to traditional oratory given on marae, and whakapapa is the story of one's ancestry. Both of these incorporate Te Reo Māori. According to historian Atholl Anderson, whakapapa utilised \"mnemonic devices, repetitive patterns [and] rhyme\" in order to leave a lasting impression. \"Casting knowledge in formulaic or other standarised story forms.. helped to fix the information in the minds of speakers and listeners\".[42] Since about 1800, the Māori language has had a tumultuous history. It started this period as the predominant language of New Zealand, and was considered the country's lingua franca.[8][43] In the 1860s, it became a minority language in the shadow of the English spoken by many settlers, missionaries, gold-seekers, and traders. In the late 19th century, the colonial governments of New Zealand and its provinces introduced an English-style school system for all New Zealanders. From the mid-19th century, due to the Native Schools Act and later the Native Schools Code, the use of Māori in schools was slowly filtered out of the curriculum in order to become more European.[44] Increasing numbers of Māori people learned English.[37] Until the Second World War (1939–1945), most Māori people spoke Māori as their first language. Worship took place in Māori; it functioned as the language of Māori homes; Māori politicians conducted political meetings in Māori, and some literature appeared in Māori, along with many newspapers.[45] Before 1880, some Māori parliamentarians suffered disadvantages because parliamentary proceedings took place in English.[46] However, by 1900, all Māori members of parliament, such as Āpirana Ngata, were university graduates who spoke fluent English. From this period greater emphasis was placed on Māori learning English, but it was not until the migration of Māori to urban areas after the Second World War (the Urban Māori) that the number of speakers of Māori began to decline rapidly.[45][37] During this period, Māori was forbidden at many schools, and any use of the language was met with corporal punishment. In recent years, prominent Māori have spoken with sadness about their experiences or experiences of their family members being caned, strapped or beaten in school.[47][48][49] By the 1980s, fewer than 20 per cent of Māori spoke the language well enough to be classed as native speakers. Even many of those people no longer spoke Māori in their homes. As a result, many Māori children failed to learn their ancestral language, and generations of non-Māori-speaking Māori emerged.[50] By the 1950s some Māori leaders had begun to recognise the dangers of the loss of te reo Māori.[51] By the 1970s there were many strategies used to save the language.[51] This included Māori-language revitalization programs such as the Kōhanga Reo movement, which from 1982 immersed infants in Māori from infancy to school age.[52] There followed in 1985 the founding of the first Kura Kaupapa Māori (Years 1 to 8 Māori-medium education programme) and later the first Wharekura (Years 9 to 13 Māori-medium education programme). In 2011 it was reported that although \"there was a true revival of te reo in the 1980s and early to mid-1990s ... spurred on by the realisation of how few speakers were left, and by the relative abundance of older fluent speakers in both urban neighbourhoods and rural communities\", the language has continued to decline.\"[52] The decline is believed \"to have several underlying causes\".[53] These include: Based on the principles of partnership, Māori-speaking government, general revitalisation and dialectal protective policy, and adequate resourcing, the Waitangi Tribunal has recommended \"four fundamental changes\":[54] Te Taura Whiri (the Māori Language Commission) should become the lead Māori language sector agency. This will address the problems caused by the lack of ownership and leadership identified by the Office of the Auditor-General.[55] Te Taura Whiri should function as a Crown–Māori partnership through the equal appointment of Crown and Māori appointees to its board. This reflects [the Tribunal's] concern that te reo revival will not work if responsibility for setting the direction is not shared with Māori. Te Taura Whiri will also need increased powers. This will ensure that public bodies are compelled to contribute to te reo's revival and that key agencies are held properly accountable for the strategies they adopt. For instance, targets for the training of te reo teachers must be met, education curricula involving te reo must be approved, and public bodies in districts with a sufficient number and/or proportion of te reo speakers and schools with a certain proportion of Māori students must submit Māori language plans for approval. These regional public bodies and schools must also consult iwi (Māori tribes or tribal confederations) in the preparation of their plans. In this way, iwi will come to have a central role in the revitalisation of te reo in their own areas. This should encourage efforts to promote the language at the grassroots.[56] The changes set forth by the Tribunal are merely recommendations; they are not binding upon government.[57] There is, however, evidence that the revitalisation efforts are taking hold, as can be seen in the teaching of te reo in the school curriculum, the use of Māori as an instructional language, and the supportive ideologies surrounding these efforts.[58] In 2014, a survey of students ranging in age from 18 to 24 was conducted; the students were of mixed ethnic backgrounds, ranging from Pākehā to Māori who lived in New Zealand. This survey showed a 62% response saying that te reo Māori was at risk.[58] Albury argues that these results come from the language either not being used enough in common discourse, or from the fact that the number of speakers was inadequate for future language development.[58] The policies for language revitalisation have been changing in attempts to improve Māori language use and have been working with suggestions from the Waitangi Tribunal on the best ways to implement the revitalisation. The Waitangi Tribunal in 2011 identified a suggestion for language revitalisation that would shift indigenous policies from the central government to the preferences and ideologies of the Māori people.[57] This change recognises the issue of Māori revitalisation as one of indigenous self-determination, instead of the job of the government to identify what would be best for the language and Māori people of New Zealand.[59] Beginning in about 2015, the Māori language underwent a revival as it became increasingly popular, as a common national heritage and shared cultural identity, even among New Zealanders without Māori roots. Surveys from 2018 indicated that \"the Māori language currently enjoys a high status in Māori society and also positive acceptance by the majority of non-Māori New Zealanders\".[60][61] As the status and prestige of the language rose, so did the demand for language classes. Businesses, including Google, Microsoft, Vodafone NZ and Fletcher Building, were quick to adopt the trend as it became apparent that using te reo made customers think of a company as \"committed to New Zealand\". The language became increasingly heard in the media and in politics. Prime Minister Jacinda Ardern—who gave her daughter a Māori middle name, and said she would learn both Māori and English—made headlines when she toasted Commonwealth leaders in 2018 with a Māori proverb, and the success of Māori musical groups such as Alien Weaponry and Maimoa further increased the language's presence in social media.[60][61][62]Disney has dubbed its animated films in Māori since Moana (2017).[63] In August 2017, Rotorua became the first city in New Zealand to declare itself as bilingual in the Māori and English languages, meaning that both languages would be promoted. In 2019, the New Zealand government launched the Maihi Karauna Māori language revitalisation strategy with a goal of 1 million people speaking te reo Māori by 2040.[64][65] Also in 2019, Kotahi Rau Pukapuka Trust and Auckland University Press began work on publishing a sizeable library of local and international literature in the language, including the Harry Potter books.[66] Some New Zealanders have pushed against the revival, debating the replacement of English-language place names with original Māori names, criticising a Police car having Māori language and graphics, and complaining about te reo Māori being used by broadcasters.[61] In March 2021, the Broadcasting Standards Authority (BSA) said it would no longer entertain complaints regarding the use of the Māori language in broadcasts. This followed a fivefold increase in complaints to the BSA. The use of Māori in itself does not breach any broadcasting standards.[67] While the preceding are all distinct languages, they remain similar enough that Tupaia, a Tahitian travelling with Captain James Cook in 1769–1770, communicated effectively with Māori.[71] Māori actors, travelling to Easter Island for production of the film Rapa-Nui noticed a marked similarity between the native tongues, as did arts curator Reuben Friend, who noted that it took only a short time to pick up any different vocabulary and the different nuances to recognisable words.[72] Speakers of modern Māori generally report that they find the languages of the Cook Islands, including Rarotongan, the easiest amongst the other Polynesian languages to understand and converse in. Nearly all speakers are ethnic Māori resident in New Zealand. Estimates of the number of speakers vary: the 1996 census reported 160,000,[73] while other estimates have reported as few as 10,000 fluent adult speakers in 1995 according to the Māori Language Commission.[74] As reported in the 2013 national census, only 21.31 per cent of Māori (self-identified) had a conversational knowledge of the language, and only around 6.5 per cent of those speakers, 1.4 per cent of the total Māori population, spoke the Māori language only. This percentage has been in decline in recent years, from around a quarter of the population[when?] to 21 per cent. In the same census, Māori speakers were 3.7 per cent of the total population.[75] The level of competence of self-professed Māori speakers varies from minimal to total. Statistics have not been gathered for the prevalence of different levels of competence. Only a minority of self-professed speakers use Māori as their main language at home.[76] The rest use only a few words or phrases (passive bilingualism).[citation needed] Māori still[update] is a community language in some predominantly Māori settlements in the Northland, Urewera and East Cape areas. Kohanga reo Māori-immersion kindergartens throughout New Zealand use Māori exclusively. Increasing numbers of Māori raise their children bilingually.[76] Urbanisation after the Second World War led to widespread language shift from Māori predominance (with Māori the primary language of the rural whānau) to English predominance (English serving as the primary language in the Pākehā cities). Therefore, Māori speakers almost always communicate bilingually, with New Zealand English as either their first or second language. Only around 9,000 people speak only in Māori.[59] The use of the Māori language in the Māori diaspora is far lower than in New Zealand itself. Census data from Australia show it as the home language of 11,747, just 8.2% of the total Australian Māori population in 2016.[77] There was originally no native writing system for Māori. It has been suggested that the petroglyphs once used by the Māori developed into a script similar to the Rongorongo of Easter Island.[78] However, there is no evidence that these petroglyphs ever evolved into a true system of writing. Some distinctive markings among the kōwhaiwhai (rafter paintings) of meeting houses were used as mnemonics in reciting whakapapa (genealogy) but again, there was no systematic relation between marks and meanings. Attempts to write Māori words using the Latin script began with Captain James Cook and other early explorers, with varying degrees of success. Consonants seem to have caused the most difficulty, but medial and final vowels are often missing in early sources. Anne Salmond[79] records aghee for aki (in the year 1773, from the North Island East Coast, p. 98), Toogee and E tanga roak for Tuki and Tangaroa (1793, Northland, p. 216), Kokramea, Kakramea for Kakaramea (1801, Hauraki, p. 261), toges for tokis, Wannugu for Uenuku and gumera for kumara (1801, Hauraki, pp. 261, 266 and 269), Weygate for Waikato (1801, Hauraki, p. 277), Bunga Bunga for pungapunga, tubua for tupua and gure for kurī (1801, Hauraki, p. 279), as well as Tabooha for Te Puhi (1823, Northern Northland, p. 385). From 1814, missionaries tried to define the sounds of the language. Thomas Kendall published a book in 1815 entitled A korao no New Zealand, which in modern orthography and usage would be He Kōrero nō Aotearoa. Beginning in 1817, professor Samuel Lee of Cambridge University worked with the Ngāpuhi chief Tītore and his junior relative Tui (also known as Tuhi or Tupaea),[80] and then with chief Hongi Hika[81] and his junior relative Waikato; they established a definitive orthography based on Northern usage, published as the First Grammar and Vocabulary of the New Zealand Language (1820).[80] The missionaries of the Church Missionary Society (CMS) did not have a high regard for this book. By 1830 the CMS missionaries had revised the orthography for writing the Māori language; for example, 'Kiddeekiddee' was changed to the modern spelling, 'Kerikeri'.[82] This orthography continues to be used, with only two major changes: the addition of wh to distinguish the voiceless bilabial fricative phoneme from the labio-velar phoneme/w/; and the consistent marking of long vowels. The Māori embraced literacy enthusiastically, and missionaries reported in the 1820s that Māori all over the country taught each other to read and write, using sometimes quite innovative materials in the absence of paper, such as leaves and charcoal, and flax.[83] Missionary James West Stack recorded the scarcity of slates and writing materials at the native schools and the use sometimes of \"pieces of board on which sand was sprinkled, and the letters traced upon the sand with a pointed stick\".[84] Māori devised ways to mark vowel length, sporadically at first. Occasional and inconsistent vowel-length markings occur in 19th-century manuscripts and newspapers written by Māori, including macron-like diacritics and doubling of letters. Māori writer Hare Hongi (Henry Stowell) used macrons in his Maori-English Tutor and Vade Mecum of 1911,[85] as does Sir Āpirana Ngata (albeit inconsistently) in his Maori Grammar and Conversation (7th printing 1953). Once the Māori language was taught in universities in the 1960s, vowel-length marking was made systematic. Bruce Biggs, of Ngāti Maniapoto descent and professor at the University of Auckland, promoted the use of double vowels (e.g. waahine); this style was standard at the university until Biggs died in 2000. Macrons (tohutō) are now the standard means of indicating long vowels,[86] after becoming the favoured option of the Māori Language Commission—set up by the Māori Language Act 1987 to act as the authority for Māori spelling and orthography.[87][88] Most news media now use macrons; Stuff websites and newspapers since 2017,[89]TVNZ[90] and NZME websites and newspapers since 2018.[91] Technical limitations in producing macronised vowels on typewriters and older computer systems are sometimes resolved by using a diaeresis[92] or circumflex[93] instead of a macron (e.g., wähine or wâhine). Double vowels continue to be used in a few exceptional cases, including: Inland Revenue continues to spell its Māori name Te Tari Taake instead of Te Tari Tāke, mainly to reduce the resemblance of tāke to the English word 'take'.[97] A considerable number of governmental and non-governmental organisations continue to use the older spelling of ⟨roopu⟩ ('association') in their names rather than the more modern form ⟨rōpū⟩. Examples include Te Roopu Raranga Whatu o Aotearoa ('the national Māori weavers' collective') and Te Roopu Pounamu (a Māori-specific organisation within the Green Party of Aotearoa New Zealand). Although it is commonly claimed that vowel realisations (pronunciations) in Māori show little variation, linguistic research has shown this not to be the case.[99][b] Vowel length is phonemic, but four of the five long vowels occur in only a handful of word roots, the exception being /aː/.[100][c] As noted above, it has recently become standard in Māori spelling to indicate a long vowel with a macron. For older speakers, long vowels tend to be more peripheral and short vowels more centralised, especially with the low vowel, which is long [aː] but short [ɐ]. For younger speakers, they are both [a]. For older speakers, /u/ is only fronted after /t/; elsewhere it is [u]. For younger speakers, it is fronted [ʉ] everywhere, as with the corresponding phoneme in New Zealand English. Due to the influence of New Zealand English, the vowel [e] is raised to be near [i], so that pī and kē (or piki and kete) now largely share the very same vowel space.[101]: 198–199 Beside monophthongs Māori has many diphthong vowel phonemes. Although any short vowel combinations are possible, researchers disagree on which combinations constitute diphthongs.[102]Formant frequency analysis distinguish /aĭ/,/aĕ/,/aŏ/,/aŭ/,/oŭ/ as diphthongs.[103] As in many other Polynesian languages, diphthongs in Māori vary only slightly from sequences of adjacent vowels, except that they belong to the same syllable, and all or nearly all sequences of nonidentical vowels are possible. All sequences of nonidentical short vowels occur and are phonemically distinct.[104][105] The pronunciation of ⟨wh⟩ is extremely variable,[106] but its most common pronunciation (its canonical allophone) is the labiodental fricative, IPA [f] (as in the English word fill). Another allophone is the voiceless bilabial fricative, IPA [ɸ], which is usually supposed to be the sole pre-European pronunciation, although linguists are not sure of the truth of this supposition.[citation needed] At least until the 1930s, the bilabial fricative was considered to be the correct pronunciation.[107] The fact that English ⟨f⟩ gets substituted by ⟨p⟩ and not ⟨wh⟩ in borrowings (for example, English February becomes Pēpuere instead of Whēpuere) would strongly hint that the Māori did not perceive English /f/ to be the same sound as their ⟨wh⟩. Because English stops /p,t,k/ primarily have aspiration, speakers of English often hear the Māori nonaspirated stops as English /b,d,ɡ/. However, younger Māori speakers tend to aspirate /p,t,k/ as in English. English speakers also tend to hear Māori /r/ as English /l/ in certain positions (cf. Japanese r). These ways of hearing have given rise to place-name spellings which are incorrect in Māori, like Tolaga Bay. (Teraki in Māori). /ŋ/ can come at the beginning of a word (like 'sing-along' without the \"si\"), which may be difficult for English speakers outside of New Zealand to manage. In some western areas of the North Island, ⟨h⟩ is pronounced as a glottal stop [ʔ] instead of [h], and the digraph⟨wh⟩ is pronounced as [ʔw] instead of [f] or [ɸ]. /ɾ/ is typically a flap, especially before /a/. However, elsewhere it is sometimes trilled. In borrowings from English, many consonants are substituted by the nearest available Māori consonant. For example, the English affricates /tʃ/ and /dʒ/, and the fricative /s/ are replaced by /h/, /f/ becomes /p/, and /l/ becomes /ɾ/ (the /l/ is sometimes retained in the southern dialect, as noted below). Syllables in Māori have one of the following forms: V, VV, CV, CVV. This set of four can be summarised by the notation, (C)V(V), in which the segments in parentheses may or may not be present. A syllable cannot begin with two consonant sounds (the digraphsng and wh represent single consonant sounds), and cannot end in a consonant, although some speakers may occasionally devoice a final vowel. All possible CV combinations are grammatical, though wo, who, wu, and whu occur only in a few loanwords from English such as wuru, \"wool\" and whutuporo, \"football\".[108] As in many other Polynesian languages, e.g., Hawaiian, the rendering of loanwords from English includes representing every English consonant of the loanword (using the native consonant inventory; English has 24 consonants to 10 for Māori) and breaking up consonant clusters. For example, \"Presbyterian\" has been borrowed as Perehipeteriana; no consonant position in the loanword has been deleted, but /s/ and /b/ have been replaced with /h/ and /p/, respectively. Stress is typically within the last four vowels of a word, with long vowels and diphthongs counting double. That is, on the last four moras. However, stressed moras are longer than unstressed moras, so the word does not have the precision in Māori that it does in some other languages. It falls preferentially on the first long vowel, on the first diphthong if there is no long vowel (though for some speakers never a final diphthong), and on the first syllable otherwise. Compound words (such as names) may have a stressed syllable in each component word. In long sentences, the final syllable before a pause may have a stress in preference to the normal stressed syllable. Biggs proposed that historically there were two major dialect groups, North Island and South Island, and that South Island Māori is extinct.[110] Biggs has analysed North Island Māori as comprising a western group and an eastern group with the boundary between them running pretty much along the island's north–south axis.[111] Within these broad divisions regional variations occur, and individual regions show tribal variations. The major differences occur in the pronunciation of words, variation of vocabulary, and idiom. A fluent speaker of Māori has no problem understanding other dialects. There is no significant variation in grammar between dialects. \"Most of the tribal variation in grammar is a matter of preferences: speakers of one area might prefer one grammatical form to another, but are likely on occasion to use the non-preferred form, and at least to recognise and understand it.\"[112] Vocabulary and pronunciation vary to a greater extent, but this does not pose barriers to communication. In the southwest of the island, in the Whanganui and Taranaki regions, the phoneme ⟨h⟩ is a glottal stop and the phoneme ⟨wh⟩ is [ʔw]. This difference was the subject of considerable debate during the 1990s and 2000s over the then-proposed change of the name of the city Wanganui to Whanganui. Part of the annotation to a Ralph Hotere exhibition at the Dunedin Public Art Gallery, written bilingually in English and southern Māori. Note several regional variations, such as Nohoka (Nohoanga, a place or seat), tikaka (tikanga, customs), āhana/ōhona (ana / ōna, alienable and inalienable \"his\"), and whaka (whanga, harbour). In South Island dialects, ng merged with k in many regions. Thus Kāi Tahu and Ngāi Tahu are variations in the name of the same iwi (the latter form is the one used in acts of Parliament). Since 2000, the government has altered the official names of several southern place names to the southern dialect forms by replacing ng with k. New Zealand's highest mountain, known for centuries as Aoraki in southern Māori dialects that merge ng with k, and as Aorangi by other Māori, was later named \"Mount Cook\". Now its sole official name is Aoraki / Mount Cook, which favours the local dialect form. Similarly, the Māori name for Stewart Island, Rakiura, is cognate with the name of the Canterbury town of Rangiora. Likewise, Dunedin's main research library, the Hocken Collections, has the name Uare Taoka o Hākena rather than the northern (standard) Te Whare Taonga o Hākena.[d] Maarire Goodall and George Griffiths say there is also a voicing of k to g, which explains why the region of Otago (southern dialect) and the settlement it is named after – Otakou (standard Māori) – vary in spelling (the pronunciation of the latter having changed over time to accommodate the northern spelling).[113] The standard Māori r is also found occasionally changed to an l in these southern dialects and the wh to w. These changes are most commonly found in place names, such as Lake Waihola,[114] and the nearby coastal settlement of Wangaloa (which would, in standard Māori, be rendered Whangaroa), and Little Akaloa, on Banks Peninsula. Goodall and Griffiths suggest that final vowels are given a centralised pronunciation as schwa or that they are elided (pronounced indistinctly or not at all), resulting in such seemingly bastardised place names as The Kilmog, which in standard Māori would have been rendered Kirimoko, but which in southern dialect would have been pronounced very much as the current name suggests.[115] This same elision is found in numerous other southern placenames, such as the two small settlements called The Kaik (from the term for a fishing village, kainga in standard Māori), near Palmerston and Akaroa, and the early spelling of Lake Wakatipu as Wagadib. In standard Māori, Wakatipu would have been rendered Whakatipua, showing further the elision of a final vowel. Despite the dialect being officially regarded as extinct,[e] its use in signage and official documentation is encouraged by many government and educational agencies in Otago and Southland.[117][118] Māori has mostly a verb-subject-object (VSO) word order.[119] It is also analytical, featuring almost no inflection, and makes extensive use of grammatical particles to indicate grammatical categories of tense, mood, aspect, case, topicalization, among others. The personal pronouns have a distinction in clusivity, singular, dual and plural numbers,[120] and the genitive pronouns have different classes (a class, o class and neutral) according to whether the possession is alienable or the possessor has control of the relationship (a category), or the possession is inalienable or the possessor has no control over the relationship (o category), and a third neutral class that only occurs for singular pronouns and must be followed by a noun.[121] There is also subject-object-verb (SOV) word order used in passive sentences. Examples of this include Nāku te ngohi i tunu (\"I cooked the fish\"; literally I the fish cooked) and Mā wai te haka e kaea? (\"Who will lead the haka?\"). Biggs (1998) developed an analysis that the basic unit of Māori speech is the phrase rather than the word.[122] The lexical word forms the \"base\" of the phrase. Biggs identifies five types of bases. Noun bases include those bases that can take a definite article, but cannot occur as the nucleus of a verbal phrase; for example: ika (fish) or rākau (tree).[123] Plurality is marked by various means, including the definite article (singular te, plural ngā),[124] deictic particles tērā rākau (that tree), ērā rākau (those trees),[125] possessives taku whare (my house), aku whare (my houses).[126] A few nouns lengthen a vowel in the plural, such as wahine (woman); wāhine (women).[127] In general, bases used as qualifiers follow the base they qualify, e.g. \"matua wahine\" (mother, female elder) from \"matua\" (parent, elder) \"wahine\" (woman).[128] Universal bases are verbs which can be used passively. When used passively, these verbs take a passive form. Biggs gives three examples of universals in their passive form: inumia (drunk), tangihia (wept for), and kīa (said).[129] Stative bases serve as bases usable as verbs but not available for passive use, such as ora, alive or tika, correct.[129] Grammars generally refer to them as \"stative verbs\". When used in sentences, statives require different syntax than other verb-like bases.[130] Possessives fall into one of two classes of prepositions marked by a and o, depending on the dominant versus subordinate relationship between possessor and possessed: ngā tamariki a te matua, the children of the parent but te matua o ngā tamariki, the parent of the children.[137] The Māori definite articles are frequently used where the equivalent, the, is not used in English, such as when referring generically to an entire class. In these cases, the singular te can even be used with a morphologically plural noun, as in In other syntactic environments, the definite article may be used to introduce a noun-phrase which is pragmatically indefinite due to the restrictions on the use of he as discussed below.[139] The indefinite article he is used most frequently in the predicate and occasionally in the subject of the sentence, although it is not allowed in subject position in all sentence types.[140] In the predicate, the indefinite article he can introduce either nouns or adjectives.[141] The article either can be translated to the English 'a' or 'some', but the number will not be indicated by he. With nouns that show morphological number, he may be used either with singular or plural forms. The indefinite article he when used with mass nouns like water and sand will always mean 'some'.[142] he tāne a man some men he kōtiro a girl some girls he kāinga a village some villages he āporo an apple some apples he tangata a person – he tāngata – some people The indefinite article he is highly restricted in its use and is incompatible with a preceding preposition. For this reason, it cannot be used in the grammatical object of the sentence as these are marked prepositionally, either with i or ki. In many cases, speakers simply use the definite articles te and ngā in positions where he is disallowed, however the indefinite articles tētahi and ētahi may be used in these situations to emphasise the indefiniteness.[143] I PST kite see ahau 1S i ACC te DEF.SG kurī. dog I kite ahau i te kurī. PST see 1S ACC DEF.SG dog \"I saw the dog.\" (\"I saw a dog.\") I PST kite see ahau 1S i ACC tētahi INDEF.SG kurī. dog I kite ahau i tētahi kurī. PST see 1S ACC INDEF.SG dog \"I saw a dog.\" In positions where both he and tētahi/ētahi may occur, there are sometimes differences of meaning between them as the following examples indicate.[144] The proper article a is used before personal and locative nouns acting as the subject of the sentence or before personal nouns and pronouns within prepositional phrases headed by prepositions ending in i (namely i, ki, kei and hei).[143] Kei PRES.LOC hea where a ART Pita? Peter Kei hea aPita? PRES.LOC where ARTPeter \"Where is Peter?\" Kei PRES.LOC hea where ia? 3S Kei hea ia? PRES.LOC where 3S \"Where is he?\" Kei PRES.LOC Tāmaki Makaurau Auckland a ART Pita Peter Kei {Tāmaki Makaurau} aPita PRES.LOC Auckland ARTPeter \"Peter is in Auckland.\" Kei PRES.LOC Tāmaki Makaurau Auckland ia 3S Kei {Tāmaki Makaurau} ia PRES.LOC Auckland 3S \"He is in Auckland.\" I PST kite see ahau 1S i ACC a ART Pita Peter I kite ahau iaPita PST see 1S ACCARTPeter \"I saw Peter.\" I PST kite see ahau 1S i ACC a ART ia 3S I kite ahau iaia PST see 1S ACCART3S \"I saw him.\" The personal nouns are not accompanied by definite or indefinite articles unless they are an intrinsic part of the name, as in Te Rauparaha.[145] Kei PRES.LOC hea where a ART Te Te Rauparaha? Rauparaha Kei hea aTeRauparaha? PRES.LOC where ARTTeRauparaha \"Where is TeRauparaha?\" Kei PRES.LOC t-ō-ku DEF.SG-INAL-1s kāinga home a ART Te Te Rauparaha. Rauparaha Kei t-ō-ku kāinga aTeRauparaha. PRES.LOC DEF.SG-INAL-1s home ARTTeRauparaha \"Te Rauparaha is at my home.\" Proper nouns are not preceded by the proper article when they are neither acting as the subject of the sentence nor in a prepositional phrase headed by i, ki, kei or hei. For example, after the focusing particle ko, the proper article is not used. Demonstratives occur after the noun and have a deictic function, and include tēnei, this (near me), tēnā, that (near you), tērā, that (far from us both), and taua, the aforementioned (anaphoric). These demonstratives, having a connection to the definite article te are termed definitives. Other definitives include tēhea? (which?), and tētahi, (a certain). The plural is formed just by dropping the t: tēnei (this), ēnei (these). The related adverbs are nei (here), nā (there, near you), rā (over there, near him).[146] Phrases introduced by demonstratives can also be expressed using the definite article te or ngā preceding a noun followed by one of the deictic particles nei, nā or rā. The t of the singular definite article appears in the singular demonstratives but is replaced by ∅ in the plural, having no connection with ngā in the majority of dialects. However, in dialects of the Waikato area, plural forms of demonstratives beginning with ng- are found, such as ngēnei 'these' instead of the more widespread ēnei (as well as and possessives such as ng(e)ōku 'my (plural, inalienable)' instead of ōku).[148] The following table shows the most common forms of demonstratives across dialects. Like other Polynesian languages, Māori has three numbers for pronouns and possessives: singular, dual and plural. For example: ia (he/she), rāua (those two), rātou (they, three or more). Māori pronouns and possessives further distinguish exclusive \"we\" from inclusive \"we\", second and third. It has the plural pronouns: mātou (we, exc), tātou (we, inc), koutou (you), rātou (they). The language features the dual pronouns: māua (me and another), tāua (me and you), kōrua (you two), rāua (those two). The difference between exclusive and inclusive lies in the treatment of the person addressed. Mātou refers to the speaker and others but not the person or persons spoken to (\"I and some others but not you\"), and tātou refers to the speaker, the person or persons spoken to and everyone else (\"you, I and others\"):[149] The possessive pronouns vary according to person, number, clusivity, and possessive class (a class or o class). Example: tāku pene (my pen), āku pene (my pens). For dual and plural subject pronouns, the possessive form is analytical, by just putting the possessive particle (tā/tō for singular objects or ā/ō for plural objects) before the personal pronouns, e.g. tā tātou karaihe (our class), tō rāua whare (their [dual] house); ā tātou karaihe (our classes). The neuter one must be followed by a noun and only occur for singular first, second and third persons. Taku is my, aku is my (plural, for many possessed items). The plural is made by deleting the initial [t].[121] A phrase spoken in Māori can be broken up into two parts: the \"nucleus\" or \"head\" and \"periphery\" (modifiers, determiners). The nucleus can be thought of as the meaning and is the centre of the phrase, whereas the periphery is where the grammatical meaning is conveyed and occurs before and/or after the nucleus.[152] Periphery Nucleus Periphery te whare nei ki te whare The nucleus whare can be translated as \"house\", the periphery te is similar to an article \"the\" and the periphery nei indicates proximity to the speaker. The whole phrase, te whare nei, can then be translated as \"this house\".[153] A definite and declarative sentence (may be a copulative sentence) begins with the declarative particle ko.[154] If the sentence is topicalized (agent topic, only in non-present sentences) the sentence begins with the particle nā (past tense) or the particle mā (future, imperfective) followed by the agent/subject. In these cases the word order changes to subject-verb-object. These agent topicalizing particles can contract with singular personal pronouns and vary according to the possessive classes: nāku can be thought of as meaning \"as for me\" and behave like an emphatic or dative pronoun.[155] Forming negative phrases in Māori is quite grammatically complex. There are several different negators which are used under various specific circumstances.[160] The main negators are as follows:[160] Negator Description kāo Negative answer to a polar question. kāore/kāhore/kāre The most common verbal negator. kore A strong negator, equivalent to 'never'. kaua e Negative imperatives; prohibitive ehara Negation for copulative phrases, topicalized and equative phrases Kīhai and tē are two negators which may be seen in specific dialects or older texts, but are not widely used.[160] The most common negator is kāhore, which may occur in one of four forms, with the kāo form only being used in response to a question.[160] Negative phrases, besides using kāore, also affect the form of verbal particles, as illustrated below. The general usage of kāhore can be seen in the following examples. The subject is usually raised in negative phrases, although this is not obligatory.[161] Each example of a negative phrase is presented with its analogue positive phrase for comparison. The passive voice of verbs is made by a suffix to the verb. For example, -ia (or just -a if the verb ends in [i]). The other passive suffixes, some of which are very rare, are: -hanga/-hia/-hina/-ina/-kia/-kina/-mia/-na/-nga/-ngia/-ria/-rina/-tia/-whia/-whina/.[163] The use of the passive suffix -ia is given in this sentence: Kua hangaia te marae e ngā tohunga (The marae has been built by the experts). The active form of this sentence is rendered as: Kua hanga ngā tohunga i te marae (The experts have built the marae). It can be seen that the active sentence contains the object marker 'i', that is not present in the passive sentence, while the passive sentence has the agent marker 'e', which is not present in the active sentence.[164] New Zealand English has gained many loanwords from Māori, mainly the names of birds, plants, fishes and places. For example, the kiwi, the national bird, takes its name from te reo. \"Kia ora\" (literally \"be healthy\") is a widely adopted greeting of Māori origin, with the intended meaning of \"hello\".[171] It can also mean \"thank you\", or signify agreement with a speaker at a meeting. The Māori greetings tēnā koe (to one person), tēnā kōrua (to two people) or tēnā koutou (to three or more people) are also widely used, as are farewells such as haere rā. The Māori phrase kia kaha, \"be strong\", is frequently encountered as an indication of moral support for someone starting a stressful undertaking or otherwise in a difficult situation. Many other words such as whānau (meaning \"family\") and kai (meaning \"food\") are also widely understood and used by New Zealanders. The Māori phrase Ka kite anō means 'until I see you again' is quite commonly used. In 2023, 47 words or expressions from New Zealand English, mostly from te reo Māori were added to the Oxford English Dictionary.[172] ^The Hocken Library contains several early journals and notebooks of early missionaries documenting the vagaries of the southern dialect. Several of them are shown at Blackman, A. Some Sources for Southern Maori dialect. ^As with many \"dead\" languages, there is a possibility that the southern dialect may be revived, especially with the encouragement mentioned. \"The Murihiku language — Mulihig' being probably better expressive of its state in 1844 — lives on in Watkin's vocabulary list and in many muttonbirding terms still in use, and may flourish again in the new climate of Maoritaka.\"[116] ^ abRoy, Eleanor Ainge (28 July 2018). \"Google and Disney join rush to cash in as Māori goes mainstream\". The Guardian. Archived from the original on 28 July 2018. Retrieved 28 July 2018. John McCaffery, a language expert at the University of Auckland school of education, says the language is thriving, with other indigenous peoples travelling to New Zealand to learn how Māori has made such a striking comeback. 'It has been really dramatic, the past three years in particular, Māori has gone mainstream,' he said. ^Banks 1771, 9 October 1769: \"we again advanc'd to the river side with Tupia, who now found that the language of the people was so like his own that he could tolerably well understand them and they him.\". ^Goodall & Griffiths 1980, page 45: \"This hill [The Kilmog]...has a much debated name, but its origins are clear to Kaitahu and the word illustrates several major features of the southern dialect. First we must restore the truncated final vowel (in this case to both parts of the name, 'kilimogo'). Then substitute r for l, k for g, to obtain the northern pronunciation, 'kirimoko'.... Though final vowels existed in Kaitahu dialect, the elision was so nearly complete that pākehā recorders often omitted them entirely.\". ^Swarbrick, Nancy (5 September 2013). \"Manners and social behaviour\". teara.govt.nz. Te Ara: The Encyclopedia of New Zealand. Archived from the original on 22 February 2018. Retrieved 21 February 2018. Harlow, Ray (1994). \"Māori Dialectology and the Settlement of New Zealand\". In Sutton, Douglas G. (ed.). The Origins of the First New Zealanders. Auckland: Auckland University Press. pp. 106–122. ISBN1-86940-098-4."}
{"url": "https://www.google.com/finance/quote/KRW-NZD", "text": "The South Korean won is the official currency of South Korea. A single won is divided into 100 jeon, the monetary subunit. The jeon is no longer used for everyday transactions, and it appears only in foreign exchange rates. The currency is issued by the Bank of Korea, based in the capital city of Seoul. Wikipedia About New Zealand Dollar  The New Zealand dollar is the official currency and legal tender of New Zealand, the Cook Islands, Niue, the Ross Dependency, Tokelau, and a British territory, the Pitcairn Islands. Within New Zealand, it is almost always abbreviated with the dollar sign. The abbreviations \"$NZ\" or \"NZ$\" are used when necessary to distinguish it from other dollar-denominated currencies. The New Zealand dollar was introduced in 1967. It is subdivided into 100 cents. Altogether it has five coins and five banknotes with the smallest being the 10-cent coin; smaller denominations have been discontinued due to inflation and production costs. In the context of currency trading, the New Zealand dollar is sometimes informally called the \"Kiwi\" or \"Kiwi dollar\", since the flightless bird, the kiwi, is depicted on its one-dollar coin. It is the tenth most traded currency in the world, representing 2.1% of global foreign exchange market daily turnover in 2019. Wikipedia Discover more You may be interested in info This list is generated from recent searches, followed securities, and other activity. Learn more All data and information is provided “as is” for personal informational purposes only, and is not intended to be financial advice nor is it for trading purposes or investment, tax, legal, accounting or other advice. Google is not an investment adviser nor is it a financial adviser and expresses no view, recommendation or opinion with respect to any of the companies included in this list or any securities issued by those companies. Please consult your broker or financial representative to verify pricing before executing any trades. Learn more"}
{"url": "https://en.wikipedia.org/wiki/Acanthodian", "text": "The popular name \"spiny sharks\" is because they were superficially shark-shaped, with a streamlined body, paired fins, a strongly upturned tail, and stout, largely immovable bony spines supporting all the fins except the tail—hence, \"spiny sharks\". However, acanthodians are not true sharks; their close relation to modern cartilaginous fish can lead them to be considered \"stem-sharks\". Acanthodians had a cartilaginousskeleton, but their fins had a wide, bony base and were reinforced on their anterior margin with a dentine spine. As a result, fossilized spines and scales are often all that remains of these fishes in ancient sedimentary rocks. The earliest acanthodians were marine, but during the Devonian, freshwater species became predominant.[citation needed] Acanthodians have been divided into four orders: Acanthodiformes, Climatiiformes, Diplacanthiformes, and Ischnacanthiformes.[2] \"Climatiiformes\" is a paraphyletic assemblage of early acanthodians such as climatiids, gyracanthids, and diplacanthids; they had robust bony shoulder girdles and many small sharp spines (\"intermediate\" or \"prepelvic\" spines) between the pectoral and pelvic fins. The climatiiform subgroup Diplacanthida has subsequently been elevated to its own order, Diplacanthiformes. Ischnacanthiforms were predators with tooth plates fused to their jaws. Acanthodiforms were filter feeders with a single dorsal fin, toothless jaws, and long gill rakers. They were the last and most specialized off the traditional acanthodians, as they survived up until the Permian period.[citation needed] The scales of Acanthodii have distinctive ornamentation peculiar to each order. Because of this, the scales are often used in determining relative age of sedimentary rock. The scales are tiny, with a bulbous base, a neck, and a flat or slightly curved diamond-shaped crown. Despite being called \"spiny sharks\", acanthodians predate sharks. Scales that have been tentatively identified as belonging to acanthodians, or \"shark-like fishes\" have been found in various Ordovician strata, though, they are ambiguous, and may actually belong to jawless fishes such as thelodonts. The earliest unequivocal acanthodian fossils date from the beginning of the Silurian Period, some 50 million years before the first sharks appeared. Later, the acanthodians colonized fresh waters, and thrived in the rivers and lakes during the Devonian and in the coal swamps of Carboniferous. By this time bony fishes were already showing their potential to dominate the waters of the world, and their competition proved too much for the spiny sharks, which died out in Permian times (approximately 250 million years ago). In a study of early jawed vertebrate relationships, Davis et al. (2012) found acanthodians to be split among the two major clades Osteichthyes (bony fish) and Chondrichthyes (cartilaginous fish). The well-known acanthodian Acanthodes was placed within Osteichthyes, despite the presence of many chondrichthyan characteristics in its braincase.[3] However, a newly described Silurian placoderm, Entelognathus, which has jaw anatomy shared with bony fish and tetrapods, has led to revisions of this phylogeny: acanthodians were then considered to be a paraphyletic assemblage leading to cartilaginous fish, while bony fish evolved from placoderm ancestors.[4] Burrow et al. 2016 provides vindication by finding chondrichthyans to be nested among Acanthodii, most closely related to Doliodus and Tamiobatis.[2] A 2017 study of Doliodus morphology points out that it appears to display a mosaic of shark and acanthodian features, making it a transitional fossil and further reinforcing this idea.[5] The oldest remains attributed acanthodian-grade chondrichthyans are Fanjingshania and Qianodus from the Early Silurian of China, dating to around 439 million years ago.[7][8] Compared to other contemporary groups of fish, acanthodians were relatively morphologically and ecologically conservative. Acanthodians rose in diversity during the Late Silurian, reaching their apex of diversity during the Lochkovian stage of the Early Devonian, declining during the Pragian but rising again during the following Emsian, which was followed by a decline in diversity during middle-Late Devonian. The diversity of the group was consistently low but stable during the Carboniferous, slightly decreasing going into the Permian.[9] The youngest records of the group are isolated scales and fin spines from Middle-Late Permian strata in the Paraná Basin of Brazil.[10]"}
{"url": "https://en.wikipedia.org/wiki/Tetrapodomorph", "text": "Tetrapodomorpha (also known as Choanata[3]) is a clade of vertebrates consisting of tetrapods (four-limbed vertebrates) and their closest sarcopterygian relatives that are more closely related to living tetrapods than to living lungfish. Advanced forms transitional between fish and the early labyrinthodonts, such as Tiktaalik, have been referred to as \"fishapods\" by their discoverers, being half-fish, half-tetrapods, in appearance and limb morphology. The Tetrapodomorpha contains the crown group tetrapods (the last common ancestor of living tetrapods and all of its descendants) and several groups of early stem tetrapods, which includes several groups of related lobe-finned fishes, collectively known as the osteolepiforms. The Tetrapodomorpha minus the crown group Tetrapoda are the stem Tetrapoda, a paraphyletic unit encompassing the fish to tetrapod transition. Among the characteristics defining tetrapodomorphs are modifications to the fins, notably a humerus with convex head articulating with the glenoid fossa (the socket of the shoulder joint). Another key trait is the internal nostril or choana. Most fish have two pairs of nostrils, one on either side of the head for incoming water (incurrent nostrils) and another pair for outgoing water (excurrent nostrils). In early tetrapodomorphs like Kenichthys, the excurrent nostrils had shifted towards the mouth's perimeter. In later tetrapodomorphs, including tetrapods, the excurrent nostril is positioned inside the mouth, where it is known as the choana.[4] The nearly-equivalent clade Choanata often refers to these later forms specifically.[2]"}
{"url": "https://www.fda.gov/drugs/resources-drugs/warning-antibiotics-dont-work-viruses-colds-and-flu", "text": "Using them for viruses will not make you feel better or get back to work faster. Antibiotics don’t fight viruses – they fight bacteria. Using antibiotics for viruses can put you at risk of getting a bacterial infection that is resistant to antibiotic treatment. Talk to your doctor or other healthcare provider about antibiotics, visit www.cdc.gov/getsmart, or call 800-CDC-INFO (800-232-4636) to learn more. Taking antibiotics for viral infections such as a cold, a cough, or the flu will NOT:"}
{"url": "https://en.wikipedia.org/wiki/FishBase", "text": "FishBase is a global species database of fish species (specifically finfish).[1] It is the largest and most extensively accessed online database on adult finfish on the web.[2] Over time it has \"evolved into a dynamic and versatile ecological tool\" that is widely cited in scholarly publications.[3][4][5] The origins of FishBase go back to the 1970s, when the fisheries scientist Daniel Pauly found himself struggling to test a hypothesis on how the growing ability of fish was affected by the size of their gills.[8] Hypotheses, such as this one, could be tested only if large amounts of empirical data were available.[9] At the time, fisheries management used analytical models which required estimates for fish growth and mortality.[10] It can be difficult for fishery scientists and managers to get the information they need on the species that concern them, because the relevant facts can be scattered across and buried in numerous journal articles, reports, newsletters and other sources. It can be particularly difficult for people in developing countries who need such information. Pauly believed that the only practical way fisheries managers could access the volume of data they needed was to assemble and consolidate all the data available in the published literature into some central and easily accessed repository.[9][11] Such a database would be particularly useful if the data has also been standardised and validated.[9] This would mean that when scientists or managers need to test a new hypothesis, the available data will already be there in a validated and accessible form, and there will be no need to create a new dataset and then have to validate it.[12] Pauly recruited Rainer Froese, and the beginnings of a software database along these lines was encoded in 1988. This database, initially confined to tropical fish, became the prototype for FishBase. FishBase was subsequently extended to cover all finfish, and was launched on the Web in August 1996. It is now the largest and most accessed online database for fish in the world.[9] In 1995 the first CD-ROM was released as \"FishBase 100\". Subsequent CDs have been released annually. The software runs on Microsoft Access which operates only on Microsoft Windows. FishBase covers adult finfish, but does not detail the early and juvenile stages of fish. In 1999 a complementary database, called LarvalBase, went online under the supervision of Bernd Ueberschär. It covers ichthyoplankton and the juvenile stage of fishes, with detailed data on fish eggs and larvae, fish identification, as well as data relevant to the rearing of young fish in aquaculture. Given FishBase's success, there was a demand for a database covering forms of aquatic life other than finfish. This resulted, in 2006, in the birth of SeaLifeBase.[9] The long-term goal of SeaLifeBase is to develop an information system modelled on FishBase, but including all forms of aquatic life, both marine and freshwater, apart from the finfish which FishBase specialises in. Altogether, there are about 300,000 known species in this category.[13] As awareness of FishBase has grown among fish specialists, it has attracted over 2,480 contributors and collaborators. Since 2000 FishBase has been supervised by a consortium of nine international institutions. The FishBase consortium has grown to twelve members. The GEOMAR – Helmholtz Centre for Ocean Research for Ocean Research Kiel (GEOMAR) in Germany, functions as the coordinating body[14][15] and, since February 2017, Quantitative Aquatics, Inc. functions as the administrative body.[16]"}
{"url": "https://en.wikipedia.org/wiki/File:Scutosaurus_BW_flipped.jpg", "text": "Licensing Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled GNU Free Documentation License.http://www.gnu.org/copyleft/fdl.htmlGFDLGNU Free Documentation Licensetruetrue attribution – You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. share alike – If you remix, transform, or build upon the material, you must distribute your contributions under the same or compatible license as the original. This licensing tag was added to this file as part of the GFDL licensing update.http://creativecommons.org/licenses/by-sa/3.0/CC BY-SA 3.0Creative Commons Attribution-Share Alike 3.0truetrue attribution – You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use."}
{"url": "https://en.m.wikipedia.org/wiki/New_Zealand_dollar", "text": "The New Zealand dollar was introduced in 1967. It is subdivided into 100 cents. Altogether it has five coins and five banknotes with the smallest being the 10-cent coin; smaller denominations have been discontinued due to inflation and production costs. In the context of currency trading, the New Zealand dollar is sometimes informally called the \"Kiwi\" or \"Kiwi dollar\",[3] since the flightless bird, the kiwi, is depicted on its one-dollar coin. It is the tenth most traded currency in the world, representing 2.1% of global foreign exchange market daily turnover in 2019.[4] Prior to the introduction of the New Zealand dollar in 1967, the New Zealand pound was the currency of New Zealand, which had been distinct from the pound sterling since 1933.[5] The pound used the £sd system, in which the pound was divided into 20 shillings and one shilling was divided into 12 pence, a system which by the 1950s was considered complicated and cumbersome. Switching to decimal currency had been proposed in New Zealand since the 1930s, although only in the 1950s did any plans come to fruition.[6] In 1957, a committee was set up by the Government to investigate decimal currency. The idea fell on fertile ground, and in 1963, the Government decided to decimalise New Zealand currency.[7] The Decimal Currency Act was passed in 1964, setting the date of transition to 10 July 1967.[8] Words such as \"fern\", \"kiwi\" and \"zeal\" were proposed to avoid confusion with the word \"dollar\", which many people associate with the United States dollar.[9][10] In the end, the word \"dollar\" was chosen anyway, and an anthropomorphic dollar note cartoon character called \"Mr. Dollar\" became the symbol of transition in a huge publicity campaign.[11] On Monday 10 July 1967 (\"Decimal Currency Day\"), the New Zealand dollar was introduced to replace the pound at a rate of two dollars to one pound (one dollar to ten shillings, ten cents to one shilling, 5⁄6 cent to a penny).[12] Some 27 million new banknotes were printed and 165 million new coins were minted for the changeover.[9] In 1971 the US devalued its dollar relative to gold, leading New Zealand on 23 December to peg its dollar at US$1.216 with a 4.5% fluctuation range, keeping the same gold value. From 9 July 1973 to 4 March 1985 the dollar's value was determined from a trade-weighted basket of currencies. On 4 March 1985, the NZ$ was floated at the initial rate of US$0.4444. Since then the dollar's value has been determined by the financial markets, and has been in the range of about US$0.39 to 0.88. The dollar's post-float low was US$0.3922 on 22 November 2000, and it reached a post-float high on 9 July 2014 of US$0.8821. Much of this medium-term variation in the exchange rate has been attributed to differences in interest rates.[citation needed] On 11 June 2007 the Reserve Bank sold an unknown worth of New Zealand dollars for nine billion USD in an attempt to drive down its value. This is the first intervention in the markets by the Bank since the float in 1985. Two suspected interventions followed, but they were not as successful as the first: the first appeared to be initially effective, with the dollar dropping to approximately US$0.7490 from near US$0.7620. However, within little more than a month it had risen to new post-float highs, reaching US$0.8103 on 23 July 2007. After reaching its post-float record high in early 2008, the value of the NZ$ plummeted throughout much of the 2nd half of 2008 and the first quarter of 2009 as a response to the global economic downturn and flight by investors away from \"riskier\" currencies such as the NZ$. The NZ$ bottomed out at approximately US$0.50 on 6 March 2009.[14] However, it rebounded strongly as the year progressed, reaching the US$0.75 range by November 2009.[14] On the introduction of the dollar, coins came in denominations of 1c, 2c, 5c, 10c, 20c, and 50c. The 1c and 2c coins were bronze, the others were cupro-nickel.[19] To ease transition, the 5c, 10c, and 20c were the same size as the sixpence, shilling and florin that they respectively replaced, and until 1970, the ten-cent coin bore the additional legend \"One Shilling\". The obverse designs of all the coins featured Arnold Machin's portrait of Queen Elizabeth II, with the legend ELIZABETH II NEW ZEALAND [date]. The reverse sides of coins introduced in 1967 did not follow the designs that were originally intended for them. Those modern art and sculpture themed designs were leaked to a newspaper and met a very negative public reaction. The final releases were given more conservative designs in line with public expectations. In 1986, New Zealand adopted Raphael Maklouf's new portrait of the Queen. The 1c and 2c coins were last minted for circulation in 1987, with collector coins being made for 1988. The coins were demonetised on 30 April 1990.[19] The lack of 1c and 2c coins meant that cash transactions were normally rounded to the nearest 5c (10c from 2006), a process known as Swedish rounding. On 11 February 1991, aluminium-bronze $1 and $2 coins were introduced to replace existing $1 and $2 notes.[19] In 1999, Ian Rank-Broadley's portrait of the Queen was introduced and the legend rearranged to read \"NEW ZEALAND ELIZABETH II\". On 11 November 2004 the Reserve Bank announced that it proposed to take the 5c coin out of circulation and to make the 50c, 20c and 10c coins smaller and use plated steel to make them lighter. After a three-month public submission period that ended on 4 February 2005, the Reserve Bank announced on 31 March that it would go ahead with the proposed changes. The changeover period started on 31 July 2006, with the old coins usable until 31 October 2006.[19] The old 50c, 20c, 10c and 5c pieces are now no longer legal tender, but are still redeemable at the Reserve Bank. Prior to the change over, these coins were similar, save for the legend and reverse artwork, to international (mainly Commonwealth) coins of the same British-derived sizes, which led to coins from other currencies, particularly older coins, being accepted by vending machines and many retailers. After the death of Queen Elizabeth II in September 2022, the Reserve Bank said it would exhaust its existing coin stocks before introducing new coins featuring King Charles III. Based on current stock levels, this would likely be several years away.[23] In 1967, notes were introduced in denominations of $1, $2, $5, $10, $20 and $100, with all except the $5 replacing their pound predecessors. The original series of dollar notes featured on the obverse a portrait of Queen Elizabeth II wearing Queen Alexandra's Kokoshnik tiara, King George's VI festoon necklace, and Queen Mary's floret earrings, while the reverse featured native birds and plants.[24] The notes were changed slightly in 1981 due to a change of printer (from De La Rue to Bradbury, Wilkinson & Co.)—the most noticeable difference being the portrait based upon a photograph by Peter Grugeon, in which Queen Elizabeth II is wearing Grand Duchess Vladimir's tiara and Queen Victoria's golden jubilee necklace.[24] The $50 note was added in 1983 to fill the long gap between the $20 and the $100 notes. $1 and $2 notes were discontinued in 1991 after being replaced with coins. A new series of notes, known as Series 5 was introduced in 1992. The obverse of each note featured a notable New Zealander, while the reverse featured a native New Zealand bird and New Zealand scenery. The Queen remained on the $20 note. In 1999, series 6 polymer notes replaced the paper notes. The designs remained much the same, but were changed slightly to accommodate new security features, with the most obvious changes being the two transparent windows. In 2015–16, new Series 7 notes were issued,[25] refreshing the note design and improving security features. As of 2021, Series 6 and 7 notes are currently legal tender.[26] In September 2022, following the death of Queen Elizabeth II, the Reserve Bank said it would exhaust its existing stocks of $20 notes before introducing new notes featuring King Charles III.[23] With the breakdown of the Bretton Woods system in 1971, both Australia and New Zealand converted the mostly-fixed foreign exchange regimes to a moving peg against the US dollar. In September 1974, Australia moved to a peg against a basket of currencies called the trade weighted index (TWI) in an effort to reduce fluctuations associated with its peg to the US dollar. The peg to the TWI was changed to a moving peg in November 1976, causing the actual value of the peg to be periodically adjusted.[27] The New Zealand dollar contributes greatly to the total global exchange market—far in excess of New Zealand's relative share of population or global GDP. According to the Bank for International Settlements, the New Zealand dollar's share of global foreign exchange market daily turnover in 2016 was 2.1% (up from 1.6% in 2010) giving it a rank of 11th.[28] Trading in the currency has climbed steadily since the same survey in 1998 when the NZD's ranking was 17th and the share of turnover was just 0.2%. ^The total sum is 200% because each currency trade is counted twice: once for the currency being bought and once for the one being sold. The percentages above represent the proportion of all trades involving a given currency, regardless of which side of the transaction it is on. For example, the US dollar is bought or sold in 88% of all currency trades, while the euro is bought or sold in 31% of all trades."}
{"url": "https://en.wikipedia.org/wiki/Chelicerata", "text": "Chelicerata split from Mandibulata by the mid-Cambrian, as evidenced by stem-group chelicerates like Habeliida and Mollisonia present by this time.[2] The surviving marine species include the four species of xiphosurans (horseshoe crabs), and possibly the 1,300 species of pycnogonids (sea spiders), if the latter are indeed chelicerates. On the other hand, there are over 77,000 well-identified species of air-breathing chelicerates, and there may be about 500,000 unidentified species. Like all arthropods, chelicerates have segmented bodies with jointed limbs, all covered in a cuticle made of chitin and proteins. The chelicerate body plan consists of two tagmata, the prosoma and the opisthosoma, except that mites have lost a visible division between these sections. The chelicerae, which give the group its name, are the only appendages that appear before the mouth. In most sub-groups, they are modest pincers used to feed. However, spiders' chelicerae form fangs that most species use to inject venom into prey. The group has the open circulatory system typical of arthropods, in which a tube-like heart pumps blood through the hemocoel, which is the major body cavity. Marine chelicerates have gills, while the air-breathing forms generally have both book lungs and tracheae. In general, the ganglia of living chelicerates' central nervous systems fuse into large masses in the cephalothorax, but there are wide variations and this fusion is very limited in the Mesothelae, which are regarded as the oldest and most basal group of spiders. Most chelicerates rely on modified bristles for touch and for information about vibrations, air currents, and chemical changes in their environment. The most active hunting spiders also have very acute eyesight. While the marine horseshoe crabs rely on external fertilization, air-breathing chelicerates use internal but usually indirect fertilization. Many species use elaborate courtship rituals to attract mates. Most lay eggs that hatch as what look like miniature adults, but all scorpions and a few species of mites keep the eggs inside their bodies until the young emerge. In most chelicerate species the young have to fend for themselves, but in scorpions and some species of spider the females protect and feed their young. The evolutionary origins of chelicerates from the early arthropods have been debated for decades. Although there is considerable agreement about the relationships between most chelicerate sub-groups, the inclusion of the Pycnogonida in this taxon has recently been questioned (see below), and the exact position of scorpions is still controversial, though they were long considered the most basal of the arachnids.[4] Although the venom of a few spider and scorpion species can be very dangerous to humans, medical researchers are investigating the use of these venoms for the treatment of disorders ranging from cancer to erectile dysfunction. The medical industry also uses the blood of horseshoe crabs as a test for the presence of contaminant bacteria. Mites can cause allergies in humans, transmit several diseases to humans and their livestock, and are serious agricultural pests. Formation of anterior segments across arthropod taxa based on previous hypothesis.[9] Note the antenna-bearing somite 1 was thought to be lost in Chelicerata. Formation of anterior segments across arthropod taxa based on gene expression and neuroanatomical observations,[10][11] Note the chelicera(Ch) and chelifore(Chf) arose from somite 1 and thus correspond to the first antenna(An/An1) of other arthropods. The Chelicerata are arthropods as they have: segmented bodies with jointed limbs, all covered in a cuticle made of chitin and proteins; heads that are composed of several segments that fuse during the development of the embryo; a much reduced coelom; a hemocoel through which the blood circulates, driven by a tube-like heart.[9] Chelicerates' bodies consist of two tagmata, sets of segments that serve similar functions: the foremost one, called the prosoma or cephalothorax, and the rear tagma is called the opisthosoma or abdomen.[12] However, in the Acari (mites and ticks) there is no visible division between these sections.[13] The prosoma is formed in the embryo by fusion of the ocular somite (referred as \"acron\" in previous literatures), which carries the eyes and labrum,[11] with six post-ocular segments (somite 1 to 6),[10] which all have paired appendages. It was previously thought that chelicerates had lost the antennae-bearing somite 1,[14] but later investigations reveal that it is retained and corresponds to a pair of chelicerae or chelifores,[15] small appendages that often form pincers. somite 2 has a pair of pedipalps that in most sub-groups perform sensory functions, while the remaining four cephalothorax segments (somite 4 to 6) have pairs of legs.[10] In basal forms the ocular somite has a pair of compound eyes on the sides and four pigment-cup ocelli (\"little eyes\") in the middle.[12] The mouth is between somite 1 and 2 (chelicerae and pedipalps). Like all arthropods, chelicerates' bodies and appendages are covered with a tough cuticle made mainly of chitin and chemically hardened proteins. Since this cannot stretch, the animals must molt to grow. In other words, they grow new but still soft cuticles, then cast off the old one and wait for the new one to harden. Until the new cuticle hardens the animals are defenseless and almost immobilized.[18] Chelicerae and pedipalps are the two pairs of appendages closest to the mouth; they vary widely in form and function and the consistent difference between them is their position in the embryo and corresponding neurons: chelicerae are deutocerebral and arise from somite 1, ahead of the mouth, while pedipalps are tritocerebral and arise from somite 2, behind the mouth.[12][10][11] The chelicerae (\"claw horns\") that give the sub-phylum its name normally consist of three sections, and the claw is formed by the third section and a rigid extension of the second.[12][19] However, spiders' have only two sections, and the second forms a fang that folds away behind the first when not in use.[16] The relative sizes of chelicerae vary widely: those of some fossil eurypterids and modern harvestmen form large claws that extended ahead of the body,[19] while scorpions' are tiny pincers that are used in feeding and project only slightly in front of the head.[20] In basal chelicerates, the pedipalps are unspecialized and subequal to the posterior pairs of walking legs.[10] However, in sea spider and arachnids, the pedipalps are more or less specialized for sensory[12] or prey-catching function[10] – for example scorpions have pincers[20] and male spiders have bulbous tips that act as syringes to inject sperm into the females' reproductive openings when mating.[16] As in all arthropods, the chelicerate body has a very small coelom restricted to small areas round the reproductive and excretory systems. The main body cavity is a hemocoel that runs most of the length of the body and through which blood flows, driven by a tubular heart that collects blood from the rear and pumps it forward. Although arteries direct the blood to specific parts of the body, they have open ends rather than joining directly to veins, and chelicerates therefore have open circulatory systems as is typical for arthropods.[22] These depend on individual sub-groups' environments. Modern terrestrial chelicerates generally have both book lungs, which deliver oxygen and remove waste gases via the blood, and tracheae, which do the same without using the blood as a transport system.[23] The living horseshoe crabs are aquatic and have book gills that lie in a horizontal plane. For a long time it was assumed that the extinct eurypterids had gills, but the fossil evidence was ambiguous. However, a fossil of the 45 millimetres (1.8 in) long eurypterid Onychopterella, from the Late Ordovician period, has what appear to be four pairs of vertically oriented book gills whose internal structure is very similar to that of scorpions' book lungs.[24] The guts of most modern chelicerates are too narrow to take solid food.[23] All scorpions and almost all spiders are predators that \"pre-process\" food in preoral cavities formed by the chelicerae and the bases of the pedipalps.[16][20] However, one predominantly herbivore spider species is known,[25] and many supplement their diets with nectar and pollen.[26] Many of the Acari (ticks and mites) are blood-sucking parasites, but there are many predatory, herbivore and scavenger sub-groups. All the Acari have a retractable feeding assembly that consists of the chelicerae, pedipalps and parts of the exoskeleton, and which forms a preoral cavity for pre-processing food.[13] Harvestmen are among the minority of living chelicerates that can take solid food, and the group includes predators, herbivores and scavengers.[27]Horseshoe crabs are also capable of processing solid food, and use a distinctive feeding system. Claws at the tips of their legs grab small invertebrates and pass them to a food groove that runs from between the rearmost legs to the mouth, which is on the underside of the head and faces slightly backwards. The bases of the legs form toothed gnathobases that both grind the food and push it towards the mouth.[17] This is how the earliest arthropods are thought to have fed.[28] Horseshoe crabs convert nitrogenous wastes to ammonia and dump it via their gills, and excrete other wastes as feces via the anus. They also have nephridia (\"little kidneys\"), which extract other wastes for excretion as urine.[17] Ammonia is so toxic that it must be diluted rapidly with large quantities of water.[29] Most terrestrial chelicerates cannot afford to use so much water and therefore convert nitrogenous wastes to other chemicals, which they excrete as dry matter. Extraction is by various combinations of nephridia and Malpighian tubules. The tubules filter wastes out of the blood and dump them into the hindgut as solids, a system that has evolved independently in insects and several groups of arachnids.[23] Chelicerate nervous systems are based on the standard arthropod model of a pair of nerve cords, each with a ganglion per segment, and a brain formed by fusion of the ganglia just behind the mouth with those ahead of it.[30] If one assume that chelicerates lose the first segment, which bears antennae in other arthropods, chelicerate brains include only one pair of pre-oral ganglia instead of two.[12] However, there is evidence that the first segment is indeed available and bears the cheliceres.[31][15] There is a notable but variable trend towards fusion of other ganglia into the brain. The brains of horseshoe crabs include all the ganglia of the prosoma plus those of the first two opisthosomal segments, while the other opisthosomal segments retain separate pairs of ganglia.[17] In most living arachnids, except scorpions if they are true arachnids, all the ganglia, including those that would normally be in the opisthosoma, are fused into a single mass in the prosoma and there are no ganglia in the opisthosoma.[23] However, in the Mesothelae, which are regarded as the most basal living spiders, the ganglia of the opisthosoma and the rear part of the prosoma remain unfused,[32] and in scorpions the ganglia of the cephalothorax are fused but the abdomen retains separate pairs of ganglia.[23] As with other arthropods, chelicerates' cuticles would block out information about the outside world, except that they are penetrated by many sensors or connections from sensors to the nervous system. In fact, spiders and other arthropods have modified their cuticles into elaborate arrays of sensors. Various touch and vibration sensors, mostly bristles called setae, respond to different levels of force, from strong contact to very weak air currents. Chemical sensors provide equivalents of taste and smell, often by means of setae.[33] Living chelicerates have both compound eyes (only in horseshoe crabs, as the compound eye in the other clades has been reduced to a cluster of no more than five pairs of ocelli), mounted on the sides of the head, plus pigment-cup ocelli (\"little eyes\"), mounted in the middle. These median ocelli-type eyes in chelicerates are assumed to be homologous with the crustacean nauplius eyes and the insect ocelli.[34] The eyes of horseshoe crabs can detect movement but not form images.[17] At the other extreme, jumping spiders have a very wide field of vision,[16] and their main eyes are ten times as acute as those of dragonflies,[35] able to see in both colors and UV-light.[36] Horseshoe crabs use external fertilization; the sperm and ova meet outside the parents' bodies. Despite being aquatic, they spawn on land in the intertidal zone on the beach.[37] The female digs a depression in the wet sand, where she will release her eggs. The male, usually more than one, then releases his sperm onto them.[38] Their trilobite-like larvae look rather like miniature adults as they have full sets of appendages and eyes, but initially they have only two pairs of book-gills and gain three more pairs as they molt.[17] Also the sea spiders have external fertilization. The male and female release their sperm and eggs into the water where fertilization occurs. The male then collects the eggs and carries them around under his body.[39] Being air-breathing animals, although many mites have become secondary aquatic,[40] the arachnids use internal fertilization. Except for opiliones and some mites, where the male have a penis used for direct fertilization,[41] fertilization in arachnids is indirect. Indirect fertilization happens in two ways; the male deposit his spermatophore (package of sperm) on the ground, which is then picked up by the female. Or the male store his sperm in appendages modified into sperm transfer organs, such as the pedipalps in male spiders, which is inserted into the female genital openings during copulation.[16]Courtship rituals are common, especially in species where the male risk being eaten before mating. Most arachnids lay eggs, but all scorpions and some mites are viviparous, giving birth to live young (even more mites are ovoviviparous, but most are oviparous).[42][43][44][45] Female pseudoscorpions carry their eggs in a brood pouch on the belly, where the growing embryos feeds on a nutritive fluid provided by the mother during development, and are therefore matrotrophic.[46] Levels of parental care for the young range from zero to prolonged. Scorpions carry their young on their backs until the first molt, and in a few semi-social species the young remain with their mother.[47] Some spiders care for their young, for example a wolf spider's brood cling to rough bristles on the mother's back,[16] and females of some species respond to the \"begging\" behavior of their young by giving them their prey, provided it is no longer struggling, or even regurgitate food.[48] There are large gaps in the chelicerates' fossil record because, like all arthropods, their exoskeletons are organic and hence their fossils are rare except in a few lagerstätten where conditions were exceptionally suited to preserving fairly soft tissues. The Burgess shale animals like Sidneyia from about 505 million years ago have been classified as chelicerates, the latter because its appendages resemble those of the Xiphosura (horseshoe crabs). However, cladistic analyses that consider wider ranges of characteristics place neither as chelicerates. There is debate about whether Fuxianhuia from earlier in the Cambrian period, about 525 million years ago, was a chelicerate. Another Cambrian fossil, Kodymirus, was originally classified as an aglaspid but may have been a eurypterid and therefore a chelicerate. If any of these was closely related to chelicerates, there is a gap of at least 43 million years in the record between true chelicerates and their nearest not-quite chelicerate relatives.[49] Reconstruction of Mollisonia plenovenatrix, the oldest known arthropod with confirmed chelicerae Sanctacaris, member of the family Sanctacarididae from the Burgess Shale of Canada, represents the oldest occurrence of a confirmed chelicerate, Middle Cambrian in age.[50] Although its chelicerate nature has been doubted for its pattern of tagmosis (how the segments are grouped, especially in the head),[49] a restudy in 2014 confirmed its phylogenetic position as the oldest chelicerate.[50] Another fossil of the site, Mollisonia, is considered a basal chelicerate and it has the oldest known chelicerae and proto-book gills.[51] Attercopus fimbriunguis, from 386 million years ago in the Devonian period, bears the earliest known silk-producing spigots, and was therefore hailed as a spider,[56] but it lacked spinnerets and hence was not a true spider.[57] Rather, it was likely sister group to the spiders, a clade which has been named Serikodiastida.[58] Close relatives of the group survived through to the Cretaceous Period.[59] Several Carboniferous spiders were members of the Mesothelae, a basal group now represented only by the Liphistiidae,[56] and fossils suggest taxa closely related to the spiders, but which were not true members of the group were also present during this Period.[60] The \"traditional\" view of the arthropod \"family tree\" shows chelicerates as less closely related to the other major living groups (crustaceans; hexapods, which includes insects; and myriapods, which includes centipedes and millipedes) than these other groups are to each other. Recent research since 2001, using both molecular phylogenetics (the application of cladistic analysis to biochemistry, especially to organisms' DNA and RNA) and detailed examination of how various arthropods' nervous systems develop in the embryos, suggests that chelicerates are most closely related to myriapods, while hexapods and crustaceans are each other's closest relatives. However, these results are derived from analyzing only living arthropods, and including extinct ones such as trilobites causes a swing back to the \"traditional\" view, placing trilobites as the sister-group of the Tracheata (hexapods plus myriapods) and chelicerates as least closely related to the other groups.[67] However, the structure of \"family tree\" relationships within the Chelicerata has been controversial ever since the late 19th century. An attempt in 2002 to combine analysis of DNA features of modern chelicerates and anatomical features of modern and fossil ones produced credible results for many lower-level groups, but its results for the high-level relationships between major sub-groups of chelicerates were unstable, in other words minor changes in the inputs caused significant changes in the outputs of the computer program used (POY).[71] An analysis in 2007 using only anatomical features produced the cladogram on the right, but also noted that many uncertainties remain.[72] In recent analyses the clade Tetrapulmonata is reliably recovered, but other ordinal relationships remain in flux.[59][73][60][74][75][76][77] The position of scorpions is particularly controversial. Some early fossils such as the Late SilurianProscorpius have been classified by paleontologists as scorpions, but described as wholly aquatic as they had gills rather than book lungs or tracheae. Their mouths are also completely under their heads and almost between the first pair of legs, as in the extinct eurypterids and living horseshoe crabs.[61] This presents a difficult choice: classify Proscorpius and other aquatic fossils as something other than scorpions, despite the similarities; accept that \"scorpions\" are not monophyletic but consist of separate aquatic and terrestrial groups;[61] or treat scorpions as more closely related to eurypterids and possibly horseshoe crabs than to spiders and other arachnids,[24] so that either scorpions are not arachnids or \"arachnids\" are not monophyletic.[61]Cladistic analyses have recovered Proscorpius within the scorpions,[58] based on reinterpretation of the species' breathing apparatus.[78] This is reflected also in the reinterpretation of Palaeoscorpius as a terrestrial animal.[79] A 2013 phylogenetic analysis[80] (the results presented in a cladogram below) on the relationships within the Xiphosura and the relations to other closely related groups (including the eurypterids, which were represented in the analysis by genera Eurypterus, Parastylonurus, Rhenopterus and Stoermeropterus) concluded that the Xiphosura, as presently understood, was paraphyletic (a group sharing a last common ancestor but not including all descendants of this ancestor) and thus not a valid phylogenetic group. Eurypterids were recovered as closely related to arachnids instead of xiphosurans, forming the group Sclerophorata within the clade Dekatriata (composed of sclerophorates and chasmataspidids). This work suggested it is possible that Dekatriata is synonymous with Sclerophorata as the reproductive system, the primary defining feature of sclerophorates, has not been thoroughly studied in chasmataspidids. Dekatriata is in turn part of the Prosomapoda, a group including the Xiphosurida (the only monophyletic xiphosuran group) and other stem-genera. A recent phylogenetic analysis of the chelicerates places the Xiphosura within the Arachnida as the sister group of Ricinulei.,[77] but others still retrieve a monophyletic arachnida.[81] Although well behind the insects, chelicerates are one of the most diverse groups of animals, with over 77,000 living species that have been described in scientific publications.[82] Some estimates suggest that there may be 130,000 undescribed species of spider and nearly 500,000 undescribed species of mites and ticks.[83] While the earliest chelicerates and the living Pycnogonida (if they are chelicerates[70]) and Xiphosura are marine animals that breathe dissolved oxygen, the vast majority of living species are air-breathers,[82] although a few spider species build \"diving bell\" webs that enable them to live under water.[84] Like their ancestors, most living chelicerates are carnivores, mainly on small invertebrates. However, many species feed as parasites, herbivores, scavengers and detritivores.[13][27][82] In the past, Native Americans ate the flesh of horseshoe crabs, and used the tail spines as spear tips and the shells to bail water out of their canoes. More recent attempts to use horseshoe crabs as food for livestock were abandoned when it was found that this gave the meat a bad taste. Horseshoe crab blood contains a clotting agent, limulus amebocyte lysate, which is used to test antibiotics and kidney machines to ensure that they are free of dangerous bacteria, and to detect spinal meningitis and some cancers.[93] In the 20th century, there were about 100 reliably reported deaths from spider bites,[109] compared with 1,500 from jellyfish stings.[110] Scorpion stings are thought to be a significant danger in less-developed countries; for example, they cause about 1,000 deaths per year in Mexico, but only one every few years in the USA. Most of these incidents are caused by accidental human \"invasions\" of scorpions' nests.[111] On the other hand, medical uses of scorpion venom are being investigated for treatment of brain cancers and bone diseases.[112][113] Ticks are parasitic, and some transmit micro-organisms and parasites that can cause diseases in humans, while the saliva of a few species can directly cause tick paralysis if they are not removed within a day or two.[114] A few of the closely related mites also infest humans, some causing intense itching by their bites, and others by burrowing into the skin. Species that normally infest other animals such as rodents may infest humans if their normal hosts are eliminated.[115] Three species of mite are a threat to honey bees and one of these, Varroa destructor, has become the largest single problem faced by beekeepers worldwide.[116] Mites cause several forms of allergic diseases, including hay fever, asthma and eczema, and they aggravate atopic dermatitis.[117] Mites are also significant crop pests, although predatory mites may be useful in controlling some of these.[82][118] ^Mittmann, B.; Scholtz, G. (2003). \"Development of the nervous system in the \"head\" of Limulus polyphemus (Chelicerata: Xiphosura): Morphological evidence for a correspondence between the segments of the chelicerae and of the (first) antennae of Mandibulata\". Dev Genes Evol. 213 (1): 9–17. doi:10.1007/s00427-002-0285-5. PMID12590348. S2CID13101102. ^Jenner, R.A. (2006), \"Challenging received wisdoms: Some contributions of the new microscopy to the new animal phylogeny\", Integrative and Comparative Biology, 46 (2): 93–103, doi:10.1093/icb/icj014, PMID21672726"}
{"url": "https://en.wikipedia.org/wiki/Diencephalon", "text": "The optic nerve (CNII) attaches to the diencephalon. The optic nerve is a sensory (afferent) nerve responsible for vision and sight ; it runs from the eye through the optic canal in the skull and attaches to the diencephalon. The retina itself is derived from the optic cup, a part of the embryonic diencephalon. This section may need to be rewritten to comply with Wikipedia's quality standards, as it uses non-scientific language and lacks citations.You can help. The talk page may contain suggestions.(November 2023) The diencephalon is the region of the embryonic vertebrate neural tube that gives rise to anterior forebrain structures including the thalamus, hypothalamus, posterior portion of the pituitary gland, and the pineal gland. The diencephalon encloses a cavity called the third ventricle. The thalamus serves as a relay centre for sensory and motor impulses between the spinal cord and medulla oblongata, and the cerebrum. It recognizes sensory impulses of heat, cold, pain, pressure etc. The floor of the third ventricle is called the hypothalamus. It has control centres for control of eye movement and hearing responses."}
{"url": "https://en.wikipedia.org/wiki/Neural_crest", "text": "The formation of neural crest during the process of neurulation. Neural crest is first induced in the region of the neural plate border. After neural tube closure, neural crest delaminates from the region between the dorsal neural tube and overlying ectoderm and migrates out towards the periphery. Neural crest was first described in the chick embryo by Wilhelm His Sr. in 1868 as \"the cord in between\" (Zwischenstrang) because of its origin between the neural plate and non-neural ectoderm.[1] He named the tissue ganglionic crest since its final destination was each lateral side of the neural tube where it differentiated into spinal ganglia.[6] During the first half of the 20th century, the majority of research on neural crest was done using amphibian embryos which was reviewed by Hörstadius (1950) in a well known monograph.[7] Cell labeling techniques advanced the field of neural crest because they allowed researchers to visualize the migration of the tissue throughout the developing embryos. In the 1960s, Weston and Chibon utilized radioisotopic labeling of the nucleus with tritiated thymidine in chick and amphibian embryo respectively. However, this method suffers from drawbacks of stability, since every time the labeled cell divides the signal is diluted. Modern cell labeling techniques such as rhodamine-lysinated dextran and the vital dye diI have also been developed to transiently mark neural crest lineages.[6] The quail-chick marking system, devised by Nicole Le Douarin in 1969, was another instrumental technique used to track neural crest cells.[8][9]Chimeras, generated through transplantation, enabled researchers to distinguish neural crest cells of one species from the surrounding tissue of another species. With this technique, generations of scientists were able to reliably mark and study the ontogeny of neural crest cells. A molecular cascade of events is involved in establishing the migratory and multipotent characteristics of neural crest cells. This gene regulatory network can be subdivided into the following four sub-networks described below. Wnt signaling has been demonstrated in neural crest induction in several species through gain-of-function and loss-of-function experiments. In coherence with this observation, the promoter region of slug (a neural crest specific gene) contains a binding site for transcription factors involved in the activation of Wnt-dependent target genes, suggestive of a direct role of Wnt signaling in neural crest specification.[10] The current role of BMP in neural crest formation is associated with the induction of the neural plate. BMP antagonists diffusing from the ectoderm generates a gradient of BMP activity. In this manner, the neural crest lineage forms from intermediate levels of BMP signaling required for the development of the neural plate (low BMP) and epidermis (high BMP).[1] Fgf from the paraxial mesoderm has been suggested as a source of neural crest inductive signal. Researchers have demonstrated that the expression of dominate-negative Fgf receptor in ectoderm explants blocks neural crest induction when recombined with paraxial mesoderm.[11] The understanding of the role of BMP, Wnt, and Fgf pathways on neural crest specifier expression remains incomplete. Signaling events that establish the neural plate border lead to the expression of a set of transcription factors delineated here as neural plate border specifiers. These molecules include Zic factors, Pax3/7, Dlx5, Msx1/2 which may mediate the influence of Wnts, BMPs, and Fgfs. These genes are expressed broadly at the neural plate border region and precede the expression of bona fide neural crest markers.[4] Experimental evidence places these transcription factors upstream of neural crest specifiers. For example, in Xenopus Msx1 is necessary and sufficient for the expression of Slug, Snail, and FoxD3.[12] Furthermore, Pax3 is essential for FoxD3 expression in mouse embryos.[13] Following the expression of neural plate border specifiers is a collection of genes including Slug/Snail, FoxD3, Sox10, Sox9, AP-2 and c-Myc. This suite of genes, designated here as neural crest specifiers, are activated in emergent neural crest cells. At least in Xenopus, every neural crest specifier is necessary and/or sufficient for the expression of all other specifiers, demonstrating the existence of extensive cross-regulation.[4] Moreover, this model organism was instrumental in the elucidation of the role of the Hedgehog signaling pathway in the specification of the neural crest, with the transcription factor Gli2 playing a key role.[14] Outside of the tightly regulated network of neural crest specifiers are two other transcription factors Twist and Id. Twist, a bHLH transcription factor, is required for mesenchyme differentiation of the pharyngeal arch structures.[15] Id is a direct target of c-Myc and is known to be important for the maintenance of neural crest stem cells.[16] Neural crest cell migration occurs in a rostral to caudal direction without the need of a neuronal scaffold such as along a radial glial cell. For this reason the crest cell migration process is termed \"free migration\". Instead of scaffolding on progenitor cells, neural crest migration is the result of repulsive guidance via EphB/EphrinB and semaphorin/neuropilin signaling, interactions with the extracellular matrix, and contact inhibition with one another.[17] While Ephrin and Eph proteins have the capacity to undergo bi-directional signaling, neural crest cell repulsion employs predominantly forward signaling to initiate a response within the receptor bearing neural crest cell.[23] Burgeoning neural crest cells express EphB, a receptor tyrosine kinase, which binds the EphrinB transmembrane ligand expressed in the caudal half of each somite. When these two domains interact it causes receptor tyrosine phosphorylation, activation of rhoGTPases, and eventual cytoskeletal rearrangements within the crest cells inducing them to repel. This phenomenon allows neural crest cells to funnel through the rostral portion of each somite.[17] Semaphorin-neuropilin repulsive signaling works synergistically with EphB signaling to guide neural crest cells down the rostral half of somites in mice. In chick embryos, semaphorin acts in the cephalic region to guide neural crest cells through the pharyngeal arches. On top of repulsive repulsive signaling, neural crest cells express β1and α4 integrins which allows for binding and guided interaction with collagen, laminin, and fibronectin of the extracellular matrix as they travel. Additionally, crest cells have intrinsic contact inhibition with one another while freely invading tissues of different origin such as mesoderm.[17] Neural crest cells that migrate through the rostral half of somites differentiate into sensory and sympathetic neurons of the peripheral nervous system. The other main route neural crest cells take is dorsolaterally between the epidermis and the dermamyotome. Cells migrating through this path differentiate into pigment cells of the dermis. Further neural crest cell differentiation and specification into their final cell type is biased by their spatiotemporal subjection to morphogenic cues such as BMP, Wnt, FGF, Hox, and Notch.[20] Neurocristopathies result from the abnormal specification, migration, differentiation or death of neural crest cells throughout embryonic development.[24][25] This group of diseases comprises a wide spectrum of congenital malformations affecting many newborns. Additionally, they arise because of genetic defects affecting the formation of neural crest and because of the action of Teratogens[26] Waardenburg's syndrome is a neurocristopathy that results from defective neural crest cell migration. The condition's main characteristics include piebaldism and congenital deafness. In the case of piebaldism, the colorless skin areas are caused by a total absence of neural crest-derived pigment-producing melanocytes.[27] There are four different types of Waardenburg's syndrome, each with distinct genetic and physiological features. Types I and II are distinguished based on whether or not family members of the affected individual have dystopia canthorum.[28] Type III gives rise to upper limb abnormalities. Lastly, type IV is also known as Waardenburg-Shah syndrome, and afflicted individuals display both Waardenburg's syndrome and Hirschsprung's disease.[29] Types I and III are inherited in an autosomal dominant fashion,[27] while II and IV exhibit an autosomal recessive pattern of inheritance. Overall, Waardenburg's syndrome is rare, with an incidence of ~ 2/100,000 people in the United States. All races and sexes are equally affected.[27] There is no current cure or treatment for Waardenburg's syndrome. Neural crest cells originating from different positions along the anterior-posterior axis develop into various tissues. These regions of neural crest can be divided into four main functional domains, which include the cranial neural crest, trunk neural crest, vagal and sacral neural crest, and cardiac neural crest. Cranial neural crest migrates dorsolaterally to form the craniofacial mesenchyme that differentiates into various cranial ganglia and craniofacial cartilages and bones.[21] These cells enter the pharyngeal pouches and arches where they contribute to the thymus, bones of the middle ear and jaw and the odontoblasts of the tooth primordia.[35] Trunk neural crest gives rise two populations of cells.[36] One group of cells fated to become melanocytes migrates dorsolaterally into the ectoderm towards the ventral midline. A second group of cells migrates ventrolaterally through the anterior portion of each sclerotome. The cells that stay in the sclerotome form the dorsal root ganglia, whereas those that continue more ventrally form the sympathetic ganglia, adrenal medulla, and the nerves surrounding the aorta.[35] Cardiac neural crest develops into melanocytes, cartilage, connective tissue and neurons of some pharyngeal arches. Also, this domain gives rise to regions of the heart such as the musculo-connective tissue of the large arteries, and part of the septum, which divides the pulmonary circulation from the aorta.[35] The semilunar valves of the heart are associated with neural crest cells according to new research.[37] Several structures that distinguish the vertebrates from other chordates are formed from the derivatives of neural crest cells. In their \"New head\" theory, Gans and Northcut argue that the presence of neural crest was the basis for vertebrate specific features, such as sensory ganglia and cranial skeleton. Furthermore, the appearance of these features was pivotal in vertebrate evolution because it enabled a predatory lifestyle.[38][39] However, considering the neural crest a vertebrate innovation does not mean that it arose de novo. Instead, new structures often arise through modification of existing developmental regulatory programs. For example, regulatory programs may be changed by the co-option of new upstream regulators or by the employment of new downstream gene targets, thus placing existing networks in a novel context.[40][41] This idea is supported by in situ hybridization data that shows the conservation of the neural plate border specifiers in protochordates, which suggest that part of the neural crest precursor network was present in a common ancestor to the chordates.[5] In some non-vertebrate chordates such as tunicates a lineage of cells (melanocytes) has been identified, which are similar to neural crest cells in vertebrates. This implies that a rudimentary neural crest existed in a common ancestor of vertebrates and tunicates.[42]"}
{"url": "https://en.wikipedia.org/wiki/Nephrozoa", "text": "Nephrozoa is a proposed major clade of bilateriananimals. It includes all bilaterians other than Xenacoelomorpha. It contrasts with the Xenambulacraria hypothesis, which instead posits that Xenocoelomorpha is most closely related to Ambulacraria.[1][2][3][4][5] Which hypothesis is correct is controversial. Authors supporting the Xenambulacraria hypothesis have suggested that the genetic evidence used to support Nephrozoa may be due to systematic error.[1]"}
{"url": "https://en.wikipedia.org/w/index.php?title=Vertebrate&action=edit&section=17", "text": "By publishing changes, you agree to the Terms of Use, and you irrevocably agree to release your contribution under the CC BY-SA 4.0 License and the GFDL. You agree that a hyperlink or URL is sufficient attribution under the Creative Commons license."}
{"url": "https://en.wikipedia.org/wiki/Archelosauria", "text": "Archelosauria is a clade grouping turtles and archosaurs (birds and crocodilians) and their fossil relatives, to the exclusion of lepidosaurs (the clade containing lizards, snakes and the tuatara). The majority of phylogenetic analyses based on molecular data (e.g. DNA and proteins) have supported a sister-group relationship between turtles and archosaurs. On the other hand, Archelosauria had not been historically supported by most morphological analyses, which have instead found turtles to either be descendants of parareptiles, early-diverging diapsids outside of Sauria, or close relatives of lepidosaurs within the clade Ankylopoda. Some recent morphological analyses have also found support for Archelosauria. Archelosauria was named in a 2015 article by Crawford et al. The name is meant to evoke the archosaurs and chelonians (turtles), the two living subgroups of the clade. Crawford et al. defined Archelosauria as the clade formed by the descendants of the most recent common ancestor of Crocodylus niloticus (the Nile crocodile) and Testudo graeca (the Greek tortoise).[1] A 2021 article by Joyce et al. modified the definition to specifically exclude the lizardLacerta agilis from the group.[8] Below is the phylogeny from Crawford et al., showing interrelationships of Testudines at family level down to Durocryptodira. Archelosauria was grouped within Sauria (the clade formed by archosaurs and lepidosaurs), as the sister branch to Lepidosauria, the clade containing lizards, snakes and the tuatara.[1] Analyses based on morphological data have generally recovered turtles either as non-diapsid reptiles nested within Parareptilia (a group of basal reptiles that lived from the Carboniferous to the Triassic), as early-diverging diapsids outside of Sauria, or as close relatives of Lepidosauria. The hypothetical clade formed by turtles and lepidosaurs to the exclusion of archosaurs is known as Ankylopoda.[8] A 2022 analysis by Simões et al. found a monophyletic Archelosauria using only morphological data for the first time, thus agreeing with most molecular analyses. Archelosauria was diagnosed by two unambiguous synapomorphies (shared derived traits): a sagittal crest on the supraoccipital bone, and the lack of an entepicondylar foramen on the humerus. A cladogram adapted from their analysis is shown below:[9]"}
{"url": "https://en.wikipedia.org/wiki/Jawless_vertebrates", "text": "Molecular data, both from rRNA[6] and from mtDNA[7] as well as embryological data,[8] strongly supports the hypothesis that both groups of living agnathans, hagfishes and lampreys, are more closely related to each other than to jawed fish, forming the clade Cyclostomi.[9] The oldest fossil agnathans appeared in the Cambrian, and two groups still survive today: the lampreys and the hagfish, comprising about 120 species in total. Hagfish are considered members of the subphylum Vertebrata, because they secondarily lost vertebrae; before this event was inferred from molecular[6][7][10] and developmental[11] data, the group Craniata was created by Linnaeus (and is still sometimes used as a strictly morphological descriptor) to reference hagfish plus vertebrates. While a few scientists still regard the living agnathans as only superficially similar, and argue that many of these similarities are probably shared basal characteristics of ancient vertebrates, newer taxonomic studies clearly place hagfish (the Myxini or Hyperotreti) with the lampreys (Hyperoartii) as being more closely related to each other than either is to the jawed fishes.[6][7][12] Agnathans are ectothermic, meaning they do not regulate their own body temperature. Agnathan metabolism is slow in cold water, and therefore they do not have to eat very much. They have no distinct stomach, but rather a long gut, more or less homogeneous throughout its length. Lampreys feed on other fish and mammals. Anticoagulant fluids preventing blood clotting are injected into the host, causing the host to yield more blood. Hagfish are scavengers, eating mostly dead animals. They use a row of sharp teeth to break down the animal. The fact that Agnathan teeth are unable to move up and down limits their possible food types. In addition to the absence of jaws, modern agnathans are characterised by absence of paired fins; the presence of a notochord both in larvae and adults; and seven or more paired gill pouches. Lampreys have a light sensitive pineal eye (homologous to the pineal gland in mammals). All living and most extinct Agnatha do not have an identifiable stomach or any appendages. Fertilization and development are both external. There is no parental care in the Agnatha class. The Agnatha are ectothermic or cold , with a cartilaginousskeleton, and the heart contains 2 chambers. In modern agnathans, the body is covered in skin, with neither dermal or epidermal scales. The skin of hagfish has copious slime glands, the slime constituting their defense mechanism. The slime can sometimes clog up enemy fishes' gills, causing them to die. In direct contrast, many extinct agnathans sported extensive exoskeletons composed of either massive, heavy dermal armour or small mineralized scales. Fertilization in lampreys is external. Mode of fertilization in hagfishes is not known. Development in both groups probably is external. There is no known parental care. Not much is known about the hagfish reproductive process. It is believed that hagfish only have 30 eggs over a lifetime.[14] There is very little of the larval stage that characterizes the lamprey. Lamprey are only able to reproduce once. After external fertilization, the lamprey's cloacas remain open, allowing a fungus to enter their intestines, killing them. Lampreys reproduce in freshwater riverbeds, working in pairs to build a nest and burying their eggs about an inch beneath the sediment. The resulting hatchlings go through four years of larval development before becoming adults. Many Ordovician, Silurian, and Devonian agnathans were armored with heavy bony-spiky plates. The first armored agnathans—the Ostracoderms, precursors to the bony fish and hence to the tetrapods (including humans)—are known from the middle Ordovician, and by the Late Silurian the agnathans had reached the high point of their evolution. Most of the ostracoderms, such as thelodonts, osteostracans, and galeaspids, were more closely related to the gnathostomes than to the surviving agnathans, known as cyclostomes. Cyclostomes apparently split from other agnathans before the evolution of dentine and bone, which are present in many fossil agnathans, including conodonts.[18] Agnathans declined in the Devonian and never recovered. Approximately 500 million years ago, two types of recombinatorial adaptive immune systems (AISs) arose in vertebrates. The jawed vertebrates diversify their repertoire of immunoglobulin domain-based T and B cell antigen receptors mainly through the rearrangement of V(D)J gene segments and somatic hypermutation, but none of the fundamental AIS recognition elements in jawed vertebrates have been found in jawless vertebrates. Instead, the AIS of jawless vertebrates is based on variable lymphocyte receptors (VLRs) that are generated through recombinatorial usage of a large panel of highly diverse leucine-rich-repeat (LRR) sequences.[19] Three VLR genes (VLRA, VLRB, and VLRC) have been identified in lampreys and hagfish, and are expressed on three distinct lymphocytes lineages. VLRA+ cells and VLRC+ cells are T-cell-like and develop in a thymus-like lympho-epithelial structure, termed thymoids. VLRB+ cells are B-cell-like, develop in hematopoietic organs, and differentiate into \"VLRB antibody\"-secreting plasma cells.[20] Myxini (hagfish) are eel-shaped slime-producing marine animals (occasionally called slime eels). They are the only known living animals that have a skull but not a vertebral column. Along with lampreys, hagfish are jawless and are living fossils; hagfish are basal to vertebrates, and living hagfish remain similar to hagfish 300 million years ago.[21] The classification of hagfish has been controversial. The issue is whether the hagfish is itself a degenerate type of vertebrate-fish (most closely related to lampreys), or else may represent a stage which precedes the evolution of the vertebral column (as do lancelets). The original scheme groups hagfish and lampreys together as cyclostomes (or historically, Agnatha), as the oldest surviving clade of vertebrates alongside gnathostomes (the now-ubiquitous jawed-vertebrates). An alternative scheme proposed that jawed-vertebrates are more closely related to lampreys than to hagfish (i.e., that vertebrates include lampreys but exclude hagfish), and introduces the category craniata to group vertebrates near hagfish. Recent DNA evidence has supported the original scheme.[9] Hyperoartia is a disputed group of vertebrates that includes the modern lampreys and their fossil relatives. Examples of hyperoartians from early in their fossil record are Endeiolepis and Euphanerops, fish-like animals with hypocercal tails that lived during the Late Devonian Period. Some paleontologists still place these forms among the \"ostracoderms\" (jawless armored \"fishes\") of the classAnaspida, but this is increasingly considered an artificial arrangement based on ancestral traits. Placement of this group among the jawless vertebrates is a matter of dispute. While today enough fossil diversity is known to make a close relationship among the \"ostracoderms\" unlikely, this has muddied the issue of the Hyperoartia's closest relatives. Traditionally the group was placed in a superclass Cyclostomata together with the Myxini (hagfishes). More recently, it has been proposed that the Myxini are more basal among the skull-bearing chordates, while the Hyperoartia are retained among vertebrates. But even though this may be correct, the lampreys represent one of the oldest divergences of the vertebrate lineage, and whether they are better united with some \"ostracoderms\" in the Cephalaspidomorphi, or not closer to these than to e.g. to other \"ostracoderms\" of the Pteraspidomorphi, or even the long-extinct conodonts, is still to be resolved. Even the very existence of the Hyperoartia is disputed, with some analyses favoring a treatment of the \"basal Hyperoartia\" as a monophyletic lineage Jamoytiiformes that may in fact be very close to the ancestral jawed vertebrates. Conodonts were eel like agnathans that lived from the Cambrian up until the beginning of the Jurassic period. They were very diverse in terms of lifestyles, with some species being filter feeders and others being macropredators. For over a century, these animals were only known because of their microscopic, phosphatic tooth structures called \"Conodont elements\". It wasn't until the mid-1980s that body fossils of conodonts were found in Scotland and Wisconsin, showing these animals true appearance. Their teeth make great index fossils, as many species lived and died out in a relatively short period of time. These fish reached their peak in diversity during the middle of the Ordovician, but were hit hard by the Ordovician-Silurian extinction event. They then reached another spike in diversity in the mid-late Devonian before again declining in the Carboniferous. They were relatively rare in the Permian, but dramatically increased in numbers in the early Triassic. Despite this, they went extinct during the lower Jurassic period, with some of the last surviving populations being in Japan. They possibly survived longer there due to the relative remoteness of the area. Originally, it was thought that they were wiped out by the large extinction at the end of the Triassic. Instead, it is now thought that they were out competed by newer Mesozoic taxa.[24][25][26][27][28][29] †Pteraspidomorphi is an extinct group of early jawless fish. The fossils show extensive shielding of the head. Many had hypocercal tails in order to generate lift to increase ease of movement through the water for their armoured bodies, which were covered in dermal bone. They also had sucking mouth parts and some species may have lived in fresh water. Thelodonti(nipple teeth) are a group of small, extinct jawless fishes with distinctive scales instead of large plates of armour. There is much debate over whether the group of Palaeozoic fish known as the Thelodonti (formerly coelolepids[30]) represent a monophyletic grouping, or disparate stem groups to the major lines of jawless and jawed fish. Thelodonts are united in possession of \"thelodont scales\". This defining character is not necessarily a result of shared ancestry, as it may have been evolved independently by different groups. Thus the thelodonts are generally thought to represent a polyphyletic group,[31] although there is no firm agreement on this point; if they are monophyletic, there is no firm evidence on what their ancestral state was.[32]: 206 \"Thelodonts\" were morphologically very similar, and probably closely related, to fish of the classes Heterostraci and Anaspida, differing mainly in their covering of distinctive, small, spiny scales. These scales were easily dispersed after death; their small size and resilience makes them the most common vertebrate fossil of their time.[33][34] The fish lived in both freshwater and marine environments, first appearing during the Ordovician, and perishing during the Frasnian–Famennian extinction event of the Late Devonian. They occupied a large variety of ecological niches, with a large amount of species preferring reef ecosystems, where their flexible bodies were more at ease than the heavily armoured bulks of other jawless fish.[35] Anaspida(without shield) is an extinct group of primitive jawless vertebrates that lived during the Silurian and Devonian periods.[36] They are classically regarded as the ancestors of lampreys.[37] Anaspids were small marine agnathans that lacked heavy bony shield and paired fins, but have a striking highly hypocercal tail. They first appeared in the Early Silurian, and flourished until the Late Devonian extinction,[38] where most species, save for lampreys, became extinct due to the environmental upheaval during that time. Cephalaspidomorphi is a broad group of extinct armored agnathans found in Silurian and Devonian strata of North America, Europe, and China, and is named in reference to the osteostracan genus Cephalaspis. Most biologists regard this taxon as extinct, but the name is sometimes used in the classification of lampreys, as lampreys are sometimes thought to be related to cephalaspids. If lampreys are included, they would extend the known range of the group from the early Silurian period through the Mesozoic, and into the present day. Cephalaspidomorphi were, like most contemporary fish, very well armoured. Particularly the head shield was well developed, protecting the head, gills and the anterior section of the innards. The body was in most forms well armoured as well. The head shield had a series of grooves over the whole surface forming an extensive lateral line organ. The eyes were rather small and placed on the top of the head. There was no proper jaw. The mouth opening was surrounded by small plates making the lips flexible, but without any ability to bite.[39] Undisputed subgroups traditionally contained with Cephaloaspidomorphi, also called \"Monorhina\", include the classesOsteostraci, Galeaspida, and Pituriaspida While the \"agnatha\" Conodonta was indeed jawless, if it would have continued to live, its descendants would still be closer related to e.g. humans than to lampreys, and also contempory it was closer related to the ancestor of humans. Due to such considerations, Agnatha can not be consolidated into a coherent grouping without either removing any non-cyclostomata, or by including all vertebrata thus rendering it into a junior synonym of vertebrata. The new phylogeny from Miyashita et al. (2019) is considered compatible with both morphological and molecular evidence.[42] ^ abJanvier, P. (November 2010). \"MicroRNAs revive old views about jawless vertebrate divergence and evolution\". Proceedings of the National Academy of Sciences of the United States of America. 107 (45): 19137–19138. Bibcode:2010PNAS..10719137J. doi:10.1073/pnas.1014583107. PMC2984170. PMID21041649. Although I was among the early supporters of vertebrate paraphyly, I am impressed by the evidence provided by Heimberg et al. and prepared to admit that cyclostomes are, in fact, monophyletic. The consequence is that they may tell us little, if anything, about the dawn of vertebrate evolution, except that the intuitions of 19th century zoologists were correct in assuming that these odd vertebrates (notably, hagfishes) are strongly degenerate and have lost many characters over time. ^Stanley, Steven M.; Luczaj, John A. (2015). Earth System History (4th ed.). Macmillan Education. p. 311. Conodonts arose late in the Early Cambrian and diversified into the Ordovician. ... Similar small teeth in very early Cambrian faunas ... may represent conodont ancestors."}
{"url": "https://en.wikipedia.org/wiki/World_Wide_Fund_for_Nature", "text": "WWF aims to \"stop the degradation of the planet's natural environment and to build a future in which humans live in harmony with nature.\"[10] The Living Planet Report has been published every two years by WWF since 1998; it is based on a Living Planet Index and ecological footprint calculation.[5] In addition, WWF has launched several notable worldwide campaigns, including Earth Hour and Debt-for-nature swap, and its current work is organized around these six areas: food, climate, freshwater, wildlife, forests, and oceans.[5][7] WWF has faced criticism for its corporate ties[11][12][13] and for supporting conservation measures that have resulted in violent conflict with local people.[14][15] WWF is part of the Steering Group of the Foundations Platform F20, an international network of foundations and philanthropic organizations.[16] The WWF was conceived to act as an international fundraising organisation to support the work of existing conservation groups, primarily the International Union for Conservation of Nature.[19] Its establishment was marked with the signing of the Morges Manifesto, the founding document that sets out the fund's commitment to assisting worthy organizations struggling to save the world's wildlife:[20] They need above all money, to carry out mercy missions and to meet conservation emergencies by buying land where wildlife treasures are threatened, and in many other ways. Money, for example, to pay guardians of wildlife refuges .... Money for education and propaganda among those who would care and help if only they understood. Money to send out experts to danger spots and to train more local wardens and helpers in Africa and elsewhere. Money to maintain a sort of 'war room' at the international headquarters of conservation, showing where the danger spots are and making it possible to ensure that their needs are met before it is too late. The WWF has set up offices and operations around the world. It originally worked by fundraising and providing grants to existing non-governmental organizations with an initial focus on the protection of endangered species. As more resources became available, its operations expanded into other areas such as the preservation of biological diversity, sustainable use of natural resources, the reduction of pollution, and climate change. The organization also began to run its own conservation projects and campaigns.[citation needed] In 1986, the organization changed its name to World Wide Fund for Nature, while retaining the WWF initials. However, it continued at that time to operate under the original name in the United States and Canada.[26] 1986 was the 25th anniversary of WWF's foundation, an event marked by a gathering in Assisi, Italy to which the organization's International President Prince Philip, the Duke of Edinburgh, invited religious authorities representing Buddhism, Christianity, Hinduism, Islam and Judaism. These leaders produced The Assisi Declarations, theological statements showing the spiritual relationship between their followers and nature that triggered a growth in the engagement of those religions with conservation around the world.[26] Stop the degradation of the planet's natural environment and to build a future in which humans live in harmony with nature, conserving the world's biological diversity; ensuring that the use of renewable natural resources is sustainable; [and] promoting the reduction of pollution and wasteful consumption. WWF researchers and many others identified 238 ecoregions that represent the world's most biologically outstanding terrestrial, freshwater and marine habitats, based on a worldwide biodiversity analysis which the organization says was the first of its kind.[27] In the early 2000s (decade), its work was focused on a subset of these ecoregions, in the areas of forest, freshwater and marine habitat conservation, endangered species conservation, climate change, and the elimination of the most toxic chemicals. We shan't save all we should like to, but we shall save a great deal more than if we had never tried. Harvard University published a case study on WWF called \"Negotiating Toward the Paris Accords: WWF & the Role of Forests in the 2015 Climate Agreement\":[28] In 2023, Prof. Adil Najam, a globally renowned climate scientist and policy expert from Pakistan, was appointed as the President of WWF International signifying the growing importance on climate change as well as of human well-being in the WWF agenda.[29] In 1947 the Conservation Foundation was formed in New York City by Fairfield Osborn. It arranged funding for scientific research into global conservation issues. It did not lobby or engage in politics. In 1985 it became an affiliate of the WWF. In 1990, it completely merged into WWF.[5] WWF's giant panda logo originated from a panda named Chi Chi that had been transferred from Beijing Zoo to London Zoo in 1958, three years before WWF was established. Being famous as the only panda residing in the Western world at that time, her uniquely recognisable physical features and status as an endangered species were seen as ideal to serve the organization's need for a strong recognisable symbol that would overcome all language barriers.[32] The organization also needed an animal that would have an impact in black and white printing. The logo was then designed by Sir Peter Scott from preliminary sketches by Gerald Watterson, a Scottish naturalist.[33][34] The logo was slightly simplified and made more geometric in 1978, and was stylized and made less detailed in 1986, at the time that the organization changed its name, with the new version featuring solid black shapes for eyes.[35] In 2000 a change was made to the font used for the initials \"WWF\" in the logo.[36] Policies of the WWF are made by board members elected for three-year terms. An Executive Team guides and develops WWF's strategy. There is also a National Council which stands as an advisory group to the board and a team of scientists and experts in conservation who research for WWF. National and international law plays an important role in determining how habitats and resources are managed and used. Laws and regulations become one of the organization's global priorities. The WWF has been opposed to the extraction of oil from the Canadian tar sands and has campaigned on this matter. Between 2008 and 2010 the WWF worked with The Co-operative Group, the UK's largest consumer co-operative to publish reports which concluded that: (1) exploiting the Canadian tar sands to their full potential would be sufficient to bring about what they described as 'runaway climate change;[37] (2) carbon capture and storage (CCS) technology cannot be used to reduce the release of carbon dioxide into the atmosphere to a level comparable to that of other methods of oil extraction;[38] (3) the $379 billion which is expected to be spent extracting oil from tar sands could be better spent on research and development in renewable energy technology;[39] and (4) the expansion of tar sands extraction poses a serious threat to the caribou in Alberta .[40] The organization convinces and helps governments and other political bodies to adopt, enforce, strengthen and/or change policies, guidelines and laws that affect biodiversity and natural resource use. It also ensures government consent and/or keeps their commitment to international instruments relating to the protection of biodiversity and natural resources.[41][42] In 2012, David Nussbaum, Chief Executive of WWF-UK, spoke out against the way shale gas is used in the UK, saying: \"...the Government must reaffirm its commitment to tackling climate change and prioritise renewables and energy efficiency.\"[43] The organisation works on a number of global issues driving biodiversity loss and unsustainable use of natural resources, including species conservation, finance, business practices, laws, and consumption choices. Local offices also work on national or regional issues.[44] WWF works with a large number of different groups to achieve its goals, including other NGOs, governments, business, investment banks, scientists, fishermen, farmers and local communities. It also undertakes public campaigns to influence decision makers, and seeks to educate people on how to live in a more environmentally friendly manner. It urges people to donate funds to protect the environment. The donors can also choose to receive gifts in return.[citation needed] In October 2020, WWF was named as one of the alliance partner's of Prince William's Earthshot Prize, to find solutions to environmental issues.[45] In March 2021, WWF announced an extension of their partnership with H&M to address sustainable supply chain practices.[46] WWF publishes the Living Planet Index in collaboration with the Zoological Society of London. Along with ecological footprint calculations, the Index is used to produce a bi-yearly Living Planet Report giving an overview of the impact of human activity on the world.[47] In 2019, WWF and Knorr jointly published the Future 50 Foods report identifying \"50 Foods for Healthier People and a Healthier Planet\".[48] In 2018, WWF, TRAFFIC and IFAW launched the Coalition to End Wildlife Trafficking Online with 21 tech companies. [49] In 2017, Instagram accounts, Sal Lavallo and Jessica Nabongo ate a trafficked, endangered pangolin at a hotel in Gabon.[50] There is often no penalty to social media accounts for cruelty to animals on social media platforms.[51][52] The organization also regularly publishes reports, fact sheets and other documents on issues related to its work, to raise awareness and provide information to policy and decision makers.[53] Peter Rose and Anne Conlon are music theatre writers, well known for their environmental musicals for children, who were commissioned by WWF-UK to write several environmental musicals as part of an education plan.[55] Some were narrated by David Attenborough, and broadcast on television in numerous countries. The British pop group S Club 7 were ambassadors for WWF-UK during their time together as a band (1999–2003).[56] Each of the members sponsored an endangered animal, and in 2000, traveled to the various locations around the world of their chosen animals for a seven-part BBC documentary series entitled S Club 7 Go Wild. From 1997 to 2007, the WWF's China office conducted its Environmental Educators' Initiative, which trained thousands of teachers, established environmental education training centers at teachers' universities, and influenced the drafting of the Ministry of Education's 2003 guidelines for public school environmental education.[60]: 145 The German public television ARD aired a documentary on 22 June 2011 that claimed to show how the WWF cooperates with corporations such as Monsanto, providing sustainability certification in exchange for donations– essentially greenwashing.[61] WWF has denied the allegations.[62] By encouraging high-impact eco-tourism, the program alleges that WWF contributes to the destruction of habitat and species it claims to protect while also harming indigenous peoples.[63] The filmmaker, German investigative journalist Wilfried Huismann, was sued by the WWF over his documentary and the book Schwarzbuch WWF published in 2012, which was based on the documentary. In an out of court settlement, he agreed to remove or revise certain claims. Speaking on behalf of WWF Germany, Marco Vollmar indicated \"[Huismann] draws a distorted picture of false statements, defamations and exaggerations, but we will accept that as expressions of opinion.\" (Translated from the original German: \"ein Zerrbild aus falschen Aussagen, Diffamierungen und Übertreibungen, aber das werden wir als Meinungsäußerungen hinnehmen.\")[64] In 2014, Huismann published a revised edition of his 2012 book, originally called The Silence of the Pandas. The original edition had become a bestseller in Germany, but was banned from Britain until 2014, when it was released under the title of PandaLeaks – The Dark Side of the WWF, after a series of injunctions and court orders.[65] The book criticizes WWF for its involvement with corporations that are responsible for large-scale destruction of the environment, such as Coca-Cola, and gives details into the existence of the secret 1001 Club, whose members, Huismann claims, continue to have an unhealthy influence on WWF's policy making.[65] WWF has denied the allegations made against it.[66] WWF has been accused by the campaigner Corporate Watch of being too close to business to campaign objectively.[11][12] WWF claims partnering with corporations such as Coca-Cola, Lafarge, Carlos Slim's and IKEA will reduce their effect on the environment.[67] WWF received €56 million (US$80 million) from corporations in 2010 (an 8% increase in support from corporations compared to 2009), accounting for 11% of total revenue for the year.[9] For their 2019 fiscal year, WWF reported 4% of their total operating revenue coming from corporations.[68] In 2017, a report by Survival International claimed that WWF-funded paramilitaries are not only committing abuses against the indigenous Baka and Bayaka in the Congo Basin, who \"face harassment and beatings, torture and death\", but are also corrupt and aid in the destruction of conserved areas. The report accused WWF and its guards of partnering with several logging companies who carried out deforestation, while the rangers ignored wildlife trafficking networks.[69] In 2019, an investigation by BuzzFeed News alleged that paramilitary groups funded by the organisation are engaged in serious human rights abuses against villagers, and the organisation has covered up the incidents and acted to protect the perpetrators from law enforcement. These armed groups were claimed to torture, sexually assault, and execute villagers based on false accusations. In one instance found by BuzzFeed News investigators, an 11-year-old boy was allegedly tortured by WWF-funded rangers in front of his parents;[70] WWF ignored all complaints against the rangers. In another incident, a ranger attempted to rape a Tharu woman and, when she resisted, attacked her with bamboo stick until she lost consciousness. While the ranger was arrested, the woman was pressured not to press charges, resulting in the ranger going free. In 2010, WWF-sponsored rangers reportedly killed a 12-year-old girl who was collecting tree bark in Bardiya National Park. Park and WWF officials allegedly obstructed investigations in these cases, by \"falsifying and destroying evidence, falsely claiming the victims were poachers, and pressuring the families of the victims to withdraw criminal complaints\".[70][71] In July 2019, Buzzfeed reported that a leaked report by the WWF accused guards of beating and raping women including pregnant women while torturing men by tying their penises with fishing lines. The investigations were cut short after paramilitary groups threatened investigators with death. The investigators accused WWF of covering up the crimes. Releasing an official statement, the WWF claimed that the report was not made public to ensure the safety of the victims and that the guards were suspended and are awaiting prosecution. However Buzzfeed accused the WWF of attempting to withhold the report to the US congressional committee investigating the human rights violations by providing highly redacted versions instead.[72][73] In the Central African Republic, WWF officials were reportedly involved in an arms deal, where the organization paid for 15 Kalashnikov assault rifles and ammunition; but part of the money went unaccounted for and they were apparently defrauded by the CAR army representatives selling the weapons.[70] The Kathmandu Post, which cooperated with BuzzFeed News on the investigations in Nepal, claimed there was intense lobbying and political pressure to release WWF-funded rangers arrested for murder. They interviewed activists who claimed they were promised donations for pressuring victims of abuse to drop charges against the rangers. When the local Tharu community protested, WWF officials carried out a counter-protest in favour of the accused and used park elephants to block Prithvi Highway.[74] In reply to the investigations, WWF stated that it takes any allegations seriously and would be launching an independent review into the cases raised. The organisation stated it has stringent policies designed to ensure it and its partners are safeguarding the rights and well-being of indigenous peoples and local communities, and should the review uncover any breaches, it is committed to taking swift action.[76] These accusations were central to a four day sit-in protest carried out by members of Extinction Rebellion's XR Youth Solidarity Network at WWF-UK's headquarters in September 2021.[77] In 2000, the World Wide Fund for Nature sued the World Wrestling Federation (now named WWE) for unfair trade practices. Both parties had shared the initials \"WWF\" since 1979. The conservation organization claimed that the professional wrestling company had violated a 1994 agreement regarding international use of the WWF initials.[78][79] On 10 August 2001, a UK court ruled in favour of the World Wide Fund for Nature. The World Wrestling Federation filed an appeal in October 2001, but later withdrew their appeal. On 5 May 2002, the World Wrestling Federation changed its Web address from WWF.com to WWE.com, and replaced every \"WWF\" reference on the existing site with \"WWE\", officially announcing their name change to \"World Wrestling Entertainment\" a day later with a \"Get the 'F' Out\" marketing campaign. The company's stock ticker also switched from WWF to WWE shortly after. The wrestling organization's abandonment of \"WWF\" initialism did not end the two organizations' legal conflict. Later in 2002, the World Wide Fund for Nature petitioned the court for $360 million in damages, but was not successful. A subsequent request to overturn by the World Wide Fund for Nature was dismissed by the British Court of Appeal on 28 June 2007. In 2003, World Wrestling Entertainment won a limited decision which permitted them to continue marketing certain pre-existing products with the abandoned WWF logo. However, WWE was mandated to issue newly branded merchandise such as apparel, action figures, video games, and DVDs with the \"WWE\" initials. Additionally, the court order required the company to remove both auditory and visual references to \"WWF\" in its library of video footage outside the United Kingdom. Starting with the 1,000th episode of Raw in July 2012, the WWF \"scratch\" logo is no longer censored in archival footage. In addition, the WWF initials are no longer censored when spoken or when written in plain text in archival footage. In exchange, WWE is no longer permitted to use WWF initials or logo in any new, original footage, packaging, or advertising, with any old-school logos for retro-themed programming now using a modification of the original WWF logo without the F. In June 2009, Touch Seang Tana, chairman of Cambodia's Commission for Conservation and Development of the Mekong River Dolphins Eco-tourism Zone, argued that the WWF had misrepresented the danger of extinction of the Mekong dolphin to boost fundraising.[80] The report stated that the deaths were caused by a bacterial disease that became fatal due to environmental contaminants suppressing the dolphins' immune systems.[81] He called the report unscientific and harmful to the Cambodian government and threatened WWF's Cambodian branch with suspension unless they met with him to discuss his claims.[82] Touch Seang Tana later said he would not press charges of supplying false information and would not make any attempt to prevent WWF from continuing its work in Cambodia, but advised WWF to adequately explain its findings and check with the commission before publishing another report. Criticism of the validity of reports critical of government action or inaction, where 'approval' has not been sought before publication, is common in Cambodia.[83] In January 2012, Touch Seang Tana signed the \"Kratie Declaration on the Conservation of the Mekong River Irrawaddy Dolphin\" along with WWF and the Cambodian Fisheries Administration, an agreement binding the parties to work together on a \"roadmap\" addressing dolphin conservation in the Mekong River.[84] In 2009, in a scorecard report that they authored on carbon emissions in G8 countries, the WWF portrayed the greenhouse gas emissions of countries who use low-carbonnuclear power in their mix as a higher amount of emissions than realistically calculated. For example, for France, the WWF displayed a false value of 362 gCO2eq/kWh which is over 400% larger than the actual emissions in France. WWF explained the manipulation as follows:[86][87] WWF does not consider nuclear power to be a viable policy option. The indicators \"emissions per capita\", \"emissions per GdP\" and \"Co2 per kWh electricity\" for all countries have therefore been adjusted as if the generation of electricity from nuclear power had produced 350 gCo2/kWh (emission factor for natural gas). Without the adjustment, the original indicators for France would have been much lower, e.g. 86 gCo2/kWh. The scorecard for Sweden was also \"adjusted\" in similar way, where the WWF replaced the actual emissions of 47 gCO2eq/kWh with 212 gCO2eq/kWh.[87] In 2011 Jochen Lamp, head of WWF Germany, was also head of Conservation Foundation German Baltic, sponsored by Nord Stream AG company building a controversial gas pipeline from Russia to Germany. While WWF headed by Lamp has been actively blocking the project using court cases, Nord Stream reached \"an out-of-court agreement\" with the Foundation, also headed by Lamp, involving transfer of 10 million EUR, after which WWF withdrew the case.[88][89] Investigative journalism by NBC and later Naomi Klein, in 2008 and 2013 respectively, uncovered that the WWF has invested and profits from, multi-million dollar investment contracts it has put into oil, gas, coal and tar sands developments and did not pull out of these, divesting, when confronted but indicated it would at the minimum wait until 2020 to do so, in some of its fossil fuel ventures, as early ending would have not been as profitable for them. The WWF does not oppose fossil fuels but engages in what it internally terms as the \"responsible development\" of fossil fuels.[90] In February 2022, WWF UK released plans to raise funds through selling NFTs (non-fungible tokens).[91] NFT is a unit of data stored on a blockchain. Critics point out transacting NFTs causes significant environmental impact.[92][93] Listing as a \"foreign agent\" and an \"undesirable organisation\" in Russia[edit] The Australian arm of WWF was established on 29 June 1978 in an old factory in Sydney, with three staff and a budget of around A$80,000 for the first year, consisting of a A$50,000 grant from the Commonwealth Government and a further A$20,500 in corporate donations. As of 2020[update], WWF-Australia is the country's biggest conservation organisation, which operates projects throughout Australia as well as the wider Oceania region.[96] Between 2015 and 2019 WWF-Australia reported an average revenue of $28.74 million per year. In 2020, WWF-Australia reported a total revenue of over $80 million driven by the global & local response to the Australian bushfires.[97] ^Scott, P. (1965). The launching of a new ark: first report of the President and Trustees of the World Wildlife Fund, an international foundation for saving the world's wildlife and wild places and countries; 1962–1965 (Collins)."}
{"url": "https://en.wikipedia.org/wiki/Telencephalon", "text": "The cerebrum is the largest part of the brain. Depending upon the position of the animal it lies either in front or on top of the brainstem. In humans, the cerebrum is the largest and best-developed of the five major divisions of the brain. The cerebrum is made up of the two cerebral hemispheres and their cerebral cortices (the outer layers of grey matter), and the underlying regions of white matter.[2] Its subcortical structures include the hippocampus, basal ganglia and olfactory bulb. The cerebrum consists of two C-shaped cerebral hemispheres, separated from each other by a deep fissure called the longitudinal fissure. The cerebral cortex, the outer layer of grey matter of the cerebrum, is found only in mammals. In larger mammals, including humans, the surface of the cerebral cortex folds to create gyri (ridges) and sulci (furrows) which increase the surface area.[3] In the developing vertebrate embryo, the neural tube is subdivided into four unseparated sections which then develop further into distinct regions of the central nervous system; these are the prosencephalon (forebrain), the mesencephalon (midbrain) the rhombencephalon (hindbrain) and the spinal cord.[7] The prosencephalon develops further into the telencephalon and the diencephalon. The dorsal telencephalon gives rise to the pallium (cerebral cortex in mammals and reptiles) and the ventral telencephalon generates the basal ganglia. The diencephalon develops into the thalamus and hypothalamus, including the optic vesicles (future retina).[8] The dorsal telencephalon then forms two lateral telencephalic vesicles, separated by the midline, which develop into the left and right cerebral hemispheres. Birds and fish have a dorsal telencephalon, like all vertebrates, but it is generally unlayered and therefore not considered a cerebral cortex. Only a layered cytoarchitecture can be considered a cortex. Note: As cerebrum is a gross division with many subdivisions and sub-regions, it is important to state that this section lists functions that cerebrum as a whole serves. See main articles on cerebral cortex and basal ganglia for more information. The cerebrum is a major part of the brain, controlling emotions, hearing, vision, personality and much more. It controls all precision of voluntary actions. The olfactory bulb, responsible for the sense of smell, takes up a large area of the cerebrum in most vertebrates. However, in humans, this part of the brain is much smaller and lies underneath the frontal lobe. The olfactory sensory system is unique since the neurons in the olfactory bulb send their axons directly to the olfactory cortex, rather than to the thalamus first. The olfaction is also the only sense that is represented by the ipsilateral side of the brain. Damage to the olfactory bulb results in a loss of olfaction (the sense of smell). Speech and language are mainly attributed to the parts of the cerebral cortex. Motor portions of language are attributed to Broca's area within the frontal lobe. Speech comprehension is attributed to Wernicke's area, at the temporal-parietal lobe junction. These two regions are interconnected by a large white matter tract, the arcuate fasciculus. Damage to the Broca's area results in expressive aphasia (non-fluent aphasia) while damage to Wernicke's area results in receptive aphasia (also called fluent aphasia). Explicit or declarative (factual) memory formation is attributed to the hippocampus and associated regions of the medial temporal lobe. This association was originally described after a patient known as HM had both his left and right hippocampus surgically removed to treat chronic [temporal lobe epilepsy]. After surgery, HM had anterograde amnesia, or the inability to form new memories. Implicit or procedural memory, such as complex motor behaviors, involves the basal ganglia. In the most primitive vertebrates, the hagfishes and lampreys, the cerebrum is a relatively simple structure receiving nerve impulses from the olfactory bulb. In cartilaginous and lobe-finned fishes and also in amphibians, a more complex structure is present, with the cerebrum being divided into three distinct regions. The lowermost (or ventral) region forms the basal nuclei, and contains fibres connecting the rest of the cerebrum to the thalamus. Above this, and forming the lateral part of the cerebrum, is the paleopallium, while the uppermost (or dorsal) part is referred to as the archipallium. The cerebrum remains largely devoted to olfactory sensation in these animals, in contrast to its much wider range of functions in amniotes.[9] In ray-finned fishes the structure is somewhat different. The inner surfaces of the lateral and ventral regions of the cerebrum bulge up into the ventricles; these include both the basal nuclei and the various parts of the pallium and may be complex in structure, especially in teleosts. The dorsal surface of the cerebrum is membranous, and does not contain any nervous tissue.[9] In the amniotes, the cerebrum becomes increasingly large and complex. In reptiles, the paleopallium is much larger than in amphibians and its growth has pushed the basal nuclei into the central regions of the cerebrum. As in the lower vertebrates, the grey matter is generally located beneath the white matter, but in some reptiles, it spreads out to the surface to form a primitive cortex, especially in the anterior part of the brain.[9] In mammals, this development proceeds further, so that the cortex covers almost the whole of the cerebral hemispheres, especially in more developed species, such as the primates. The paleopallium is pushed to the ventral surface of the brain, where it becomes the olfactory lobes, while the archipallium becomes rolled over at the medial dorsal edge to form the hippocampus. In placental mammals, a corpus callosum also develops, further connecting the two hemispheres. The complex convolutions of the cerebral surface (see gyrus, gyrification) are also found only in higher mammals.[9] Although some large mammals (such as elephants) have particularly large cerebra, dolphins are the only species (other than humans) to have cerebra accounting for as much as 2 percent of their body weight.[10] The cerebra of birds are similarly enlarged to those of mammals, by comparison with reptiles. The increased size of bird brains was classically attributed to enlarged basal ganglia, with the other areas remaining primitive, but this view has been largely abandoned.[11] Birds appear to have undergone an alternate process of encephalization,[12] as they diverged from the other archosaurs, with few clear parallels to that experienced by mammals and their therapsid ancestors."}
{"url": "https://en.wikipedia.org/wiki/War_of_the_Triple_Alliance", "text": "The Paraguayan War, also known as the War of the Triple Alliance,[a] was a South American war that lasted from 1864 to 1870. It was fought between Paraguay and the Triple Alliance of Argentina, the Empire of Brazil, and Uruguay. It was the deadliest and bloodiest inter-state war in Latin American history.[6] Paraguay sustained large casualties, but the approximate numbers are disputed. Paraguay was forced to cede disputed territory to Argentina and Brazil. The war began in late 1864, as a result of a conflict between Paraguay and Brazil caused by the Uruguayan War. Argentina and Uruguay entered the war against Paraguay in 1865, and it then became known as the \"War of the Triple Alliance.\" After Paraguay was defeated in conventional warfare, it conducted a drawn-out guerrilla resistance, a strategy that resulted in the further destruction of the Paraguayan military and the civilian population. Much of the civilian population died due to battle, hunger, and disease. The guerrilla war lasted for 14 months until president Francisco Solano López was killed in action by Brazilian forces in the Battle of Cerro Corá on 1 March 1870. Argentine and Brazilian troops occupied Paraguay until 1876. Background Territorial disputes The Platine region in 1864. The shaded areas are disputed territories. Since their independence from Portugal and Spain in the early 19th century, the Empire of Brazil and the Spanish-American countries of South America were troubled by territorial disputes. Each nation in this region had boundary conflicts with multiple neighbors. Most had overlapping claims to the same territories, due to unresolved questions which stemmed from their former metropoles. Signed by Portugal and Spain in 1494, the Treaty of Tordesillas proved ineffective in the following centuries, as both colonial powers expanded their frontiers in South America and elsewhere. The outdated boundary lines did not represent the actual occupation of lands by the Portuguese and Spanish. By the early 1700s, the Treaty of Tordesillas was deemed not useful, and it was clear to both parties that a newer treaty had to be drawn based on feasible boundaries. In 1750, the Treaty of Madrid separated the Portuguese and Spanish areas of South America in lines that mostly corresponded to present-day boundaries. Neither Portugal nor Spain was satisfied with the results, and new treaties were signed in the following decades that either established new territorial lines or repealed them. The final accord signed by both powers, the 1801 Treaty of Badajoz, reaffirmed the validity of the previous Treaty of San Ildefonso (1777), which had derived from the older Treaty of Madrid. The territorial disputes became worse when the Viceroyalty of the Río de la Plata collapsed in the early 1810s, leading to the rise of Argentina, Paraguay, Bolivia, and Uruguay. Historian Pelham Horton Box wrote: \"Imperial Spain bequeathed to the emancipated Spanish-American nations not only her own frontier disputes with Portuguese Brazil but problems which had not disturbed her, relating to the exact boundaries of her own viceroyalties, captaincies general, audiencias and provinces.\"[7] Once separated the three countries quarreled over lands that were mostly uncharted or unknown. They were either sparsely populated or settled by indigenous tribes that answered to no parties.[8][9] In the case of Paraguay and Brazil, the problem was to define whether the Apa or Branco rivers should represent their actual boundary, a persistent issue that had confused Spain and Portugal in the late 18th century. A few indigenous tribes populated the region between the two rivers, and these tribes would attack Brazilian and Paraguayan settlements that were local to them.[10][11] Political situation before the war There are several theories regarding the origins of the war. The traditional view emphasizes that the policies of Paraguayan president Francisco Solano López used the Uruguayan War as a pretext to gain control of the Platine basin. That caused a response from the regional hegemons, Brazil and Argentina, both of which exercised influence over the much smaller republics of Uruguay and Paraguay. The war has also been attributed to the aftermath of colonialism in South America with border conflicts between the new states, the struggle for power among neighboring nations over the strategic Río de la Plata region, Brazilian and Argentine meddling in internal Uruguayan politics (which had already caused the Platine War), Solano López's efforts to help his allies in Uruguay (which had been defeated by the Brazilians), and his presumed expansionist ambitions.[12] A strong military was developed because Paraguay's larger neighbors, Argentina and Brazil, had territorial claims against it and wanted to dominate it politically, much as both had already done in Uruguay. Paraguay had recurring boundary disputes and tariff issues with Argentina and Brazil for many years during the rule of Solano Lopez's predecessor and father, Carlos Antonio López. Regional tension In the time since Brazil and Argentina had become independent, their struggle for hegemony in the Río de la Plata region had profoundly marked the diplomatic and political relations among the countries of the region.[13] Brazil was the first country to recognize the independence of Paraguay, in 1844. At this time Argentina still considered it a breakaway province. While Argentina was ruled by Juan Manuel Rosas (1829–1852), a common enemy of both Brazil and Paraguay, Brazil contributed to the improvement of the fortifications and development of the Paraguayan army, sending officials and technical help to Asunción. As no roads linked the inland province of Mato Grosso to Rio de Janeiro, Brazilian ships needed to travel through Paraguayan territory, going up the Paraguay River to arrive at Cuiabá. However, Brazil had difficulty obtaining permission from the government in Asunción to freely use the Paraguay River for its shipping needs. in 1855, at the request of the Uruguayan government and Venancio Flores, leader of the Colorado Party, which was traditionally supported by the Brazilian Empire; in 1864, against Atanasio Aguirre. This last intervention would lead to the Paraguayan War. On 19 April 1863, Uruguayan general Venancio Flores, who was then an officer in the Argentine army as well as the leader of the Colorado Party of Uruguay,[14] invaded his country, starting the Cruzada Libertadora with the open support of Argentina, which supplied the rebels with arms, ammunition and 2,000 men.[15] Flores wanted to overthrow the Blanco Party government of president Bernardo Berro,[16]: 24 which was allied with Paraguay.[16]: 24 Paraguayan president López sent a note to the Argentine government on 6 September 1863, asking for an explanation, but Buenos Aires denied any involvement in Uruguay.[16]: 24 From that moment, mandatory military service was introduced in Paraguay; in February 1864, an additional 64,000 men were drafted into the army.[16]: 24 One year after the beginning of the Cruzada Libertadora, in April 1864, Brazilian minister José Antônio Saraiva arrived in Uruguayan waters with the Imperial Fleet, to demand payment for damages caused to Rio Grande do Sul farmers in border conflicts with Uruguayan farmers. Uruguayan president Atanasio Aguirre, from the Blanco Party, rejected the Brazilian demands, presented his own demands, and asked Paraguay for help.[17] To settle the growing crisis, Solano López offered himself as a mediator of the Uruguayan crisis, as he was a political and diplomatic ally of the Uruguayan Blancos, but the offer was turned down by Brazil.[18] Brazilian soldiers on the northern borders of Uruguay started to provide help to Flores' troops and harassed Uruguayan officers, while the Imperial Fleet pressed hard on Montevideo.[19] During the months of June–August 1864 a Cooperation Treaty was signed between Brazil and Argentina at Buenos Aires, for mutual assistance in the Plate Basin Crisis.[20] Brazilian minister Saraiva sent an ultimatum to the Uruguayan government on 4 August 1864: either comply with the Brazilian demands, or the Brazilian army would retaliate.[21] The Paraguayan government was informed of all this and sent to Brazil a message, which stated in part: The government of the Republic of Paraguay will consider any occupation of the Oriental territory [i.e. Uruguay] as an attempt against the equilibrium of the states of the Platine Region which interests the Republic of Paraguay as a guarantee for its security, peace, and prosperity; and that it protests in the most solemn manner against the act, freeing itself for the future of every responsibility that may arise from the present declaration. The Brazilian government, probably believing that the Paraguayan threat would be only diplomatic, answered on 1 September, stating that \"they will never abandon the duty of protecting the lives and interests of Brazilian subjects.\" But in its answer, two days later, the Paraguayan government insisted that \"if Brazil takes the measures protested against in the note of August 30th, 1864, Paraguay will be under the painful necessity of making its protest effective.\"[23] On 12 October, despite the Paraguayan notes and ultimatums, Brazilian troops under the command of general João Propício Mena Barreto [pt] invaded Uruguay.[16]: 24 This was not the start of the Paraguayan war, however, for Paraguay continued to maintain diplomatic relations with Brazil for another month. On 11 November the Brazilian ship Marquês de Olinda, on her routine voyage up the River Paraguay to the Brazilian Mato Grosso, and carrying the new governor of that province, docked at Asunción and took on coal. Completing the formalities, she continued on her journey. According to one source, López hesitated whether to break the peace for a whole day, saying \"If we don't have a war now with Brazil, we shall have one at a less convenient time for ourselves\".[24] López then ordered the Paraguayan ship Tacuarí to pursue her and compel her return. On 12 November Tacuarí caught up with Marquês de Olinda in the vicinity of Concepción, fired across her bows, and ordered her to return to Asunción; when she arrived on the 13th, all on board were arrested. On the 12th Paraguay informed the Brazilian minister in Asunción that diplomatic relations had been broken off.[25] The conflict between Brazil and Uruguay was settled in February 1865. News of the war's end was brought by Pereira Pinto and met with joy in Rio de Janeiro. Brazilian emperor Pedro II found himself waylaid by a crowd of thousands in the streets amid acclamations.[26][27] However, public opinion quickly changed for the worse when newspapers began running stories painting the convention of 20 February as harmful to Brazilian interests, for which the cabinet was blamed. The newly promoted Viscount of Tamandaré and Mena Barreto (now Baron of São Gabriel) had supported the peace accord.[28] Tamandaré changed his mind soon afterward and played along with the allegations. A member of the opposition party, José Paranhos, Viscount of Rio Branco, was used as a scapegoat by the emperor and the government and was recalled in disgrace to the imperial capital.[29] The accusation that the convention had failed to meet Brazilian interests proved to be unfounded. Not only had Paranhos managed to settle all Brazilian claims, but by preventing the death of thousands, he gained a willing and grateful Uruguayan ally instead of a dubious and resentful one, which provided Brazil with an important base of operations during the acute clash with Paraguay that shortly ensued.[30] Opposing forces Paraguay According to some historians,[who?] Paraguay began the war with over 60,000 trained men—38,000 of whom were already under arms—400 cannons, a naval squadron of 23 steamboats and five river-navigating ships (among them, the gunboatTacuarí).[31] Communication in the Río de la Plata basin was maintained solely by river, as very few roads existed. Whoever controlled the rivers would win the war, so Paraguay had built fortifications on the banks of the lower end of the Paraguay River.[16]: 28–30 However, recent studies[which?] suggest many problems. Although the Paraguayan army had between 70,000 and 100,000 men at the beginning of the conflict, they were badly equipped. Most infantry armaments consisted of inaccurate smooth-bore muskets and carbines, slow to reload and short-ranged. The artillery was similarly poor. Military officers had no training or experience, and there was no command system, as all decisions were made personally by López. Food, ammunition, and armaments were scarce, with logistics and hospital care deficient or nonexistent.[32] The nation of about 450,000 people could not stand against the Triple Alliance of 11 million people. The Paraguayan army during peacetime prior to the war was made up of eight infantry battalions of 800 men each but had only been able to muster 4,084; five cavalry regiments, nominally 2,500 (2,522 in reality) and two artillery regiments, with 907 men. By March 1865, six new infantry battalions and eight cavalry regiments had been formed. In addition, the Paraguayans could rely on their militia which consisted of all able-bodied men which, as the war continued, began to include increasingly younger and older men.[33] Brazil and its allies At the beginning of the war, the military forces of Brazil, Argentina, and Uruguay were far smaller than Paraguay's. Argentina had approximately 8,500 regular troops and a naval squadron of four steamers and one schooner. Uruguay entered the war with fewer than 2,000 men and no navy. Many of Brazil's 16,000 troops were located in its southern garrisons.[34] The Brazilian advantage, though, was in its navy, comprising 45 ships with 239 cannons and about 4,000 well-trained crew. A great part of the squadron was already in the Rio de la Plata basin, where it had acted under the Marquis of Tamandaré in the intervention against Aguirre's government. Brazil, however, was unprepared to fight a war. Its army was disorganized. The troops it used in Uruguay were mostly armed contingents of gauchos and the National Guard. While some Brazilian accounts of the war described their infantry as volunteers (Voluntários da Pátria), other Argentine revisionist and Paraguayan accounts disparaged the Brazilian infantry as mainly recruited from slaves and the landless (largely black) underclass, who were promised free land for enlisting.[35] The cavalry was formed from the National Guard of Rio Grande do Sul. Ultimately, a total of about 146,000 Brazilians fought in the war from 1864 to 1870, consisting of the 10,025 army soldiers stationed in Uruguayan territory in 1864, 2,047 that were in the province of Mato Grosso, 55,985 Fatherland Volunteers, 60,009 National Guardsmen, 8,570 ex-slaves who had been freed to be sent to war, and 9,177 navy personnel. Another 18,000 National Guard troops stayed behind to defend Brazilian territory.[36] Course of the war Paraguayan offensive In Mato Grosso Brazilian expedition to Mato Grosso: Encampment of the Expeditionary Division in the virgin forests of Goiás (L'Illustration, 1866) Paraguay took the initiative during the first phase of the war, launching the Mato Grosso Campaign by invading the Brazilian province of Mato Grosso on 14 December 1864,[16]: 25 followed by an invasion of the Rio Grande do Sul province in the south in early 1865 and the Argentine Corrientes Province. Two separate Paraguayan forces invaded Mato Grosso simultaneously. An expedition of 3,248 troops, commanded by Vicente Barrios, was transported by a naval squadron under the command of frigate captain Pedro Ignacio Meza up the Paraguay River to the town of Concepción.[16]: 25 There they attacked the Nova Coimbra fort on 27 December 1864.[16]: 26 The Brazilian garrison of 154 men resisted for three days, under the command of Hermenegildo Portocarrero (later Baron of Fort Coimbra). When their munitions were exhausted, the defenders abandoned the fort and withdrew up the river towards Corumbá on board the gunship Anhambaí.[16]: 26 After occupying the fort, the Paraguayans advanced further north, taking the cities of Albuquerque, Tage and Corumbá in January 1865.[16]: 26 Solano López then sent a detachment to attack the military frontier post of Dourados. On 29 December 1864, this detachment, led by Martín Urbieta, encountered tough resistance from Antônio João Ribeiro and his 16 men, who were all eventually killed. The Paraguayans continued to Nioaque and Miranda, defeating the troops of José Dias da Silva. The city of Coxim was taken in April 1865. The second Paraguayan column, formed from some of the 4,650 men led by Francisco Isidoro Resquín at Concepción, penetrated into Mato Grosso with 1,500 troops.[16]: 26 Despite these victories, the Paraguayan forces did not continue to Cuiabá, the capital of the province, where Augusto Leverger had fortified the camp of Melgaço. Their main objective was the capture of the gold and diamond mines, disrupting the flow of these materials into Brazil until 1869.[16]: 27 Brazil sent an expedition to fight the invaders in Mato Grosso. A column of 2,780 men led by Manuel Pedro Drago left Uberaba in Minas Gerais in April 1865 and arrived at Coxim in December, after a difficult march of more than 2,000 kilometres (1,200 mi) through four provinces. However, Paraguay had already abandoned Coxim by December. Drago arrived at Miranda in September 1866, and the Paraguayans had left once again. Colonel Carlos de Morais Camisão assumed command of the column in January 1867—now with only 1,680 men—and decided to invade Paraguayan territory, which he penetrated as far as Laguna[37] where Paraguayan cavalry forced the expedition to retreat. Despite the efforts of Camisão's troops and the resistance in the region, which succeeded in liberating Corumbá in June 1867, a large portion of Mato Grosso remained under Paraguayan control. The Brazilians withdrew from the area in April 1868, moving their troops to the main theatre of operations, in the south of Paraguay. Paraguayan invasion of Corrientes and Rio Grande do Sul Paraguayan invasion of Corrientes and Allied counterattack in 1865 The invasion of Corrientes and Rio Grande do Sul was the second phase of the Paraguayan offensive. In order to support the Uruguayan Blancos, the Paraguayans had to travel across Argentine territory. In January 1865, Solano López asked Argentina's permission for an army of 20,000 men (led by general Wenceslao Robles) to travel through the province of Corrientes.[16]: 29–30 Argentine president Bartolomé Mitre refused Paraguay's request and a similar one from Brazil.[16]: 29 After this refusal, the Paraguayan Congress gathered at an emergency meeting on 5 March 1865. After several days of discussions, on 23 March Congress decided to declare war on Argentina for its policies, hostile to Paraguay and favourable to Brazil, and then they conferred to Francisco Solano López the rank of Field Marshal of the Republic of Paraguay. The declaration of war was sent on 29 March 1865 to Buenos Aires.[38] On 13 April 1865, a Paraguayan squadron sailed down the Paraná River and attacked two Argentine ships in the port of Corrientes. Immediately general Robles' troops took the city with 3,000 men, and a cavalry force of 800 arrived the same day. Leaving a force of 1,500 men in the city, Robles advanced southwards along the eastern bank.[16]: 30 Along with Robles' troops, a force of 12,000 soldiers under colonel Antonio de la Cruz Estigarribia crossed the Argentine border south of Encarnación in May 1865, driving for Rio Grande do Sul. They traveled down the Uruguay River and took the town of São Borja on 12 June. Uruguaiana, to the south, was taken on 6 August with little resistance. By invading Corrientes, Solano López had hoped to gain the support of the powerful Argentine caudilloJusto José de Urquiza, governor of the provinces of Corrientes and Entre Ríos, who was known to be the chief federalist hostile to Mitre and the central government in Buenos Aires.[39] However, Urquiza gave his full support to an Argentine offensive.[16]: 31 The forces advanced approximately 200 kilometres (120 mi) south before ultimately ending the offensive in failure. Following the invasion of the Corrientes Province by Paraguay on 13 April 1865, a great uproar stirred in Buenos Aires as the public learned of Paraguay's declaration of war. President Bartolomé Mitre made a famous speech to the crowds on 4 May 1865: ...My fellow countrymen, I promise you: in three days we shall be at the barracks. In three weeks, at the frontiers. And in three months in Asunción![40] The same day, Argentina declared war on Paraguay;[16]: 30–31 however, on 1 May 1865, Brazil, Argentina, and Uruguay had signed the secret Treaty of the Triple Alliance in Buenos Aires. They named Bartolomé Mitre, president of Argentina, as supreme commander of the allied forces.[39] The signatories of the treaty were Rufino de Elizalde (Argentina), Otaviano de Almeida (Brazil) and Carlos de Castro (Uruguay). Brazilian steamers ramming Paraguayan ships in the Battle of Riachuelo On 11 June 1865, at the naval Battle of Riachuelo, the Brazilian fleet commanded by admiral Francisco Manoel Barroso da Silva destroyed the Paraguayan navy and prevented the Paraguayans from permanently occupying Argentine territory. For all practical purposes, this battle decided the outcome of the war in favor of the Triple Alliance; from that point onward, it controlled the waters of the Río de la Plata basin up to the entrance to Paraguay.[41] A separate Paraguayan division of 3,200 men that continued towards Uruguay under the command of Pedro Duarte, who was then defeated by Allied troops under Venancio Flores in the bloody Battle of Yatay, on the banks of the Uruguay River, near Paso de los Libres. While Solano López ordered the retreat of the forces that had occupied Corrientes, the Paraguayan troops that invaded São Borja advanced, taking Itaqui and Uruguaiana. The situation in Rio Grande do Sul was chaotic, and the local Brazilian military commanders were incapable of mounting effective resistance to the Paraguayans.[42] The baron of Porto Alegre set out for Uruguaiana, a small town in the province's west, where the Paraguayan army was besieged by a combined force of Brazilian, Argentine and Uruguayan units.[43] Porto Alegre assumed command of the Brazilian army in Uruguaiana on 21 August 1865.[44] On 18 September, the Paraguayan garrison surrendered without further bloodshed.[45] Allied counterattack In subsequent months, the Paraguayans were driven out of the cities of Corrientes and San Cosme, the only Argentine territory still in Paraguayan possession. By the end of 1865, the Triple Alliance was on the offensive. Its armies numbered 42,000 infantry and 15,000 cavalry as they invaded Paraguay in April.[16]: 51–52 The Paraguayans scored small victories against major forces in the Battle of Corrales (also known as battle of Pehuajó or Itati) in the Corrientes Province, but it could not stop the invasion.[46] Invasion of Paraguay Allied troops entrenched in Tuyutí On 16 April 1866, the Allied armies invaded Paraguayan mainland by crossing the Paraná River.[47] López launched counter-attacks, but they were repelled by general Manuel Luís Osório, who took victories in the battles of Itapirú and Isla Cabrita. Yet, the Allied advance was checked in the first major battle of the war, at Estero Bellaco, on 2 May 1866.[48] Solano López, believing that he could deal a fatal blow to the Allies, launched a major offensive with 25,000 men against 35,000 Allied soldiers at the Battle of Tuyutí on 24 May 1866, one of the bloodiest battles in Latin-American history.[49] Despite being very close to victory at Tuyutí, López's plan was shattered by the Allied army's fierce resistance and the decisive action of the Brazilian artillery.[50] Both sides sustained heavy losses: more than 12,000 casualties for Paraguay versus 6,000 for the Allies.[51][52] By 18 July, the Paraguayans had recovered, defeating forces commanded by Mitre and Flores in the Battle of Sauce and Boquerón, losing more than 2,000 men against the Allied 6,000 casualties.[53] However, Brazilian general Porto Alegre[54] won the Battle of Curuzú, putting the Paraguayans in a desperate situation.[55] On 12 September 1866, after the defeat in the Battle of Curuzú, Solano López invited Mitre and Flores to a conference in Yataytí Corá, which resulted in a \"heated argument\" among both leaders.[16]: 62 López had realized that the war was lost and was ready to sign a peace treaty with the Allies.[56] However, no agreement was reached, since Mitre's conditions for signing the treaty were that every article of the Treaty of the Triple Alliance was to be carried out, a condition that Solano López refused.[56] Article 6 of the treaty made truce or peace with López nearly impossible, as it stipulated that the war was to continue until the then government ceased to be, which meant the removal of Solano López. Allied setback at Curupayty: their advance comes to a halt Paraguayan artillery redoubts at the battle of Curuzú, by Cándido López After the conference, the Allies marched into Paraguayan territory, reaching the defensive line of Curupayty. Trusting their numerical superiority and the possibility of attacking the flank of the defensive line through the Paraguay River by using the Brazilian ships, the Allies made a frontal assault on the defensive line, supported by the flank fire of the battleships.[57] However, the Paraguayans, commanded by general José E. Díaz, stood strong in their positions and set up for a defensive battle, inflicting tremendous damage on the attacking Allied troops, resulting in over 8,000 casualties on the Brazil-Argentine army against no more than 250 losses of the Paraguayans.[58] The Battle of Curupayty resulted in an almost catastrophic defeat for the Allied forces, ending their offensive for ten months, until July 1867.[16]: 65 The Allied leaders blamed each other for the disastrous failure at Curupayty. General Flores left for Uruguay in September 1866 shortly after the battle and was later murdered there in 1867. Porto Alegre and Tamandaré found common ground in their distaste for the Brazilian commander of the 1st Corps, field marshal Polidoro Jordão. General Jordão was ostracized for supporting Mitre and for being a member of the Conservative Party, while Porto Alegre and Tamandaré were Progressives.[59] General Porto Alegre also blamed Mitre for the tremendous defeat, saying: Here is the result of the Brazilian government's lack of confidence in its generals and giving its Armies to foreign generals.[60] Mitre had a harsh opinion of the Brazilians and said that \"Porto Alegre and Tamandaré, who are cousins, and cousins even in lack of judgement have made a family pact to monopolize, in practice, the command of war.\" He further criticized Porto Alegre: \"It is impossible to imagine a greater military nullity than this general, to which it can be added Tamandaré's dominating bad influence over him and the negative spirit of both in relation to the allies, owning to passions and petty interests.\"[59] Caxias assumes command The Brazilian government decided to create a unified command over Brazilian forces operating in Paraguay and turned to the 63-year-old Luís Alves de Lima e Silva, the Marquess of Caxias, as the new leader on 10 October 1866.[61] Osório was sent to organize a 5,000-strong third corps of the Brazilian army in Rio Grande do Sul.[16]: 68 Caxias arrived in Itapiru on 17 November.[62] His first measure was to dismiss vice-admiral Tamandaré. The government had appointed Caxias' fellow Conservative vice-admiral Joaquim José Inácio—later the Viscount of Inhaúma—to lead the navy.[62] The Marquess of Caxias assumed command on 19 November.[63] He aimed to end the never-ending squabbling among the allied commanders and to increase his autonomy from the Brazilian government.[64] With the departure of president Mitre in February 1867, Caxias assumed overall command of the Allied forces.[16]: 65 He found the army practically paralyzed and devastated by disease. During this period Caxias trained his soldiers, re-equipped the army with new guns, improved the quality of the officer corps, and upgraded the health corps and overall hygiene of the troops, putting an end to epidemics.[65] From October 1866 until July 1867, all offensive operations were suspended.[66] Military operations were limited to skirmishes with the Paraguayans and bombarding Curupayty. Solano López took advantage of the disorganization of the enemy to reinforce the Fortress of Humaitá.[16]: 70 The advance resumes: fall of Humaitá As the Brazilian army was ready for combat, Caxias sought to encircle Humaitá and force its capitulation by siege. To aid the war effort, Caxias used observation balloons to gather information of the enemy lines.[67] With the 3rd Corps ready for combat, the Allied army started its flanking march around Humaitá on 22 July.[67] The march to outflank the left-wing of the Paraguayan fortifications constituted the basis of Caxias' tactics. He wanted to bypass the Paraguayan strongholds, cut the connections between Asunción and Humaitá and finally encircle the Paraguayans. The 2nd Corps was stationed in Tuyutí, while the 1st corps and the newly created 3rd Corps were used by Caxias to encircle Humaitá.[68] President Mitre returned from Argentina and re-assumed overall command on 1 August.[69] With the capture on 2 November by Brazilians troops of the Paraguayan position of Tahí, at the shores of the river, Humaitá would become isolated from the rest of the country by land.[70][b] The combined Brazilian–Argentine–Uruguayan army continued advancing north through hostile territory to surround Humaitá. The Allied force advanced to San Solano on the 29th and Tayi on 2 November, isolating Humaitá from Asunción.[72] Before dawn on 3 November, Solano López reacted by ordering the attack on the rearguard of the allies in the Second Battle of Tuyutí.[16]: 73 The Paraguayans, commanded by general Bernardino Caballero breached the Argentine lines, causing enormous damage to the Allied camp and successfully capturing weapons and supplies, very needed by López for the war effort.[73] Only thanks to the intervention of Porto Alegre and his troops, the Allied army recovered.[74] During the Second Battle of Tuyutí, Porto Alegre fought with his saber in hand-to-hand combat and lost two horses.[75] In this battle, the Paraguayans lost over 2,500 men, while the allies had just over 500 casualties.[76] By 1867, Paraguay had lost 60,000 men to battle casualties, injuries, or disease. Due to the growing manpower shortage, López conscripted another 60,000 soldiers from slaves and children. Women were entrusted with all support functions alongside the soldiers. Many Paraguayan soldiers went into battle without shoes or uniforms. López enforced the strictest discipline, executing even his two brothers and two brothers-in-law for alleged defeatism.[77] By December 1867, there were 45,791 Brazilians, 6,000 Argentines and 500 Uruguayans at the front. After the death of Argentine vice president Marcos Paz, Mitre relinquished his position for the second and final time on 14 January 1868.[78] Allied representatives in Buenos Aires abolished the position of Allied commander-in-chief on 3 October, although the Marquess of Caxias continued to fill the role of Brazilian supreme commander.[79] On 19 February, Brazilian ironclads successfully made a passage up the Paraguay River under heavy fire, gaining full control of the river and isolating Humaitá from resupply by water.[80] Humaitá fell on 25 July 1868, after a long siege.[16]: 86 López with the bulk of his army escaped from the siege of Humaitá. Before doing so he tried a daring manouevre: to capture on or more allied ironclads by human wave boarding tactics. The assault on the warships Lima Barros and Cabral was a naval action that took place in the early hours of 2 March 1868, when Paraguayan canoes, joined two by two, disguised with branches and manned by 50 soldiers each, approached the ironclads Lima Barros and Cabral. The Imperial Fleet, which had already achieved the Passage of Humaitá, was anchored in the Paraguay river, before the Taji stronghold near Humaitá. Taking advantage of the dense darkness of the night and the hyacinths that descended on the current, a squadron of canoes covered by branches and foliage and tied two by two, crewed by 1,500 Paraguayans armed with machetes, hatchets and approaching swords, went to approach Cabral and Lima Barros. The fighting continued until dawn when the warships Brasil, Herval, Mariz e Barros and Silvado approached and shot the Paraguayans, who gave up the attack, losing 400 men and 14 canoes.[81] Fall of Asunción En route to Asunción, the Allied army went 200 kilometres (120 mi) north to Palmas, stopping at the Piquissiri River. There Solano López had concentrated 12,000 Paraguayans in a fortified line that exploited the terrain and supported the forts of Angostura and Itá-Ibaté. Resigned to frontal combat, Caxias ordered the so-called Piquissiri maneuver. While a squadron attacked Angostura, Caxias made the army cross to the west side of the river. He ordered the construction of a road in the swamps of the Gran Chaco along which the troops advanced to the northeast. At Villeta the army crossed the river again, between Asunción and Piquissiri, behind the fortified Paraguayan line. Instead of advancing to the capital, already evacuated and bombarded, Caxias went south and attacked the Paraguayans from the rear in December 1868, in an offensive which became known as \"Dezembrada\".[16]: 89–91 Caxias' troops were ambushed while crossing the Itororó during an initial advance, during which the Paraguayans inflicted severe damage on the Brazilian armies.[82] Days later, however, the Allies destroyed a whole Paraguayan division at the Battle of Avay.[16]: 94 Weeks later, Caxias won another decisive victory at the Battle of Lomas Valentinas and captured the last stronghold of the Paraguayan Army in Angostura. On 24 December, Caxias sent a note to Solano López asking for surrender, but Solano López refused and fled to Cerro León.[16]: 90–100 Alongside the Paraguayan president was the American Minister-Ambassador, Martin T. McMahon, who after the war became a fierce defender of López's cause.[83] Asunción was occupied on 1 January 1869, by Brazilian general João de Souza da Fonseca Costa, father of the future marshal Hermes da Fonseca. On 5 January, Caxias entered the city with the rest of the army.[16]: 99 Most of Caxias army settled in Asunción, where also 4,000 Argentine and 200 Uruguayan troops soon arrived together with about 800 soldiers and officers of the Paraguayan Legion. By this time, Caxias was ill and tired. On 17 January, he fainted during a mass; he relinquished his command the next day, and the day after that left for Montevideo.[84] Very soon the city hosted about 30,000 Allied soldiers; for the next few months these looted almost every building, including diplomatic missions of European nations.[84] Provisional government The Count of Eu with the Viscount of Rio Branco among Brazilian officers. The Viscount of Rio Branco represented the Triple Alliance in forming the provisional government of Paraguay. With Solano López on the run, the country lacked a government. Pedro II sent his Foreign minister José Paranhos to Asunción where he arrived on 20 February 1869 and began consultations with the local politicians. Paranhos had to create a provisional government that could sign a peace accord and recognize the border claimed by Brazil between the two nations.[85] According to historian Francisco Doratioto, Paranhos, \"the then-greatest Brazilian specialist on Platine affairs\", had a \"decisive\" role in the installation of the Paraguayan provisional government.[86] With Paraguay devastated, the power vacuum resulting from Solano López's overthrow was quickly filled by emerging domestic factions which Paranhos had to accommodate. On 31 March, a petition was signed by 335 leading citizens asking Allies for a Provisional government. This was followed by negotiations between the Allied countries, which put aside some of the more controversial points of the Treaty of the Triple Alliance; on 11 June, agreement was reached with Paraguayan opposition figures that a three-man Provisional government would be established. On 22 July, a National Assembly met in the National Theatre and elected Junta Nacional of 21 men which then selected a five-man committee to select three men for the Provisional government. They selected Carlos Loizaga, Juan Francisco Decoud, and José Díaz de Bedoya. Decoud, being pro-Argentine, was unacceptable to Paranhos, who had him replaced with Cirilo Antonio Rivarola. The government was finally installed on 15 August but was just a front for the continued Allied occupation.[84] After the death of Lopez, the Provisional Government issued a proclamation on 6 March 1870 in which it promised to support political liberties, to protect commerce and to promote immigration. The Provisional Government did not last. In May 1870, José Díaz de Bedoya resigned; on 31 August 1870, so did Carlos Loizaga. The remaining member, Antonio Rivarola, was then immediately relieved of his duties by the National Assembly, which established a provisional Presidency, to which it elected Facundo Machaín, who assumed his post that same day. However, the next day, 1 September, he was overthrown in a coup that restored Rivarola to power. Most important were the Battle of Piribebuy and the Battle of Acosta Ñu, in which more than 5,000 Paraguayans died.[87] After a successful beginning which included victories over the remnants of Solano López's army, the Count fell into depression and Paranhos became the unacknowledged, de facto commander-in-chief.[88] Death of Solano López President Solano López organized the resistance in the mountain range northeast of Asunción. At the end of the war, with Paraguay suffering severe shortages of weapons and supplies, Solano López reacted with draconian attempts to keep order, ordering troops to kill any of their colleagues, including officers, who talked of surrender.[89] Paranoia prevailed in the army, and soldiers fought to the bitter end in a resistance movement, resulting in more destruction in the country.[89] Two detachments were sent in pursuit of Solano López, who was accompanied by 200 men in the forests in the north. On 1 March 1870, the troops of General José Antônio Correia da Câmara surprised the last Paraguayan camp in Cerro Corá. During the ensuing battle, Solano López was wounded and separated from the remainder of his army. Too weak to walk, he was escorted by his aide and a pair of officers, who led him to the banks of the Aquidaban-nigui River. The officers left Solano López and his aide there while they looked for reinforcements. Before they returned, Câmara arrived with a small number of soldiers. Though he offered to permit Solano López to surrender and guaranteed his life, Solano López refused. Shouting \"I die with my homeland!\", he tried to attack Câmara with his sword. He was quickly killed by Câmara's men, bringing an end to the long conflict in 1870.[90][91] Casualties of the war Paraguay suffered massive casualties, and the war's disruption and disease also cost civilian lives. Some historians estimate that the nation lost the majority of its population. The specific numbers are hotly disputed and range widely. A survey of 14 estimates of Paraguay's pre-war population varied between 300,000 and 1,337,000.[92] Later academic work based on demographics produced a wide range of estimates, from a possible low of 21,000 (7% of population) (Reber, 1988) to as high as 69% of the total prewar population (Whigham, Potthast, 1999). Because of the local situation, all casualty figures are a very rough estimate; accurate casualty numbers may never be determined. After the war, an 1871 census recorded 221,079 inhabitants, of which 106,254 were women, 28,746 were men, and 86,079 were children (with no indication of sex or upper age limit).[93] The worst reports are that up to 90% of the male population was killed, though this figure is without support.[89] One estimate places total Paraguayan losses—through both war and disease—as high as 1.2 million people, or 90% of its pre-war population,[94] but modern scholarship has shown that this number depends on a population census of 1857 that was a government invention.[95] A different estimate places Paraguayan deaths at approximately 300,000 people out of 500,000 to 525,000 pre-war inhabitants.[96] During the war, many men and boys fled to the countryside and forests. In the estimation of Vera Blinn Reber, however, \"The evidence demonstrates that the Paraguayan population casualties due to the war have been enormously exaggerated\".[97] Homeless Paraguayan families during the Paraguayan War, 1867 A 1999 study by Thomas Whigham from the University of Georgia and Barbara Potthast (published in the Latin American Research Review under the title \"The Paraguayan Rosetta Stone: New Evidence on the Demographics of the Paraguayan War, 1864–1870\", and later expanded in the 2002 essay titled \"Refining the Numbers: A Response to Reber and Kleinpenning\") used a methodology to yield more accurate figures. To establish the population before the war, Whigham used an 1846 census and calculated, based on a population growth rate of 1.7% to 2.5% annually (which was the standard rate at that time), that the immediately pre-war Paraguayan population in 1864 was approximately 420,000–450,000. Based on a census carried out after the war ended, in 1870–1871, Whigham concluded that 150,000–160,000 Paraguayan people had survived, of whom only 28,000 were adult males. In total, 60–70% of the population died as a result of the war,[98] leaving a woman/man ratio of 4 to 1 (as high as 20 to 1, in the most devastated areas).[98] For academic criticism of the Whigham-Potthast methodology and estimates see the main article Paraguayan War casualties. A Brazilian priest with Paraguayan refugees coming from San Pedro, 1869 or 1870 Steven Pinker wrote that, assuming a death rate of over 60% of the Paraguayan population, this war was proportionally one of the most destructive in modern times for any nation state.[99][page needed] Allied losses As was common before antibiotics were developed, disease caused more deaths than war wounds. Bad food and poor sanitation contributed to disease among troops and civilians. Among the Brazilians, two-thirds of the dead died either in a hospital or on the march. At the beginning of the conflict, most Brazilian soldiers came from the north and northeast regions;[citation needed] the change from a hot to a colder climate, combined with restricted food rations, may have weakened their resistance. Entire battalions of Brazilians were recorded as dying after drinking water from rivers. Therefore, some historians believe cholera, transmitted in the water, was a leading cause of death during the war.[citation needed] Gender and ethnic aspects Women in the Paraguayan War Jovita Feitosa joined the Imperial Army as a Fatherland Volunteer disguised as a boy. Later, she revealed to be a woman, and was eventually accepted. Paraguayan women played a significant role in the Paraguayan War. During the period just before the war began many Paraguayan women were the heads of their households, meaning they held a position of power and authority. They received such positions by being widows, having children out of wedlock, or their husbands having worked as peons. When the war began women started to venture out of the home, becoming nurses, working with government officials, and establishing themselves into the public sphere. When The New York Times reported on the war in 1868, it considered Paraguayan women equal to their male counterparts.[100] Paraguayan women's support of the war effort can be divided into two stages. The first is from the time the war began in 1864 to the Paraguayan evacuation of Asunción in late 1868. During this period of the war, peasant women became practically the sole producers of agricultural goods.[101] The second stage begins when the war turned to a more guerrilla form; it started when the capital of Paraguay fell and ended with the death of Paraguay's president Francisco Solano López in 1870. At this stage, the number of women becoming victims of war was increasing.[citation needed] The government press, with doubtful veracity, claimed that battalions of women were formed to fight the Allies and exalted the role of Ramona Martínez (who was a woman enslaved by López) as \"the American Joan of Arc\" for her fighting and rallying of injured troops.[102] Women helped sustain Paraguayan society during a very unstable period. Though Paraguay did lose the war, the outcome might have been even more disastrous without women performing specific tasks. Women worked as farmers, soldiers, nurses, and government officials. They became a symbol for national unification, and at the end of the war, the traditions women maintained were part of what held the nation together.[103] A 2012 piece in The Economist argued that with the death of most of Paraguay's male population, the Paraguayan War distorted the sex ratio to women greatly outnumbering men and has impacted the sexual culture of Paraguay to this day. Because of the depopulation, men were encouraged after the war to have multiple children with multiple women, even supposedly celibate Catholic priests. A columnist linked this cultural idea to the paternity scandal of former president Fernando Lugo, who fathered multiple children while he was a supposedly celibate priest.[104] Paraguayan indigenous people Prior to the war, indigenous people occupied very little space in the minds of the Paraguayan elite. Paraguayan president Carlos Antonio Lopez even modified the country's constitution in 1844 to remove any mention of Paraguay's Hispano-Guarani character.[105] This marginalization was undercut by the fact that Paraguay had long prized its military as its only honorable and national institution and the majority of the Paraguayan military was indigenous and spoke Guarani. However, during the war, the indigenous people of Paraguay came to occupy an even larger role in public life, especially after the Battle of Estero Bellaco. For this battle, Paraguay put its \"best\" men, who happened to be of Spanish descent, front and center. Paraguay overwhelmingly lost this battle, as well as \"the males of all the best families in the country.\"[106] The now remaining members of the military were \"old men who had been left in Humaita, Indians, slaves and boys.\"[106] The war also bonded the indigenous people of Paraguay to the project of Paraguayan nation-building. In the immediate lead up to the war, they were confronted with a barrage of nationalist rhetoric (in Spanish and Guarani) and subject to loyalty oaths and exercises.[107] Paraguayan president Francisco Solano Lopez, son of Carlos Antonio Lopez, was well aware that the Guarani speaking people of Paraguay had a group identity independent of the Spanish-speaking Paraguayan elite. He knew he would have to bridge this divide or risk it being exploited by the 'Triple Alliance.' To a certain extent, Lopez succeeded in getting the indigenous people to expand their communal identity to include all of Paraguay. As a result of this, any attack on Paraguay was considered to be an attack on the Paraguayan nation, despite rhetoric from Brazil, Uruguay and Argentina saying otherwise. This sentiment increased after the terms of the Treaty of the Triple Alliance were leaked, especially the clause stating that Paraguay would pay for all the damages incurred by the conflict. Afro-Brazilians Racist cartoon in Paraguayan military newspaper. The Brazilian government allowed the creation of black-only units or \"zuavos\" in the military at the outset of the war, following the proposal of Afro-Brazilian Quirino Antônio do Espírito Santo, a veteran of the Brazilian War of Independence.[108] Over the course of the war, the zuavos became an increasingly attractive option for many enslaved Afro-Brazilian men, especially given the zuavos’ negative opinion toward slavery.[109] Once the zuavos had enlisted or forcibly recruited them, it became difficult for their masters to regain possession of them, since the government was desperate for soldiers.[109] By 1867, black-only units were no longer permitted, with the entire military being integrated just as it had been prior to the war. The overarching rationale behind this was that the \"country needed recruits for its existing battalions, not more independently organized companies.\"[110] This did not mean the end of black soldiers in the Brazilian military. On the contrary, \"impoverished gente de cor constituted the greater part of the soldiery in every Brazilian infantry battalion.\"[111] Afro-Brazilian women played a key role in sustaining the Brazilian military as \"vivandeiras.\" Vivandeiras were poor women who traveled with the soldiers to undertake \"logistic tasks such as carrying tents, preparing food and doing laundry.\"[112] For most of these women, the principal reason they became vivandeiras was because their male loved ones had joined as soldiers, and they wanted to take care of them. However, the Brazilian government actively worked to minimize the importance of their work by labeling it \"service to their male kin, not the nation\" and considering it to be \"natural\" and \"habitual.\"[112] The reality was that the government depended heavily on these women and officially required their presence in the camps.[112] Poor Afro-Brazilian women also served as nurses, with most of them being trained upon entry into the military to assist male doctors in the camps. These women were \"seeking gainful employment to compensate for the loss of income from male kin who had been drafted into the war.\"[112] Territorial changes and treaties Paraguay after the war Paraguay permanently lost its claim to territories which, before the war, were in dispute between it and Brazil or Argentina, respectively. In total, about 140,000 square kilometres (54,000 sq mi) were affected. Those disputes had been longstanding and complex. After the war Brazil signed a separate Loizaga–Cotegipe Treaty of peace and borders with Paraguay on 9 January 1872, in which it obtained freedom of navigation on the Paraguay River. Brazil also retained the northern regions it had claimed before the war.[114] Those regions are now part of its State of Mato Grosso do Sul. Disputes with Argentina Misiones In colonial times the missionary Jesuits established numerous villages in lands between the rivers Paraná and Uruguay. After the Jesuits were expelled from Spanish territory in 1767, the ecclesiastical authorities of both Asunción and Buenos Aires made claim to religious jurisdiction in these lands and the Spanish government sometimes awarded it to one side, sometimes to the other; sometimes they split the difference. After independence, the Republic of Paraguay and the Argentine Confederation succeeded to these disputes.[115] On 19 July 1852, the governments of the Argentine Confederation and Paraguay signed a treaty, by which Paraguay relinquished its claim to the Misiones.[116] However, this treaty did not become binding, because it required to be ratified by the Argentine Congress, which refused.[117] Paraguay's claim was still alive on the eve of the war. After the war the disputed lands definitively became the Argentine national territory of Misiones, now Misiones Province. Gran Chaco The Gran Chaco is an area lying to the west of the River Paraguay. Before the war it was \"an enormous plain covered by swamps, chaparral and thorn forests ... home to many groups of feared Indians, including the Guaicurú, Toba and Mocoví.\"[117] There had long been overlapping claims to all or parts of this area by the Argentine Confederation, Bolivia and Paraguay. With some exceptions, these were paper claims, because none of those countries was in effective occupation of the area: essentially, they were claims to be the true successor to the Spanish Empire, in an area never effectively occupied by Spain itself, and wherein Spain had no particular motive for prescribing internal boundaries. The exceptions were as follows. First, to defend itself against Indian incursions, both in colonial times and after, the authorities in Asunción had established some border fortlets on the west bank of the river Paraguay—a coastal strip within the Chaco. By the same treaty of 19 July 1852, between Paraguay and the Argentine Confederation, an undefined area in the Chaco north of the Bermejo River was implicitly conceded to belong to Paraguay. As already stated, the Argentine Congress refused to ratify this treaty; and it was protested by the government of Bolivia as inimical to its own claims. The second exception was that in 1854, the government of Carlos Antonio López established a colony of French immigrants on the right bank of the River Paraguay at Nueva Burdeos; when it failed, it was renamed Villa Occidental.[118] After 1852, and more especially after the State of Buenos Aires rejoined the Argentine Confederation, Argentina's claim to the Chaco hardened; it claimed territory all the way up to the border with Bolivia. By Article XVI of the Treaty of the Triple Alliance Argentina was to receive this territory in full. However, the Brazilian government disliked what its representative in Buenos Aires had negotiated in this respect and resolved that Argentina should not receive \"a handsbreadth of territory\" above the Pilcomayo River. It set out to frustrate Argentina's further claim, with eventual success. The post-war border between Paraguay and Argentina was resolved through long negotiations, completed 3 February 1876, by signing the Machaín-Irigoyen Treaty. This treaty granted Argentina roughly one third of the area it had originally desired. Argentina became the strongest of the River Plate countries. When the two parties could not reach consensus on the fate of the Chaco Boreal area between the Río Verde and the main branch of Río Pilcomayo, the President of the United States, Rutherford B. Hayes, was asked to arbitrate. His award was in Paraguay's favor. The Paraguayan Presidente Hayes Department is named in his honor. Consequences of the war Paraguay There was destruction of the existing state, loss of neighboring territories and ruin of the Paraguayan economy, so that even decades later, it could not develop in the same way as its neighbors. Paraguay is estimated to have lost up to 69% of its population, most of them due to illness, hunger and physical exhaustion, of whom 90% were male, and also maintained a high debt of war with the allied countries that, not completely paid, ended up being pardoned in 1943 by the Brazilian President Getúlio Vargas. A new pro-Brazil government was installed in Asunción in 1869, while Paraguay remained occupied by Brazilian forces until 1876, when Argentina formally recognized the independence of that country, guaranteeing its sovereignty and leaving it a buffer state between its larger neighbors. Brazil Preparations for the victory celebration in Brazil, 1870 The War helped the Brazilian Empire to reach its peak of political and military influence, becoming the Great Power of South America, and also helped to bring about the end of slavery in Brazil, moving the military into a key role in the public sphere.[119] However, the war caused a ruinous increase of public debt, which took decades to pay off, severely limiting the country's growth. The war debt, alongside a long-lasting social crisis after the conflict,[120][121] are regarded as crucial factors for the fall of the Empire and proclamation of the First Brazilian Republic.[122][123] During the war the Brazilian army took complete control of Paraguayan territory and occupied the country for six years after 1870. In part this was to prevent the annexation of even more territory by Argentina, which had wanted to seize the entire Chaco region. During this time, Brazil and Argentina had strong tensions, with the threat of armed conflict between them. During the wartime sacking of Asunción, Brazilian soldiers carried off war trophies. Among the spoils taken was a large caliber gun called Cristiano, named because it was cast from church bells of Asunción melted down for the war. In Brazil the war exposed the fragility of the Empire and dissociated the monarchy from the army. The Brazilian army became a new and influential force in national life. It developed as a strong national institution that, with the war, gained tradition and internal cohesion. The Army would take a significant role in the later development of the history of the country. The economic depression and the strengthening of the army later played a large role in the deposition of the emperor Pedro II and the republican proclamation in 1889. Marshal Deodoro da Fonseca became the first Brazilian president. As in other countries, \"wartime recruitment of slaves in the Americas rarely implied a complete rejection of slavery and usually acknowledged masters' rights over their property.\"[124] Brazil compensated owners who freed slaves for the purpose of fighting in the war, on the condition that the freedmen immediately enlist. It also impressed slaves from owners when needing manpower, and paid compensation. In areas near the conflict, slaves took advantage of wartime conditions to escape, and some fugitive slaves volunteered for the army. Together these effects undermined the institution of slavery. But, the military also upheld owners' property rights, as it returned at least 36 fugitive slaves to owners who could satisfy its requirement for legal proof. Significantly, slavery was not officially ended until the 1880s.[124] Brazil spent close to 614,000 réis (the Brazilian currency at the time), which were gained from the following sources: réis, thousands source 49 Foreign loans 27 Domestic loans 102 Paper emission 171 Title emission 265 Taxes Due to the war, Brazil ran a deficit between 1870 and 1880, which was finally paid off. At the time foreign loans were not significant sources of funds.[125] Argentina Following the war, Argentina faced many federalist revolts against the national government. Economically it benefited from having sold supplies to the Brazilian army, but the war overall decreased the national treasure. The national action contributed to the consolidation of the centralized government after revolutions were put down, and the growth in influence of Army leadership. It has been argued the conflict played a key role in the consolidation of Argentina as a nation-state.[126] That country became one of the wealthiest in the world, by the early 20th century.[127] It was the last time that Brazil and Argentina openly took such an interventionist role in Uruguay's internal politics.[128] Uruguay Uruguay suffered lesser effects, although nearly 5,000 soldiers were killed. As a consequence of the war, the Colorados gained political control of Uruguay and, despite rebellions, retained it until 1958. Modern interpretations of the war Interpretation of the causes of the war and its aftermath has been a controversial topic in the histories of participating countries, especially in Paraguay. There it has been considered either a fearless struggle for the rights of a smaller nation against the aggression of more powerful neighbors, or a foolish attempt to fight an unwinnable war that almost destroyed the nation. Several revisionist historians consider the mass extermination of the Paraguayan people during the war to be a case of genocide.[130][131] In 2022, the Mercosur Parliament formed the Sub-Commission for Truth and Justice on the War of the Triple Alliance, within its Human Rights Commission, to investigate the potential crimes (including genocide) committed during the war and then arrive at a \"consensual truth\" on the matter within the parliament.[132] In December 1975, after presidents Ernesto Geisel and Alfredo Stroessner signed a treaty of friendship and co-operation[133] in Asunción, the Brazilian government returned some of its spoils of war to Paraguay, but has kept others. In April 2013 Paraguay renewed demands for the return of the \"Christian\" cannon. Brazil has had this on display at the former military garrison, now used as the National History Museum, and says that it is part of its history as well.[134] Theories about British influence on the outbreak of war A popular belief among Paraguayans and Argentine revisionists since the 1960s contends that the outbreak of war was due to the machinations of the British government, a theory which historians have noted has little to no basis in historical evidence. In Brazil, some have claimed that the United Kingdom was the primary source of financing for the Triple Alliance during the war, with British aid being given in order to advance Britain's economic interests in the region; something which historians have noted that has little evidence to support it as well; noting that from 1863 to 1865 Brazil and Great Britain were engaged in a diplomatic incident, and five months after the outbreak of the Paraguayan war the two countries temporarily broke off relations. They have also noted that in 1864, a British diplomat wrote a letter to Solano López asking him to avoid initiating hostilities in the region, and there remains no evidence that Britain \"forced\" the allies to attack Paraguay.[135] The ignorant and barbaric people of Paraguay believe that it is under the protection of the most illustrious of the governments (...) and only with foreign intervention, or a war, they will be relieved from their error.[142] Charles Washburn, who was the Minister of the United States to Paraguay and Argentina, claimed that Thornton spoke of Paraguay, months before the outbreak of the conflict, as: ... Worst than Abyssinia, and López (is) worst than King Tewodros II. The extinction [of Paraguay] as a nation will be benefit, to all the world.[143][144] However, historian E.N. Tate noted that: Whatever his dislike of Paraguay, Thornton appears to have had no wish that its quarrels with Argentina and Brazil, rapidly worsening at the time of his visit to Asunción, should develop into war. His influence in Buenos Aires seems to have been used consistently during the next few months in the interests of peace.[145] Other historians have also disputed the claims of British influence in the outbreak of war, pointing out that there is no documented evidence for it.[146][135][147] They note that, although the British economy and commercial interests benefited from the war, the British government opposed it from the start. In addition, they also noted that the war damaged international commerce (including Britain's), and the British government disapproved of the secret clauses in the Treaty of the Triple Alliance.[148] Britain at the time already was increasing their imports of Egyptian and Indian cotton and as such did not need any from Paraguay.[149][150] William Doria (the British Chargé d'Affaires in Paraguay who briefly acted in Thornton's place), joined French and Italian diplomats in condemning Argentina's President Bartolomé Mitre's involvement in Uruguay. But when Thornton returned to the job in December 1863, Doria threw his full backing behind Mitre.[151] Effects on yerba mate industry Since colonial times, yerba mate had been a major cash crop for Paraguay. Until the war, it had generated significant revenues for the country. The war caused a sharp drop in harvesting of yerba mate in Paraguay, reportedly by as much as 95% between 1865 and 1867.[152] Soldiers from all sides used yerba mate to diminish hunger pangs and alleviate combat anxiety.[153] Much of the 156,415 square kilometers (60,392 sq mi) lost by Paraguay to Argentina and Brazil was rich in yerba mate, so by the end of the 19th century, Brazil became the leading producer of the crop.[153] Foreign entrepreneurs entered the Paraguayan market and took control of its remaining yerba mate production and industry.[152] Notes ^According to historian Chris Leuchars, it is known as \"the War of the Triple Alliance, or the Paraguayan War, as it is more popularly termed.\" See Leuchars 2002, p. 33. ^Mitre systematized the exchange of correspondence with Caxias, in the previous month, about the Allied advance, in a document entitled Memoria Militar, in which included his military plans and the planning of attack of Humaitá.[71] ^An early 20th-century estimate is that from a prewar population of 1,337,437, the population fell to 221,709 (28,746 men, 106,254 women, 86,079 children) by the end of the war (War and the Breed, David Starr Jordan, p. 164. Boston, 1915; Applied Genetics, Paul Popenoe, New York: Macmillan Company, 1918) ^Byron Farwell, The Encyclopedia of Nineteenth-Century Land Warfare: An Illustrated World View, New York: WW Norton, 2001. p. 824, ^ abcdIpsen, Wiebke (2012). \"Patricias, Patriarchy, and Popular Demobilization: Gender and Elite Hegemony in Brazil at the End of the Paraguayan War\". Hispanic American Historical Review. 92 (2): 312. doi:10.1215/00182168-1545701. ^ abKraay, Hendrik; Whigham, Thomas L. (2004). \"I die with my country:\" Perspectives on the Paraguayan War, 1864–1870. Dexter, Michigan: Thomson-Shore. ISBN978-0-8032-2762-0, p. 16 Quote: \"During the 1960s, revisionists influenced by both left-wing dependency theory and, paradoxically, an older, right-wing nationalism (especially in Argentina) focused on Britain's role in the region. They saw the war as a plot hatched in London to open up a supposedly wealthy Paraguay to the international economy. With more enthusiasm than evidence revisionists presented the loans contracted in London by Argentina, Uruguay, and Brazil as proof of the insidious role of foreign capital. Little evidence for these allegations about Britain's role has emerged, and the one serious study to analyze this question has found nothing in the documentary base to confirm the revisionist claim.\" ^Galeano, Eduardo. \"Open Veins of Latin America: Five Centuries of the Pillage of a Continent,\" Monthly Review Press, 1997"}
{"url": "https://en.wikipedia.org/wiki/Geologic_ages", "text": "The geologic time scale is a way of representing deep time based on events that have occurred throughout Earth's history, a time span of about 4.54 ± 0.05 Ga (4.54 billion years).[5] It chronologically organises strata, and subsequently time, by observing fundamental changes in stratigraphy that correspond to major geological or paleontological events. For example, the Cretaceous–Paleogene extinction event, marks the lower boundary of the Paleogene System/Period and thus the boundary between the Cretaceous and Paleogene systems/periods. For divisions prior to the Cryogenian, arbitrary numeric boundary definitions (Global Standard Stratigraphic Ages, GSSAs) are used to divide geologic time. Proposals have been made to better reconcile these divisions with the rock record.[6][3] Historically, regional geologic time scales were used[3] due to the litho- and biostratigraphic differences around the world in time equivalent rocks. The ICS has long worked to reconcile conflicting terminology by standardising globally significant and identifiable stratigraphic horizons that can be used to define the lower boundaries of chronostratigraphic units. Defining chronostratigraphic units in such a manner allows for the use of global, standardised nomenclature. The ICC represents this ongoing effort. The relative relationships of rocks for determining their chronostratigraphic positions use the overriding principles of:[7][8][9][10] Superposition – Newer rock beds will lie on top of older rock beds unless the succession has been overturned. Lateral continuity – Originally deposited layers of rock extend laterally in all directions until either thinning out or being cut off by a different rock layer. Biologic succession (where applicable) – This states that each stratum in a succession contains a distinctive set of fossils. This allows for a correlation of the stratum even when the horizon between them is not continuous. The GTS is divided into chronostratigraphic units and their corresponding geochronologic units. These are represented on the ICC published by the ICS; however, regional terms are still in use in some areas. Chronostratigraphy is the element of stratigraphy that deals with the relation between rock bodies and the relative measurement of geological time.[11] It is the process where distinct strata between defined stratigraphic horizons are assigned to represent a relative interval of geologic time. A chronostratigraphic unit is a body of rock, layered or unlayered, that is defined between specified stratigraphic horizons which represent specified intervals of geologic time. They include all rocks representative of a specific interval of geologic time, and only this time span.[11] Eonothem, erathem, system, series, subseries, stage, and substage are the hierarchical chronostratigraphic units.[11]Geochronology is the scientific branch of geology that aims to determine the age of rocks, fossils, and sediments either through absolute (e.g., radiometric dating) or relative means (e.g., stratigraphic position, paleomagnetism, stable isotope ratios).[12] A geochronologic unit is a subdivision of geologic time. It is a numeric representation of an intangible property (time).[12] Eon, era, period, epoch, subepoch, age, and subage are the hierarchical geochronologic units.[11]Geochronometry is the field of geochronology that numerically quantifies geologic time.[12] A Global Standard Stratigraphic Age (GSSA)[15] is a numeric only, chronologic reference point used to define the base of geochronologic units prior to the Cryogenian. These points are arbitrarily defined.[11] They are used where GSSPs have not yet been established. Research is ongoing to define GSSPs for the base of all units that are currently defined by GSSAs. The numeric (geochronometric) representation of a geochronologic unit can, and is more often subject to change when geochronology refines the geochronometry, while the equivalent chronostratigraphic unit remains the same, and their revision is less common. For example, in early 2022 the boundary between the Ediacaran and Cambrianperiods (geochronologic units) was revised from 541 Ma to 538.8 Ma but the rock definition of the boundary (GSSP) at the base of the Cambrian, and thus the boundary between the Ediacaran and Cambrian systems (chronostratigraphic units) has not changed, merely the geochronometry has been refined. The numeric values on the ICC are represented by the unit Ma (megaannum) meaning \"million years\", i.e., 201.4 ± 0.2 Ma, the lower boundary of the Jurassic Period, is defined as 201,400,000 years old with an uncertainty of 200,000 years. Other SI prefix units commonly used by geologists are Ga (gigaannum, billion years), and ka (kiloannum, thousand years), with the latter often represented in calibrated units (before present). An epoch is the second smallest geochronologic unit. It is equivalent to a chronostratigraphic series.[11][16] There are 37 defined epochs and one informal one. There are also 11 subepochs which are all within the Neogene and Quaternary.[2] The use of subepochs as formal units in international chronostratigraphy was ratified in 2022.[17] An age is the smallest hierarchical geochronologic unit and is equivalent to a chronostratigraphic stage.[11][16] There are 96 formal and five informal ages.[2] The Early and Late subdivisions are used as the geochronologic equivalents of the chronostratigraphic Lower and Upper, e.g., Early Triassic Period (geochronologic unit) is used in place of Lower Triassic Series (chronostratigraphic unit). Rocks representing a given chronostratigraphic unit are that chronostratigraphic unit, and the time they were laid down in is the geochronologic unit, i.e., the rocks that represent the Silurian Series are the Silurian Series and they were deposited during the Silurian Period. Formal, hierarchical units of the geologic time scale (largest to smallest) The names of geologic time units are defined for chronostratigraphic units with the corresponding geochronologic unit sharing the same name with a change to the latter (e.g. Phanerozoic Eonothem becomes the Phanerozoic Eon). Names of erathems in the Phanerozoic were chosen to reflect major changes in the history of life on Earth: Paleozoic (old life), Mesozoic (middle life), and Cenozoic (new life). Names of systems are diverse in origin, with some indicating chronologic position (e.g., Paleogene), while others are named for lithology (e.g., Cretaceous), geography (e.g., Permian), or are tribal (e.g., Ordovician) in origin. Most currently recognised series and subseries are named for their position within a system/series (early/middle/late); however, the ICS advocates for all new series and subseries to be named for a geographic feature in the vicinity of its stratotype or type locality. The name of stages should also be derived from a geographic feature in the locality of its stratotype or type locality.[11] Informally, the time before the Cambrian is often referred to as the Precambrian or pre-Cambrian (Supereon).[6][note 3] Of the stupidity and ignorance of those who imagine that these creatures were carried to such places distant from the sea by the Deluge...Why do we find so many fragments and whole shells between the different layers of stone unless they had been upon the shore and had been covered over by earth newly thrown up by the sea which then became petrified? And if the above-mentioned Deluge had carried them to these places from the sea, you would find the shells at the edge of one layer of rock only, not at the edge of many where may be counted the winters of the years during which the sea multiplied the layers of sand and mud brought down by the neighboring rivers and spread them over its shores. And if you wish to say that there must have been many deluges in order to produce these layers and the shells among them it would then become necessary for you to affirm that such a deluge took place every year. These views of da Vinci remained unpublished, and thus lacked influence at the time; however, questions of fossils and their significance were pursued and, while views against Genesis were not readily accepted and dissent from religious doctrine was in some places unwise, scholars such as Girolamo Fracastoro shared da Vinci's views, and found the attribution of fossils to the 'Deluge' absurd.[33] Niels Stensen, more commonly known as Nicolas Steno (1638–1686), is credited with establishing four of the guiding principles of stratigraphy.[33] In De solido intra solidum naturaliter contento dissertationis prodromus Steno states:[7][38] When any given stratum was being formed, all the matter resting on it was fluid and, therefore, when the lowest stratum was being formed, none of the upper strata existed. ...strata which are either perpendicular to the horizon or inclined to it were at one time parallel to the horizon. When any given stratum was being formed, it was either encompassed at its edges by another solid substance or it covered the whole globe of the earth. Hence, it follows that wherever bared edges of strata are seen, either a continuation of the same strata must be looked for or another solid substance must be found that kept the material of the strata from being dispersed. If a body or discontinuity cuts across a stratum, it must have formed after that stratum. Respectively, these are the principles of superposition, original horizontality, lateral continuity, and cross-cutting relationships. From this Steno reasoned that strata were laid down in succession and inferred relative time (in Steno's belief, time from Creation). While Steno's principles were simple and attracted much attention, applying them proved challenging.[33] These basic principles, albeit with improved and more nuanced interpretations, still form the foundational principles of determining the correlation of strata relative to geologic time. Over the course of the 18th-century geologists realised that: Sequences of strata often become eroded, distorted, tilted, or even inverted after deposition Strata laid down at the same time in different areas could have entirely different appearances The strata of any given area represented only part of Earth's long history The apparent, earliest formal division of the geologic record with respect to time was introduced by Thomas Burnet who applied a two-fold terminology to mountains by identifying \"montes primarii\" for rock formed at the time of the 'Deluge', and younger \"monticulos secundarios\" formed later from the debris of the \"primarii\".[39][33] This attribution to the 'Deluge', while questioned earlier by the likes of da Vinci, was the foundation of Abraham Gottlob Werner's (1749–1817) Neptunism theory in which all rocks precipitated out of a single flood.[40] A competing theory, Plutonism, was developed by Anton Moro (1687–1784) and also used primary and secondary divisions for rock units.[41][33] In this early version of the Plutonism theory, the interior of Earth was seen as hot, and this drove the creation of primary igneous and metamorphic rocks and secondary rocks formed contorted and fossiliferous sediments. These primary and secondary divisions were expanded on by Giovanni Targioni Tozzetti (1712–1783) and Giovanni Arduino (1713–1795) to include tertiary and quaternary divisions.[33] These divisions were used to describe both the time during which the rocks were laid down, and the collection of rocks themselves (i.e., it was correct to say Tertiary rocks, and Tertiary Period). Only the Quaternary division is retained in the modern geologic time scale, while the Tertiary division was in use until the early 21st century. The Neptunism and Plutonism theories would compete into the early 19th century with a key driver for resolution of this debate being the work of James Hutton (1726–1797), in particular his Theory of the Earth, first presented before the Royal Society of Edinburgh in 1785.[42][8][43] Hutton's theory would later become known as uniformitarianism, popularised by John Playfair[44] (1748–1819) and later Charles Lyell (1797–1875) in his Principles of Geology.[9][45][46] Their theories strongly contested the 6,000 year age of the Earth as suggested determined by James Ussher via Biblical chronology that was accepted at the time by western religion. Instead, using geological evidence, they contested Earth to be much older, cementing the concept of deep time. During the early 19th century William Smith, Georges Cuvier, Jean d'Omalius d'Halloy, and Alexandre Brongniart pioneered the systematic division of rocks by stratigraphy and fossil assemblages. These geologists began to use the local names given to rock units in a wider sense, correlating strata across national and continental boundaries based on their similarity to each other. Many of the names below erathem/era rank in use on the modern ICC/GTS were determined during the early to mid-19th century. During the 19th century, the debate regarding Earth's age was renewed, with geologists estimating ages based on denudation rates and sedimentary thicknesses or ocean chemistry, and physicists determining ages for the cooling of the Earth or the Sun using basic thermodynamics or orbital physics.[5] These estimations varied from 15,000 million years to 0.075 million years depending on method and author, but the estimations of Lord Kelvin and Clarence King were held in high regard at the time due to their pre-eminence in physics and geology. All of these early geochronometric determinations would later prove to be incorrect. The establishment of the IUGS in 1961[52] and acceptance of the Commission on Stratigraphy (applied in 1965)[53] to become a member commission of IUGS led to the founding of the ICS. One of the primary objectives of the ICS is \"the establishment, publication and revision of the ICS International Chronostratigraphic Chart which is the standard, reference global Geological Time Scale to include the ratified Commission decisions\".[1] Following on from Holmes, several A Geological Time Scale books were published in 1982,[54] 1989,[55] 2004,[56] 2008,[57] 2012,[58] 2016,[59] and 2020.[60] However, since 2013, the ICS has taken responsibility for producing and distributing the ICC citing the commercial nature, independent creation, and lack of oversight by the ICS on the prior published GTS versions (GTS books prior to 2013) although these versions were published in close association with the ICS.[2] Subsequent Geologic Time Scale books (2016[59] and 2020[60]) are commercial publications with no oversight from the ICS, and do not entirely conform to the chart produced by the ICS. The ICS produced GTS charts are versioned (year/month) beginning at v2013/01. At least one new version is published each year incorporating any changes ratified by the ICS since the prior version. The following five timelines show the geologic time scale to scale. The first shows the entire time from the formation of the Earth to the present, but this gives little space for the most recent eon. The second timeline shows an expanded view of the most recent eon. In a similar way, the most recent era is expanded in the third timeline, the most recent period is expanded in the fourth timeline, and the most recent epoch is expanded in the fifth timeline. Horizontal scale is Millions of years (above timelines) / Thousands of years (below timeline) First suggested in 2000,[61] the Anthropocene is a proposed epoch/series for the most recent time in Earth's history. While still informal, it is a widely used term to denote the present geologic time interval, in which many conditions and processes on Earth are profoundly altered by human impact.[62] As of April 2022[update] the Anthropocene has not been ratified by the ICS; however, in May 2019 the Anthropocene Working Group voted in favour of submitting a formal proposal to the ICS for the establishment of the Anthropocene Series/Epoch.[63] Nevertheless, the definition of the Anthropocene as a geologic time period rather than a geologic event remains controversial and difficult.[64][65][66][67] An international working group of the ICS on pre-Cryogenian chronostratigraphic subdivision have outlined a template to improve the pre-Cryogenian geologic time scale based on the rock record to bring it in line with the post-Tonian geologic time scale.[6] This work assessed the geologic history of the currently defined eons and eras of the pre-Cambrian,[note 3] and the proposals in the \"Geological Time Scale\" books 2004,[68]2012,[3] and 2020.[69] Their recommend revisions[6] of the pre-Cryogenian geologic time scale were (changes from the current scale [v2023/09] are italicised): Three divisions of the Archean instead of four by dropping Eoarchean, and revisions to their geochronometric definition, along with the repositioning of the Siderian into the latest Neoarchean, and a potential Kratian division in the Neoarchean. Siderian (?–2450 Ma) – moved from Proterozoic to end of Archean, no start time given, base of Paleoproterozoic defines the end of the Siderian Refinement of geochronometric divisions of the Proterozoic, Paleoproterozoic, repositioning of the Statherian into the Mesoproterozoic, new Skourian period/system in the Paleoproterozoic, new Kleisian or Syndian period/system in the Neoproterozoic. The book, Geologic Time Scale 2012, was the last commercial publication of an international chronostratigraphic chart that was closely associated with the ICS.[2] It included a proposal to substantially revise the pre-Cryogenian time scale to reflect important events such as the formation of the Solar System and the Great Oxidation Event, among others, while at the same time maintaining most of the previous chronostratigraphic nomenclature for the pertinent time span.[70] As of April 2022[update] these proposed changes have not been accepted by the ICS. The proposed changes (changes from the current scale [v2023/09]) are italicised: Vaalbaran Period/System (3490–3020 Ma) – based on the names of the Kapvaal (Southern Africa) and Pilbara (Western Australia) cratons, to reflect the growth of stable continental nuclei or proto-cratonic kernels.[58] Pongolan Period/System (3020–2780 Ma) – named after the Pongola Supergroup, in reference to the well preserved evidence of terrestrial microbial communities in those rocks.[58] Oxygenian Period/System (2420–2250 Ma) – named for displaying the first evidence for a global oxidising atmosphere.[58] Jatulian or Eukaryian Period/System (2250–2060 Ma) – names are respectively for the Lomagundi–Jatuli δ13C isotopic excursion event spanning its duration, and for the (proposed)[73][74] first fossil appearance of eukaryotes.[58] The following table summarises the major events and characteristics of the divisions making up the geologic time scale of Earth. This table is arranged with the most recent geologic periods at the top, and the oldest at the bottom. The height of each table entry does not correspond to the duration of each subdivision of time. As such, this table is not to scale and does not accurately represent the relative time-spans of each geochronologic unit. While the Phanerozoic Eon looks longer than the rest, it merely spans ~539 million years (~12% of Earth's history), whilst the previous three eons[note 3] collectively span ~3,461 million years (~76% of Earth's history). This bias toward the most recent eon is in part due to the relative lack of information about events that occurred during the first three eons compared to the current eon (the Phanerozoic).[6][75] The use of subseries/subepochs has been ratified by the ICS.[17] Some other planets and satellites in the Solar System have sufficiently rigid structures to have preserved records of their own histories, for example, Venus, Mars and the Earth's Moon. Dominantly fluid planets, such as the gas giants, do not comparably preserve their history. Apart from the Late Heavy Bombardment, events on other planets probably had little direct influence on the Earth, and events on Earth had correspondingly little effect on those planets. Construction of a time scale that links the planets is, therefore, of only limited relevance to the Earth's time scale, except in a Solar System context. The existence, timing, and terrestrial effects of the Late Heavy Bombardment are still a matter of debate.[note 14] The geologic history of Earth's Moon has been divided into a time scale based on geomorphological markers, namely impact cratering, volcanism, and erosion. This process of dividing the Moon's history in this manner means that the time scale boundaries do not imply fundamental changes in geological processes, unlike Earth's geologic time scale. Five geologic systems/periods (Pre-Nectarian, Nectarian, Imbrian, Eratosthenian, Copernican), with the Imbrian divided into two series/epochs (Early and Late) were defined in the latest Lunar geologic time scale.[95] The Moon is unique in the Solar System in that it is the only other body from which we have rock samples with a known geological context. The geological history of Mars has been divided into two alternate time scales. The first time scale for Mars was developed by studying the impact crater densities on the Martian surface. Through this method four periods have been defined, the Pre-Noachian (~4,500–4,100 Ma), Noachian (~4,100–3,700 Ma), Hesperian (~3,700–3,000 Ma), and Amazonian (~3,000 Ma to present).[96][97] Martian time periods (millions of years ago) A second time scale based on mineral alteration observed by the OMEGA spectrometer on-board the Mars Express. Using this method, three periods were defined, the Phyllocian (~4,500–4,000 Ma), Theiikian (~4,000–3,500 Ma), and Siderikian (~3,500 Ma to present).[98] ^It is now known that not all sedimentary layers are deposited purely horizontally, but this principle is still a useful concept. ^Time spans of geologic time units vary broadly, and there is no numeric limitation on the time span they can represent. They are limited by the time span of the higher rank unit they belong to, and to the chronostratigraphic boundaries they are defined by. ^ abcPrecambrian or pre-Cambrian is an informal geological term for time before the Cambrian period ^This denomination of time is not universally recognized by geologists ^ abThe Tertiary is a now obsolete geologic system/period spanning from 66 Ma to 2.6 Ma. It has no exact equivalent in the modern ICC, but is approximately equivalent to the merged Palaeogene and Neogene systems/periods. ^ abGeochronometric date for the Ediacaran has been adjusted to reflect ICC v2023/09 as the formal definition for the base of the Cambrian has not changed. ^Kratian time span is not given in the article. It lies within the Neoarchean, and prior to the Siderian. The position shown here is an arbitrary division. ^Desnoyers, J. (1829). \"Observations sur un ensemble de dépôts marins plus récents que les terrains tertiaires du bassin de la Seine, et constituant une formation géologique distincte; précédées d'un aperçu de la nonsimultanéité des bassins tertiares\" [Observations on a set of marine deposits [that are] more recent than the tertiary terrains of the Seine basin and [that] constitute a distinct geological formation; preceded by an outline of the non-simultaneity of tertiary basins]. Annales des Sciences Naturelles (in French). 16: 171–214, 402–491.From p. 193:\"Ce que je désirerais ... dont il faut également les distinguer.\" (What I would desire to prove above all is that the series of tertiary deposits continued – and even began in the more recent basins – for a long time, perhaps after that of the Seine had been completely filled, and that these later formations – Quaternary (1), so to say – should not retain the name of alluvial deposits any more than the true and ancient tertiary deposits, from which they must also be distinguished.) However, on the very same page, Desnoyers abandoned the use of the term \"Quaternary\" because the distinction between Quaternary and Tertiary deposits wasn't clear. From p. 193: \"La crainte de voir mal comprise ... que ceux du bassin de la Seine.\" (The fear of seeing my opinion in this regard be misunderstood or exaggerated, has made me abandon the word \"quaternary\", which at first I had wanted to apply to all deposits more recent than those of the Seine basin.)"}
{"url": "https://en.wikipedia.org/wiki/Molybdenum", "text": "Most molybdenum compounds have low solubility in water, but when molybdenum-bearing minerals contact oxygen and water, the resulting molybdate ion MoO2− 4 is quite soluble. Industrially, molybdenum compounds (about 14% of world production of the element) are used in high-pressure and high-temperature applications as pigments and catalysts. Molybdenum-bearing enzymes are by far the most common bacterial catalysts for breaking the chemical bond in atmospheric molecular nitrogen in the process of biological nitrogen fixation. At least 50 molybdenum enzymes are now known in bacteria, plants, and animals, although only bacterial and cyanobacterial enzymes are involved in nitrogen fixation. These nitrogenases contain an iron–molybdenum cofactor FeMoco, which is believed to contain either Mo(III) or Mo(IV).[12][13] This is distinct from the fully oxidized Mo(VI) found complexed with molybdopterin in all other molybdenum-bearing enzymes, which perform a variety of crucial functions.[14] The variety of crucial reactions catalyzed by these latter enzymes means that molybdenum is an essential element for all higher eukaryote organisms, including humans. Molybdenum is a transition metal with an electronegativity of 2.16 on the Pauling scale. It does not visibly react with oxygen or water at room temperature, but is attacked by halogens and hydrogen peroxide. Weak oxidation of molybdenum starts at 300 °C (572 °F); bulk oxidation occurs at temperatures above 600 °C, resulting in molybdenum trioxide. Like many heavier transition metals, molybdenum shows little inclination to form a cation in aqueous solution, although the Mo3+ cation is known to form under carefully controlled conditions.[18] Gaseous molybdenum consists of the diatomic species Mo2. That molecule is a singlet, with two unpaired electrons in bonding orbitals, in addition to 5 conventional bonds. The result is a sextuple bond.[19][20] There are 39 known isotopes of molybdenum, ranging in atomic mass from 81 to 119, as well as 13 metastable nuclear isomers. Seven isotopes occur naturally, with atomic masses of 92, 94, 95, 96, 97, 98, and 100. Of these naturally occurring isotopes, only molybdenum-100 is unstable.[8] Molybdenum forms chemical compounds in oxidation states −4 and from −2 to +6. Higher oxidation states are more relevant to its terrestrial occurrence and its biological roles, mid-level oxidation states are often associated with metal clusters, and very low oxidation states are typically associated with organomolybdenum compounds. The chemistry of molybdenum and tungsten show strong similarities. The relative rarity of molybdenum(III), for example, contrasts with the pervasiveness of the chromium(III) compounds. The highest oxidation state is seen in molybdenum(VI) oxide (MoO3), whereas the normal sulfur compound is molybdenum disulfide MoS2.[23] From the perspective of commerce, the most important compounds are molybdenum disulfide (MoS 2) and molybdenum trioxide (MoO 3). The black disulfide is the main mineral. It is roasted in air to give the trioxide:[23] 2 MoS 2 + 7 O 2 → 2 MoO 3 + 4 SO 2 The trioxide, which is volatile at high temperatures, is the precursor to virtually all other Mo compounds as well as alloys. Molybdenum has several oxidation states, the most stable being +4 and +6 (bolded in the table at left). Molybdenum(VI) oxide is soluble in strong alkaline water, forming molybdates (MoO42−). Molybdates are weaker oxidants than chromates. They tend to form structurally complex oxyanions by condensation at lower pH values, such as [Mo7O24]6− and [Mo8O26]4−. Polymolybdates can incorporate other ions, forming polyoxometalates.[27] The dark-blue phosphorus-containing heteropolymolybdate P[Mo12O40]3− is used for the spectroscopic detection of phosphorus.[28] The broad range of oxidation states of molybdenum is reflected in various molybdenum chlorides:[23] Molybdenite—the principal ore from which molybdenum is now extracted—was previously known as molybdena. Molybdena was confused with and often utilized as though it were graphite. Like graphite, molybdenite can be used to blacken a surface or as a solid lubricant.[32] Even when molybdena was distinguishable from graphite, it was still confused with the common lead ore PbS (now called galena); the name comes from Ancient GreekΜόλυβδοςmolybdos, meaning lead.[17] (The Greek word itself has been proposed as a loanword from AnatolianLuvian and Lydian languages).[33] Although (reportedly) molybdenum was deliberately alloyed with steel in one 14th-century Japanese sword (mfd. c. 1330), that art was never employed widely and was later lost.[34][35] In the West in 1754, Bengt Andersson Qvist examined a sample of molybdenite and determined that it did not contain lead and thus was not galena.[36] By 1778 Swedish chemist Carl Wilhelm Scheele stated firmly that molybdena was (indeed) neither galena nor graphite.[37][38] Instead, Scheele correctly proposed that molybdena was an ore of a distinct new element, named molybdenum for the mineral in which it resided, and from which it might be isolated. Peter Jacob Hjelm successfully isolated molybdenum using carbon and linseed oil in 1781.[17][39] For the next century, molybdenum had no industrial use. It was relatively scarce, the pure metal was difficult to extract, and the necessary techniques of metallurgy were immature.[40][41][42] Early molybdenum steel alloys showed great promise of increased hardness, but efforts to manufacture the alloys on a large scale were hampered with inconsistent results, a tendency toward brittleness, and recrystallization. In 1906, William D. Coolidge filed a patent for rendering molybdenum ductile, leading to applications as a heating element for high-temperature furnaces and as a support for tungsten-filament light bulbs; oxide formation and degradation require that molybdenum be physically sealed or held in an inert gas.[43] In 1913, Frank E. Elmore developed a froth flotation process to recover molybdenite from ores; flotation remains the primary isolation process.[44] During World War I, demand for molybdenum spiked; it was used both in armor plating and as a substitute for tungsten in high-speed steels. Some British tanks were protected by 75 mm (3 in) manganese steel plating, but this proved to be ineffective. The manganese steel plates were replaced with much lighter 25 mm (1.0 in) molybdenum steel plates allowing for higher speed, greater maneuverability, and better protection.[17] The Germans also used molybdenum-doped steel for heavy artillery, like in the super-heavy howitzer Big Bertha,[45] because traditional steel melts at the temperatures produced by the propellant of the one ton shell.[46] After the war, demand plummeted until metallurgical advances allowed extensive development of peacetime applications. In World War II, molybdenum again saw strategic importance as a substitute for tungsten in steel alloys.[47] Molybdenum is the 54th most abundant element in the Earth's crust with an average of 1.5 parts per million and the 25th most abundant element in its oceans, with an average of 10 parts per billion; it is the 42nd most abundant element in the Universe.[17][48] The Soviet Luna 24 mission discovered a molybdenum-bearing grain (1 × 0.6 µm) in a pyroxene fragment taken from Mare Crisium on the Moon.[49] The comparative rarity of molybdenum in the Earth's crust is offset by its concentration in a number of water-insoluble ores, often combined with sulfur in the same way as copper, with which it is often found. Though molybdenum is found in such minerals as wulfenite (PbMoO4) and powellite (CaMoO4), the main commercial source is molybdenite (MoS2). Molybdenum is mined as a principal ore and is also recovered as a byproduct of copper and tungsten mining.[10] The world's production of molybdenum was 250,000 tonnes in 2011, the largest producers being China (94,000 t), the United States (64,000 t), Chile (38,000 t), Peru (18,000 t) and Mexico (12,000 t). The total reserves are estimated at 10 million tonnes, and are mostly concentrated in China (4.3 Mt), the US (2.7 Mt) and Chile (1.2 Mt). By continent, 93% of world molybdenum production is about evenly shared between North America, South America (mainly in Chile), and China. Europe and the rest of Asia (mostly Armenia, Russia, Iran and Mongolia) produce the remainder.[50] World production trend In molybdenite processing, the ore is first roasted in air at a temperature of 700 °C (1,292 °F). The process gives gaseous sulfur dioxide and the molybdenum(VI) oxide:[23] Copper, an impurity in molybdenite, is separated at this stage by treatment with hydrogen sulfide.[23] Ammonium molybdate converts to ammonium dimolybdate, which is isolated as a solid. Heating this solid gives molybdenum trioxide:[51] Molybdenum had a value of approximately $30,000 per tonne as of August 2009. It maintained a price at or near $10,000 per tonne from 1997 through 2003, and reached a peak of $103,000 per tonne in June 2005.[53] In 2008, the London Metal Exchange announced that molybdenum would be traded as a commodity.[54] The Knaben mine in southern Norway, opened in 1885, was the first dedicated molybdenum mine. Closed in 1973 but reopened in 2007,[55] it now produces 100,000 kilograms (98 long tons; 110 short tons) of molybdenum disulfide per year. Large mines in Colorado (such as the Henderson mine and the Climax mine)[56] and in British Columbia yield molybdenite as their primary product, while many porphyry copper deposits such as the Bingham Canyon Mine in Utah and the Chuquicamata mine in northern Chile produce molybdenum as a byproduct of copper-mining. About 86% of molybdenum produced is used in metallurgy, with the rest used in chemical applications. The estimated global use is structural steel 35%, stainless steel 25%, chemicals 14%, tool & high-speed steels 9%, cast iron 6%, molybdenum elemental metal 6%, and superalloys 5%.[57] Molybdenum can withstand extreme temperatures without significantly expanding or softening, making it useful in environments of intense heat, including military armor, aircraft parts, electrical contacts, industrial motors, and supports for filaments in light bulbs.[17][58] Molybdenum is also used in steel alloys for its high corrosion resistance and weldability.[48][50] Molybdenum contributes corrosion resistance to type-300 stainless steels (specifically type-316) and especially so in the so-called superaustenitic stainless steels (such as alloy AL-6XN, 254SMO and 1925hMo). Molybdenum increases lattice strain, thus increasing the energy required to dissolve iron atoms from the surface.[contradictory] Molybdenum is also used to enhance the corrosion resistance of ferritic (for example grade 444)[59] and martensitic (for example 1.4122 and 1.4418) stainless steels.[citation needed] Because of its lower density and more stable price, molybdenum is sometimes used in place of tungsten.[48] An example is the 'M' series of high-speed steels such as M2, M4 and M42 as substitution for the 'T' steel series, which contain tungsten. Molybdenum can also be used as a flame-resistant coating for other metals. Although its melting point is 2,623 °C (4,753 °F), molybdenum rapidly oxidizes at temperatures above 760 °C (1,400 °F) making it better-suited for use in vacuum environments.[58] TZM (Mo (~99%), Ti (~0.5%), Zr (~0.08%) and some C) is a corrosion-resisting molybdenum superalloy that resists molten fluoride salts at temperatures above 1,300 °C (2,370 °F). It has about twice the strength of pure Mo, and is more ductile and more weldable, yet in tests it resisted corrosion of a standard eutectic salt (FLiBe) and salt vapors used in molten salt reactors for 1100 hours with so little corrosion that it was difficult to measure.[60][61] Other molybdenum-based alloys that do not contain iron have only limited applications. For example, because of its resistance to molten zinc, both pure molybdenum and molybdenum-tungsten alloys (70%/30%) are used for piping, stirrers and pump impellers that come into contact with molten zinc.[62] Molybdenum powder is used as a fertilizer for some plants, such as cauliflower.[48] Elemental molybdenum is used in NO, NO2, NOx analyzers in power plants for pollution controls. At 350 °C (662 °F), the element acts as a catalyst for NO2/NOx to form NO molecules for detection by infrared light.[63] Molybdenum disulfide (MoS2) is used as a solid lubricant and a high-pressure high-temperature (HPHT) anti-wear agent. It forms strong films on metallic surfaces and is a common additive to HPHT greases — in the event of a catastrophic grease failure, a thin layer of molybdenum prevents contact of the lubricated parts.[67] When combined with small amounts of cobalt, MoS2 is also used as a catalyst in the hydrodesulfurization (HDS) of petroleum. In the presence of hydrogen, this catalyst facilitates the removal of nitrogen and especially sulfur from the feedstock, which otherwise would poison downstream catalysts. HDS is one of the largest scale applications of catalysis in industry.[68] Molybdenum oxides are important catalysts for selective oxidation of organic compounds. The production of the commodity chemicals acrylonitrile and formaldehyde relies on MoOx-based catalysts.[51] Lead molybdate (wulfenite) co-precipitated with lead chromate and lead sulfate is a bright-orange pigment used with ceramics and plastics.[70] The molybdenum-based mixed oxides are versatile catalysts in the chemical industry. Some examples are the catalysts for the oxidation of carbon monoxide, propylene to acrolein and acrylic acid, the ammoxidation of propylene to acrylonitrile.[71][72] Molybdenum carbides, nitride and phosphides can be used for hydrotreatment of rapeseed oil.[73] Molybdenum is an essential element in most organisms; a 2008 research paper speculated that a scarcity of molybdenum in the Earth's early oceans may have strongly influenced the evolution of eukaryotic life (which includes all plants and animals).[74] In terms of function, molybdoenzymes catalyze the oxidation and sometimes reduction of certain small molecules in the process of regulating nitrogen, sulfur, and carbon.[78] In some animals, and in humans, the oxidation of xanthine to uric acid, a process of purinecatabolism, is catalyzed by xanthine oxidase, a molybdenum-containing enzyme. The activity of xanthine oxidase is directly proportional to the amount of molybdenum in the body. An extremely high concentration of molybdenum reverses the trend and can inhibit purine catabolism and other processes. Molybdenum concentration also affects protein synthesis, metabolism, and growth.[79] Mo is a component in most nitrogenases. Among molybdoenzymes, nitrogenases are unique in lacking the molybdopterin.[80][81] Nitrogenases catalyze the production of ammonia from atmospheric nitrogen: Structure of the FeMoco active site of nitrogenaseThe molybdenum cofactor (pictured) is composed of a molybdenum-free organic complex called molybdopterin, which has bound an oxidized molybdenum(VI) atom through adjacent sulfur (or occasionally selenium) atoms. Except for the ancient nitrogenases, all known Mo-using enzymes use this cofactor. Acute toxicity has not been seen in humans, and the toxicity depends strongly on the chemical state. Studies on rats show a median lethal dose (LD50) as low as 180 mg/kg for some Mo compounds.[89] Although human toxicity data is unavailable, animal studies have shown that chronic ingestion of more than 10 mg/day of molybdenum can cause diarrhea, growth retardation, infertility, low birth weight, and gout; it can also affect the lungs, kidneys, and liver.[90][91]Sodium tungstate is a competitive inhibitor of molybdenum. Dietary tungsten reduces the concentration of molybdenum in tissues.[48] Molybdenum deficiency has also been reported as a consequence of non-molybdenum supplemented total parenteral nutrition (complete intravenous feeding) for long periods of time. It results in high blood levels of sulfite and urate, in much the same way as molybdenum cofactor deficiency. Since pure molybdenum deficiency from this cause occurs primarily in adults, the neurological consequences are not as marked as in cases of congenital cofactor deficiency.[96] A congenital molybdenum cofactor deficiency disease, seen in infants, is an inability to synthesize molybdenum cofactor, the heterocyclic molecule discussed above that binds molybdenum at the active site in all known human enzymes that use molybdenum. The resulting deficiency results in high levels of sulfite and urate, and neurological damage.[97][98] Most molybdenum is excreted from the human body as molybdate in the urine. Furthermore, urinary excretion of molybdenum increases as dietary molybdenum intake increases. Small amounts of molybdenum are excreted from the body in the feces by way of the bile; small amounts also can be lost in sweat and in hair.[99][100] High levels of molybdenum can interfere with the body's uptake of copper, producing copper deficiency. Molybdenum prevents plasma proteins from binding to copper, and it also increases the amount of copper that is excreted in urine. Ruminants that consume high levels of molybdenum suffer from diarrhea, stunted growth, anemia, and achromotrichia (loss of fur pigment). These symptoms can be alleviated by copper supplements, either dietary and injection.[101] The effective copper deficiency can be aggravated by excess sulfur.[48][102] Copper reduction or deficiency can also be deliberately induced for therapeutic purposes by the compound ammonium tetrathiomolybdate, in which the bright red anion tetrathiomolybdate is the copper-chelating agent. Tetrathiomolybdate was first used therapeutically in the treatment of copper toxicosis in animals. It was then introduced as a treatment in Wilson's disease, a hereditary copper metabolism disorder in humans; it acts both by competing with copper absorption in the bowel and by increasing excretion. It has also been found to have an inhibitory effect on angiogenesis, potentially by inhibiting the membrane translocation process that is dependent on copper ions.[103] This is a promising avenue for investigation of treatments for cancer, age-related macular degeneration, and other diseases that involve a pathologic proliferation of blood vessels.[104][105] In some grazing livestock, most strongly in cattle, molybdenum excess in the soil of pasturage can produce scours (diarrhea) if the pH of the soil is neutral to alkaline; see teartness. In 2000, the then U.S. Institute of Medicine (now the National Academy of Medicine, NAM) updated its Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for molybdenum. If there is not sufficient information to establish EARs and RDAs, an estimate designated Adequate Intake (AI) is used instead. An AI of 2 micrograms (μg) of molybdenum per day was established for infants up to 6 months of age, and 3 μg/day from 7 to 12 months of age, both for males and females. For older children and adults, the following daily RDAs have been established for molybdenum: 17 μg from 1 to 3 years of age, 22 μg from 4 to 8 years, 34 μg from 9 to 13 years, 43 μg from 14 to 18 years, and 45 μg for persons 19 years old and older. All these RDAs are valid for both sexes. Pregnant or lactating females from 14 to 50 years of age have a higher daily RDA of 50 μg of molybdenum. The European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL defined the same as in United States. For women and men ages 15 and older the AI is set at 65 μg/day. Pregnant and lactating women have the same AI. For children aged 1–14 years, the AIs increase with age from 15 to 45 μg/day. The adult AIs are higher than the U.S. RDAs,[107] but on the other hand, the European Food Safety Authority reviewed the same safety question and set its UL at 600 μg/day, which is much lower than the U.S. value.[108] For U.S. food and dietary supplement labeling purposes, the amount in a serving is expressed as a percent of Daily Value (%DV). For molybdenum labeling purposes 100% of the Daily Value was 75 μg, but as of May 27, 2016 it was revised to 45 μg.[109][110] A table of the old and new adult daily values is provided at Reference Daily Intake. Average daily intake varies between 120 and 240 μg/day, which is higher than dietary recommendations.[90] Pork, lamb, and beef liver each have approximately 1.5 parts per million of molybdenum. Other significant dietary sources include green beans, eggs, sunflower seeds, wheat flour, lentils, cucumbers, and cereal grain.[17] Molybdenum dusts and fumes, generated by mining or metalworking, can be toxic, especially if ingested (including dust trapped in the sinuses and later swallowed).[89] Low levels of prolonged exposure can cause irritation to the eyes and skin. Direct inhalation or ingestion of molybdenum and its oxides should be avoided.[111][112]OSHA regulations specify the maximum permissible molybdenum exposure in an 8-hour day as 5 mg/m3. Chronic exposure to 60 to 600 mg/m3 can cause symptoms including fatigue, headaches and joint pains.[113] At levels of 5000 mg/m3, molybdenum is immediately dangerous to life and health.[114] ^Pope, Michael T.; Müller, Achim (1997). \"Polyoxometalate Chemistry: An Old Field with New Dimensions in Several Disciplines\". Angewandte Chemie International Edition. 30: 34–48. doi:10.1002/anie.199100341. ^Krupp, Alfred; Wildberger, Andreas (1888). The metallic alloys: A practical guide for the manufacture of all kinds of alloys, amalgams, and solders, used by metal-workers ... with an appendix on the coloring of alloys. H.C. Baird & Co. p. 60. ^Kean, Sam (2011-06-06). The Disappearing Spoon: And Other True Tales of Madness, Love, and the History of the World from the Periodic Table of the Elements (Illustrated ed.). Back Bay Books. pp. 88–89. ISBN978-0-316-05163-7."}
{"url": "https://en.wikipedia.org/wiki/El_Dorado_International_Airport", "text": "El Dorado has consistently been ranked since 2016 by World Airport Awards and Skytrax as among the best airports in the world. It was ranked as the best airport in South America by World Airport Awards and the best airport in Latin America overall by Skytrax with a four-star certification by Skytrax in 2022.[6][7] It was also ranked as having the best staff in South America and placed 35th in Skytrax's World's Top 100 Airports. As well as ranking as the 2nd cleanest airport in Latin America by Skytrax in 2023.[8][9] An Avianca B707-359B (HK-1410) at El Dorado International Airport in 1972.Main entrance for domestic departures at El Dorado International AirportBaggage claim hall at Bogota El Dorado AirportInternational Departures gate in the Terminal 1Just outside of the airport in 2018 The airport was designed and built during the government of General Gustavo Rojas Pinilla as part of his campaign in modernize the country. Construction began in 1955 and entered in service by December 1959, replacing the aging Techo International Airport, which had been the city's main airport since 1930.[10] Before its inauguration, Soledad International Airport in Barranquilla was the nation's air hub. It was relegated to secondary importance in the country when El Dorado Airport opened. The Sociedad de Mejoras y Ornato de Bogotá was in charge of the external decoration of the airport. On September 29, after a meeting of the Board of Directors of project, the inauguration date was set for December 10, 1959.[10] Before its completion on October 28, 1959, the airports's manager, René Van Meerbeke, recommended that the airport's name be placed in the central part of the building's façade; however, the name was the subject of much reflection: a short, easy-to-pronounce, terrigenous name was sought that would recall the aboriginal culture that occupied the Bogotá Savannah. Thus, El Dorado was chosen as the most appropriate name for the new terminal. After much discussion among academics led by Professor Luis López de Mesa, Father Félix Restrepo and Alberto Miramón, the name of the airport was determined by decree, written without spaces, forming a single word.[10] On December 10, 1959, the Eldorado International Air was inaugurated with a spectacular aeronautical exhibition beginning on December 6 which included 16 airplanes, 12 jet-propelled and four additional B-26s that caused the greatest impression on the public.[10] In the archives of the firm of architects and engineers Cuéllar, Serrano, Gómez y Cía., which participated in the construction of the airport, and of the Sociedad de Mejoras y Ornato, there is no record of the company that manufactured the Eldorado sign; neither is the weight of the eight letters known, since they did not need maintenance since they were installed. The airport reached a milestone in 1973 with over three million passengers and nearly 5 million units of luggage processed. Plans began for a second runway at El Dorado with concerns that the explosive growth would lead to over congestion in the future.[citation needed] In 1998, the airport's second runway was inaugurated, which has received much disagreement from the residents of the nearby area of Fontibón, due to the incessant air operations during the day and night. In 2012, in accordance with Law 75 of 1989, which honors the memory of Luis Carlos Galán, Law 1529 of 2012 made official the name change from \"El Dorado International Airport\" to \"Luis Carlos Galán Sarmiento El Dorado International Airport\", including the space between \"El\" and \"Dorado\", and the name of the politician from Bumán. This name change generated a strong controversy, especially because of the costs associated with it, and because of the non-existent relationship between Galán and the airport.[11] At the end of 2017, all the works corresponding to El Dorado International Airport were fully handed over. The airport's passenger terminal grew from 173,000 to 235,000 m² (square meters), with the potential to serve up to 43 million people per year. In 1981, Avianca undertook the construction of the Puente Aéreo Terminal inaugurated by President Julio César Turbay Ayala. The name Puente Aéreo means Air Bridge in English. Avianca's original purpose for the terminal was for flights serving Cali, Medellín, Miami and New York. During the first years of operation and until 2005 Avianca gradually moved all of its domestic operations to the Puente Aéreo and moved the Miami and New York operations to the main terminal. This allowed them to streamline their operations by using space previously assigned to customs and immigration for passenger gates and lounges. The culmination of this process came in 2006 when the airline undertook extensive renovations on the building. However, the airline was mindful of the impending and current renovations of El Dorado. One possible plan will be demolishing the Puente Aéreo Terminal, Main terminal and old cargo buildings which will be replaced with a new mega terminal. Many of the renovations made to the terminal in 2006 were obviously temporary and designed to be cheap but effective. For example, the walkways for the new gates are simply floor tiles placed over the old tarmac and the structure is made of aluminum with plastic sheets instead of glass windows. Passengers must cross the lanes used by buses, baggage carts and other vehicles in order to reach the aircraft. Once at the gate travellers must climb stairs to access the plane, the norm in the 1950s and 1960s but has for many years been surpassed by jetways. In February 2008, Avianca opened a pioneer store called Avianca store which sells different products including: toy airplanes, hats, umbrellas, clothing, stuffed toys, pens, mugs and other such products, all embossed with the company logo. The store was an instant success and the airline expanded the concept to various other cities in Colombia. On 28 April 2018, Avianca moved its entire domestic operation to Terminal 1 and local carriers Satena and EasyFly started operating from Puente Aéreo or Terminal 2 On 3 September 1932 it was launched the first Military Transport Service in Colombia, when a Junkers F-13 carried Colonel Luis Acevedo and his party to Leticia. Colonel Acevedo also served as Colombia's General Director of aviation. Although the military air transport infrastructure was not formed yet, that mission was accomplished during the conflict with Peru in a rudimentary but effective way, with aircraft like the Junkers W 34, Ju 52 and BT-32 Condor. In 1954 he created a \"Liaison Squadron\" operating under direct orders of the President of the Republic, at the time, Gen. Gustavo Rojas Pinilla. The Squadron was located in the Airport of Techo, the first airport of Bogotá. Its success led to the creation of a Military Airlift Group, which reached the category of Transportation Base in 1959. By then El Dorado International Airport was finished, so the Colombian Air Force ordered the transfer of the Unit to an area adjacent to the new Airport of El Dorado, using the civil airport facilities, while finishing the construction of a new base. The base was baptized as Comando Aéreo de Transporte Militar (Military Transportation Air Command) or CATAM. The base was inaugurated on 28 May 1963. The base acquired the status of Operations and Logistics Support Center by FAC Directive No. 4429 of 8 July 1963, starting operations on 25 October. In 1968 the first two Hercules C-130B, with Colombian airplane military numbers FAC-1001 and FAC-1002, were delivered to this base. These aircraft, clearly designed for war missions and troop and materials transport, were able to use short and unpaved runways used in military operations through the country, fulfilling the needs of Colombian Air Force. In 1977, the Military Transport Aviation Command was named after the Colombian aviation pioneer, Honorary Brigadier General Camilo Daza Alvarez. In order to expand its capacity for troop and cargo transportation in support of surface forces, in their fight against subversion and drug trafficking, the Air Force acquired new C-130 Hercules aircraft that been used for security purposes but also for humanitarian assistance. Between 1990 and 1991 the base received from the U.S. government six C-130B aircraft to support operations to combat drug trafficking and guerrillas. In 1996 the base opened new ground accesses through an area devoted to the Military Transport Aviation Command. The narrow street that impeded the entrance and exit of vehicles was replaced by a dual carriageway and a tunnel that allows access to vehicular traffic passing below the airplane access ramp to runway number 2 of El Dorado International Airport. The parking lot was also enlarged to serve up to 260 vehicles. The base hosts the Colombian Air Force Museum, which has planes in display that represent the various types used in service during the 85 years history of the force.[13] In 2003 NVG equipment for night vision air operations was installed in Hercules C-130 and CN-235 Nurtanio airplanes. This increased the operational and support capacity of the base given to ground Army force, by allowing transportation, parachuting and aeromedical evacuation on combat runways lacking illumination. In this way Colombian Air Force almost doubled its operating capacity at this base, since it works 24 hours a day.[14] The main passenger terminal is known as Terminal 1 (T1). The T1 building is shaped like a lowercase \"h\" and is divided into two piers or concourses: the international one to the north side and the domestic pier/concourse on the south side. Terminal 1 has four airline lounges (operated by LATAM, Avianca, and Copa), in addition to the El Dorado Lounge by Mastercard in the international concourse and one airline lounge (operated by Avianca) in the domestic concourse. It also offers a variety of food options, both on the air and land side, and several retail stores and cafés in the duty-free area. There are also car rental facilities, ticket counters, ATMs, telephones, restrooms, luggage storage space and even a small casino. The terminal has complimentary Wi-Fi service. T1 has several check-in counter areas, check-in kiosks and expanded immigration lanes compared to the previous terminal. \"Express lanes\" were added for holders of biometric passports and Global Entry Membership. The new terminal has moving walkways, escalators and elevators to guarantee universal access and faster connections. The new terminal contains 32 gates: 10 for international flights and 22 for domestic flights, five of which are remote stands. The \"Puente Aéreo\" is currently Terminal 2 (T2). It had previously been Avianca's exclusive terminal for domestic flights. On April 29, 2018, the airline moved the remainder of its domestic operation from T2 to T1, which in turn meant the switch from T1 to T2 of EasyFly and Satena, who are currently the sole operators at the terminal. Terminal 2 contains a revamped food plaza, some retail stores, and ATMs. Due to the high demand for passengers, the need has become apparent to build a new, more modern airport with a larger capacity for both commercial and cargo flights. The process began with the creation of the new terminal. On 7 February 2007, the airport gave a concession to the consortium Opain. The national government accepted the proposal with Opain (airport operating company), to demolish the airport on 14 March 2008, after having given its concession. Initially the grant provided for the modernization of existing buildings and the construction of some additional buildings connected to the main terminal, but during the upgrading works (see below, Milestone 1), structural defects were discovered, which do not compromise the integrity of the building today. Opain from the beginning had proposed to demolish the aging terminal and had even submitted a new design to replace it, but the government had strongly opposed it due to pressing budget and legal issues (because it would be a big change to the terms of the concession, which could make Opain as well as other competitors who participated in the tender submitted claims), although many sectors of public opinion agreed with Opain. After the structural problems were discovered, the government agreed to the demolition of the airport and compensation for the renovations that Opain had already been hired to perform (Milestone 1). For the airport to handle 16 million passengers annually and 1.5 million tons of cargo, Opain plans to move the cargo terminal to allow the expansion of the passenger terminal and ensure access for at least an additional avenue to 26th Street. On 19 September 2007, the implementation of Milestone 1 of the plan for modernization and expansion of the airport began. This consists of expanding the current Central Arrivals Hall of the terminal and installation of the CUTE system at the terminal. This was completed in March 2008. Additionally, the construction of the new cargo terminal, a new building for the office of civil aviation, a new fire station, an administrative center and quarantine were completed in September 2009. The third milestone of the project began in late November 2009. Terminal 2, located on the north side of the current terminal, will handle all international passengers and its construction was set for 2012. The old building or Terminal 1 will handle only national passengers, except for Avianca's which will continue being served on Terminal Puente Aereo. Soon after Terminal 2 begins its operation, the old Terminal 1 building will be demolished in order to build a new terminal for domestic passengers. On 17 October, the new Terminal 2 was inaugurated and on the 19th, every international operation was moved from Terminal 1 to Terminal 2. The new El Dorado International Airport, designed by Zyscovich Architects, was the largest infrastructure project in the city, when it was completed in July 2014.[19] In January 2015, a two-stage plan was proposed to improve Bogota's aerial access, as part of a greater endeavor to modernize Colombia's airports. The plan included a major expansion to the current main terminal with the effect of increasing the number of gates from 37 to 56 and thus raising the capacity of the airport from 27 million passengers to 40 million. Phase 1 also includes improvements to the runway to increase efficiency. The time scale for phase one was announced as approximately 24 months. Phase 2 involves the construction of a brand new secondary airport, currently called El Dorado II, in the suburb of Facatativa west of Bogota. The new airport was due to open in 2023.[20] It was expected that El Dorado and El Dorado II will be connected via a commuter/light rail project.[21] In 2023, plans were announced to double the capacity of El Dorado Airport with capacity to reach 60 million passengers with a contract awarded by 2025.[22] ^1Edelweiss Air's flight from Bogotá to Zurich makes a stop in Cartagena. However, the airline does not have eighth freedom traffic rights to transport passengers solely between Bogotá and Cartagena. The flight from Zurich to Bogotá is non-stop. ^2KLM's flight from Bogotá to Amsterdam makes a stop in Cartagena. However, the airline does not have eighth freedom traffic rights to transport passengers solely between Bogotá and Cartagena. The flight from Amsterdam to Bogotá is non-stop. ^3Turkish Airlines' flight from Bogotá to Istanbul makes a stop in Panama City. However, the airline does not have traffic rights to transport passengers solely between Bogotá and Panama City. The flight from Istanbul to Bogotá is non-stop. On 19 April 1960, a Lloyd Aéreo Colombiano Curtiss C-46 Commando crashed on approach to BOG because of loss of height and speed in the final approach turn. Six crew and 31 passengers out of 51 on board were killed.[39] On 22 September 1966, an AviancaDouglas DC-4 struck trees attempting to return to the airport following engine problems. Both occupants died.[40] On 18 December 1966, a Aerocondo-Aerovías Cóndor de Colombia Lockheed L-1649 Starliner crashed on approach 10-20m short of the runway threshold. Four crew and 13 passengers died out of the 59 on board.[41] On 27 August 1973, an Aerocondor-Aerovías Cóndor de Colombia Lockheed L-188 Electra flew into a mountain 12km (7.4mi) SE of BOG because the flight crew did not follow the prescribed departure procedure following takeoff. All six crew and 36 passengers died.[44] On 10 July 1975, an Aerocondor Lockheed L-188 Electra veered to the right shortly after lift off, sank back and struck another aircraft, causing a fire that destroyed both aircraft. Two occupants out of the four occupants died.[45] On 29 April 1978, a LAC ColombiaDouglas DC-6 crashed 1km NW of the airport because the aircraft was configured for cargo, not for carrying passengers. All three crew and five out the nine passengers were killed.[46] On 22 August 1979, a SATERA Hawker Siddeley HS 748 was stolen by a 23 year old man who was an airline mechanic who was just fired. The aircraft crashed into a Bogota suburb, killing the pilot and three on the ground.[47] On 24 January 1980, Douglas C-53D HK-2214 of Aerotal Colombia crashed after an in-flight engine failure following which the propeller on the engine was feathered. The aircraft was on a test flight. All four on board were killed.[48] On 8 February 1986, Douglas DC-3 HK-3031 of SAEP Colombia crashed on approach. The port engine had lost power shortly after take-off on a cargo flight to Rondon Airport and the decision was made to return to Bogotá. Although the aircraft was destroyed in the post-impact fire, all five people on board survived.[49] On 20 February 1993, a Cargo Three Panama Convair CV-440 crashed into a field 3km (1.9mi) W of the airport because of an engine fire. Both occupants perished.[50] On 20 April 1998, Air France Flight 422 from Eldorado Airport to Quito, Ecuador, using an aircraft leased from TAME and flown with Ecuadorian crew, crashed less than two minutes after taking off into a mountain in eastern Bogotá. All 43 passengers and 10 crew died.[51] On 11 October 2007, a Rio Jur Beechcraft Super King Air impacted the ground after banking right abruptly, passed a hangar, then impacted houses near the approach end of the runway. All five occupants were killed along with two on the ground. [52] On 7 July 2008, a Kalitta AirBoeing 747-209B, operating as Centurion Air Cargo Flight 164 on an aircraft that had been leased to Centurion Air Cargo, crashed shortly after departing from El Dorado International Airport in Bogotá at 3:55 am. The plane was en route to Miami, Florida, with a shipment of flowers. After reporting a fire in one of the engines, the plane attempted returning to the airport but crashed near the village of Madrid, Colombia. One of the plane's engines hit a farm house, killing an adult and two children who lived there. The crew of eight survived.[53][54] In 2016 and 2017 the airport was named the best in South America by World Airport Awards.[6] It received four-star rating by Skytrax and was listed in the top 50 of the \"World's Top 100 Airports\" list in both years.[8] Its staff was rated the best in South America by the World Airport Awards in 2017.[7][55]"}
{"url": "https://en.wikipedia.org/wiki/Catholic_Church_in_Latin_America", "text": "In the later part of the 20th century, however, the rise of Liberation theology has challenged such close alliances between church and state. Pope Francis has embraced many elements of liberation theology, especially the dedication of the Church to the poor and marginalized. In comparison to Europe and other Western nations, the Catholic Church still has a major influence in Latin American society. The Requerimiento of 1512 served as a legal doctrine mandating that the Amerindians accept the Spanish monarch's power over the region and Christianity. The doctrine called for the Amerindians who abided by these demands to be considered “loyal vassals,” but justified war against the Amerindians if they opposed the Spaniards’ power and allowed for an aggressive conquest, resulting in the Amerindians being “deprived of their liberty and property.”[6][7] The Requerimiento briefly alludes to the enslavement of the Amerindians as a result of the Spaniards' militaristic conquest of the region.[8] Slavery was part of the local population's culture before the arrival of the conquistadors. Christian missionaries provided existing slaves with an opportunity to escape their situation by seeking out the protection of the missions. On December 1511, the Dominican friar Antonio de Montesinos openly rebuked the Spanish authorities governing Hispaniola for their mistreatment of the American natives, telling them \"... you are in mortal sin ... for the cruelty and tyranny you use in dealing with these innocent people\".[9][10][11]King Ferdinand enacted the Laws of Burgos and Valladolid in response. Enforcement was lax, and while some blame the Church for not doing enough to liberate the Indians, others point to the Church as the only voice raised on behalf of indigenous peoples.[12] Francisco de Vitoria, an acclaimed Theology Professor of the colonial era, opposed the idea of the Amerindians being “forcibly converted” to Catholicism on the premise that they would not truly accept the religion.[13] However, in contrast with de Montesinos’ views, de Vitoria reasoned that if the Amerindians were to oppose the Catholic faith with “blasphemies,” war against them would be justified.[14] During the colonial period, the Catholic missions also included efforts by the friars to educate the Amerindians. [15] Although the missionaries focused on the “conversion,” the friars also worked to educate the Amerindians about Spanish cultural expectations, social customs, and about “political organization through the mission system.\"[15] Pedro de Gante, one of the first missionaries to arrive in Latin America during the colonial era, underscores in his letter to King Charles V of Spain the Spanish missionaries’ efforts to educate the Amerindians.[16] In the letter, he argued that the Amerindians' workload under the Spanish colonists did not allow them to properly “provide for their families and the opportunity to become good Christians.[16] In his letter, de Gante specifically requests that the king provide annual funding to run a local school and diminish the Amerindians’ workload to provide them with a “spiritual instruction.”[17] Nevertheless, Amerindian populations suffered serious decline due to new diseases, inadvertently introduced through contact with Europeans, which created a labor vacuum in the New World.[18] In 1522, the first Franciscan missionaries arrived in Mexico, establishing schools, model farms and hospitals. The ‘apostolic twelve’ were one of the first groups of friars to arrive in Mexico during the colonial period. [19] The group initiated the “organized effort to evangelize the native people of Mexico.”[19] The Franciscans’ views of Amerindians religious beliefs and evangelization strategies are highlighted letter by Friar Francisco Angelorum, providing instructions on their evangelization tasks in Mexico.[20] Angelorum concludes that the Amerindians’ idols were a result of being “deceived by satanic wiles” and identifies preaching about the “Eternal Father” and spiritual “salvation” as the best means of evangelizing the Amerindians.[21] When some Europeans questioned whether the Indians were truly human and worthy of baptism, Pope Paul III in the 1537 bull Sublimis Deus confirmed that \"their souls were as immortal as those of Europeans\" and they should neither be robbed nor turned into slaves.[22][23][24] Over the next 150 years, missions expanded into southwestern North America.[25] Native people were often legally defined as children, and priests took on a paternalistic role, sometimes enforced with corporal punishment.[26] Junípero Serra, the Franciscan priest in charge of this effort, founded a series of missions which became important economic, political, and religious institutions.[27] These missions brought grain, cattle and a new way of living to the Indian tribes of California. Overland routes were established from New Mexico that resulted in the colonization of San Francisco in 1776 and Los Angeles in 1781. However, by bringing Western civilization to the area, these missions and the Spanish government have been held responsible for wiping out nearly a third of the native population, primarily through disease.[28] Only in the 19th century, after the breakdown of most Spanish and Portuguese colonies, was the Vatican able to take charge of Catholic missionary activities through its Propaganda Fide organization.[29] In a challenge to Spanish and Portuguese policy, Pope Gregory XVI, began to appoint his own candidates as bishops in the colonies, condemned slavery and the slave trade in the 1839 papal bull In supremo apostolatus, and approved the ordination of native clergy in the face of government racism.[30] Yet in spite of these advances, the Amerindian population continued to suffer decline from exposure to European diseases.[31] Dominicans The Dominican missionaries were part of the Catholic Church's Dominican Order.[32] The Dominicans favored “doctrinal preaching and philosophical argument with religious opponents” as their specific method of evangelization.[32] Dominican friars gained immense fame as the Amerindians’ advocates against “the Spaniards’ abuse” and “exploitation of the Indians.”[33] Pedro de Gante was one of the first Dominican friars to arrive in Latin America, and in his letter to King Charles V of Spain, he advocated for the Amerindians’ rights. He argued that the Spanish colonists’ should avoid continuing to make harsh labor demands of Amerindians by noting how the native people did “not even have time to look after their subsistence” and would “die of hunger.”[34] Bartolome de Las Casas, another famed Dominican friar, also defended the Amerindians' rights and opposed the Spaniards’ view of the indigenous people as “barbarians” as an acceptable justification to massacre the indigenous population.[35] In his work, In Defense of the Indians, de Las Casas underscored the Amerindians’ advanced “political states” and “architecture” to demonstrate that the Amerindians were not barbaric and indicate that the indigenous people had the capacity for rational thought and were “very ready to accept” Christianity.[36] Jesuit missions in Latin America were very controversial in Europe, especially in Spain and Portugal where they were seen as interfering with the proper colonial enterprises of the royal governments. The Jesuits were often the only force standing between the Native Americans and slavery. Together throughout South America but especially in present-day Brazil and Paraguay they formed Christian Native American city-states, called \"reductions\" (Spanish Reducciones, Portuguese Reduções). These were societies set up according to an idealized theocratic model. It is partly because the Jesuits, such as Antonio Ruiz de Montoya, protected the natives (whom certain Spanish and Portuguese colonizers wanted to enslave) that the Society of Jesus was suppressed. In these regions the Jesuit reductions were different from the reductions in other regions, because the Indians were expected to adopt Christianity but not European culture.[37] Under the Jesuit leadership of the Indians through native \"puppet\" caciques, the reductions achieved a high degree of autonomy within the Spanish and Portuguese colonial empires. With the use of Indian labour, the reductions became economically successful. When their existence was threatened by the incursions of Bandeirante slave traders, Indian militia were created that fought effectively against the colonists.[37] The resistance by the Jesuit reductions to slave raids, as well as their high degree of autonomy and economic success, have been cited as contributing factors to the expulsion of the Jesuits from the Americas in 1767.[38] The Jesuit reductions present a controversial chapter of the evangelisational history of the Americas, and are variously described as jungle utopias or as theocratic regimes of terror.[37] The conquest was immediately accompanied by evangelization, and new, local forms of Catholicism appeared. The Virgin of Guadalupe is one of Mexico's oldest religious image, and is said to have appeared to Juan Diego Cuauhtlatoatzin in 1531. News of the 1534 apparition on Tepayac Hill spread quickly through Mexico; and in the seven years that followed, 1532 through 1538, the Indian people accepted the Spaniards and 8 million people were converted to the Catholic faith. Thereafter, the Aztecs no longer practiced human sacrifice or native forms of worship. In 2001 the Italian Movement of Love Saint Juan Diego was created, and launched evangelization projects in 32 states. A year later, Juan Diego was canonized by Pope John Paul II. Guadalupe is often considered a mixture of the cultures which blend to form Mexico, both racially[39] and religiously[40] Guadalupe is sometimes called the \"first mestiza\"[41] or \"the first Mexican\". [42] Mary O'Connor writes that Guadalupe \"bring[s] together people of distinct cultural heritages, while at the same time affirming their distinctness.\"[43] One theory is that the Virgin of Guadalupe was presented to the Aztecs as a sort of \"Christianized\" Tonantzin, necessary for the clergymen to convert the indigenous people to their faith. As Jacques Lafaye wrote in Quetzalcoatl and Guadalupe, \"...as the Christians built their first churches with the rubble and the columns of the ancient pagan temples, so they often borrowed pagan customs for their own cult purposes.[44] For most of the history of post-colonial Latin America, religious rights have been regularly violated, and even now, tensions and conflict in the area of religion remain. Religious human rights, in the sense of freedom to exercise and practice one's religion, are almost universally guaranteed in the laws and constitutions of Latin America today, although they are not universally observed in practice. Moreover, it has taken Latin America much longer than other parts of the West to adopt religious freedom in theory and in practice, and the habit of respect for those rights is only gradually being developed. The slowness to embrace religious freedom in Latin America is related to its colonial heritage and to its post-colonial history. The Aztec and the Inca both made substantial use of religion to support their authority and power. This pre-existing role of religion in pre-Columbian culture made it relatively easy for the Spanish conquistadors to replace native religious structures with those of a Catholicism that was closely linked to the Spanish throne.[45] Anti-clericalism was an integral feature of 19th-century liberalism in Latin America. This anti-clericalism was based on the idea that the clergy (especially the prelates who ran the administrative offices of the Church) were hindering social progress in areas such as public education and economic development. The Catholic Church was one of the largest land owning groups in most of Latin America's countries. As a result, the Church tended to be rather conservative politically. Beginning in the 1820s, a succession of liberal regimes came to power in Latin America.[46] Some members of these liberal regimes sought to imitate the Spain of the 1830s (and revolutionary France of a half-century earlier) in expropriating the wealth of the Catholic Church, and in imitating the 18th-century benevolent despots in restricting or prohibiting the religious institutes. As a result, a number of these liberal regimes expropriated Church property and tried to bring education, marriage and burial under secular authority. The confiscation of Church properties and changes in the scope of religious liberties (in general, increasing the rights of non-Catholics and non-observant Catholics, while licensing or prohibiting the institutes) generally accompanied secularist, and later, Marxist-leaning, governmental reforms.[47] The Mexican Constitution of 1824 had required the Republic to prohibit the exercise of any religion other the Roman Catholic and Apostolic faith.[48] The Constitution of 1857 retained many of the Roman Catholic Church's Colonial era privileges and revenues, but, unlike the earlier constitution, did not mandate that the Catholic Church be the nation's exclusive religion, and strongly restricted the Church's right to own property. Such reforms were unacceptable to the leadership of the clergy and the Conservatives. Comonfort and members of his administration were excommunicated, and a revolt was then declared. Starting in 1855, US-backed President Benito Juárez issued decrees nationalizing church property, separating church and state, and suppressing religious institutes. Church properties were confiscated and basic civil and political rights were denied to religious institutes and the clergy. The Church supported the regime of Juárez's successor, Porfirio Diaz, who was opposed to land reform. The first of the Liberal Reform Laws were passed in 1855. The Juárez Law, named after Benito Juárez, restricted clerical privileges, specifically the authority of Church courts,[49] by subverting their authority to civil law. It was conceived of as a moderate measure, rather than abolish church courts altogether. However, the move opened latent divisions in the country. Archbishop Lázaro de la Garza in Mexico City condemned the Law as an attack on the Church itself, and clerics went into rebellion in the city of Puebla in 1855–56.[50] Other laws attacked the privileges traditionally enjoyed by the military, which was significant since the military had been instrumental in putting and keeping Mexican governments in office since Emperor Agustín de Iturbide in the 1820s.[49] The next Reform Law was called the lerdo law, after Miguel Lerdo de Tejada. Under this new law, the government began to confiscate Church land.[49] This proved to be considerably more controversial than the Juárez Law. The purpose of the law was to convert lands held by corporate entities such as the Church into private property, favoring those who already lived on it. It was thought that such would encourage development and the government could raise revenue by taxing the process.[50] Lerdo de Tejada was the Minister of Finance and required that the Church sell much of its urban and rural land at reduced prices. If the Church did not comply, the government would hold public auctions. The Law also stated that the Church could not gain possession of properties in the future. However, the Lerdo Law did not apply only to the Church. It stated that no corporate body could own land. Broadly defined, this would include ejidos, or communal land owned by Indian villages. Initially, these ejidos were exempt from the law, but eventually these Indian communities suffered and extensive loss of land.[49] By 1857, additional anti-clerical legislation, such as the Iglesias Law (named after José María Iglesias) regulated the collection of clerical fees from the poor and prohibited clerics from charging for baptisms, marriages, or funeral services.[51] Marriage became a civil contract, although no provision for divorce was authorized. Registry of births, marriages and deaths became a civil affair, with President Benito Juárez registering his newly born son in Veracruz. The number of religious holidays was reduced and several holidays to commemorate national events introduced. Religious celebrations outside churches was forbidden, use of church bells restricted and clerical dress was prohibited in public.[52] One other significant Reform Law was the Law for the Nationalization of Ecclesiastical Properties, which would eventually secularize nearly all of the country's monasteries and convents. The government had hoped that this law would bring in enough revenue to secure a loan from the United States but sales would prove disappointing from the time it was passed all the way to the early 20th century.[52] Following the revolution of 1910, the new Mexican Constitution of 1917 contained further anti-clerical provisions. Article 3 called for secular education in the schools and prohibited the Church from engaging in primary education; Article 5 outlawed monastic orders; Article 24 forbade public worship outside the confines of churches; and Article 27 placed restrictions on the right of religious organizations to hold property. Most obnoxious to Catholics was Article 130, which deprived clergy members of basic political rights. Many of these laws were resisted, leading to the Cristero Rebellion of 1927–1929. The suppression of the Church included the closing of many churches and the killing and forced marriage of priests. The persecution was most severe in Tabasco under the atheist governor Tomás Garrido Canabal. Between 1926 and 1929 an armed conflict in the form of a popular uprising broke out against the anti-Catholic\\ anti-clerical Mexican government, set off specifically by the anti-clerical provisions of the Mexican Constitution of 1917. Discontent over the provisions had been simmering for years. The conflict is known as the Cristero War. A number of articles of the 1917 Constitution were at issue. Article 5 outlawed monastic religious orders. Article 24 forbade public worship outside of church buildings, while Article 27 restricted religious organizations' rights to own property. Finally, Article 130 took away basic civil rights of members of the clergy: priests and religious leaders were prevented from wearing their habits, were denied the right to vote, and were not permitted to comment on public affairs in the press. The Cristero War was eventually resolved diplomatically, largely with the influence of the U.S. Ambassador. The conflict claimed the lives of some 90,000: 56,882 on the federal side, 30,000 Cristeros, and numerous civilians and Cristeros who were killed in anticlerical raids after the war's end. As promised in the diplomatic resolution, the laws considered offensive to the Cristeros remained on the books, but no organized federal attempts to enforce them were put into action. Nonetheless, in several localities, persecution of Catholic priests continued based on local officials' interpretations of the law.[citation needed] The effects of the war on the Church were profound. Between 1926 and 1934 at least 40 priests were killed.[53] Between 1926 and 1934, over 3,000 priests were exiled or assassinated.[54][55] In an effort to prove that \"God would not defend the Church\", Calles ordered \"hideous desecration of churches ... there were parodies of (church) services, nuns were raped and any priests captured ... were shot ...\".[56] Calles was eventually deposed[56] and despite the persecution, the Church in Mexico continued to grow. A 2000 census reported that 88 percent of Mexicans identify as Catholic.[57] Where there were 4,500 priests serving the people before the rebellion, in 1934 there were only 334 priests licensed by the government to serve fifteen million people, the rest having been eliminated by emigration, expulsion and assassination.[53][58] It appears that ten states were left without any priests.[58] The tension between civilian and clerical authority dominated Ecuador's history for much of the 19th and early 20th centuries. This issue was one of the bases for the lasting dispute between Conservatives, who represented primarily the interests of the Sierra and the church, and the Liberals, who represented those of the costa and anticlericalism. Although Colombia enacted anticlerical legislation and its enforcement during more than three decades (1849–84), it soon restored “full liberty and independence from the civil power” to the Catholic Church. When the Liberal Party came to power in 1930, anticlerical Liberals pushed for legislation to end Church influence in public schools. These Liberals held that the Church and its intellectual backwardness were responsible for a lack of spiritual and material progress in Colombia. Liberal-controlled local, departmental and national governments ended contracts with religious communities who operated schools in government-owned buildings, and set up secular schools in their place. These actions were sometimes violent, and were met by a strong opposition from clerics, Conservatives, and even a good number of more moderate Liberals. Across the country, militants attacked churches, convents, and monasteries, killing priests and looking for arms, since a conspiracy theory maintained that the religious had guns, and this despite the fact that not a single serviceable weapon was located in the raids.[61] Liberal anti-clericalists of the 1880s established a new pattern of church-state relations in which the official constitutional status of the Church was preserved while the state assumed control of many functions formerly the province of the Church. Conservative Catholics, asserting their role as definers of national values and morality, responded in part by joining in the rightist religio-political movement known as Catholic Nationalism which formed successive opposition parties. This began a prolonged period of conflict between church and state that persisted until the 1940s when the Church enjoyed a restoration of its former status under the presidency of Colonel Juan Perón. Perón claimed that Peronism was the \"true embodiment of Catholic social teaching\" - indeed, more the embodiment of Catholicism than the Catholic Church itself. In 1954, Perón reversed the fortunes of the church by threatening total disestablishment and retracting critical functions, including the teaching of religious education in public schools. As a result, Argentina saw extensive destruction of churches, denunciations of clergy and confiscation of Catholic schools as Perón attempted to extend state control over national institutions.[62] The renewed rupture in church-state relations was completed when Perón was excommunicated. However, in 1955, overthrown by a military general who was a leading member of the Catholic Nationalist movement. In 1983, the civilian president, Raúl Alfonsín, attempted to restore a liberal democratic state. Alfonsín's opposition to the church-military alliance, conjoined with his strongly secular emphasis contravening traditional Catholic positions, incited opposition that served to curtail his agenda. Cuba, under atheist Fidel Castro, succeeded in reducing the Church's ability to work by deporting the archbishop and 150 Spanish priests, discriminating against Catholics in public life and education and refusing to accept them as members of the Communist Party.[63] The subsequent flight of 300,000 people from the island also helped to diminish the Church there. In later year Fidel Castro converted back to Catholicism and lifted the ban on the catholic church in Cuba [63] In the 1960s, growing social awareness and politicization in the Latin American Church gave birth to liberation theology which openly supported anti-imperialist movements. The Peruvian priest, Gustavo Gutiérrez, became its primary proponent[64] and, in 1979, the bishops' conference in Mexico officially declared the Latin American Church's \"preferential option for the poor\".[65] Archbishop Óscar Romero, a supporter of the movement, became the region's most famous contemporary martyr in 1980, when he was murdered while saying mass by forces allied with the government.[66] Both Pope John Paul II and Pope Benedict XVI (as Cardinal Ratzinger) denounced the movement.[67] The Brazilian theologian Leonardo Boff was twice ordered to cease publishing and teaching.[68] While Pope John Paul II was criticized for his severity in dealing with proponents of the movement, he maintained that the Church, in its efforts to champion the poor, should not do so by resorting to violence or partisan politics.[64] The movement is still alive in Latin America today, though the Church now faces the challenge of Pentecostal revival in much of the region.[69] ^Koschorke, A History of Christianity in Asia, Africa, and Latin America (2007), p. 287 ^Dussel, Enrique, A History of the Church in Latin America, Wm B Eerdmans Publishing, 1981, pp. 45, 52, 53 quote: \"The missionary Church opposed this state of affairs from the beginning, and nearly everything positive that was done for the benefit of the indigenous peoples resulted from the call and clamor of the missionaries. The fact remained, however, that widespread injustice was extremely difficult to uproot ... Even more important than Bartolome de Las Casas was the Bishop of Nicaragua, Antonio de Valdeviso, who ultimately suffered martyrdom for his defense of the Indian.\" ^Johansen, p. 110, quote: \"In the Papal bull Sublimis deus (1537), Pope Paul III declared that Indians were to be regarded as fully human, and that their souls were as immortal as those of Europeans. This edict also outlawed slavery of Indians in any form ...\" ^Casas, Bartolomé de las (1974). In Defense of the Indians: The Defense of the Most Reverend Lord, Don Fray Bartolomé De Las Casas, of the Order of Preachers, Late Bishop of Chiapa, against the Persecutors and Slanderers of the Peoples of the New World Discovered Across the Seas. Northern Illinois University Press. pp. 42–45. ^Casas, Bartolomé de las (1974). In Defense of the Indians: The Defense of the Most Reverend Lord, Don Fray Bartolomé De Las Casas, of the Order of Preachers, Late Bishop of Chiapa, against the Persecutors and Slanderers of the Peoples of the New World Discovered Across the Seas. Northern Illinois University Press. p. 44. ^Elizondo, Virgil. \"Our Lady of Guadalupe. A Guide for the New Millennium.\" St. Anthony Messenger Magazine Online. December 1999. [2]Archived 2007-10-26 at the Wayback Machine, accessed 3 December 2006 ^Lafaye, Jacques. Quetzalcoatl and Guadalupe. The Formation of Mexican National Consciousness. Chicago: University of Chicago Press. 1976 ^Sigmund, Paul E. (1996). \"Religious Human Rights in the World Today: A Report on the 1994 Atlanta Conference: Legal Perspectives on Religious Human Rights: Religious Human Rights in Latin America\". Emory International Law Review. Emory University School of Law. ^Bergquist, Charles; David J. Robinson (1997–2005). \"Colombia\". Microsoft Encarta Online Encyclopedia 2005. Microsoft Corporation. Archived from the original on 2007-11-11. Retrieved April 16, 2006.On April 9, 1948, Gaitán was assassinated outside his law offices in downtown Bogotá. The assassination marked the start of a decade of bloodshed, called La Violencia (the violence), which took the lives of an estimated 180,000 Colombians before it subsided in 1958."}
{"url": "https://en.wikipedia.org/wiki/Population_history_of_Indigenous_peoples_of_the_Americas", "text": "Population figures for the Indigenous peoples of the Americas prior to European colonization have been difficult to establish. By the end of the 20th century, most scholars gravitated toward an estimate of around 50 million, with some historians arguing for an estimate of 100 million or more.[1][2] Pre-Columbian population figures are difficult to estimate because of the fragmentary nature of the evidence. Estimates range from 8–112 million.[10] Scholars have varied widely on the estimated size of the Indigenous populations prior to colonization and on the effects of European contact.[11] Estimates are made by extrapolations from small bits of data. In 1976, geographer William Denevan used the existing estimates to derive a \"consensus count\" of about 54 million people. Nonetheless, more recent estimates still range widely.[12] In 1992, Denevan suggested that the total population was approximately 53.9 million and the populations by region were, approximately, 3.8 million for the United States and Canada, 17.2 million for Mexico, 5.6 million for Central America, 3 million for the Caribbean, 15.7 million for the Andes and 8.6 million for lowland South America.[13] A 2020 genetic study suggests that prior estimates for the pre-Columbian Caribbean population may have been at least tenfold too large.[14] Historian David Stannard estimates that the extermination of Indigenous peoples took the lives of 100 million people: \"...the total extermination of many American Indian peoples and the near-extermination of others, in numbers that eventually totaled close to 100,000,000.\"[15] A 2019 study estimates the pre-Columbian Indigenous population contained more than 60 million people, but dropped to 6 million by 1600, based on a drop in atmospheric CO2 during that period.[16][17] Other studies have disputed this conclusion.[18][19] The Indigenous population of the Americas in 1492 was not necessarily at a high point and may actually have already been in decline in some areas. Indigenous populations in most areas of the Americas reached a low point by the early 20th century.[20] Using an estimate of approximately 37 million people in Mexico, Central and South America in 1492 (including 6 million in the Aztec Empire, 5–10 million in the Mayan States, 11 million in what is now Brazil, and 12 million in the Inca Empire), the lowest estimates give a death toll from all causes of 80% by the end of the 17th century (nine million people in 1650).[21] Latin America would match its 15th-century population early in the 19th century; it numbered 17 million in 1800, 30 million in 1850, 61 million in 1900, 105 million in 1930, 218 million in 1960, 361 million in 1980, and 563 million in 2005.[21] In the last three decades of the 16th century, the population of present-day Mexico dropped to about one million people.[21] The Maya population is today estimated at six million, which is about the same as at the end of the 15th century, according to some estimates.[21] In what is now Brazil, the Indigenous population declined from a pre-Columbian high of an estimated four million to some 300,000. Over 60 million Brazilians possess at least one Native South American ancestor, according to a DNA study.[22] While it is difficult to determine exactly how many Natives lived in North America before Columbus,[23] estimates range from 3.8 million, as mentioned above, to 7 million[24] people to a high of 18 million.[25] Scholars vary on the estimated size of the Indigenous population in what is now Canada prior to colonization and on the effects of European contact.[26] During the late 15th century is estimated to have been between 200,000[27] and two million,[28] with a figure of 500,000 currently accepted by Canada's Royal Commission on Aboriginal Health.[29] Although not without conflict, European Canadians' early interactions with First Nations and Inuit populations were relatively peaceful.[30] However repeated outbreaks of European infectious diseases such as influenza, measles and smallpox (to which they had no natural immunity),[31] combined with other effects of European contact, resulted in a twenty-five percent to eighty percent Indigenous population decrease post-contact.[27] Roland G Robertson suggests that during the late 1630s, smallpox killed over half of the Wyandot (Huron), who controlled most of the early North American fur trade in the area of New France.[32] In 1871 there was an enumeration of the Indigenous population within the limits of Canada at the time, showing a total of only 102,358 individuals.[33] From 2006 to 2016, the Indigenous population has grown by 42.5 percent, four times the national rate.[34] According to the 2011 Canadian Census, Indigenous peoples (First Nations – 851,560, Inuit – 59,445 and Métis – 451,795) numbered at 1,400,685, or 4.3% of the country's total population.[35] The population debate has often had ideological underpinnings.[36] Low estimates were sometimes reflective of European notions of cultural and racial superiority. Historian Francis Jennings argued, \"Scholarly wisdom long held that Indians were so inferior in mind and works that they could not possibly have created or sustained large populations.\"[37] In 1998, Africanist Historian David Henige said many population estimates are the result of arbitrary formulas applied from unreliable sources.[38] Population size for Native American tribes is difficult to state definitively, but at least one writer has made estimates, often based on an assumed proportion of the number of warriors to total population for the tribe.[50] Typical proportions were 5 people per one warrior and at least 1 up to 5 warriors (therefore at least 5-25 people) per lodge, cabin or house. Genetic diversity and population structure in the American land mass using DNA micro-satellite markers (genotype) sampled from North, Central, and South America have been analyzed against similar data available from other Indigenous populations worldwide.[85][86] The Amerindian populations show a lower genetic diversity than populations from other continental regions.[86] Decreasing genetic diversity with increasing geographic distance from the Bering Strait can be seen, as well as a decreasing genetic similarity to Siberian populations from Alaska (genetic entry point).[85][86] A higher level of diversity and lower level of population structure in western South America compared to eastern South America is observed.[85][86] A relative lack of differentiation between Mesoamerican and Andean populations is a scenario that implies coastal routes were easier than inland routes for migrating peoples (Paleo-Indians) to traverse.[85] The overall pattern that is emerging suggests that the Americas were recently colonized by a small number of individuals (effective size of about 70–250), and then they grew by a factor of 10 over 800–1,000 years.[87][88] The data also show that there have been genetic exchanges between Asia, the Arctic and Greenland since the initial peopling of the Americas.[88][89] A new study in early 2018 suggests that the effective population size of the original founding population of Native Americans was about 250 people.[90][91] One estimate of population collapse in Central Mexico brought on by successive epidemics in the early colonial period. Note: Other scholars' estimates vary widely. Early explanations for the population decline of the Indigenous peoples of the Americas include the brutal practices of the Spanish conquistadores, as recorded by the Spaniards themselves, such as the encomienda system, which was ostensibly set up to protect people from warring tribes as well as to teach them the Spanish language and the Catholic religion, but in practice was tantamount to serfdom and slavery.[92] The most notable account was that of the DominicanfriarBartolomé de las Casas, whose writings vividly depict Spanish atrocities committed in particular against the Taínos.[93] The second European explanation was a perceived divine approval, in which God removed the natives as part of His \"divine plan\" to make way for a new Christian civilization. Many Native Americans viewed their troubles in a religious framework within their own belief systems.[94] According to later academics such as Noble David Cook, a community of scholars began \"quietly accumulating piece by piece data on early epidemics in the Americas and their relation to the subjugation of native peoples.\" Scholars like Cook believe that widespread epidemic disease, to which the natives had no prior exposure or resistance, was the primary cause of the massive population decline of the Native Americans.[95] One of the most devastating diseases was smallpox, but other deadly diseases included typhus, measles, influenza, bubonic plague, cholera, malaria, tuberculosis, mumps, yellow fever and pertussis, which were chronic in Eurasia.[96] However, recently scholars have studied the link between physical colonial violence such as warfare, displacement, and enslavement, and the proliferation of disease among Native populations.[4][97][98] For example, according to Coquille scholar Dina Gilio-Whitaker, \"In recent decades, however, researchers challenge the idea that disease is solely responsible for the rapid Indigenous population decline. The research identifies other aspects of European contact that had profoundly negative impacts on Native peoples' ability to survive foreign invasion: war, massacres, enslavement, overwork, deportation, the loss of will to live or reproduce, malnutrition and starvation from the breakdown of trade networks, and the loss of subsistence food production due to land loss.\"[99] Further, Andrés Reséndez of the University of California, Davis points out that, even though the Spanish were aware of deadly diseases such as smallpox, there is no mention of them in the New World until 1519, implying that, until that date, epidemic disease played no significant part in the depopulation of the Antilles. The practices of forced labor, brutal punishment, and inadequate necessities of life, were the initial and major reasons for depopulation.[100]Jason Hickel estimates that a third of Arawak workers died every six months from forced labor in these mines.[101] In this way, \"slavery has emerged as a major killer\" of the indigenous populations of the Caribbean between 1492 and 1550, as it set the conditions for diseases such as smallpox, influenza, and malaria to flourish.[100] Unlike the populations of Europe who rebounded following the Black Death, no such rebound occurred for the Indigenous populations.[100] Similarly, historian Jeffrey Ostler at the University of Oregon has argued that population collapses in North America throughout colonization were not due mainly to lack of Native immunity to European disease. Instead, he claims that \"When severe epidemics did hit, it was often less because Native bodies lacked immunity than because European colonialism disrupted Native communities and damaged their resources, making them more vulnerable to pathogens.\" In specific regard to Spanish colonization of northern Florida and southeastern Georgia, Native peoples there \"were subject to forced labor and, because of poor living conditions and malnutrition, succumbed to wave after wave of unidentifiable diseases.\" Further, in relation to British colonization in the Northeast, Algonquian speaking tribes in Virginia and Maryland \"suffered from a variety of diseases, including malaria, typhus, and possibly smallpox.\" These diseases were not solely a case of Native susceptibility, however, because \"as colonists took their resources, Native communities were subject to malnutrition, starvation, and social stress, all making people more vulnerable to pathogens. Repeated epidemics created additional trauma and population loss, which in turn disrupted the provision of healthcare.\" Such conditions would continue, alongside rampant disease in Native communities, throughout colonization, the formation of the United States, and multiple forced removals, as Ostler explains that many scholars \"have yet to come to grips with how U.S. expansion created conditions that made Native communities acutely vulnerable to pathogens and how severely disease impacted them. ... Historians continue to ignore the catastrophic impact of disease and its relationship to U.S. policy and action even when it is right before their eyes.\"[6] Historian David Stannard says that by \"focusing almost entirely on disease ... contemporary authors increasingly have created the impression that the eradication of those tens of millions of people was inadvertent—a sad, but both inevitable and \"unintended consequence\" of human migration and progress,\" and asserts that their destruction \"was neither inadvertent nor inevitable,\" but the result of microbial pestilence and purposeful genocide working in tandem.[102] He also wrote:[103] ...Despite frequent undocumented assertions that disease was responsible for the great majority of indigenous deaths in the Americas, there does not exist a single scholarly work that even pretends to demonstrate this claim on the basis of solid evidence. And that is because there is no such evidence, anywhere. The supposed truism that more native people died from disease than from direct face-to-face killing or from gross mistreatment or other concomitant derivatives of that brutality such as starvation, exposure, exhaustion, or despair is nothing more than a scholarly article of faith... In contrast, historian Russel Thornton has pointed out that there were disastrous epidemics and population losses during the first half of the sixteenth century \"resulting from incidental contact, or even without direct contact, as disease spread from one American Indian tribe to another.\"[104] Thornton has also challenged higher Indigenous population estimates, which are based on the Malthusian assumption that \"populations tend to increase to, and beyond, the limits of the food available to them at any particular level of technology.\"[105] The European colonization of the Americas resulted in the deaths of so many people it contributed to climatic change and temporary global cooling, according to scientists from University College London.[106][107] A century after the arrival of Christopher Columbus, some 90% of Indigenous Americans had perished from \"wave after wave of disease\", along with mass slavery and war, in what researchers have described as the \"great dying\".[108] According to one of the researchers, UCL Geography Professor Mark Maslin, the large death toll also boosted the economies of Europe: \"the depopulation of the Americas may have inadvertently allowed the Europeans to dominate the world. It also allowed for the Industrial Revolution and for Europeans to continue that domination.\"[109] When Old World diseases were first carried to the Americas at the end of the fifteenth century, they spread throughout the southern and northern hemispheres, leaving the Indigenous populations in near ruins.[96][110] No evidence has been discovered that the earliest Spanish colonists and missionaries deliberately attempted to infect the American natives, and some efforts were made to limit the devastating effects of disease before it killed off what remained of their labor force (compelled to work under the encomienda system).[96][110] The cattle introduced by the Spanish contaminated various water reserves which Native Americans dug in the fields to accumulate rainwater. In response, the Franciscans and Dominicans created public fountains and aqueducts to guarantee access to drinking water.[21] But when the Franciscans lost their privileges in 1572, many of these fountains were no longer guarded and so deliberate well poisoning may have happened.[21] Although no proof of such poisoning has been found, some historians believe the decrease of the population correlates with the end of religious orders' control of the water.[21] In the centuries that followed, accusations and discussions of biological warfare were common. Well-documented accounts of incidents involving both threats and acts of deliberate infection are very rare, but may have occurred more frequently than scholars have previously acknowledged.[111][112] Many of the instances likely went unreported, and it is possible that documents relating to such acts were deliberately destroyed,[112] or sanitized.[113][114] By the middle of the 18th century, colonists had the knowledge and technology to attempt biological warfare with the smallpox virus. They well understood the concept of quarantine, and that contact with the sick could infect the healthy with smallpox, and those who survived the illness would not be infected again. Whether the threats were carried out, or how effective individual attempts were, is uncertain.[96][112][113] One such threat was delivered by fur trader James McDougall, who is quoted as saying to a gathering of local chiefs, \"You know the smallpox. Listen: I am the smallpox chief. In this bottle I have it confined. All I have to do is to pull the cork, send it forth among you, and you are dead men. But this is for my enemies and not my friends.\"[115] Likewise, another fur trader threatened Pawnee Indians that if they didn't agree to certain conditions, \"he would let the smallpox out of a bottle and destroy them.\" The Reverend Isaac McCoy was quoted in his History of Baptist Indian Missions as saying that the white men had deliberately spread smallpox among the Indians of the southwest, including the Pawnee tribe, and the havoc it made was reported to General Clark and the Secretary of War.[115][116] Artist and writer George Catlin observed that Native Americans were also suspicious of vaccination, \"They see white men urging the operation so earnestly they decide that it must be some new mode or trick of the pale face by which they hope to gain some new advantage over them.\"[117] So great was the distrust of the settlers that the Mandan chief Four Bears denounced the white man, whom he had previously treated as brothers, for deliberately bringing the disease to his people.[118][119][120] During the siege of British-held Fort Pitt in the Seven Years' War, Colonel Henry Bouquet ordered his men to take smallpox-infested blankets from their hospital and gave them as gifts to two neutral Lenape Indian dignitaries during a peace settlement negotiation, according to the entry in the Captain's ledger, \"To convey the Smallpox to the Indians\".[113][121][122] In the following weeks, Sir Jeffrey Amherst conspired with Bouquet to \"Extirpate this Execreble Race\" of Native Americans, writing, \"Could it not be contrived to send the small pox among the disaffected tribes of Indians? We must on this occasion use every stratagem in our power to reduce them.\" His Colonel agreed to try.[112][121] Most scholars have asserted that the 1837 Great Plains smallpox epidemic was \"started among the tribes of the upper Missouri River by failure to quarantine steamboats on the river\",[115] and Captain Pratt of the St. Peter \"was guilty of contributing to the deaths of thousands of innocent people. The law calls his offense criminal negligence. Yet in light of all the deaths, the almost complete annihilation of the Mandans, and the terrible suffering the region endured, the label criminal negligence is benign, hardly befitting an action that had such horrendous consequences.\"[119] However, some sources attribute the 1836–40 epidemic to the deliberate communication of smallpox to Native Americans, with historian Ann F. Ramenofsky writing, \"Variola Major can be transmitted through contaminated articles such as clothing or blankets. In the nineteenth century, the U. S. Army sent contaminated blankets to Native Americans, especially Plains groups, to control the Indian problem.\"[123] In Brazil, well into the 20th century, deliberate infection attacks continued as Brazilian settlers and miners transported infections intentionally to the native groups whose lands they coveted.[110] After Edward Jenner's 1796 demonstration that the smallpox vaccination worked, the technique became better known and smallpox became less deadly in the United States and elsewhere. Many colonists and natives were vaccinated, although, in some cases, officials tried to vaccinate natives only to discover that the disease was too widespread to stop. At other times, trade demands led to broken quarantines. In other cases, natives refused vaccination because of suspicion of whites. The first international healthcare expedition in history was the Balmis expedition which had the aim of vaccinating Indigenous peoples against smallpox all along the Spanish Empire in 1803. In 1831, government officials vaccinated the Yankton Sioux at Sioux Agency. The Santee Sioux refused vaccination and many died.[36] An 1899 chromolithograph of U.S. cavalry pursuing American Indians, artist unknown.An 1899 chromolithograph from the Werner Company of Akron, Ohio titled Custer Massacre at Big Horn, Montana – June 25, 1876. While epidemic disease was a leading factor of the population decline of the American Indigenous peoples after 1492, there were other contributing factors, all of them related to European contact and colonization. One of these factors was warfare. According to demographer Russell Thornton, although many people died in wars over the centuries, and war sometimes contributed to the near extinction of certain tribes, warfare and death by other violent means was a comparatively minor cause of overall native population decline.[124] From the U.S. Bureau of the Census in 1894, wars between the government and the Indigenous peoples ranged over 40 in number over the previous 100 years. These wars cost the lives of approximately 19,000 white people, and the lives of about 30,000 Indians, including men, women, and children. They safely estimated that the amount of Native people who were killed or wounded was actually around fifty percent more than what was recorded.[125] There is some disagreement among scholars about how widespread warfare was in pre-Columbian America,[126] but there is general agreement that war became deadlier after the arrival of the Europeans and their firearms.[citation needed] The South or Central American infrastructure allowed for thousands of European conquistadors and tens of thousands of their Indian auxiliaries to attack the dominant Indigenous civilization. Empires such as the Incas depended on a highly centralized administration for the distribution of resources. Disruption caused by the war and the colonization hampered the traditional economy, and possibly led to shortages of food and materials.[127] Across the western hemisphere, war with various Native American civilizations constituted alliances based out of both necessity or economic prosperity and, resulted in mass-scale intertribal warfare.[128] European colonization in the North American continent also contributed to a number of wars between Native Americans, who fought over which of them should have first access to new technology and weaponry—like in the Beaver Wars.[129] Some Spaniards objected to the encomienda system of labor, notably Bartolomé de las Casas, who insisted that the Indigenous people were humans with souls and rights. Because of many revolts and military encounters, Emperor Charles V helped relieve the strain on both the native laborers and the Spanish vanguards probing the Caribana for military and diplomatic purposes.[130] Later on New Laws were promulgated in Spain in 1542 to protect isolated natives, but the abuses in the Americas were never entirely or permanently abolished. The Spanish also employed the pre-Columbian draft system called the mita,[131] and treated their subjects as something between slaves and serfs. Serfs stayed to work the land; slaves were exported to the mines, where large numbers of them died. In other areas the Spaniards replaced the ruling Aztecs and Incas and divided the conquered lands among themselves ruling as the new feudal lords with often, but unsuccessful lobbying to the viceroys of the Spanish crown to pay Tlaxcalan war demnities. The infamous Bandeirantes from São Paulo, adventurers mostly of mixed Portuguese and native ancestry, penetrated steadily westward in their search for Indian slaves. Serfdom existed as such in parts of Latin America well into the 19th century, past independence.[132] Historian Andrés Reséndez argues that even though the Spanish were aware of the spread of smallpox, they made no mention of it until 1519, a quarter century after Columbus arrived in Hispaniola.[133] Instead he contends that enslavement in gold and silver mines was the primary reason why the Native American population of Hispaniola dropped so significantly.[132][133] and that even though disease was a factor, the native population would have rebounded the same way Europeans did following the Black Death if it were not for the constant enslavement they were subject to.[133] He further contends that enslavement of Native Americans was in fact the primary cause of their depopulation in Spanish territories;[133] that the majority of Indians enslaved were women and children compared to the enslavement of Africans which mostly targeted adult males and in turn they were sold at a 50% to 60% higher price,[134] and that 2,462,000 to 4,985,000 Amerindians were enslaved between Columbus's arrival and 1900.[135][134] While some California tribes were settled on reservations, others were hunted down and massacred by 19th century American settlers. It is estimated that at least 9,400 to 16,000 California Indians were killed by non-Indians, mostly occurring in more than 370 massacres (defined as the \"intentional killing of five or more disarmed combatants or largely unarmed noncombatants, including women, children, and prisoners, whether in the context of a battle or otherwise\").[138][139] Throughout history, Indigenous people have been subjected to the repeated and forced removal from their land. Beginning in the 1830s, there was the relocation of an estimated 100,000 Indigenous people in the United States called the \"Trail of Tears\".[140] The tribes affected by this specific removal were the Five Civilized Tribes: The Cherokee, Creek, Chickasaw, Choctaw, and Seminole. The treaty of New Echota,[141] was enacted, which stated that the United States \"would give Cherokee land west of the Mississippi in exchange for $5,000,000\".[140] According to Jeffrey Ostler, \"Of the 80,000 Native people who were forced west from 1830 into the 1850s, between 12,000 and 17,000 perished.\" Ostler states that \"the large majority died of interrelated factors of starvation, exposure and disease\".[142] In addition to the removal of the Southern Tribes, there were multiple other removals of Northern Tribes also known as \"Trails of Tears.\" For example, \"In the free labor states of the North, federal and state officials, supported by farmers, speculators and business interests, evicted Shawnees, Delawares, Senecas, Potawatomis, Miamis, Wyandots, Ho-Chunks, Ojibwes, Sauks and Meskwakis.\" These Nations were moved West of the Mississippi into what is now known as Eastern Kansas, and numbered 17,000 on arrival. According to Ostler, \"by 1860, their numbers had been cut in half\" because of low fertility, high infant mortality, and increased disease caused by conditions such as polluted drinking water, few resources, and social stress.[142] Ostler also writes that the areas that Northern tribes were removed to were already inhabited: \"The areas west of the Mississippi River were home to other Indigenous nations— Osages, Kanzas, Omahas, Ioways, Otoes and Missourias. To make room for thousands of people from the East, the government dispossessed these nations of much their lands.\" Ostler writes that in 1840, when Northern Nations were moved onto their land, \"The combined population of these western nations was 9,000 ... 20 years later, it had fallen to 6,000.\"[142] On 8 September 2000, the head of the United States Bureau of Indian Affairs (BIA) formally apologized for the agency's participation in the ethnic cleansing of Western tribes.[143][144][145] In a speech before representatives of Native American peoples in June 2019, California governor Gavin Newsom apologized for the \"California Genocide.\" Newsom said, \"That's what it was, a genocide. No other way to describe it. And that's the way it needs to be described in the history books.\"[146] ^Extrapolated from 30,000 warriors (× 5) in year 1762, according to James Gorrell. Almost a century later, in 1841, George Catlin estimated the Sioux as up to 50,000 people, and mentioned that they had just lost approx. 8,000 dead to smallpox a few years prior. ^ abcOstler, Jeffrey (2019). Surviving Genocide: Native Nations and the United States from the American Revolution to Bleeding Kansas. Yale University Press. pp. 11–17, 381. ISBN978-0-300-24526-4. Since 1992, the argument for a total, relentless, and pervasive genocide in the Americas has become accepted in some areas of Indigenous studies and genocide studies. For the most part, however, this argument has had little impact on mainstream scholarship in U.S. history or American Indian history. Scholars are more inclined than they once were to gesture to particular actions, events, impulses, and effects as genocidal, but genocide has not become a key concept in scholarship in these fields. ^Dunbar-Ortiz, Roxanne (2014). An Indigenous Peoples History of the United States. Beacon Press. ISBN978-0-8070-0041-0. ^Feinstein, Stephen (2006). \"God, Greed, and Genocide: The Holocaust Through the Centuries, by Arthur Grenke\". Canadian Journal of History. 41 (1): 197–199. doi:10.3138/cjh.41.1.197. ISSN0008-4107. For the most part, however, the diseases that decimated the Natives were caused by natural contact. These Native peoples were greatly weakened, and as a result, they were less able to resist the Europeans. However, diseases themselves were rarely the sources of the genocides nor were they the sources of the deaths which were caused by genocidal means. The genocides were caused by the aggressive actions of one group towards another. ^Denevan, William M. (September 1992). \"The Pristine Myth: The Landscape of the Americas in 1492\". Annals of the Association of American Geographers. 82 (3): 369–385. doi:10.1111/j.1467-8306.1992.tb01965.x. ^Curthoys, Ann; Docker, John (2001). \"Introduction: Genocide: definitions, questions, settler-colonies\". Aboriginal History. 25: 1–15. ISSN0314-8769. JSTOR45135468. Some of the worst examples of escalating death by sickness and disease occurred on the Spanish Christian missions in Florida, Texas, California, Arizona, and New Mexico in the period 1690-1845. After the military delivered captive Indians to the missions, they were expected to perform arduous agricultural labour while being provided with no more than 1400 calories per day in low-nutrient foods, with some missions supplying as little as 715 calories per day. ^Russel, Thornton (1994). \"Book reviews - American Holocaust: Columbus and the Conquest of the New World by David E. Stannard\". The Journal of American History. 80 (4): 1428. doi:10.2307/2080617. JSTOR2080617. ^Cartwright, Mark (October 2015). \"Inca Government\". World History Encyclopedia. Knights of Vatican. Retrieved 19 July 2017. Eventually 40,000 Incas would govern some 10 million subjects speaking over 30 different languages. Consequently, the centralised Inca government, employing a vast network of administrators, governed over a patchwork empire which, in practice, touched local populations to varying degrees. ^Reséndez estimates between 2.462 and 4.985 million Indigenous people were enslaved.Reséndez, Andrés (2017). The other slavery: The uncovered story of Indian enslavement in America. p. 324. ISBN978-0-544-94710-8. ^Madley, Benjamin, An American Genocide, The United States and the California Catastrophe, 1846–1873, Yale University Press, 2016, 692 pages, ISBN978-0-300-18136-4, pp. 11, 351 ^For example, The Oxford Companion to American Military History (Oxford University Press, 1999) states that \"if Euro-Americans committed genocide anywhere on the continent against Native Americans, it was in California.\""}
{"url": "https://en.wikipedia.org/wiki/The_Revenant_(2015_film)", "text": "In August 2001, Akiva Goldsman purchased Punke's manuscript. Iñárritu signed on to direct The Revenant in August 2011; in April 2014, after several delays due to other projects, Iñárritu confirmed that he was beginning work on it and that DiCaprio had the lead role. Principal photography began in October 2014. Location and crew concerns delayed production from May to August 2015. The Revenant premiered at the TCL Chinese Theatre in Los Angeles, California on December 16, 2015. It had a limited release on December 25 and a wide release on January 8, 2016. It was a blockbuster, grossing $533 million worldwide. It received positive reviews, with praise for the performances, particularly for DiCaprio and Hardy, Iñárritu's direction, and Lubezki's cinematography; however, some criticism went to its runtime.[7][8][9] In late 1823, fur trapper Hugh Glass guides Captain Andrew Henry's trappers through the territory of the present-day Dakotas. While he and his half-Pawnee son, Hawk, are hunting, the company's camp is attacked by an Arikara war party which is seeking to recover its chief's abducted daughter, Powaqa. Many of the trappers are killed during the fight, and the rest of them escape onto a boat. Guided by Glass, the survivors begin a trek to Fort Kiowa on foot because Glass believes that traveling downriver will make them vulnerable. After docking, the crew stash their pelts near the shore. While he is scouting game, Glass is mauled and left near death by a female grizzly bear which is guarding its cubs. Trapper John Fitzgerald, fearing another Arikara attack, argues that the group must mercy-kill Glass and keep moving. Henry agrees, but he is unable to pull the trigger. Instead, he offers money in order to pay someone to stay with Glass and bury him after he dies. When the only volunteers are Hawk and the young Jim Bridger, Fitzgerald agrees to stay for money, in order to recoup his losses from the abandoned pelts. After the others leave, Fitzgerald attempts to smother Glass but is stopped by Hawk, who intervenes. Hawk threatens to shoot Fitzgerald and shouts for Bridger, who is away gathering water so Fitzgerald stabs Hawk to death as Glass watches helplessly. The next morning, Fitzgerald convinces Bridger, who is unaware of Hawk's murder, that the Arikara are approaching and they must abandon Glass. At first, Bridger protests, but he ultimately follows Fitzgerald after the latter leaves Glass half-buried alive in a makeshift grave. Bridger leaves his canteen, in which he engraved a spiral symbol, with Glass. After they depart, Fitzgerald admits that he lied about the approaching Arikara. When Fitzgerald and Bridger later meet Henry at the fort, Fitzgerald tells Henry that Glass died and Hawk vanished. Bridger is complicit in the lie about Glass's death, but he knows nothing about Hawk's murder. Glass begins his arduous journey through the wilderness. He performs a crude cauterization of his wounds and eludes the pursuing Arikara by jumping into river rapids. He later encounters Pawnee refugee Hikuc, who says that \"revenge is in the Creator's hands.\" The men share bison meat and travel. As a storm approaches, Hikuc constructs a makeshift sweat lodge for a feverish Glass to shelter in. After a hallucinogenic experience in the lodge, Glass emerges to discover that his wounds are healing, but French hunters have lynched Hikuc. He infiltrates their camp and sees the leader raping Powaqa. Glass frees her, and while she castrates her rapist, he kills several hunters, and recovers Hikuc's horse. The following day, Glass is ambushed and driven over a cliff on his horse by the Arikara. He survives the stormy night by eviscerating the dead horse and sheltering inside its carcass. A frightened French survivor staggers into Fort Kiowa, and Bridger recognizes his spiral engraved canteen as Glass's. Believing that it might be a surviving Hawk, Henry organizes a search party. Realizing that Glass is alive, Fitzgerald empties the outpost's safe and flees. The search party finds the exhausted Glass. Furious, Henry orders the arrest of Bridger, but Glass vouches for Bridger by stating that he was not present when Fitzgerald murdered Hawk and he was later deceived and threatened by the higher-ranking Fitzgerald. Glass and Henry set out in pursuit of Fitzgerald. After the two men split up, Fitzgerald ambushes, kills, and scalps Henry. Glass finds Henry's corpse, places it on his horse in an attempt to act as a decoy, and shoots Fitzgerald in the arm. He pursues Fitzgerald to a riverbank, where they engage in a brutal fight. Glass is about to kill Fitzgerald, but he spots a band of Arikara downstream. He remembers Hikuc's words and pushes Fitzgerald downstream into the hands of the Arikara. Their chief Elk Dog kills and scalps Fitzgerald, and the Arikara (who have found Powaqa) spare Glass. Glass retreats into the mountains, where he is visited by his wife's spirit. Once Iñárritu agreed to direct, he began working with Smith on script rewrites. In an interview with Creative Screenwriting, Smith admitted that during this process, he was unsure if Iñárritu would even be able to film some of the sequences they wrote. He recalled, \"He would have some ideas and I would say, 'Alejandro, we can't pull this off. It's not going to work', and he would say, 'Mark, trust me, we can do this.' In the end, he was right.\"[19] The project was put on hold in March 2012, as New Regency hired Iñárritu to direct an adaptation of Flim-Flam Man, Jennifer Vogel's non-fiction book about her criminal father.[20] Penn was also under consideration for the lead role in that film.[21] In December 2012, Iñárritu announced that his next film would be Birdman or (The Unexpected Virtue of Ignorance), a comedy-drama about an actor who once played a famous superhero. He ended up winning the Academy Awards for Best Director and Best Original Screenplay, with the film winning Best Picture. Filming took place in March 2013.[22]The Revenant was scheduled to begin production right after Birdmanwrapped.[23] Principal photography for The Revenant began in October 2014.[26] A planned two-week break from filming in December was extended to six weeks which forced Tom Hardy to drop out of Suicide Squad. In February 2015, the director, who shot the film using natural lighting, stated that production would last \"until the end of April or May\", as the crew is \"shooting in such remote far-away locations that, by the time we arrive and have to return, we have already spent 40% of the day\".[27][28][29] Ultimately, principal photography wrapped in August 2015.[citation needed] The waterfall scenes were filmed at the Kootenai Falls near Libby, Montana. Though the initial plan was to film the last scenes in Canada, the weather was ultimately too warm, leading the filmmakers to locations near the Río Olivia in Tierra del Fuego, Argentina with snow on the ground, to shoot the ending.[26][30] Crew members often complained about difficult filming, with many quitting or being fired. Mary Parent was then brought in as a producer.[26] Iñárritu stated that some of the crew members had left the production, explaining that \"as a director, if I identify a violin that is out of tune, I have to take that from the orchestra.\" On his experience, DiCaprio stated: \"I can name 30 or 40 sequences that were some of the most difficult things I've ever had to do. Whether it's going in and out of frozen rivers, or sleeping in animal carcasses, or what I ate on set. [I was] enduring freezing cold and possible hypothermia constantly.\"[31][32] The director had stated that he originally wanted to film chronologically, a process that would have added $7 million to the production budget.[33] Later he confirmed that it was shot in sequence, though Hardy had stated that it was impossible due to weather conditions.[34][35] In July 2015, it was reported that the budget had ballooned from the original $60 million to $95 million, and by the time production wrapped it had reached $135 million.[3] The musical score was composed by Japanese musician Ryuichi Sakamoto and German electronic musician Alva Noto with additional music composed by Bryce Dessner.[37] The main body of the score was recorded at the Seattlemusic Scoring Stage in the Bastyr Chapel in greater Seattle, Washington by musicians of the Northwest Sinfonia. Sakamoto conducted these sessions. Bryce Dessner's portion of the score was performed by the 25-piece Berlin-based orchestra known as \"s t a r g a z e\" under conductor André de Ridder.[38][39] Additional licensed music includes \"Become Ocean\", the Pulitzer Prize and Grammy Award-winning work of John Luther Adams as recorded by the Seattle Symphony with conductor Ludovic Morlot and an excerpt of \"Jetsun Mila\" from French musician and composer Eliane Radigue.[40] A soundtrack album was released online on December 25, 2015, and on CD on January 8, 2016. Milan Records released a vinyl pressing of the soundtrack in April 2016.[39] The movie was accompanied by a 44-minute documentary, named A World Unseen, highlighting the process of making the production. A World Unseen was released on January 21, 2016, on YouTube.[42][43] Eliot Rausch served as its director.[44][45] In the documentary A World Unseen, the director stated that for The Revenant's main themes, he revisits the issues and concerns of intense parental and filial relations, which are readily recognizable as a recurrent theme from his previous works. Regarding the theme of revenge, he mentioned that the approach of vengeance in the film needs to be significantly tempered by anyone who would want to see vengeance as either an effective or useful moral to be applied in life. In the end, he said, there can only be a disappointment and lack of fulfillment for anyone who looks to revenge as providing a higher purpose for living or a life-defining purpose.[42][43] The Guardian reported, \"The backstory about Glass's love for a Pawnee woman is fiction. It has been suggested the real Glass had such a relationship, but there's no firm evidence—and no evidence that he had any children. ... As for the ending, it has been changed in one significant way: in real life, nobody got killed.\"[47] Canadian actor Roy Dupuis was strongly critical of the portrayal of French-Canadianvoyageurs as murderous rapists. Dupuis was originally offered a role as a voyageur, but he rejected it due to perceptions of anti-French bias and historical inaccuracies.[48][49][50][51] According to Allan Greer, the Canada Research Chair of colonial North America, \"generally the American traders had a worse reputation than the Canadians.\"[48] The filmmakers made a special point of emphasizing the importance of historical issues of ethnicity approached in the movie and represented in the mixed ethnic background of Hugh Glass's son portrayed in the film (portrayed as half Pawnee by Glass's wife) as relating to his own life and his identification with ethnic concerns. The director has referred to having encountered constant xenophobia and stated that: \"These constant and relentless xenophobic (comments) have been widely spread by the media without shame, embraced and cheered by leaders and communities around the US. The foundation of all this is so outrageous that it can easily be minimized as an SNLsketch, a mere entertainment, a joke ... I debated with myself, if I should bring up this uncomfortable subject tonight but in light of the constant and relentless xenophobic comments that have been expressed recently against my Mexican fellows, it is inevitable.\"[52] Bruce Bradley of True West Magazine pointed out several anachronisms hinted in the film such as John Fitzgerald talking about his father having friends in the Texas Rangers and about going to Texas and joining the army himself. However, in 1823, Texas was still part of Mexico and therefore would not have had an American army, let alone the Texas Rangers. Another example being Hugh Glass discovering a mountain of buffalo skulls while working his way to Fort Kiowa despite the buffalo slaughter not occurring until the 1870s.[53] To portray Arikara culture accurately, the filmmakers hired several cultural consultants[54][55] and teamed up with two linguists to provide faithful Arikara and Pawnee language lines for actors.[56][57] Hikuc, the Pawnee man who helps Glass survive, is played by a Navajo actor.[58] However, in one scene, a Pawnee character rescuing Glass is accompanied by a voiceover in Inupiaq, which is spoken in Arctic Alaska, thousands of kilometres away and a different language family from Pawnee. The voiceover was a recording of Doreen Nutaaq Simmonds[59] reading a poem from a John Luther Adams recording; the words originally came from an Inuit woman named Uvavnuk, an angakkuq (shaman) and oral poet.[60] The Revenant had a limited release in the United States on December 25, 2015, including Los Angeles—making it eligible for the 88th Academy Awards—before being released nationwide on January 8, 2016.[61][62] The film opened in Australia on January 7, 2016, and in the United Kingdom on January 15, 2016.[63][64] In the Philippines, the film's release date was originally set for January 27, 2016, but it was eventually delayed one week to February 3, 2016.[65][66] Although studios initially chose not to pursue a theatrical release in China, following its three wins at the 88th Academy Awards on February 28, 2016, the film was granted a release in China but with several cuts.[67] It was released on March 18, 2016.[25] The DVD, Blu-ray and 4K Ultra HD Blu-ray were released on April 12, 2016, in the US.[68] Opening sales of the DVD along with online streaming orders placed The Revenant as number one in sales at Amazon.[69] Distribution to major rental outlets in the US was done on May 17, 2016. On December 20, 2015, less than one week before its release, screener copies of The Revenant were leaked online. The FBI linked this to Alcon Entertainment CEO Andrew Kosove, who denied responsibility.[70] In October 2016, a former 20th Century Fox employee was fined $1.12 million in a separate case for uploading both The Revenant and The Peanuts Movie online.[71] The Revenant grossed $183.6 million in the United States and Canada and $349.3 million in other countries for a worldwide total of $533 million, against a production budget of $135 million.[4] In March 2016, before the film had completed its theatrical domestic and international runs, Deadline Hollywood calculated the net profit of the film to be $61.6 million when factoring together all expenses and revenues.[72] In North America, The Revenant opened in limited release on December 25, 2015, and over the weekend grossed $474,560 from four theaters in New York City and Los Angeles ($118,640 per screen), finishing twenty-third at the box office.[73] It was the second-biggest theater average of 2015 behind the $130,000 four-screen debut of Steve Jobs.[74] The film earned a total of $1.6 million from its two-week limited run before expanding wide on January 8, 2016, across 3,371 theaters.[75][76] It earned $2.3 million from its early Thursday preview showings from 2,510 theaters.[75] On its opening day, the film earned $14.4 million, ranking first at the box office.[77] The film grossed $39.8 million in its opening weekend from 3,375 theaters, exceeding initial projections by 70%, and finishing second at the box office behind Star Wars: The Force Awakens ($42.4 million), which was on its fourth weekend of play. It was the director's biggest opening of all time, and the fourth-biggest for DiCaprio and supporting actor Tom Hardy.[78] Critics noted that The Force Awakens had an advantage from playing at 781 more theaters, family-friendly Sunday matinees, and all North American IMAX theaters.[78] Nevertheless, The Revenant played very balanced across the US and overperformed in all states except the Northeast region.[78] Its wide release weekend is among the top openings in January.[79] It topped the box office in its fifth weekend overall and third weekend in wide release after competing with Ride Along 2 in its second weekend. It added $16 million in its third weekend, which was down 49.7% but topped the box office, despite a blizzard blanketing most of the East Coast which reportedly hurt many films' box office performance.[80][81][82][83] The following weekend it was overtaken by Fox's own animated movie Kung Fu Panda 3 thereby topping the box office for one weekend.[84] Following the announcement of the Oscar nominees on January 14, The Revenant witnessed the biggest boost among the Best Picture category, jumping from $54.1 million to $170.5 million, an increase of +215% up to the Oscar ceremony in the weekend ending February 28.[85] Outside North America, the film secured a release in 78 countries.[86] It made $20.5 million from 2,407 screens in just 18 markets, placing behind The Force Awakens at the international box office chart and first among newly released films.[87] The following weekend, it added $32.3 million from 25 markets on 4,849 screens.[86] The film topped the international box office in its third weekend—the same weekend when it topped the US box office—overtaking The Force Awakens with $33.7 million from 48 markets.[88] In the United Kingdom and Ireland, it took the number 1 spot with $7.87 million or £5.2 million ($7.4 million) from 589 theaters and remained there for a second weekend declining by 24% with £3.86 million ($5.5 million), and for a third weekend.[86][89][90][91][92] Similarly, in Russia, it passed The Force Awakens to take the top spot with $7.5 million from 1,063 screens.[87] In France, it has the biggest opening day in Paris and the third-biggest opening weekend of 2016 with $8.2 million.[93] It also opened at No. 1 in Mexico ($5.1 million), Spain ($4 million), the Netherlands ($1.3 million), Belgium ($1.1 million), Argentina ($955,000), Sweden ($914,000), South Korea, Denmark, Norway, Israel, Egypt and Portugal among other markets.[86][88][91][94] In Germany ($4.6 million) and Australia ($2.9 million), it debuted at No. 2 both behind The Force Awakens and in Brazil ($2.17 million) behind The Ten Commandments.[87][94] It had one of the top ten openings of all time for a Fox film, not accounting for inflation in South Korea with $5.7 million and went on to top the box office there for a second weekend with $3.22 million despite cold weather affecting theater attendance resulting in low box office performance.[95][96] In Russia, though not opening at number 1, it topped the box office in its second weekend with $4.4 million—more than The Force Awakens—and went on to the top for a third weekend with $3.6 million.[86][97] In China, it had an opening day of around $11 million from more than 11,000 screens, including $250,000 in midnight previews, and $23 million in two days.[98][99][100] In its opening weekend, it grossed $31 million, coming in second place behind the animated Zootopia. IMAX comprised $2.3 million on 278 screens.[101] In terms of total earnings, its largest markets outside of the US and Canada are China ($58.6 million), the United Kingdom ($32.8 million), Germany ($28.7 million) and France ($28.2 million).[102][103] The film opened in Japan on March 23.[93][104] On Rotten Tomatoes, the film has an approval rating of 78%, based on 399 reviews, with an average rating of 7.80/10. The website's critical consensus reads: \"As starkly beautiful as it is harshly uncompromising, The Revenant uses Leonardo DiCaprio's committed performance as fuel for an absorbing drama that offers punishing challenges—and rich rewards.\"[105] On Metacritic, the film has a weighted average score of 76 out of 100, based on 50 critics, indicating \"generally favorable reviews\".[106] Audiences polled by CinemaScore gave the film an average grade of \"B+\" on an A+ to F scale, and PostTrak reported audiences gave it an overall positive score of 85% and a 59% \"definite recommend\".[107][108] Reviewers cited in a CBS News survey of critics highly praised DiCaprio's performance, referring to it as an \"astonishing testament to his commitment to a role\" and as an \"anchoring performance of ferocious 200 percent commitment\".[8]Peter Travers of Rolling Stone called DiCaprio's acting \"a virtuoso performance, thrilling in its brute force and silent eloquence\".[109] Writing for Vulture, David Edelstein called the film a \"tour de force\" and \"[b]leak as hell but considerably more beautiful\", but noted it had \"traditional masculinity instead of a search for what illuminates man's inhumanity to man\".[citation needed] Justin Chang of Variety wrote Iñárritu \"increasingly succumbs to the air of grim overdetermination that has marred much of [his] past work\" and it was \"an imposing vision... but also an inflated and emotionally stunted one\".[8] Stephanie Zacharek, writing for Time magazine, gave a positive review to the film stating: \"Inarritu may have fashioned The Revenant as the ultimate endurance test, but as Glass, DiCaprio simply endures. He gives the movie a beating heart, offering it up, figuratively speaking, alive and bloody on a platter. It—he—is the most visceral effect in the movie: revenge served warm. Bon appetit.\"[7]Richard Brody of The New Yorker was critical of the film, and said that Emmanuel Lubezki's images were mere \"pictorial ornament[s] to Alejandro G. Iñárritu's bland theatrical stagings\".[110] A critic for Santa Barbara Independent considered it a remake of Man in the Wilderness (1971).[111] Slant Magazine's writer Ed Gonzalez suggested that the Slant staff mostly disliked the film: \"Our contempt for The Revenant knows no limits.\"[110] Gonzalez unfavorably compared Iñárritu's work to Terrence Malick's 2005 film The New World. In the official review, Slant writer Jaime N. Christley wrote: \"The Revenant [is] a misery-fest that plants its narrative flags as carelessly as a Roland Emmerich blockbuster, guaranteeing us a viewing experience almost as arduous as the trials depicted on screen, before reaching a conclusion that's sealed the moment audiences first meet the key players. After an obligatory false calm, The Revenant's proper opening scene is a show-stopping massacre at a fur trapper's campsite. It's the kind of thing Howard Hawks would have handled—and did, in The Big Sky—in under 90 seconds, with mostly off-camera particulars and minimal effects, but Iñárritu forces it to resemble the Normandy Beach sequence in Saving Private Ryan as much as history or sense will allow, and then some.\"[112] On May 2, 2016, Time magazine included both DiCaprio and Iñárritu in its issue of the 100 Most Influential People of 2015, with a cover photograph of DiCaprio on the magazine. John Kerry, then the US Secretary of State, wrote a short testimonial to DiCaprio for this issue of Time stating that DiCaprio's dedication drives him to succeed and \"that's how he takes himself back 200 years to create an Oscar-winning, bear-brawling, powerhouse performance in The Revenant.\"[125] ^Evry, Max (January 21, 2015). \"First Look at Leonardo DiCaprio in The Revenant\". ComingSoon.net. Archived from the original on December 8, 2015. Retrieved December 28, 2015. Through April, Leonardo DiCaprio will be shooting in the wilds of Calgary amid the Canadian Rockies playing a fur trapper hunting the men who left him for dead in The Revenant..."}
{"url": "https://en.wikipedia.org/wiki/File:Miguel_Hidalgo_con_estandarte.jpg", "text": "The official position taken by the Wikimedia Foundation is that \"faithful reproductions of two-dimensional public domain works of art are public domain\". This photographic reproduction is therefore also considered to be in the public domain in the United States. In other jurisdictions, re-use of this content may be restricted; see Reuse of PD-Art photographs for details. This image has been assessed under the valued image criteria and is considered the most valued image on Commons within the scope: Retrato Miguel Hidalgo y Costilla con estandarte por Antonio Fabrés en Palacio Nacional (México) (Portrait of Miguel Hidalgo y Costilla with banner by Antonio Fabrés in National Palace (Mexico)). You can see its nomination here."}
{"url": "https://en.wikipedia.org/wiki/Mule", "text": "A female mule that has oestrus cycles, and so could, in theory, carry a foetus, is called a \"molly\" or \"Molly mule\", although the term is sometimes used to refer to female mules in general. A male mule is properly called a \"horse mule\", although it is often called a \"john mule\", which is the correct term for a gelded mule. A young male mule is called a \"mule colt\", and a young female is called a \"mule filly\".[4] Breeding of mules became possible only when the range of the domestic horse, which originated in Central Asia in about 3500 BC, extended into that of the domestic ass, which originated in north-eastern Africa. This overlap probably occurred in Anatolia and Mesopotamia in Western Asia, and mules were bred there before 1000 BC.[5]: 37 George Washington bred mules at his Mount Vernon home. At the time, they were not common in the United States, but Washington understood their value, as they were \"more docile than donkeys and cheap to maintain.\"[11] In the nineteenth century, they were used in various capacities as draught animals – on farms, especially where clay made the soil slippery and sticky; pulling canal boats; and famously for pulling, often in teams of 20 or more animals, wagonloads of borax out of Death Valley, California from 1883 to 1889. The wagons were among the largest ever pulled by draught animals, designed to carry 10 short tons (9 metric tons) of borax ore at a time.[12] Mules were used by armies to transport supplies, occasionally as mobile firing platforms for smaller cannons, and to pull heavier field guns with wheels over mountainous trails such as in Afghanistan during the Second Anglo-Afghan War.[13] In general terms, in both the mule and the hinny, the foreparts and head of the animal are similar to those of the father sire, while the hindparts and tail tend to resemble those of the dam.[5]: 36 A mule is generally larger than a hinny, with longer ears and a heavier head; the tail is usually covered with long hair like that of its mare mother.[5]: 37 A mule has the thin limbs, small narrow hooves and short mane of the donkey, while its height, the shape of the neck and body, and the uniformity of its coat and teeth are more similar to those of the horse.[16] Mules vary widely in size, from small miniature mules under 125 cm (50 in) to large and powerful draught mules standing up to 180 cm (70 in) at the withers.[17]: 86 The median weight range is between about 370 and 460 kg (820 and 1000 lb).[18] The coat may be of any color seen in the horse or in the donkey. Mules usually display the light points commonly seen in donkeys: pale or mealy areas on the belly and the insides of the thighs, on the muzzle, and around the eyes. They often have primitive markings such as dorsal stripe, shoulder stripe or zebra stripes on the legs.[5]: 37 The mule exhibits hybrid vigor.[19]Charles Darwin wrote: \"The mule always appears to me a most surprising animal. That a hybrid should possess more reason, memory, obstinacy, social affection, powers of muscular endurance, and length of life, than either of its parents, seems to indicate that art has here outdone nature\".[20] The mule inherits from the donkey the traits of intelligence, sure-footedness, toughness, endurance, disposition, and natural cautiousness. From the horse it inherits speed, conformation, and agility.[21]: 5–6, 8 Mules are reputed to exhibit a higher cognitive intelligence than their parent species, but robust scientific evidence to back up these claims is lacking. Preliminary data exist from at least two evidence-based studies, but they rely on a limited set of specialized cognitive tests and a small number of subjects.[22][23] Mules are generally taller at the shoulder than donkeys and have better endurance than horses, although a lower top speed.[22] In the early twentieth century the mule was preferred to the horse as a pack animal – its skin is harder and less sensitive than that of a horse, and it is better able to bear heavy weights.[16] A mule has 63 chromosomes, intermediate between the 64 of the horse and the 62 of the donkey.[24] Mules are usually infertile for this reason.[25] Pregnancy is rare, but can occasionally occur naturally, as well as through embryo transfer. A few mare mules have produced offspring when mated with a horse or donkey stallion.[26][27]Herodotus gives an account of such an event as an ill omen of Xerxes' invasion of Greece in 480 BC: \"There happened also a portent of another kind while he was still at Sardis—a mule brought forth young and gave birth to a mule\" (Herodotus The Histories 7:57), and a mule's giving birth was a frequently recorded portent in antiquity, although scientific writers also doubted whether it was really possible (see e.g. Aristotle, Historia animalium, 6.24; Varro, De re rustica, 2.1.28). Between 1527 and 2002 approximately sixty such births were reported.[27] In Morocco in early 2002 and Colorado in 2007, mare mules produced colts.[27][28][29] Blood and hair samples from the Colorado birth verified that the mother was indeed a mule and the foal was indeed her offspring.[29] A 1939 article in the Journal of Heredity describes two offspring of a fertile mare mule named \"Old Bec,\" which was owned at the time by Texas A&M University in the late 1920s. One of the foals was a female, sired by a jack. Unlike her mother, she was sterile. The other, sired by a five-gaited Saddlebred stallion, exhibited no characteristics of any donkey. That horse, a stallion, was bred to several mares, which gave birth to live foals that showed no characteristics of the donkey.[30] In a more recent instance, a group from the Federal University of Minas Gerais in 1995 described a female mule that was pregnant for a seventh time, having previously produced two donkey sires, two foals with the typical 63 chromosomes of mules, and several horse stallions that had produced four foals. The three of the latter available for testing each bore 64 horse-like chromosomes. These foals phenotypically resembled horses, though they bore markings absent from the sire's known lineages, and one had ears noticeably longer than those typical of her sire's breed. The elder two horse-like foals had proved fertile at the time of publication, with their progeny being typical of horses.[31] While a few mules can carry live weight up to 160 kg (353 lb), the superiority of the mule becomes apparent in their additional endurance.[32] In general, a mule can be packed with dead weight up to 20% of its body weight, or around 90 kg (198 lb).[32] Although it depends on the individual animal, mules trained by the Army of Pakistan are reported to be able to carry up to 72 kg (159 lb) and walk 26 km (16.2 mi) without resting.[33] The average equine in general can carry up to roughly 30% of its body weight in live weight, such as a rider.[34] About 3.5 million donkeys and mules are slaughtered each year for meat worldwide.[35] Mule trains have been part of working portions of transportation links as recently as 2005 by the World Food Programme,[36] and are still used extensively to transport cargo in rugged, roadless regions.[citation needed] ^\"Mule\". The Encyclopædia Britannica: A Dictionary of Arts, Sciences, and General. Vol. XVII. Henry G. Allen and Company. 1888. p. 15. Archived from the original on 2 April 2023. Retrieved 27 March 2016."}
{"url": "https://en.wikipedia.org/wiki/Susana_Baca", "text": "Susana Baca grew up in a coastal fishing village Chorrillos, a district of the Lima Province of Peru, and part of greater Lima.[4] Her music is a mixture of traditional and contemporary. Her backing band features indigenous Peruvian instruments such as the cajón (\"wooden box\", whose origins lie in an upturned fruit crate), udu (clay pot), and quijada (jawbone of a burro) cheko a dried gourd, as well as acoustic guitar and electric upright baby bass. Although many of her songs are based on traditional forms such as the landó or vals, she also incorporates elements of Cuban and Brazilian music. Her debut album for the label Luaka Bop, produced by Greg Landau, brought her to the attention of World Music audiences worldwide. Baca is an important figure in the revival of Afro-Peruvian music within Peru (see, for example, dancers from the Perú Negro troupe, as well as \"Festejo\" music), which, like the culture that produced it, had previously been little recognized, but which is now regarded as an important part of Peruvian culture. Baca has contributed much to its international popularity, which began in 1995 with the release of the compilation CD Afro-Peruvian Classics: The Soul of Black Peru. The album, featuring the Baca song \"Maria Lando\", was released by the Luaka Bop record label, which belongs to ex-Talking Heads frontman David Byrne.[5] According to several important critics, Baca is more than just an Afro-Peruvian diva. Michael Heumann from Stylus Magazine says: \"Maria Lando put Susana Baca on the world music map. Since then, she has released a number of very popular albums, most on Luaka Bop, and all of incredibly high quality.\" Timothy G. Merello from PopMatters said of her performance in Old Town School of Folk Music, Chicago: \"Susana Baca, more than just a singer, is a poet, a historian, a spelunker and explorer of Afro-Peruvian folklore and music [...] she and her band entertain, educate and entrance a sold out crowd with a musical melange of rhythms, melodies, beats, and dance.\" While Deanne Sole, also from PopMatters, says of Baca's 2009 album Seis Poemas (Six Poems): \"After watching Peruvian singer Susana Baca perform on an open stage in burning heat with malfunctioning equipment, one can't help but respect Baca not only as a voice but as a performer, as a person [...] Seis Poemas, a small album but a charming one.\" Baca founded the Instituto Negrocontinuo (Black Continuum Institute) in her seafront home in Chorrillos, to foster the collection, preservation and creation of Afro-Peruvian culture, music and dance.[6] Her 2020 album A Capella was nominated for Best Folk Album at the 21st Annual Latin Grammy Awards.[9][10] She ended up winning the award and said she dedicated it to the Peruvian youth, specially those who work on building Perú higher every day.[11] In July 2011, the newly elected President of Peru, Ollanta Humala, announced that Baca would become his Minister of Culture. On 28 July she was sworn in, becoming the second Afro-Peruvian cabinet minister in the history of independent Peru. She resigned due to a cabinet reshuffle on 11 December 2011."}
{"url": "https://en.wikipedia.org/wiki/Monument_to_Christopher_Columbus_(Buenos_Aires)", "text": "The Monument to Christopher Columbus was located in the plaza behind the government house in the city of Buenos Aires in Argentina. It was located in the Columbus Park, between the Casa Rosada and La Avenida La Rabida. The monument was a gift celebrating the 1910 Centennial of Argentine independence from Spain. It was sponsored by the Argentine-Italian community, led by Italian immigrant businessman Antonio Devoto. Work of the Italian sculptor Arnaldo Zocchi, the foundation stone of the monument was placed on May 24, 1910 and the inauguration took place on June 15, 1921. The statue was a source of pride for the Buenos Aires Italian community, planned for the centennial of Argentine independence in 1910. For previously maligned Italian immigrants to Argentina, sponsoring the statue, which had pride of place in front of the Casa Rosada, brought them a level of respect they had not previously enjoyed. Nineteenth-century liberal thinker Juan Bautista Alberdi did not considered southern European Catholics, such as Italians, desirable immigrants. The centennial of Argentine independence was an occasion to create new monuments in the capital. In particular, immigrant communities were invited to submit proposals to the Centennial Commission. The immigrant communities of Italy, Spain, France, and Germany vied for prominent placement of their monuments. The site for the Columbus statue behind the Casa Rosada in the Parque Colón was already occupied by an enormous fountain, which was ordered moved. The statue of Columbus was on a high column, with the navigator holding a map in his hand and facing the sea, looking toward Europe. An inscription on the monument from Seneca's Medea alluded to foreknowledge of the existence of the New World, and the \"discovery\" by Columbus as the fulfillment of a prophecy. There were a series of allegorical statues at the base of the column depicting science, civilization, and genius. On another part of the base allegorical figures to Christian faith and justice are meant to convey European civilization's benefits brought to the New World by Columbus. The original plans did not directly tie Columbus to Argentina, and the Centennial Commission requested additions. This resulted in bas reliefs of Columbus, one with his requesting permission from the Catholic Monarchs of Spain, Isabel and Ferdinand, to sail West. The other shows Columbus on his return, bringing indigenous slaves. The monument was made of Italian marble by an Italian sculptor in Italy, and until the requested changes in design, it had nothing to do with Argentina or the Italian immigrant community. On a number of points, the placement and symbolism of the Columbus statue became problemic for a number of Argentines after the 1992, the 500th anniversary of Columbus's voyage.[1] The statue became a source of controversy in 2013, when President Cristina Fernández de Kirchner decided to replace Columbus with one of Juana Azurduy de Padilla, a Mestiza revolutionary army leader during the war of independence. The statue to Azurduy was commissioned with funding with the help of Bolivian president Evo Morales and inaugurated in July 2015. In 2013 the statue to Columbus was dismantled and for two years lay in pieces on the ground while the statue to Azurduy by Argentine sculptor Andrés Zerneri was constructed. The Argentine Italian community and other Argentines were outraged at the change. Legal battles were fought about the destination of the Columbus statue. In 2017 it was moved within the capital and placed at the waterfront on Avenida Costanera Rafael Obligado, adjacent to Aeroparque Jorge Newbery. The two years when it was in pieces caused damage to the marble, and it had to be restored. The new site required reinforcement of the ground, to support the tons of marble. The Azurduy statue was moved in 2017 to a less prominent place in the central core of Buenos Aires, in front of the Kirchner Cultural Center. The controversy over the Columbus and Azurduy statues highlights conflicts in Argentina over historical memory, national identity, and claims to public space.[2]"}
{"url": "https://en.wikipedia.org/wiki/Special:BookSources/978-0-86531-213-5", "text": "This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). If you arrived at this page by clicking an ISBN link in a Wikipedia page, you will find the full range of relevant search links for that specific book by scrolling to the find links below. To search for a different book, type that book's individual ISBN into this ISBN search box. Spaces and hyphens in the ISBN do not matter. Also, the number starts after the colon for \"ISBN-10:\" and \"ISBN-13:\" numbers. An ISBN identifies a specific edition of a book. Any given title may therefore have a number of different ISBNs. See #Find other editions below for finding other editions. An ISBN registration, even one corresponding to a book page on a major book distributor database, is not definite proof that such a book actually exists. A title may have been cancelled or postponed after the ISBN was assigned. Check to see if the book exists or not. Google Books and Amazon.com may be helpful if you want to verify citations in Wikipedia articles, because they often let you search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available). At the Open Library (part of the Internet Archive) you can borrow and read entire books online. Luxembourg Montenegro Netherlands Find this book in the Dutch-Union Catalogue that searches simultaneously in more than 400 Dutch electronic library systems (including regional libraries, university libraries, research libraries and the Royal Dutch library) Book-swapping websites Non-English book sources If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language."}
{"url": "https://en.wikipedia.org/w/index.php?title=Latin_America&action=edit&section=49", "text": "By publishing changes, you agree to the Terms of Use, and you irrevocably agree to release your contribution under the CC BY-SA 4.0 License and the GFDL. You agree that a hyperlink or URL is sufficient attribution under the Creative Commons license."}
{"url": "https://reproductiverights.org/our-regions/latin-america-caribbean/", "text": "Related Content Follow the Center Join Now Latin America and the Caribbean The Center for Reproductive Rights Latin America and Caribbean (LAC) Program works to ensure that reproductive rights are recognized as human rights at the national, regional and international levels. Summary Along with its partners in the region, the Center uses strategic litigation to hold governments accountable, secure equal access to justice, and obtain redress and reparation for sexual and reproductive health and rights (SRHR) violations and gender-based violence faced by the area’s most vulnerable women, girls and adolescents. Based in Bogotá, Colombia, and operating in the region for nearly 20 years, the Center’s LAC Program has secured wide-reaching legal victories in national courts, regional accountability bodies, and international human rights bodies that advance reproductive rights and access. Key Facts 80% Of sexual violations of girls and adolescents are concentrated in victims between 10 and 14 years old. 4x Girls under 15 are four times more likely to die during pregnancy or childbirth than an adult woman. 83% Of women of reproductive age live in countries with restrictive abortion laws. (Spanish-language publication) In two unprecedented decisions, the UN Working Group on Arbitrary Detention called on El Salvador to release and provide reparations to four women criminalized for suffering obstetric emergencies, deeming their detention arbitrary. Read more. (Spanish-language publication) This report shows that glyphosate, the herbicide used by the Colombian government to eradicate illicit crops, has negative impacts on people’s reproductive health. We call on the Colombian government to stop the fumigation program from moving forward. Read more. (Spanish-language publication) Every girl deserves a childhood free from abuse–yet many in Latin America are being failed by their governments. We took the cases of four girls–Fatima, Lucía, Susana, and Norma–to the UN Human Rights Committee to get justice. Read their stories in this fact-sheet. Read more. (Spanish language fact sheet) In a landmark ruling in Paola Guzmán Albarracín v. Ecuador, the Inter-American Court of Human Rights established legally binding standards to prevent sexual violence and harassment in schools throughout Latin America and the Caribbean, and held Ecuador responsible for failing to protect Paola Guzmán Albarracín. Read more about the case in this fact sheet. Privacy Overview This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience. Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. Analytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics the number of visitors, bounce rate, traffic source, etc. Cookie Duration Description _ga 2 years This cookie is installed by Google Analytics. The cookie is used to calculate visitor, session, campaign data and keep track of site usage for the site's analytics report. The cookies store information anonymously and assign a randomly generated number to identify unique visitors. _gat_UA-6619340-1 1 minute No description _gid 1 day This cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the wbsite is doing. The data collected including the number visitors, the source where they have come from, and the pages viisted in an anonymous form. _parsely_session 30 minutes This cookie is used to track the behavior of a user within the current session. HotJar: _hjAbsoluteSessionInProgress 30 minutes No description HotJar: _hjFirstSeen 30 minutes No description HotJar: _hjid 1 year This cookie is set by Hotjar. This cookie is set when the customer first lands on a page with the Hotjar script. It is used to persist the random user ID, unique to that site on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID. HotJar: _hjIncludedInPageviewSample 2 minutes No description HotJar: _hjIncludedInSessionSample 2 minutes No description HotJar: _hjTLDTest session No description SSCVER 1 year 24 days The domain of this cookie is owned by Nielsen. The cookie is used for online advertising by creating user profile based on their preferences. Advertisement cookies are used to provide visitors with relevant ads and marketing campaigns. These cookies track visitors across websites and collect information to provide customized ads. Cookie Duration Description _fbp 3 months This cookie is set by Facebook to deliver advertisement when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website. fr 3 months The cookie is set by Facebook to show relevant advertisments to the users and measure and improve the advertisements. The cookie also tracks the behavior of the user across the web on sites that have Facebook pixel or Facebook social plugin. IDE 1 year 24 days Used by Google DoubleClick and stores information about how the user uses the website and any other advertisement before visiting the website. This is used to present users with ads that are relevant to them according to the user profile. IMRID 1 year 24 days The domain of this cookie is owned by Nielsen. The cookie is used for storing the start and end of the user session for nielsen statistics. It helps in consumer profiling for online advertising. personalization_id 2 years This cookie is set by twitter.com. It is used integrate the sharing features of this social media. It also stores information about how the user uses the website for tracking and targeting. TDID 1 year The cookie is set by CloudFare service to store a unique ID to identify a returning users device which then is used for targeted advertising. test_cookie 15 minutes This cookie is set by doubleclick.net. The purpose of the cookie is to determine if the user's browser supports cookies."}
{"url": "https://en.wikipedia.org/wiki/The_Revenant_(novel)", "text": "The Revenant: A Novel of Revenge is a 2002 novel by American author Michael Punke, based on a series of events in the life of American frontiersman Hugh Glass in 1823 Missouri Territory.[1] The word \"revenant\" means someone who has risen from the grave to terrorize the living. Many of the outdoor adventures depicted in the novel are based on Punke's outdoor interests and hobbies, such as fishing and exploring the backcountry, which he enjoyed while growing up in Wyoming.[1] Punke had originally intended to write a political novel.[1] He started archival research and writing in 1997, and it took four years for him to complete the novel, with the long hours taking a toll on his health.[1]Maxim reporter Walter Bonner interviewed Punke's brother who described the novel as an attempt by Punke to \"write his way out of D.C. and back to the big sky country\".[1] The novel is dedicated to his high school English teacher, Roger Clark. The novel was republished in January 2015 in anticipation of the upcoming film release, but Punke's role as an ambassador to the World Trade Organization (WTO) prevented him from participating in pre-release publicity.[1] Reception of the novel has been largely positive. Publishers Weekly described the novel as \"Told in simple expository language, this is a spellbinding tale of heroism and obsessive retribution.\"[2] Similarly, Kirkus Reviews called the novel \"A good adventure yarn, with plenty of historical atmosphere and local color.\"[3] Writer William Nauenberg describes the story as \"the epitome of determination and willpower in action.\"[4] When reviewing the novel for its 2015 re-release to coincide with the film, critic Brian Ted Jones described the novel as not fulfilling the expectation of \"the novel's higher bar\", describing it as more like Punke's non-fiction, and stylistically not very well executed.[5] Jones also described the novel's subtitle \"A Novel of Revenge\", as misrepresentative, claiming the novel's real plot device \"is actually more prosaic: He wants his stuff back.\"[5] A review of the same edition by Library Journal, called the novel \"A must-read for fans of Westerns and frontier fiction and recommended for anyone interested in stories that test the limit of how much the human body and spirit can endure.\"[6] After the novel was optioned for adaptation, publishers chose to republish it in hardback in January 2015. However, because Punke had become a Deputy United States Trade Representative and US Ambassador to the World Trade Organization, the State Department prohibited him from doing any publicity for his book in preparation for the film release (one representative noted that \"He can't even sign copies\").[1]"}
{"url": "https://en.wikipedia.org/wiki/Ordinary_People", "text": "The Jarretts are an upper-middle-class family in Lake Forest, a wealthy suburb north of Chicago. They are trying to return to normal life after experiencing the accidental death of their older teenage son, Buck, and the attempted suicide of their younger and surviving son, Conrad. Conrad has recently returned home after spending four months in a psychiatric hospital. He feels alienated from his friends and family and seeks help from a psychiatrist, Dr. Tyrone Berger, who discovers that Conrad was involved in the sailing accident that caused Buck's death. Conrad is now dealing with post-traumatic stress disorder and is seeking help to cope with his emotions. Conrad's father, Calvin, attempts to connect with his surviving son and understand his wife, while Conrad's mother, Beth, denies her loss, hoping to maintain her composure and restore her family to what it once was. She appears to have favored her older son and has grown cold toward Conrad due to his suicide attempt. Beth is determined to maintain the appearance of perfection and normality, and her efforts only serve to alienate Conrad further. Conrad works with Dr. Berger and begins to learn how to deal with his emotions rather than control them. He starts dating a fellow student, Jeannine, who helps him regain a sense of optimism. However, Conrad still struggles to communicate and establish normal relationships with his parents and schoolmates. Beth and Conrad often argue while Calvin tries to referee, generally taking Conrad's side for fear of pushing him over the edge again. Tensions escalate near Christmas when Conrad becomes furious at Beth for not wanting to take a photo with him, swearing at her in front of his grandparents. Afterwards, Beth discovers Conrad has been lying about his after-school whereabouts. This leads to a heated confrontation between Conrad and Beth in which Conrad points out that Beth never visited him in the hospital; Conrad argues that if Buck had been hospitalized in his place, she would have gone to see him, to which Beth curtly replies that Buck would never have been in the hospital in the first place. Beth and Calvin take a trip to see Beth's brother Ward in Houston, where Calvin presses Beth about her evasive attitude. Conrad suffers a setback when he learns that Karen, a friend from the psychiatric hospital, has died by suicide. A cathartic breakthrough session in the middle of the night with Dr. Berger allows Conrad to stop blaming himself for Buck's death and accept his mother's frailties. However, when Conrad tries to show affection, Beth is unresponsive, leading Calvin to emotionally confront her one last time. He questions their love and asks whether she is capable of truly loving anyone. Stunned, Beth packs her bags and goes back to Houston. Calvin and Conrad are left to come to terms with their new family situation, affirming their father-son love. Ordinary People received critical acclaim. On Rotten Tomatoes, the film has an approval rating of 89%, based on 102 reviews, with an average rating of 8.50/10. The site's critical consensus reads, \"Robert Redford proves himself a filmmaker of uncommon emotional intelligence with Ordinary People, an auspicious debut that deftly observes the fractioning of a family unit through a quartet of superb performances.\"[6] Roger Ebert gave it a full four stars and praised how the film's setting \"is seen with an understated matter-of-factness. There are no cheap shots against suburban lifestyles or affluence or mannerisms: The problems of the people in this movie aren't caused by their milieu, but grow out of themselves. ... That's what sets the film apart from the sophisticated suburban soap opera it could easily have become.\"[7] He later named it the fifth best film of the year 1980; while colleague Gene Siskel ranked it the second best film of 1980.[8] Writing for The New York Times,Vincent Canby called it \"a moving, intelligent and funny film about disasters that are commonplace to everyone except the people who experience them.\"[9] The film marked a career breakout for Mary Tyler Moore from the personalities of her other two famous roles: Laura Petrie on The Dick Van Dyke Show and Mary Richards on The Mary Tyler Moore Show. Moore's nuanced portrayal of the mother to Hutton's character was highly acclaimed, and earned her a Best Actress nomination.[10] Donald Sutherland's performance as the father was also well received and earned him a Golden Globe nomination. Despite his co-stars receiving nominations, Sutherland was overlooked for an Academy Award, which Entertainment Weekly has described as one of the biggest acting snubs in the history of the awards.[11] Judd Hirsch's portrayal of Dr. Berger was a departure from his work on the sitcom Taxi, and drew praise from many in the psychiatric community as one of the rare times their profession is shown in a positive light in film.[12] Hirsch was also nominated for Best Supporting Actor, losing out to co-star Hutton. Additionally, Ordinary People launched the career of Elizabeth McGovern who played Hutton's character's love interest, and who received special permission to film while attending Juilliard. The film's prominent usage of Pachelbel's Canon, which had been relatively obscure for centuries, helped to usher the piece into mainstream popular culture.[13] Julia L. Hall, a journalist who has written extensively about narcissistic personality disorder, wrote in 2017 upon Moore's death that she \"portrays her character's narcissism to a tee in turn after turn.\"[14] She praised Moore for taking such a career risk so soon after having played such a memorable and likable character on television, \"scaffolding gaping emptiness with a persona of perfection, supported by denial, blame, rejection, and rage.\"[14]"}
{"url": "https://en.wikipedia.org/wiki/Argo_(2012_film)", "text": "Argo received widespread critical acclaim for the acting (particularly Arkin and Goodman's), Affleck's direction, Terrio's screenplay, the editing, and Desplat's score. Commentators and participants in the actual operation criticized failures in historical accuracy. The film received seven nominations at the 85th Academy Awards and won three, for Best Picture, Best Adapted Screenplay, and Best Film Editing. Mendez contacts John Chambers, a Hollywoodmake-up artist who had previously worked for the CIA. Chambers puts Mendez in touch with film producer Lester Siegel. Together, they set up a phony film production company, publicize their plans, and successfully establish the pretense of developing Argo, a \"science fantasy adventure\" in the style of Star Wars, to lend the cover story credibility. Meanwhile, the escapees grow restless. The revolutionaries reassemble embassy photographs shredded before the takeover and realize that some personnel are unaccounted for. Posing as a producer for Argo, Mendez enters Iran under the alias Kevin Harkins and meets with the six escapees. He provides them with Canadian passports and fake identities. Although afraid to trust Mendez's scheme, they reluctantly go along, knowing that he is risking his own life too. A scouting visit to the bazaar to maintain their cover story takes a bad turn when they are harassed by a hostile shopkeeper, but their Iranian culture contact hustles them away from the hostile crowd. Mendez is told the operation has been cancelled in favor of a planned military rescue of the hostages. He pushes ahead anyway, forcing his boss, Deputy Director Jack O'Donnell to hastily re-obtain authorization for the mission and rebook their cancelled tickets on a Swissair flight. Tensions rise at the airport, where the escapees' new ticket reservations are confirmed only at the last minute, and the head guard's call to the fake production company in Hollywood is answered only at the last second. The escapees board the plane, and at about the same time, the airport authorities are alerted to the ruse. They try to stop them, but the plane is able to take off. To protect the hostages remaining in Tehran from retaliation, all U.S. involvement in the rescue is suppressed, and full credit is given to the Canadian government and its ambassador (who shuts down the embassy and leaves Iran with his wife as the operation is underway). The ambassador's Iranian housekeeper, who had known about the Americans and lied to the revolutionaries to protect them, escapes to Iraq. Mendez is awarded the Intelligence Star, but due to the mission's classified nature, he receives the medal in secret and has to return it afterward. Mendez returns to his wife and son in Virginia. The film ends by explaining what happened after the depicted events: the hostages were freed after 444 days, Mendez and Chambers remained friends until Chambers' death in 2001, Bill Clinton returned Mendez's star in 1997 after the Canadian Caper is declassified, and that he lives with his family in rural Maryland. As the credits roll, President Jimmy Carter is heard commenting on the operation. In 2007, the producers George Clooney, Grant Heslov and David Klawans set up a project based on the article. Affleck's participation was announced in February 2011.[6] The following June, Alan Arkin was the first person cast in the film.[7] After the rest of the roles were cast, filming began in Los Angeles[8] in August 2011. Additional filming took place in McLean, Virginia; Washington, D.C.; and Istanbul.[9] The scene in which Mendez drives up to and walks into the CIA headquarters lobby was filmed with permission at the CIA's original headquarters building in Virginia; all other scenes set at the CIA were filmed in the basement of the Los Angeles Times Building.[10] The screenplay used by the CIA to create their cover story was an adaptation of Roger Zelazny's 1967 novel Lord of Light. Producer Barry Geller had spearheaded an earlier attempt to produce the film using the original title. After that production attempt failed, the screenplay was renamed Argo and used by the CIA.[4][13] According to Tony Mendez, Studio Six—the phony Hollywood production office he helped create at the core of the CIA plan—proved so convincing that even weeks after the rescue was complete and the office had folded, 26 scripts were delivered to its address, including one from Steven Spielberg.[14] In April 2016, research by VICE, based on documents received under the Freedom of Information Act, revealed that the CIA's public relations arm was involved in the production of the 2012 Argo,[15][16] just as it provided information to a number of other American entertainment productions (such as the well-established case of the 2012 film Zero Dark Thirty[17]). Argo was acclaimed by critics, praising Ben Affleck's direction, the cast (especially Arkin and Goodman), the script, and the editing. On the review aggregator website Rotten Tomatoes, 96% of 364 critics' reviews are positive, with an average rating of 8.4/10. The website's consensus reads: \"Tense, exciting, and often darkly comic, Argo recreates a historical event with vivid attention to detail and finely wrought characters.\"[18]Metacritic, which uses a weighted average, assigned the film a score of 86 out of 100, based on 45 critics, indicating \"universal acclaim\".[19] Audiences polled by CinemaScore gave the film an average grade of \"A+\" on an A+ to F scale.[20] Naming Argo one of the best 11 films of 2012, critic Stephen Holden of The New York Times wrote: \"Ben Affleck's seamless direction catapults him to the forefront of Hollywood filmmakers turning out thoughtful entertainment.\"[21]The Washington Times said it felt \"like a movie from an earlier era — less frenetic, less showy, more focused on narrative than sensation,\" but that the script included \"too many characters that he doesn't quite develop.\"[22] The craft in this film is rare. It is so easy to manufacture a thriller from chases and gunfire, and so very hard to fine-tune it out of exquisite timing and a plot that's so clear to us we wonder why it isn't obvious to the Iranians. After all, who in their right mind would believe a space opera was being filmed in Iran during the hostage crisis?[23] Ebert gave the film four out of four stars, calling it \"spellbinding\" and \"surprisingly funny\", and chose it as the best film of the year, the last film he would choose for this honor before his death in 2013.[23] He also correctly predicted that it would win the Academy Award for Best Picture, following its presentation at the Toronto International Film Festival.[24] Literary critic Stanley Fish says that the film is a standard caper film in which \"some improbable task has to be pulled off by a combination of ingenuity, training, deception and luck.\" He goes on to describe the film's structure: \"(1) the presentation of the scheme to reluctant and unimaginative superiors, (2) the transformation of a ragtag bunch of ne'er-do-wells and wackos into a coherent, coordinated unit and (3) the carrying out of the task.\"[25] Although he thinks the film is good at building and sustaining suspense, he concludes, This is one of those movies that depend on your not thinking much about it; for as soon as you reflect on what's happening rather than being swept up in the narrative flow, there doesn't seem much to it aside from the skill with which suspense is maintained despite the fact that you know in advance how it's going to turn out. ... Once the deed is successfully done, there's really nothing much to say, and anything that is said seems contrived. That is the virtue of an entertainment like this; it doesn't linger in the memory and provoke afterthoughts.[25] Abolhassan Banisadr, foreign minister and then president during the incident, argued that the movie does not take into account the fact that most of the cabinet members advocated freeing all the American personnel quickly.[26]Jian Ghomeshi, a Canadian writer and radio figure of Iranian descent, thought the film had a \"deeply troubling portrayal of the Iranian people.\" Ghomeshi asserted \"among all the rave reviews, virtually no one in the mainstream media has called out [the] unbalanced depiction of an entire ethnic national group, and the broader implications of the portrait.\" He also suggested that the timing of the film was poor, as American and Iranian political relations were at a low point.[27] University of Michigan history professor Juan Cole had a similar assessment, writing that the film's narrative fails to provide adequate historical context for the events it portrays, and such errors of omission lead all of the Iranian characters in the film to be depicted as ethnic stereotypes.[28] A November 3, 2012 article in the Los Angeles Times claimed that the film had received very little attention in Tehran, though Masoumeh Ebtekar, who was the spokesperson of the students who took the hostages and called only \"Tehran Mary\" in the film's credits, said that the film did not show \"the real reasons behind the event\".[29] The film also ignores the importance of the date of the Embassy takeover. Mark Bowden, in his book on the subject, noted that November 4 was recognized as National Students' Day to acknowledge the student protesters killed by the Shah's police the year before. He also pointed out that this was the same date that the Ayatollah Khomeini had been exiled 15 years before.[30] Bootleg DVDs have become popular and are estimated at \"several hundreds of thousands\" of copies. Interpretations of the film's popularity in Iran have varied, ranging from the fact that the movie portrays the excesses of the revolution and the hostage crisis, which had been long glorified in Iran, to Iranians viewing it as a reminder of what caused the poor relations with America and the ensuing cost to Iran, decades after the embassy takeover. The high DVD sales suggests a form of silent protest against the government's ongoing hostility to relations with America.[31][32] Standing in the Golden Globe pressroom with his directing trophy, Affleck acknowledged that it was frustrating not to get an Oscar nod when many felt he deserved one. But he's keeping a sense of humor. \"I mean, I also didn't get the acting nomination,\" he pointed out. \"And no one's saying I got snubbed there!\"[37] After the film was previewed at the 2012 Toronto International Film Festival,[38] many critics said that it unfairly glorified the CIA's role and minimized the Canadian government's role (particularly that of Ambassador Taylor) in the extraction operation.[39]Maclean's asserted that \"the movie rewrites history at Canada's expense, making Hollywood and the CIA the saga's heroic saviours while Taylor is demoted to a kindly concierge.\"[40] The postscript text said that the CIA let Taylor take the credit for political purposes, which some critics thought implied that he did not deserve the accolades he received.[41] In response to this criticism, Affleck changed the postscript text to read: \"The involvement of the CIA complemented efforts of the Canadian embassy to free the six held in Tehran. To this day the story stands as an enduring model of international co-operation between governments.\"[42] The Toronto Star wrote, \"Even that hardly does Canada justice.\"[43] People welcoming the six American diplomats back to the United States and expressing thanks to Canada, 1980 In a CNN interview, former U.S. president Jimmy Carter addressed the controversy: 90% of the contributions to the ideas and the consummation of the plan was Canadian. And the movie gives almost full credit to the American CIA. And with that exception, the movie is very good. But Ben Affleck's character in the film was... only in Tehran a day and a half. And the main hero, in my opinion, was Ken Taylor, who was the Canadian ambassador who orchestrated the entire process.[44] Taylor noted, \"In reality, Canada was responsible for the six and the CIA was a junior partner. But I realize this is a movie and you have to keep the audience on the edge of their seats.\"[42] In the film, Taylor is shown as having been ordered to close down the Canadian embassy. This did not happen, and the Canadians never considered abandoning the six Americans who had taken refuge under their protection.[42] Affleck asserted: Because we say it's based on a true story, rather than this is a true story, we're allowed to take some dramatic license. There's a spirit of truth .... the kinds of things that are really important to be true are—for example, the relationship between the U.S. and Canada. The U.S. stood up collectively as a nation and said, \"We like you, we appreciate you, we respect you, and we're in your debt.\"... There were folks who didn't want to stick their necks out and the Canadians did. They said, \"We'll risk our diplomatic standing, our lives, by harboring six Americans because it's the right thing to do.\" Because of that, their lives were saved.\"[40] Upon its release in October 2012, the film was criticized for its suggestion that British and New Zealand embassies had turned away the American diplomats in Tehran. In fact both embassies, together with the Canadians, helped the Americans. The British had initially hosted the American diplomats; however, the location was deemed unsafe as the British embassy itself had been targeted and surrounded by mobs and all involved officials from the various nations believed the Canadian ambassador's residence to be a safer location. New Zealand diplomats organized a place for the diplomats to hide if they needed to change their location,[47] and drove the Americans to the airport when they made their escape from Tehran.[48] British diplomats also assisted other American hostages beyond the escaped group of six.[49] Bob Anders, the U.S. consular agent played in the film by Tate Donovan, said, \"They put their lives on the line for us. We were all at risk. I hope no one in Britain will be offended by what's said in the film. The British were good to us and we're forever grateful.\"[49] My immediate reaction on hearing about this was one of outrage. I have since simmered down, but am still very distressed that the film-makers should have got it so wrong. My concern is that the inaccurate account should not enter the mythology of the events in Tehran in November 1979.\"[49] The then-British chargé d'affaires in Tehran said that, had the Americans been discovered in the British embassy, \"I can assure you we'd all have been for the high jump.\"[49]Martin Williams, secretary to Sir John Graham in Iran at the time, was the one who found the Americans, after searching for them in his own British car (the only Austin Maxi in Iran) and first sheltered them in his own house.[citation needed] Affleck is quoted as saying to The Sunday Telegraph: \"I struggled with this long and hard, because it casts Britain and New Zealand in a way that is not totally fair. But I was setting up a situation where you needed to get a sense that these six people had nowhere else to go. It does not mean to diminish anyone.\"[49] On March 12, 2013, the New Zealand House of Representatives censured Affleck by unanimously agreeing to the following motion, initiated by New Zealand First leader Winston Peters: ... this House acknowledge[s] with gratitude the efforts of former New Zealand diplomats Chris Beeby and Richard Sewell in assisting American hostages in Tehran during the hostage crisis in 1979, and express[es] its regret that the director of the movie Argo saw fit to mislead the world about what actually happened during that crisis when, in reality, our courageous diplomats' inspirational actions were of significant help to the American hostages and deserve the factual and historical record to be corrected.[50][51] In the film, the diplomats face suspicious glances from Iranians whenever they go out in public, and appear close to being caught at many steps along the way to their freedom. In reality, the diplomats never appeared to be in imminent danger.[40][42][52] Taylor's wife bought three sets of plane tickets from three different airlines ahead of time, without any issues.[40][42] The film depicts a dramatic last-minute cancellation of the mission by the Carter administration and Mendez declaring he will proceed with the mission. Carter delayed authorization by only 30 minutes, and that was before Mendez had left Europe for Iran.[53] The film portrays a tense situation when the crew tries to board the plane, and their identities are nearly discovered. No such confrontation with security officials took place at the departure gate.[53][54] The film has a dramatic chase sequence as the plane takes off; this did not occur.[55] As Mark Lijek described it, \"Fortunately for us, there were very few Revolutionary Guards in the area. It is why we turned up for a flight at 5.30 in the morning; even they weren't zealous enough to be there that early. The truth is the immigration officers barely looked at us and we were processed out in the regular way. We got on the flight to Zurich and then we were taken to the US ambassador's residence in Bern. It was that straightforward.\"[52] The screenplay does not include the six days Bob Anders, Mark and Cora Lijek and Joe and Kathy Stafford were on the run before taking refuge with the Canadians nor where Lee Schatz was until he joined the group at John and Zena Sheardown's home. The screenplay has the escapees—Mark and Cora Lijek, Bob Anders, Lee Schatz, and Joe and Kathy Stafford—settling down to enforced cohabitation at the residence of the Canadian ambassador Ken Taylor. In reality, the group of five (Lee Schatz joined the Lijeks and Bob Anders about ten days later) was split between the Taylor house and the home of another Canadian official, John Sheardown and his wife Zena.[52][56] It was in fact the friendship between Bob Anders and John Sheardown that led Bob to call John to seek sanctuary for the group when the haven with Thai cook Sam (Somchai) began to unravel. John's response to Bob was, \"Why didn't you call sooner?\" \"It's not true we could never go outside. John Sheardown's house had an interior courtyard with a garden and we could walk there freely,\" Mark Lijek says.[52] Lester Siegel, played by Alan Arkin, is not a real person. His name and some contribution are based on Robert Sidell's while his personality is based on Jack Warner's.[57]Concept art for Lord of Light by Jack Kirby In the depiction of a frantic effort by CIA headquarters, in Langley, to get President Jimmy Carter to re-authorize the mission so that previously purchased airline tickets would still be valid, a CIA officer is portrayed as getting the White House telephone operator to connect him to Chief of Staff Hamilton Jordan by impersonating a representative of the school attended by Jordan's children. In reality, Jordan was unmarried and had no children at the time.[58] The film depicts Mendez discovering the script with the title of Argo. In reality the script was titled Lord of Light, based on the book of the same name by Roger Zelazny. The CIA changed the title to Argo.[4][59] Comic book artist Jack Kirby did not do storyboard work for the fabricated CIA film production. Kirby created concept art for a proposed production of Lord of Light a few years before the Iranian hostage situation.[4][59] The Hollywood Sign is shown dilapidated as it had been in the 1970s. The sign had been repaired in 1978, two years before the events described in the film.[60] The Swissair flight that took Mendez and the hostage group out of Tehran is shown operated with a Boeing 747-300, a type which entered service in 1983, and is painted in a livery that Swissair introduced in 1980. In real life, the group departed Iran on a Swissair Douglas DC-8 named \"Aargau\".[61][62] ^Jennifer Vineyard (October 10, 2012). \"Ben Affleck on Why He Got to Look Hot in Argo\". Vulture. Vox Media, LLC. Archived from the original on April 11, 2023. Retrieved April 11, 2023. Affleck: \"I haven't done a movie that I haven't ripped off from another one! [Laughs.] This movie, we ripped off All the President's Men, for the CIA stuff, a John Cassavetes movie called The Killing of a Chinese Bookie, which we really used as a reference for the California stuff, and then there was kind of a Battle of Algiers, Z/Missing/Costa-Gavras soup of movies, that we used for the rest of it.\" ^\"Ken Taylor, the 'main hero' of Iran hostage escape, dies at 81\". The Washington Post. October 16, 2015. Archived from the original on January 6, 2021. Retrieved January 4, 2021. In addition to shielding the Americans from Iranian capture, Taylor also played a crucial role in plotting their escape ... Taylor obtained for the Americans six Canadian passports containing forged Iranian visas that ultimately allowed them to board a flight to Switzerland. He undertook all these covert actions at a high personal risk ... Taylor's contributions were widely recognized upon the Americans' safe return in 1980. He was inducted into the Order of Canada, awarded the U.S. Congressional Gold Medal"}
{"url": "https://en.wikipedia.org/wiki/One_Flew_Over_the_Cuckoo%27s_Nest_(film)", "text": "Originally announced in 1962 with Kirk Douglas starring, the film took 13 years to develop.[5] Filming finally began in January 1975 and lasted three months, on location in Salem, Oregon and the surrounding area, and in Depoe Bay on the north Oregon coast. The producers shot the film in the Oregon State Hospital, an actual psychiatric hospital, which is also the setting of the novel. The hospital is in operation, though the original buildings in the film have been demolished. The film was released on November 19, 1975. Considered by many to be one of the greatest films ever made, One Flew Over the Cuckoo's Nest is No. 33 on the American Film Institute's 100 Years... 100 Movies list. Ratched sees McMurphy's lively, rebellious presence as a threat to her authority, which she responds to by confiscating and rationing the patients' cigarettes and suspending their card-playing privileges. McMurphy finds himself in a battle of wills against Ratched. He steals a school bus, picks up his girlfriend Candy and escapes with several patients to go fishing on the Pacific Coast. He exposes them to the outside world and encourages them to discover their own abilities and find self-confidence. After an orderly tells him that his prison sentence can become indefinite, since he has been committed, McMurphy makes plans to escape with Chief. He also learns that he, Chief, and Taber are the only non-chronic patients who have been involuntarily committed; the others have committed themselves and may leave at any time, but are too afraid to do so. After Cheswick bursts into a fit and demands his cigarettes from Ratched, McMurphy starts a fight with the orderlies and Chief intervenes to help him. McMurphy, Chief, and Cheswick are sent to the disturbed ward after the fight, and Chief reveals to McMurphy that he can speak and hear normally. He has feigned deaf-muteness to avoid engaging with anyone, remembering the way in which alcoholism destroyed his father's life. After being subjected to electroconvulsive therapy, McMurphy returns to the ward pretending to be brain-damaged, but then reveals that the treatment has made him even more determined to defeat Ratched. McMurphy and Chief make plans to escape, but decide to throw a secret Christmas party for their friends after Ratched and the orderlies leave for the night. McMurphy sneaks Candy and her friend Rose into the ward, each bringing bottles of alcohol for the party. He then bribes the night orderly Turkle to allow the party. Afterward, McMurphy and Chief prepare to escape, inviting Billy to come with them. Billy refuses, but asks for a \"date\" with Candy; McMurphy arranges for him to have sex with her. McMurphy and the others get drunk, and McMurphy falls asleep instead of making his escape with Chief. Ratched arrives in the morning to find the ward in disarray and most of the patients passed out. She discovers Billy and Candy together, and aims to embarrass Billy in front of everyone. Billy manages to overcome his stutter and stands up to Ratched. When she threatens to tell his mother, Billy cracks under the pressure and reverts to stuttering. Ratched has him placed in the doctor's office. Moments later, McMurphy punches an orderly when trying to escape out of a window with the Chief, causing the other orderlies to intervene. This attempt to escape is disrupted when Billy suddenly commits suicide by slitting his throat with broken glass. Ratched tries to control the situation by calling for the day's routine to continue as usual, but her nonchalant reaction enrages McMurphy and he strangles Ratched. The orderlies subdue McMurphy, saving Ratched's life. Some time later, Ratched is wearing a neck brace and speaking with a weak voice, and Harding leads the now-unsuspended card-playing. McMurphy is nowhere to be found, leading to rumors that he has escaped. Later that night, Chief sees McMurphy being returned to his bed. Chief greets him, elated that McMurphy had kept his promise not to escape without him, but discovers that McMurphy has been lobotomized. After tearfully hugging McMurphy, Chief smothers him to death with a pillow. He then tears a hydrotherapy console free of its floor mountings, just like when McMurphy attempted to do so earlier, and throws it through a window, and escapes as Taber and the other inmates awaken and cheer for him. In 1962, Kirk Douglas's company Joel Productions announced that it had acquired the rights to make Broadway stage and film adaptations of One Flew Over the Cuckoo's Nest with Douglas starring as McMurphy in both the play and the film, Dale Wasserman writing the stageplay, and George Roy Hill directing the film based on Wasserman's play. Jack Nicholson had also tried to buy the film rights to the novel but was outbid by Douglas.[6] Wasserman's 1963–1964 Broadway stage adaptation successfully opened, but Douglas was unable to find a studio willing to make it with him.[5] Wasserman subsequently sold his film rights to Douglas in 1970, but then delayed the film for several more years with lawsuits.[5] In 1971, Kirk Douglas's son Michael Douglas convinced his father to allow him to produce the film, as he was drawn to the novel's \"one man against the system\" plot due to his involvement with student activism at the University of California, Santa Barbara.[2] Douglas optioned the film to director Richard Rush, but he was unable to secure financing from major studios.[7][8] In March 1973, Douglas announced a new deal in which he would co-produce the film with Saul Zaentz as the first project of Fantasy Records' new film division.[2][5][7] Zaentz, a voracious reader, felt an affinity with Kesey, and so after Hauben's first attempt he asked Kesey to write the screenplay.[2] Kesey participated in the early stages of script development, but withdrew after creative differences with the producers over casting and narrative point of view; ultimately he filed suit against the production and won a settlement.[9] Although Kesey was paid for his work, his screenplay from the first-person point of view of Chief Bromden was not used. Instead, Lawrence Hauben and Bo Goldman wrote a new screenplay from a third-person perspective.[5] Hal Ashby was hired to replace Rush as director in 1973, but he was also replaced by Forman after he successfully fled to the United States. Although Douglas and Zaentz were unaware that he had been his father's first choice to direct, they began considering him after Hauben showed them Forman's 1967 Czechoslovak film The Firemen's Ball.[5][2] Douglas later said the film \"had the sort of qualities we were looking for: it took place in one enclosed situation, with a plethora of unique characters he had the ability to juggle.\"[2] Although Forman was suffering from a mental health crisis and refused to leave his Hotel Chelsea room in New York City for months, Douglas and Zaentz sent him a copy of the novel. Although Forman was not aware that the novel was the one which Douglas's father had hired him to direct in the 1960s, he quickly decided that it was \"the best material I’d come across in America\" and flew to California to discuss the film with Douglas and Zaentz further.[6] They quickly hired Forman because, in Douglas's words, \"Unlike the other directors we saw, who kept their cards close to their chest, he went through the script page by page and told us what he would do.\"[2] Forman wrote in 2012: \"To me, [the story] was not just literature, but real life, the life I lived in Czechoslovakia from my birth in 1932 until 1968. The Communist Party was my Nurse Ratched, telling me what I could and could not do; what I was or was not allowed to say; where I was and was not allowed to go; even who I was and was not\".[10] Although Kirk Douglas allowed his son to produce the film, he remained interested in playing McMurphy. However, Ashby and Forman felt Kirk Douglas was too old for the role and decided to recast him. This decision would strain relations between Kirk and Michael Douglas for many years, although Michael Douglas claimed it had not been his decision to recast him.[11][12]Gene Hackman,[13][14]James Caan,[15]Marlon Brando,[13][14] and Burt Reynolds[16] were all considered for the role of McMurphy. All four turned down the role, which ultimately went to 37-year-old Jack Nicholson on the suggestion of Ashby.[17] Nicholson had never played this type of role before. Production was delayed for about six months because of Nicholson's schedule. Douglas later stated in an interview that \"that turned out to be a great blessing: it gave us the chance to get the ensemble right\".[2] Nicholson did extensive research for the role and even met patients in a psychiatric ward to watch electroconvulsive shock therapy to prepare for the role. Danny DeVito was the first to be cast, reprising his role as the patient Martini from the 1971 off-Broadway production. Chief Bromden (who turns out to be the title character), played by Will Sampson, was referred by Mel Lambert (who portrayed the harbormaster in the fishing scene), a used car dealer Douglas met on an airplane flight when Douglas told him they wanted a \"big guy\" to play the part. Lambert's father often sold cars to Native American customers and six months later called Douglas to say: \"the biggest sonofabitch Indian came in the other day!\"[2] Jeanne Moreau, Angela Lansbury, Colleen Dewhurst, Geraldine Page, Ellen Burstyn, Anne Bancroft, and Jane Fonda all were considered to portray Nurse Ratched before Lily Tomlin was ultimately cast in the role.[5][6] However, Forman became interested in recasting Tomlin with Louise Fletcher, who had a supporting role in the film, after viewing her film Thieves Like Us (1974). A mutual acquaintance, the casting director Fred Roos, had already mentioned Fletcher's name as a possibility. Even so, it took four or five meetings, across one year, for Fletcher to secure the role of Nurse Ratched.[18][5] Her final audition was late in 1974, with Forman, Zaentz, and Douglas. The day after Christmas, her agent called to say she was expected at the Oregon State Hospital in Salem on January 4 to begin rehearsals.[19] Tomlin subsequently left the film to replace Fletcher in Nashville (1975). In 2016, Fletcher recalled that Nicholson's salary was \"enormous\", while the rest of the cast worked at or close to scale. She put in 11 weeks, grossing US$10,000 (equivalent to $57,000 in 2023).[19] Prior to commencement of filming, a week of rehearsals started on January 4, 1975, in Oregon shortly after Nicholson concluded his previous film The Fortune (1975).[5] The cast watched the patients in their daily routine and at group therapy. Jack Nicholson and Louise Fletcher also witnessed electroconvulsive therapy being performed on a patient.[2] Principal photography began on January 13, 1975, and concluded approximately three months later.[5] The film was shot on location in Salem, Oregon, the surrounding area, and the coastal town of Depoe Bay, Oregon.[5][21][22] The producers decided to shoot the film in the Oregon State Hospital, an actual mental hospital, as this is also the setting of the novel.[23] The hospital's director, Dean Brooks, was supportive of the filming and eventually ended up playing the character of Dr. John Spivey in the film. Brooks identified a patient for each of the actors to shadow, and some of the cast even slept on the wards at night. He also wanted to incorporate his patients into the crew, to which the producers agreed. Douglas recalls that it was not until later that he found out that many of them were criminally insane.[2] For the group therapy scenes, Forman and his cinematographer Haskell Wexler used three cameras to record all shots for the scene simultaneously. Although this was unusual for the time and more expensive, it allowed Forman and Wexler to capture the actors' authentic reactions to each other.[2] As Forman did not allow the actors to see the day's filming, this led to the cast losing confidence in him, while Nicholson also began to wonder about his performance. Douglas convinced Forman to show Nicholson something, which he did, and restored the actor's confidence.[2] Haskell Wexler was fired as cinematographer and replaced by Bill Butler. Wexler believed his dismissal was due to his concurrent work on the documentary Underground, in which the radical militant group the Weather Underground were being interviewed while hiding from the law. However, Forman said he had terminated Wexler's services over artistic differences. Douglas also claimed Wexler wanted to get Forman fired in order to direct the film himself, and was fueling the cast's distrust of Forman and lack of confidence in their own performances. Both Wexler and Butler received Academy Award nominations for Best Cinematography for One Flew Over the Cuckoo's Nest, though Wexler said there was \"only about a minute or two minutes in that film I didn't shoot\".[24] According to Butler, Nicholson refused to speak to Forman: \"...[Jack] never talked to Miloš at all, he only talked to me\".[25] The production went over the initial budget of $2 million and over-schedule, but Zaentz, who was personally financing the movie, was able to come up with the difference by borrowing against his company, Fantasy Records. The total production budget came to $4.4 million.[2] Critics praised the film, sometimes with reservations. Roger Ebert said: \"Miloš Forman's One Flew Over the Cuckoo's Nest is a film so good in so many of its parts that there's a temptation to forgive it when it goes wrong. But it does go wrong, insisting on making larger points than its story really should carry, so that at the end, the human qualities of the characters get lost in the significance of it all. And yet, there are those moments of brilliance.\"[28] Ebert later put the film on his \"Great Movies\" list.[29] A.D. Murphy of Variety wrote a mixed review as well,[30] as did Vincent Canby in The New York Times: \"A comedy that can't quite support its tragic conclusion, which is too schematic to be honestly moving, but it is acted with such a sense of life that one responds to its demonstration of humanity if not to its programmed metaphors.\"[31] The film opens and closes with original music by composer Jack Nitzsche, featuring an eerie bowed saw (performed by Robert Armstrong) and wine glasses. On the score, reviewer Steven McDonald: \"The edgy nature of the film extends into the score, giving it a profoundly disturbing feel at times–even when it appears to be relatively normal. The music has a tendency to always be a little off-kilter, and from time to time, it tilts completely over into a strange little world of its own\"[32] Kesey claimed never to have seen the movie, but said he disliked what he knew of it,[9] which was confirmed by Chuck Palahniuk, who wrote: \"The first time I heard this story, it was through the movie starring Jack Nicholson. A movie that Kesey once told me he disliked.\"[34] Pantera singer Phil Anselmo released a music video called \"Choosing Mental Illness\" with his band Philip H. Anselmo & The Illegals. It pays tribute to One Flew Over the Cuckoo's Nest. The music video shows scenes recreated from the film with Anselmo playing McMurphy and the rest of the band playing other characters from the film, and Nurse Ratched played by actor Michael St. Michaels.[37] Danny Devito's role in the parody is significant since he was cast in the original film as the character \"Martini\". In the episode of It's Always Sunny in Philadelphia, a reference is made to Devito's original role with a character in the parody named \"Martini\". Additionally, the 1975 film featured Will Sampson as Chief Bromden. In the parody, Tim Sampson, son of Will Sampson, plays \"Chief\" in mirroring his father's role in the film.[1]"}
{"url": "https://en.wikipedia.org/w/index.php?title=The_Revenant_(2015_film)&action=edit&section=17", "text": "By publishing changes, you agree to the Terms of Use, and you irrevocably agree to release your contribution under the CC BY-SA 4.0 License and the GFDL. You agree that a hyperlink or URL is sufficient attribution under the Creative Commons license."}
{"url": "https://web.archive.org/web/20151208182502/http://www.comingsoon.net/movies/trailers/462785-the-revenant-trailer-leonardo-dicaprio", "text": "These crawls are part of an effort to archive pages as they are created and archive the pages that they refer to. That way, as the pages that are referenced are changed or taken from the web, a link to the version that was live when the page was written will be preserved. Then the Internet Archive hopes that references to these archived pages will be put in place of a link that would be otherwise be broken, or a companion link to allow people to see what was originally intended by a page's authors. This is a collection of web page captures from links added to, or changed on, Wikipedia pages. The idea is to bring a reliability to Wikipedia outlinks so that if the pages referenced by Wikipedia articles are changed, or go away, a reader can permanently find what was originally referred to. The Revenant Trailer: Leonardo DiCaprio and Tom Hardy in the Christmas Release The Revenant trailer starring Leonardo DiCaprio and Tom Hardy 20th Century Fox has released The Revenant trailer! Opening in limited theaters on Christmas Day, the action adventure expands wide on January 8, 2016. Inspired by true events, The Revenant is an immersive and visceral cinematic experience capturing one man’s epic adventure of survival and the extraordinary power of the human spirit. In an expedition of the uncharted American wilderness, legendary explorer Hugh Glass (Leonardo DiCaprio) is brutally attacked by a bear and left for dead by members of his own hunting team. In a quest to survive, Glass endures unimaginable grief as well as the betrayal of his confidant John Fitzgerald (Tom Hardy). Guided by sheer will and the love of his family, Glass must navigate a vicious winter in a relentless pursuit to live and find redemption. Also starring Domhnall Gleeson and Will Poulter, the film is based on the novel by Michael Punke. Mark Smith and Iñárritu wrote the script for the movie, produced by Arnon Milchan, Steve Golin, David Kanter, Keith Redmon, and Iñárritu."}
{"url": "https://web.archive.org/web/20160206203132/http://www.philstar.com/entertainment/2016/01/30/1547740/dicaprios-film-revenant-stays-fierce", "text": "These crawls are part of an effort to archive pages as they are created and archive the pages that they refer to. That way, as the pages that are referenced are changed or taken from the web, a link to the version that was live when the page was written will be preserved. Then the Internet Archive hopes that references to these archived pages will be put in place of a link that would be otherwise be broken, or a companion link to allow people to see what was originally intended by a page's authors. This is a collection of web page captures from links added to, or changed on, Wikipedia pages. The idea is to bring a reliability to Wikipedia outlinks so that if the pages referenced by Wikipedia articles are changed, or go away, a reader can permanently find what was originally referred to. DiCaprio’s film Revenant stays fierce The legend of Hugh Glass, as he is regarded to be the revenant, someone who came back from the dead, is played by Leonardo DiCaprio. MANILA, Philippines – With multiple wins at the recent Golden Globes and major nominations in the upcoming 88th Academy Awards, The Revenant, starring Leonardo DiCaprio and directed by Academy Award winner Alejandro Iñárritu, is fiercely strong at the US box-office with an incredible opening (Jan. 8) of $14.4M, edging out Star Wars: The Force Awakens ($10.75M) that day. The Revenant, best seen on the big screen, is on its way to $100M with an estimated gross of $97M to date. Inspired by true events, the film is physically intense and emotionally gripping story of a man presumed to be dead but came back to life. The legend of Hugh Glass, as he is regarded to be the revenant, someone who came back from the dead, is played by DiCaprio. Part thriller, part wilderness journey, The Revenant explores primal drives not only for life itself but for dignity, justice, faith, family and home. Glass’ mythology began in 1823, when he was among thousands joining the fur trade, a driving new force in the US economy. It was a time when many saw the wild as a spiritual void that demanded to be tamed and conquered by the steeliest of men. And so they poured into the unknown, plying unmapped rivers, disappearing into impossibly lush forests, seeking not only excitement and adventure but also profits — often in fierce competition with the Native tribes for whom these lands had long been home. Many such men died anonymously, but Glass entered the annals of American folklore by flat-out refusing to die. His legend sparked after he faced one of the West’s most feared dangers: A startled grizzly bear. For even the most tested frontiersmen that should have been the end. But not for Glass. In Iñárritu’s telling of the tale, a mauled Glass clings to life — then suffers a human betrayal that fuels him to continue at any cost. In spite of tremendous loss, Glass pulls himself from an early grave — clawing his way through a gauntlet of unknown perils and unfamiliar cultures on a journey that becomes not just a search for reckoning but for redemption. As Glass moves through the frontier in turmoil, he comes to reject the urge for destruction that once drove him. He has become a “revenant” — one returned from the dead. Adds DiCaprio: “The Revenant is an incredible journey through the harshest elements of an uncharted America. It’s about the power of a man’s spirit. Hugh Glass’ story is the stuff of campfire legends, but Alejandro uses that folklore to explore what it really means to have all the chips stacked against you, what the human spirit can endure and what happens to you when you do endure.” “There are powerful themes for me in the film: The will to live and our relationship with wilderness,” explains DiCaprio of his immediate attraction to the story. “I’ve also previously played a lot of characters who were incredibly articulate in different ways and had a lot to say, so this was a unique challenge for me. It was about conveying things without words or in a different language. A lot of it was about adapting in the moment, about reacting to what nature was giving us and to what Glass was going through as we filmed. It was about exploring the most internal elements of the survival instinct.” Entertainment ( Article MRec ), pagematch: 1, sectionmatch: 1 Based in part on Michael Punke’s novel The Revenant: A Novel of Revenge, 20th Century Fox and New Regency present The Revenant also starring Tom Hardy, Domhnall Gleeson, Will Poulter, Forrest Goodluck, Paul Anderson, Kristoffer Joner, Joshua Burge and Duane Howard. Powerful epic saga unfolds on the big screen when The Revenant opens in cinemas come Feb. 3 from 20th Century Fox to be distributed by Warner Bros."}
{"url": "https://web.archive.org/web/20140904170619/http://www.thewrap.com/movies/column-post/babel-director-alejandro-gonzalez-inarritu-attached-wbs-revenant-exclusive-30191/", "text": "These crawls are part of an effort to archive pages as they are created and archive the pages that they refer to. That way, as the pages that are referenced are changed or taken from the web, a link to the version that was live when the page was written will be preserved. Then the Internet Archive hopes that references to these archived pages will be put in place of a link that would be otherwise be broken, or a companion link to allow people to see what was originally intended by a page's authors. This is a collection of web page captures from links added to, or changed on, Wikipedia pages. The idea is to bring a reliability to Wikipedia outlinks so that if the pages referenced by Wikipedia articles are changed, or go away, a reader can permanently find what was originally referred to. The director of \"Biutiful\" and \"Babel\", Inarritu gravitates toward foreboding, character-driven stories. And this qualifies. In \"The Revenant,\" a fur trapper is mauled by a bear. The captain of his ship tries to carry him back to civilization, but the terrain is impossible, and he can't do it. So the captain hires two men to stay behind and bury him after he dies. Instead of making his last days comfortable, the men rob him and leave to die alone in the cold. He survives and heads off to seek his revenge. The movie is based on Michael Punke's novel. Mark L. Smith wrote the screenplay. Akiva Goldsman is producing via his Weed Road Pictures."}
{"url": "https://web.archive.org/web/20160321014450/http://deadline.com/2016/03/the-revenant-china-box-office-opening-weekend-leonardo-dicaprio-1201723193/", "text": "‘The Revenant’ Traps Big China Bow; $30M+ Opening Weekend In Sights The official Chinese premiere of The Revenant doesn’t actually take place until Sunday, but the film has already snared big box office since bowing in local theaters on Friday. The first day’s gross came in at about $11M, and with another $13M or so on Saturday, it’s set up for what’s expected to be a $30M+ weekend. The start for the Alejandro G Inarritu-helmed frontier revenge tale is a rare feat for an R-rated Hollywood pic — not many such films secure China releases or open quite so hot. Oscar-winning star Leonardo DiCaprio, affectionately adored by locals as “Little Lee,” will attend the Beijing premiere tomorrow. The actor’s popularity in the Middle Kingdom can’t be underestimated. He’s been a huge star there since Titanic, and his pro-environmental leanings are also understood to be widely appreciated. Local social media exploded upon his Oscar win last month. The film’s local release is a win for backer New Regency. President/CEO Brad Weston tells me The Revenant’s slot in China was part of a “really important initiative… Birdman, 12 Years A Slave, Gone Girl and Noah weren’t released there, so this was a big deal for us.” The Revenant went out via Fox in the rest of the world, but New Regency held back China, and as part of a three-picture deal with local company Guangdong Alpha Animation and Culture, sold it the Middle Kingdom distribution rights in exchange for a piece of equity. Alpha then partnered with Bona Film and Huaxia to distribute. Recent Comments So i guess you guys have to adjust that number 19 profitibility sheet from a few days... phantom 21 hours DiCaprio's sheer presence turned a pricy, almost entirely silent art film into a 500M international hit. Now... Conversations to release the film in the PROC began in December, Weston says. That was well before the Oscars which resulted in three wins including for DiCaprio and Inarritu, and a hat-trick Best Cinematographer prize for Emmanuel Lubezki. A date-range of late February through early April was discussed, “and when the date came together, it came together very quickly. It was a coordinated effort between Alpha, Bona, Huaxia and IMAX.” I hear The Revenant has all the IMAX screens in China this weekend; we’ll have full figures tomorrow. It’s been reported that the movie had to go through some cuts to please the censors, but Weston says it was “in total, maybe 90 seconds, maybe less.” The post-release premiere for The Revenant in China is not unique. The Mexican red carpet also came a few days after the bow, with DiCaprio and Inarritu in attendance. That helped push mid-weeks and the following frame. The Revenant will similarly get a bump from Little Lee’s appearances tomorrow, but with Batman V Superman: Dawn Of Justice looming on the horizon it will be interesting to watch how Revenant holds next week. As of last frame’s actuals, The Revenant was at $263.9M internationally and the global total at roughly $443M. Offshore holds have been solid — last frame drops were all less than 50%, including in some places where the film was in its 10th session. China could quickly become the biggest overseas play for the film, unseating the UK. And there’s more to come with Japan, the last territory, still due to bow in April. DiCaprio is headed there this week for the film’s Wednesday premiere. DiCaprio’s sheer presence turned a pricy, almost entirely silent art film into a 500M international hit. Now that’s star power. After seeing even proven draws struggle this last year, it is nice to see at least one still on his feet proving the “star power is dead” naysayers wrong."}
{"url": "https://web.archive.org/web/20160105145312/https://www.yahoo.com/movies/leonardo-dicaprio-on-fighting-a-1281529422913590.html", "text": "Leonardo DiCaprio on Fighting a Bear in 'The Revenant' and Film vs. TV Leonardo DiCaprio says that spending nine months in the most remote and frigid regions of Canada and Argentina filming The Revenant profoundly changed him — and the actor predicts that director Alejandro González Iñárritu’s intense new film will have an equally seismic impact on movie fans. “It’s going to be one of the most unique film-going experiences that audiences have seen in modern times,” the 40-year-old Oscar nominee told Yahoo Movies on Friday about The Revenant, the gruesome true-life tale that hits theaters on December 25th. In the film, DiCaprio stars as Hugh Glass, an 1820s frontiersman who’s left for dead by his fellow travelers after a vicious bear attack. Glass wakes ups under a pile of dirt in the freezing wilderness, and after recovering, he sets out on a lonely, treacherous journey back to civilization — where he plans on enacting his revenge on those who abandoned him, especially a fellow trapper named John Fitzgerald (played by Tom Hardy). In his conversation with Yahoo, DiCaprio was not shy about hyping up the film, which could earn him his fifth Academy Award acting nomination. He also spoke about the difficulty of the shoot — which led to the departure of several crewmembers — and the future of film. Watching the trailer, I couldn’t figure out how you shot the bear attack — was it a real bear? A fake bear? CGI? Well, I’m not going to give any specifics on how we did it — you’ll figure it out on your own. But I will tell you it involved cables, it involved me flying around the forest, and it involved a tremendous amount of rehearsal. And it was pretty agonizing to do. [Those scenes] — amongst many other sequences — were some of the more difficult things I’ve ever had to do in my entire career. But the end result is going to be one of the most immersive experiences audiences will ever have with what it would be like to come face-to-face with an animal of that magnitude that is incredibly primal. It is absolutely startling and shocking, and it is the closest thing to documentary filmmaking — but in a completely stylized way that a lot of people never experienced. You spent nine months in freezing cold Canada and Argentina shooting this. Was there any point where you felt like you were going to break? The truth is that I knew what I was getting into. This was a film that had been floating around for quite some time, but nobody was crazy enough to really take this on, simply because of the logistics of where we needed to shoot and the amount of work and rehearsal that would have to be done to achieve Alejandro and [cinematographer Emmanuel Lubezki] vision. They’re very specific about their shots and what they want to achieve, and that — compounded with the fact that we were in an all-natural environment, succumbing to whatever nature gave us — was something that became more of a profoundly intense chapter of our lives than we ever thought it was going to be. It’s epic poetry, an existential journey through nature, and this man finding a will to live against all odds. Yet he changes, nature changes him and I think those elements changed him while we were doing the movie. Were there any moments where you asked yourself, “What the hell have I gotten into?” I can name 30 or 40 sequences that were some of the most difficult things I’ve ever had to do. Whether it’s going in and out of frozen rivers, or sleeping in animal carcasses, or what I ate on set. [I was] enduring freezing cold and possible hypothermia constantly. What did you have to eat during it? I certainly don’t eat raw bison liver on a regular basis. When you see the movie, you’ll see my reaction to it, because Alejandro kept it in. It says it all. It was an instinctive reaction. There were reports that it was a difficult shoot, and that people had to leave the shoot at different points. Did you ever feel that it was getting out of control, or moments that you thought the whole thing couldn’t continue? Look, what we were trying to pull off was so incredibly ambitious, and [Alejandro] is vigilant and incredibly specific about what he wants up on screen, and he won’t accept anything less. So along the way, if we had departments that didn’t agree or departments that didn’t work correctly with the situation … I wasn’t privy to exactly what went on. But to me, most of the people were there working incredibly hard, and they should be given due credit for the effort they gave this movie. Because everyone I saw was giving their heart and soul to pursuing his vision. There are fewer and fewer movies like this, as studios play it safe and go with franchises and superheroes. Do you worry about that, as both an actor and producer? I do. I put a lot of thought into that, actually, because The Revenant is a film that is so incredibly unique. It’s a very linear and simple story, but it doesn’t have all the formulaic plot points [found in] a lot of films have coming out of the studio system. What’s interesting to watch, as television becoming that much better, I think a lot of these stories are going to be going toward television. Television is better than it’s ever been in history. A lot of stories are being pushed — because of how complicated they are to make —toward Netflix and other channels on cable. But we’re going to see whether or not movies like this can survive anymore. It’s like the Last of the Mohicans — these type of movies. I don’t know if [these kinds of films] going to get financed anymore. So all I can do as a lover of film is, when I see these opportunities, to jump on them. [They’re] maybe a dying breed. Is TV something your production company, Appian Way, wants to do? Sure, we’ve got a few things in development right now, and a few things that look like they’re going to go, [but are] just a little premature to talk about. [On] television right now, things like Bloodline — these shows are incredible, and the acting is astounding and it’s on par with movies nowadays. To me, it’s going to have to make movies even that much better [in order to be] green-lit. The Revenant is coming out on December 25th — around the same time as Star Wars: The Force Awakens, as well as other holiday movies. What’s the pitch for people to see this one? There’s always room at the top. I’m going to be in line for Star Wars as well. What’s great about this time of year is, as you know, they save the best for last in most cases. And hopefully, it just means more people are excited to go to the movies. I think that people know this time of year is where a lot of great pieces of art are going to come out. I would never think that we’d be able to compete with Star Wars — that’s something that’s embedded in our cultural DNA. But I think when people begin to hear about this movie, they’ll want to go see it. And that’s all we can hope for."}
{"url": "https://en.wikipedia.org/wiki/Survival_film", "text": "The survival film is a film genre in which one or more characters make an effort at physical survival.[1] The genre focuses on characters' life-or-death struggles, often set against perilous circumstances. Survival films explore the human will to live, individual motivations, and personal desires, prompting audiences to reflect on broader aspects of humanity and personal values. They balance realism and believability with slow-burning suspense to maintain a sense of urgency. While some survival films may have epic scopes and lengthy running times, their effectiveness lies in creating an atmosphere where every moment poses a passive threat to the protagonist's existence.[2] The genre often overlaps with other film genres. It is a subgenre of the adventure film, along with swashbuckler films, war films, and safari films.[1] Survival films are darker than most other adventure films and usually focus their storyline on a single character, usually the protagonist. The films tend to be \"located primarily in a contemporary context\", so film audiences are familiar with the setting, and the characters' activities are less romanticized.[3] In a 1988 book, Thomas Sobchack compared the survival film to romance film: \"They both emphasize the heroic triumph over obstacles which threaten social order and the reaffirmation of predominant social values such as fair play and respect for merit and cooperation.\"[3] The author said survival films \"identify and isolate a microcosm of society\", such as the surviving group from the plane crash in The Flight of the Phoenix (1965) or those on the overturned ocean liner in The Poseidon Adventure (1972). Sobchack explained, \"Most of the time in a survival film is spent depicting the process whereby the group, cut off from the securities and certainties of the ordinary support networks of civilized life, forms itself into a functioning, effective unit.\" The group often varies in types of characters, sometimes to the point of caricature. While women have historically been stereotyped in such films, they \"often play a decisive role in the success or failure of the group.\"[4]"}
{"url": "https://en.wikipedia.org/wiki/Melaw_Nakehk%27o", "text": "Melaw Nakehk'o is an actress, artist, traditional moose hide tanner, and co-founder of the First Nations organization Dene Nahjo. She is primarily known for her role as the kidnapped Arikara woman Powaqa in the 2015 film The Revenant. She was at the grocery store with her two young sons one Saturday morning when there was an open casting call in Yellowknife, Northwest Territories. \"People kept texting me and telling me I should go do this thing,\" she said. When the grocery store clerk also told her to try out for the role, she went to the casting call, arriving an hour before it was over. For her audition, Melaw had to improvise a scene with other actors about trading furs for a horse. \"I didn’t have to read anything; it was all just presence and just being badass,\" she said.[2] For the red carpet premiere of the movie in Hollywood, she received media attention when she wore a dress that was a collaboration between Metis artist Christi Belcourt and Rome-based house of Valentino.[5][6] \"It was really important to be able to represent Indigenous designers at such a high level of fashion, because today there’s a lot of cultural appropriation with a lot of the huge fashion houses and people just being disrespectful wearing headdresses and Native American-inspired stuff,\" Nakehk'o stated.[7] Nakehk'o is a visual artist who paints, sews and beads, as well as a traditional moose hide tanner based out of Yellowknife, Northwest Territories. She has taught moosehide tanning at the Dechinta Bush University Center for Research and Learning. In 2012, she received a Minister's Cultural Award for \"sharing the knowledge of her elders and for bringing a renewed interest to the art of moose hide tanning.\"[8][9] She is a co-founder of Dene Nahjo, an organization that promotes leadership and social and environmental justice for indigenous people of the northern territories.[10][11]"}
{"url": "https://en.wikipedia.org/wiki/4K_Ultra_HD_Blu-ray", "text": "Ultra HD Blu-ray (4K Ultra HD, UHD-BD, or 4K Blu-ray)[2][3] is a digitaloptical discdata storage format that is an enhanced variant of Blu-ray.[4] Ultra HD Blu-ray discs are incompatible with existing standard Blu-ray players.[1] Ultra HD Blu-ray supports 4KUHD (3840 × 2160 pixel resolution) video at frame rates up to 60 progressive frames per second,[4] encoded using High-Efficiency Video Coding.[4] The discs support both high dynamic range by increasing the color depth to 10-bit per color and a greater color gamut than supported by conventional Blu-ray video by using the Rec. 2020color space. Ultra HD Blu-Ray discs also support a 12-bit per color container via Dolby Vision.[5] Dolby Vision content on 4K UHD Blu-Ray can also be mastered for 10,000 nits peak brightness, whereas standard HDR10 can only achieve a maximum of 4,000 nits of brightness.[6] Moreover, Dolby Vision makes use of dynamic metadata, which adjusts the brightness and tone mapping per scene. In contrast, standard HDR10 only makes use of static metadata, which sets the same brightness and tone mapping for the entirety of the content.[7] The first Ultra HD Blu-ray Discs were officially released in the United States on February 14, 2016.[14] To differentiate retail Ultra HD Blu-ray releases, the format usually uses a black opaque or largely transparent keep case packaging format (as opposed to blue). The case size is the same as that of a normal Blu-ray disc. The specification for 4K Blu-ray allows for three disc capacities, each with its own data rate: 50 GB at 72 or 92 Mbit/s, and 66 GB and 100 GB at 92, 123, or 144 Mbit/s. On 66 GB and 100 GB discs, the pits and lands are not narrower than those of a standard Blu-ray Disc, but shorter, which increases the capacity of each layer from 25 GB to 33 1/3 GB. This also means that each revolution of such a disc transfers more data than that of a standard Blu-ray Disc, which means the transfer rate is higher with the same linear velocity. In addition, the disc can be encoded to have the drive hold the full 5,000 rpm until it reaches a point largely away from the innermost part of the disc if an even higher transfer rate is needed. 50 and 66 GB use two layers, and 100 GB uses three layers.[16][4] Ultra HD Blu-ray technology was licensed in mid-2015, and players had an expected release date of Christmas 2015.[4] Ultra HD Blu-ray uses a new revision of AACSDRM: AACS 2. AACS 2.1 is used on certain releases such as Stand by Me, Fury, The Patriot, and Zombieland. On February 14, 2016, the BDA released Ultra HD Blu-ray with mandatory support for HDR10 Media Profile video and optional support for Dolby Vision.[19][20] In December 2017, the BDA also settled the specification for an 8K Blu-ray format for use in Japan. More than two hours of 8K UHD content can be recorded on BDXL discs (128GB – quad-layer, 100GB triple-layer).[15] On January 23, 2018, the BDA spec v3.2 gained optional support for HDR10+ and for SL-HDR2 (developed by Philips and Technicolor) also known as Advanced HDR by Technicolor.[21] However, no Ultra HD Blu-ray player has ever supported SL-HDR2, and no discs encoded in SL-HDR2 have been released. According to Technicolor, Technicolor “has developed a single-layer solution, known as Technicolor HDR (ETSI standard SL-HDR1), which ensures backwards compatibility with all non-HDR screens and non-HDR equipment. Broadcasters just need to produce a single feed, and the technology allows content to be converted into a format for both legacy screens and HDR screens.”[23][24] Most retail Ultra HD Blu-ray discs are encoded with Ateme TITAN.[25] Ultra HD Blu-ray discs use HDMV or BD-J for menus. Subtitles use Presentation Graphic Stream, which is the same format as normal Blu-ray discs. Only computers with activated Software Guard Extensions (SGX) support Ultra HD Blu-ray playback. Intel introduced SGX in the Skylake generation Core processors in 2016, enabling PCs to play protected Blu-ray discs for the first time. In January 2022, Intel deprecated support for SGX for the Rocket Lake and Alder Lake generation desktop processors, leading to Ultra HD Blu-ray discs being unplayable on those systems, even with licensed software such as PowerDVD.[26][27][28] However, on systems without SGX support, Ultra HD Blu-ray discs can be ripped using a drive with patched firmware (LibreDrive) and compatible software such as MakeMKV, DVDFab, or AnyDVD HD.[29]"}
{"url": "https://en.wikipedia.org/wiki/Innu", "text": "The Innu / Ilnu (\"man\", \"person\") or Innut / Innuat / Ilnuatsh (\"people\"), formerly called Montagnais from the French colonial period (French for \"mountain people\", English pronunciation: /ˌmɔːntənˈjeɪ/), are the Indigenous inhabitants of territory in the northeastern portion of the present-day province of Labrador and some portions of Quebec. They refer to their traditional homeland as Nitassinan (\"Our Land\", ᓂᑕᔅᓯᓇᓐ) or Innu-assi (\"Innu Land\"). The Innu are divided into several bands, with the Montagnais being the southernmost group and the Naskapi being the northernmost. Their ancestors were known to have lived on these lands as hunter-gatherers for several thousand years. To support their seasonal hunting migrations, they created portable tents made of animal skins. Their subsistence activities were historically centred on hunting and trapping caribou, moose, deer, and small game. Their language, Ilnu-Aimun or Innu-Aimun (popularly known since the French colonial era as Montagnais),[2] is spoken throughout Nitassinan, with certain dialect differences. It is part of the Cree language group, and is unrelated to the Inuit languages of other nearby peoples. The \"Innu / Ilnu\" consist of two regional tribal groups, which differ in dialect and partly also in their way of life and culture: the Ilnu, Nehilaw or \"Western/Southern Montagnais\" in the south, speak the \"l\"-dialect (Ilnu-Aimun or Nenueun/Neːhlweːuːn), and the Innu or \"Eastern Montagnais\" (\"Central/Moisie Montagnais\", \"Eastern/Lower North Shore Montagnais\", and \"Labrador/North West River Montagnais\") live further north; they speak the \"n\"-dialect (Innu-Aimun) Today, about 18,000 Innu live in eleven settlements within reserves in Quebec and Labrador. To avoid confusion with the Inuit, who belong to the Eskimo peoples, today only the singular form \"Innu / Ilnu\" is used for the Innu, members of the large Cree-language family. The plural form of \"Innut / Innuat / Ilnuatsh\" has been abandoned. The Naskapi (also known as Innu and Iyiyiw), live farther north and are less numerous. The Innu recognize several distinctions among their people (e.g. Mushuau Innuat, Maskuanu, Uashau Innuat) based on different regional affiliations and speakers of various dialects of the Innu language.Innu communities of Quebec and Labrador and the two Naskapi communities (Kawawachikamach and Natuashish) The word Naskapi was first recorded by French colonists in the 17th century. They applied it to distant Innu groups who were beyond the reach of Catholic missionary influence. It was particularly applied to those people living in the lands that bordered Ungava Bay and the northern Labrador coast, near the Inuit communities of northern Quebec and northern Labrador. Gradually it came to refer to the people known today as the Naskapi First Nation. The Naskapi are traditionally nomadic peoples, in contrast with the more sedentary Montagnais, who establish settled territories. The Mushuau Innuat (plural), while related to the Naskapi, split off from the tribe in the 1900s. They were subject to a government relocation program at Davis Inlet. Some of the families of the Naskapi Nation of Kawawachikamach have close relatives in the Cree village of Whapmagoostui, on the eastern shore of Hudson Bay. Since 1990, the Montagnais people have generally chosen to be officially referred to as the Innu, which means human being in Innu-aimun. The Naskapi have continued to use the word Naskapi. Betsiamites (Pessamu in standardized orthography, home of the Bande des Innus de Pessamit, known also as 'Pessamit Innu Band', Reserve: Betsiamites, ca. 252 km2, Population: 4,041)and the capital of the Innus The Innu were historically allied with neighbouring Atikamekw, Maliseet and Algonquin peoples against their enemies, the Algonquian-speaking Mi'kmaq and Iroquoian-speaking Five Nations of the Iroquois Confederacy (known as Haudenosaunee. During the Beaver Wars (1609-1701), the Iroquois repeatedly invaded the Innu territories from their homelands south of the Great Lakes. They took women and young males as captive slaves, and plundered their hunting grounds in search of more furs. Since these raids were made by the Iroquois with unprecedented brutality, the Innu themselves adopted the torment, torture, and cruelty of their enemies. The Naskapi, on the other hand, usually had to confront the southward advancing Inuit in the east of the peninsula.[citation needed] Roman Catholic procession of First Nations people in the Labrador peninsula Innu oral tradition describes the original encounters of the Innu and the French explorers led by Samuel de Champlain as fraught with distrust. Neither group understood the language of the other, and the Innu were concerned about the motives of the French explorers.[13] The French asked permission to settle on the Innu's coastal land, which the Innu called Uepishtikueiau. This eventually developed as Quebec City. According to oral tradition, the Innu at first declined their request. The French demonstrated their ability to farm wheat on the land and promised they would share their bounty with the Innu in the future, which the Innu accepted.[14] Two distinct versions of the oral history describe the outcome. In the first, the French used gifts of farmed food and manufactured goods to encourage the Innu to become dependent on them. Then, the French changed it to a mercantile relationship: trading these items to the Innu in exchange for furs. When the nomadic Innu went inland for the winter, the French increased the size and population of their settlement considerably, eventually completely displacing the Innu.[15] The second, and more widespread, version of the oral history describes a more immediate conflict. In this version, the Innu taught the French how to survive in their traditional lands. Once the French had learned enough to survive on their own, they began to resent the Innu. The French began to attack the Innu, who retaliated in an attempt to reclaim their ancestral territory. The Innu had a disadvantage in numbers and weaponry, and eventually began to avoid the area rather than risk further defeat. During this conflict, the French colonists took many Innu women as wives. French women did not immigrate to New France in the early period.[16] French explorer Samuel de Champlain eventually became involved in the Innu's conflict with the Iroquois, who were ranging north from their traditional territory around the Great Lakes in present-day New York and Pennsylvania. On July 29, 1609, at Ticonderoga or Crown Point, New York, (historians are not sure which of these two places), Champlain and his party encountered a group of Iroquois, likely Mohawk, who were the easternmost tribe of the Five Nations of the Iroquois Confederacy. A battle began the next day. As two hundred Iroquois advanced on Champlain's position, a native guide pointed out the three enemy chiefs to the French. According to legend, Champlain fired his arquebus and killed two of the Mohawk chiefs with one shot; one of his men shot and killed the third. The Mohawk reportedly fled the scene. Although the French also traded extensively with the Mohawk and other Iroquois, and converted some to Catholicism, they also continued to have armed conflicts with them. The Innu of Labrador and those living on the north shore of the Gulf of Saint-Lawrence in the Canadian Shield region have never officially ceded their territory to Canada by way of treaty or other agreement. But, as European-Canadians began widespread forest and mining operations at the turn of the 20th century, the Innu became increasingly settled in coastal communities and in the interior of Quebec. The Canadian and provincial governments, the Catholic, Moravian, and Anglican churches, all encouraged the Innu to settle in more permanent, majority-style communities, in the belief that their lives would improve with this adaptation. This coercive assimilation resulted in the Innue giving up some traditional activities (hunting, trapping, fishing). Because of these social disruptions and the systemic disadvantages faced by Indigenous peoples, community life in the permanent settlements often became associated with high levels of substance abuse, domestic violence, and suicide among the Innu. In 1999, Survival International published a study of the Innu communities of Labrador. It assessed the adverse effects of the Canadian government's relocating the people far from their ancestral lands and preventing them from practising their ancient way of life.[17] The Innu people of Labrador formally organized the Naskapi Montagnais Innu Association in 1976 to protect their rights, lands, and way of life against industrialization and other outside forces. The organization changed its name to the Innu Nation in 1990 and functions today as the governing body of the Labrador Innu. The group has won recognition for its members as status Indians under Canada's Indian Act in 2002 and is currently involved in land claim and self-governance negotiations with the federal and provincial governments.[4] In addition to the Innu Nation, residents at both Natuashish and Sheshatshiu elect Band Councils to represent community concerns. The chiefs of both councils sit on the Innu Nation's board of directors and the three groups work in cooperation with one another. The Innu Nation's efforts to raise awareness about the environmental impacts of a mining project in Voisey's Bay were documented in Marjorie Beaucage's 1997 film Ntapueu ... i am telling the truth.[18]: 342 In 1999, Survival International published a study of the Innu communities of Labrador. It assessed the adverse effects of the Canadian government's relocating the people far from their ancestral lands and preventing them from practising their ancient way of life.[17] Survival International concluded that these policies violated contemporary international law in human rights, and drew parallels with the treatment of Tibetans by the People's Republic of China. According to the study, from 1990 to 1997, the Innu community of Davis Inlet had a suicide rate more than twelve times the Canadian average, and well over three times the rate often observed in isolated northern villages.[17] By 2000, the Innu island community of Davis Inlet asked the Canadian government to assist with a local addiction public health crisis. At their request, the community was relocated to a nearby mainland site, now known as Natuashish. At the same time, the Canadian government created the Natuashish and Sheshatshiu band councils under the Indian Act. The Naskapi Nation of Kawawachikamach, of Quebec, signed a comprehensive land claims settlement, the Northeastern Quebec Agreement; they did so in 1978. As a consequence, the Naskapi of Kawawachikamach are no longer subject to certain provisions of the Indian Act. All the Innu communities of Quebec are still subject to the Act. The New York Power Authority's proposed contract in 2009 with the province of Quebec to buy power from its extensive hydroelectric dam facilities has generated controversy, because it was dependent on construction of a new dam complex and transmission lines that would have interfered with the traditional ways of the Innu.[19] According to the Sierra Club: [t]he \"New York Power Authority is in preliminary discussions and considering the liability of a new contract with Hydro Quebec,\" a Canadian supplier of hydroelectricity. Chief Georges-Ernest Grégoire of the Innu community in Eastern Quebec urged the governor not to proceed with a plan to buy hydroelectric power from Canada, saying the dam complex that would be built would affect the traditional way of life for his people. Although Innu have only been in Sheshatshiu since fur trading posts were established by the Hudson's Bay Company in Northwest River in the mid-1700s and only in Davis Inlet/Natuashish since the Moravians set up along the Inuit Coast in 1771, Newfoundland and Labrador Premier Danny Williams struck a deal on September 26, 2008 with Labrador's Innu to permit construction of a hydroelectricmegaproject to proceed on the proposed Lower Churchill site. They also negotiated compensation for another project on the Upper Churchill, where large tracts of traditional Innu hunting lands were flooded. Traditional Innu craft is demonstrated in the Innu tea doll. These children's toys originally served a dual purpose for nomadic Innu tribes. When travelling vast distances over challenging terrain, the people left nothing behind. They believed that \"Crow\" would take it away. Everyone, including young children, helped to transport essential goods. Innu women made intricate dolls from caribou hides and scraps of cloth. They filled the dolls with tea and gave them to young girls to carry on long journeys. The girls could play with the dolls while also carrying important goods. Every able-bodied person carried something. Men generally carried the heavier bags and women would carry young children. Men wore caribou pants and boots with a buckskin long shirt, all made by women. With the introduction of trade cloth from the French and English, people began replacing the buckskin shirts with ones made of cloth. Most still wore boots and pants made from caribou hide. Women wore long dresses of buckskin. Contemporary Innu women have often replaced these with manufactured pants and jackets. Women traditionally wore their hair long or in two coils. Men wore theirs long. Both genders wore necklaces made of bone and bead. Smoke pipes were used by both genders, marked for women as shorter. If a man killed a bear, it was a sign of joy and initiation into adulthood and the man would wear a necklace made from the bear's claws. Plants traditionally eaten included raspberries, blueberries, strawberries, cherries, wild grapes, hazelnuts, crab apples, red martagon bulbs, Indian potato, and maple-tree sap for sweetening. Cornmeal was traded with Iroquois, Algonquin, and Abenaki First Nations peoples, and made into apon (cornbread), which sometimes also included oat or wheat flour when it became available. Pine-needle tea kept away infections and colds resulting from the harsh weather. Traditionally, buckskin was a most important material used for clothing, boots, moccasins, house covers and storage. Women prepared the hides and many of the products made from it. They scraped the hides to remove all fur, then left them outside to freeze. The next step was to stretch the hide on a frame. They rubbed it with a mixture of animal brain and pine needle tea to soften it. The dampened hide was formed into a ball and left overnight. In the morning, it would be stretched again, then placed over a smoker to smoke and tan it. The hide was left overnight. The finished hide was called buckskin. In traditional Innu communities, people walked or used snow shoes. While people still walk and use snow shoes where necessary for hunting or trapping, many Innu communities rely heavily on trucks, SUVs, and cars; in Northern Innu communities, people use snowmobiles for hunting and general transportation. ^Vincent, Sylvie (2006). \"The Uepishtikueiau Narrative: The Arrival of the French at the site of Québec City according to Innu Oral Tradition\". In Christie, Gordon (ed.). Aboriginality and Governance: A Multidisciplinary Approach. Penticton Indian Reserve, British Columbia: Theytus Books. pp. 7–9. ISBN1894778243."}
{"url": "https://en.wikipedia.org/wiki/Eastern_Shoshone", "text": "The Eastern Shoshone adopted horses much sooner than their neighbours to the North, the Blackfoot Confederacy (made up of three related groups, the Piegan, Siksika, and Kainai). With the advantages that horses provided in battle, such as speed and mobility, the Eastern Shoshone were able to expand to the north and soon occupied much of present-day southern and central Alberta, most of Montana, and parts of Wyoming, and raided the Blackfoot frequently. Meanwhile, their close cousins, the Comanche, split off and migrated south to present-day western Texas. Once the Piegan, in particular, had access to horses of their own and guns obtained from the Hudson's Bay Company via the Cree and Assiniboine, the situation changed. By 1787 David Thompson reports that the Blackfoot had completely conquered most of Shoshone territory, and frequently captured Shoshone women and children and forcibly assimilated them into Blackfoot society, further increasing their advantages over the Shoshone. Thompson reports that Blackfoot territory in 1787 was from the North Saskatchewan River in the north to the Missouri River in the South, and from Rocky Mountains in the west out to a distance of 300 miles (480 km) to the east.[5] Through the early 1800s, the Eastern Shoshone and Crow fought over the contested Wind River Basin, a prime bison hunting area, culminating in an incident at Crow Heart Butte, where Washakie challenged and defeated a leading Crow warrior for possession of the Wind River Valley. The Eastern Shoshone participated significantly in the Rocky Mountain Fur Trade and bison hide trade from the 1820s and 1840s. The rendezvous sites along the Wind River Range were established in areas previously used by the Shoshone for trade fairs.[6] By the 1850s, Washakie had emerged as a leader among the Shoshone, known for his war prowess as well as his ability to negotiate with whites. Fluent in English and a friend and father-in-law of Jim Bridger, Washakie championed the establishment of the Wind River Indian Reservation through negotiations at the 1863 and 1868 treaties at Fort Bridger.[7] After the reservation period, the Eastern Shoshone saw the arrival of Northern Arapaho on the Wind River Indian Reservation in 1878.[8] Later negotiations reduced the size of the reservation[9][10] and resulted in settlement of lands within the Wind River Reclamation Project. In 1938 the Eastern Shoshone won the case United States vs. Shoshone Tribe of Indians,[11] securing rights to timber and mineral resources on the reservation reserved to them under the Fort Bridger Treaties. This lawsuit argued by George Tunison ruled that the Shoshone were owed payment for the location of the Northern Arapaho to the Wind River Indian Reservation.[12] In the 1970s, Eastern Shoshone tribal members uncovered that oil field workers on the reservation were stealing oil without paying royalties, a scandal that led to reforms.[13] Haivodika or Haiwodekanee (Dove Eaters, so named by their Kuccuntikka kin, because they allegedly behaved timidly on buffalo hunts, also called Blacks Fork Indians, about 1825 they broke off from the main body of Kuccuntikka to live nearer and with white settlements and trading posts lived the greater part of the year along the creeks of Green River in the Bridger Basin in western Wyoming and particular at Henrys Fork in southeastern Idaho, they served as go-betweens between the nomadic Eastern and Northern Shoshone bands and Utes, Flathead, Nez Perce, and occasionally Crow Indians and the whites at the trading post Fort Bridger; they bought skins from the Plains Indians and sold them at the Fort and distributed the white Traders' goods among the Ute and Navajo. It is even known that they went to the Mormons at Great Salt Lake and exchanged skins for agricultural products and textiles, with the end of the Fur Trade and the bison hunting the Haivodika lost their social function and their identity as a separate Eastern Shoshone band, they chose to live with their Mixed-blood relatives in the surrounding white settlements or their Kuccuntikka kin on the Wind River Reservation)"}
{"url": "https://en.wikipedia.org/wiki/Chetco_people", "text": "The Chetco (Chetco: chit-dee-ni, chit-dee-ne or Chit-dv-ne' [1]) are a tribe of Native Americans who originally lived along the lower Chetco River and Winchuck River in Curry County in the U.S. state of Oregon. The name Chetco comes from the word meaning \"close to the mouth of the Chetco River\" in their own language, which is part of the Athapascan languages.[2] Although they were once one of the largest tribes on the Pacific coast of Oregon, \"the last known full-blooded Chetco\" living on the Chetco River (Lucy Dick) died in 1940. Many of Lucy Dick’s and Amelia Van Pelt’s (Chetco/Tututni) descendants continue to live in the Chetco region, and are members of the Confederated Tribes of Siletz Indians.[3] the Chetco people formerly lived in 9 associated villages, but after the Rogue River Wars in 1856, were removed to the Siletz Reservation and became part of the Confederated Tribes of Siletz Indians, living in one large village there. All of the Oregon Coast Tribes were removed to and confederated on the Siletz Reservation. Some Chetco descendants are enrolled in other federally recognized tribes, Cher-Ae Heights Indian Community of the Trinidad Rancheria, located in Humboldt County, California.[4] The Chetco language is a member of the Athapascan languages, which also includes most native languages in Alaska, the Apache and Navajo languages in the southwest United States, and the languages spoken by the Rogue River and Tolowa tribes in Oregon. The name \"Chetco\" comes from the word Chit-taa-ghii-li (or Chit) in their own language, meaning \"close to the mouth of the Chetco River\".[2] The nine villages of the tribe on the Chit-see-ghii-li (Chetco River) were named Chettanne, Chettannene (twin villages at both sides of the mouth of the river), Khuniliikhwut (south side of Chetco River), Nakwutthume (Chetco River above all the other villages), Nukhwuchutun (Nukhsuchutun) (south side of Chetco River), Setthatun (south side of Chetco River), Siskhaslitun (south side of Chetco River), Tachukhaslitun (south side of Chetco River), and Thlcharghilitun (on the upper course of a south branch of Chetco River).[2] The endings \"anne/nene/t̟ûn-nĕ\" mean \"inhabitants of a place/village\" or \"people\"; a place/village site is designated as \"dun\". Other village names are also mentioned (sometimes they are identical - only in a modern transcription): Chit-dvn (\"Chetco River Village\"), Duu-srxuu-shi'n (\"Winchuck River village\"), Lhch'aa-ghii~-lii~-dvn (\"Chetco River Forks Village\"), Sri'-ch'as-lii~-dvn (\"Village upriver from North Fork of the Chetco River\"), T'uu-k'wvt (\"Village near Gold Beach\"), Yaa~-shuu-chit-yan'-ne (\"Village at Gold Beach\") Map of southwest Oregon and northern California, showing the traditional homeland of the Chetco The Chetco are believed to have come to coastal Oregon between 3000 and 1000 years ago.[5] They had nine villages on the lower 14 miles (23 km) of the Chetco River, with their principal villages at the mouth, where the river flows into the Pacific Ocean.[2] The Chetco territory extended a short distance on either side of the river, along the Pacific coast from Pistol River in the north to the Winchuck River in the south.[3] They were the most populous of the 12 coastal tribes in southern Oregon.[5] The Chetco were hunter-gatherers with a diet based on hunting deer and elk, gathering acorns and mussels, and fishing. They used dugout canoes on the ocean and river and worked with stone tools. The Chetco cooked on open fires or with simple pots, and were culturally very similar to the Tolowa tribe to the south, \"who shared the same customs regulating social relationships and frequently intermarried\".[6] The tribe is thought to have had perhaps one thousand members at its peak,[3] but its numbers declined after European-American settlers came into contact with the Chetco in the 19th century.[6] Settlers destroyed the Chetco villages in 1853 and the surviving members of the tribe were forcibly removed to the Siletz Reservation in Tillamook County, Oregon (in 1879 the land the reservation is on became part of Lincoln County). In 1854 there were 241 members of the tribe on the reservation: 83 women, 117 men, and 41 children. By 1861 there were 262 on the reservation: 96 women, 62 men, and 104 children; by 1871 the total on the reservation had dropped to 63.[2] Lucy Dick, who died in 1940 was \"the last known full-blooded Chetco\"; as of 2009.[3]"}
{"url": "https://en.wikipedia.org/wiki/Harrying_of_the_North", "text": "Contemporary chronicles vividly record the savagery of the campaign, the huge scale of the destruction and the widespread famine caused by looting, burning and slaughtering. Some present-day scholars have labelled the campaigns a genocide, although others doubt whether William could have assembled enough troops to inflict so much damage and have suggested that the records may have been exaggerated or misinterpreted. Records from the Domesday Book of 1086 suggest that as much as 75% of the population could have died or never returned. Background At the time of the Norman Conquestthe North consisted of what became Yorkshire, Durham, and Northumberland in the east and Lancashire with the southern parts of Cumberland and Westmorland in the west.[2] The population of the north pre-conquest can be described as \"Anglo-Scandinavian\" carrying a cultural continuity from a mixing of Viking and Anglo-Saxon traditions. The dialect of English spoken in Yorkshire may well have been different to people from the south of England, and the aristocracy south of the Tees (an area roughly analogous to modern day Yorkshire) was partly Danish in origin.[3] At the time of the Norman conquest, the counties north of Yorkshire had not been conquered. Yorkshire in 1086 was larger than it is now.[a] Further, communications between the north and south were difficult, partly due to the terrain but also because of the poor state of the roads. The more popular route between York and the south was by ship.[5] In 962 Edgar the Peaceful had granted legal autonomy to the Northern earls of the Danelaw in return for their loyalty; this had limited the powers of the Anglo-Saxon kings who succeeded him north of the Humber. The Anglo-Saxon earldom of Northumbria bordering the Danelaw stretched from the Tees to the Tweed[3] Copsi, a supporter of Tostig (a previous Anglo-Saxon earl of Northumbria who had been banished by Edward the Confessor), was a native of Northumbria and his family had a history of being rulers of Bernicia, and at times Northumbria. Copsi had fought in Harald Hardrada's army with Tostig, against Harold Godwinson at the Battle of Stamford Bridge in 1066. He had managed to escape after Harald's defeat. When Copsi offered homage to William at Barking in 1067, William rewarded him by making him earl of Northumbria.[9] After just five weeks as earl, Copsi was murdered by Osulf, son of Earl Eadulf III of Bernicia. When, in turn, the usurping Osulf was also killed, his cousin, Cospatrick, bought the earldom from William. He was not long in power before he joined Edgar Ætheling in rebellion against William in 1068.[9] With two earls murdered and one changing sides, William decided to intervene personally in Northumbria.[8] He marched north and arrived in York during the summer of 1068. The opposition melted away, with some of them – including Edgar – taking refuge at the court of the Scottish king Malcolm III.[10] Back in Northumbria, William changed tactics and appointed a Norman, Robert de Comines, as earl, rather than an Anglo-Saxon. Despite warnings from the bishop, Ethelwin, that a rebel army was mobilised against him, Robert rode into Durham with a party of men on 28 January 1069,[11] where he and his men were surrounded and slaughtered[8][12] The rebels then turned their attention to York where they killed the guardian of the castle there plus a large number of his men[8][12] William's response was swift and brutal: he returned to York, where he fell on the besiegers, killing or putting them to flight.[13] Possibly emboldened by the fighting in the north, rebellions broke out in other parts of the country. William sent earls to deal with problems in Dorset, Shrewsbury and Devon, while he dealt with rebels in the Midlands and Stafford.[6] Edgar Ætheling had sought assistance from the king of Denmark, Sweyn II, a nephew of King Canute. Sweyn assembled a fleet of ships under the command of his sons. The fleet sailed up the east coast of England raiding as they went. The Danes with their English allies retook the city of York.[14] Then, in the winter of 1069, William marched his army from Nottingham to York with the intention of engaging the rebel army. However, by the time William's army had reached York, the rebel army had fled, with Edgar returning to Scotland. As they had nowhere suitable on land to stay for the winter, the Danes decided to go back to their ships in the Humber Estuary. After negotiation with William, it was agreed that, if he made payment to them, then they would go home to Denmark without a fight.[15] With the Danes having returned home, William then turned to the rebels. As they were not prepared to meet his army in pitched battle, he employed a strategy that would attack the rebel army's sources of support and their food supply.[16] The Harrying William's strategy, implemented during the winter of 1069–70 (he spent Christmas 1069 in York), has been described by William E. Kapelle and some other modern scholars as an act of genocide.[17][18][c] Contemporary biographers of William considered it to be his cruellest act and a \"stain upon his soul\".[19] Writing about the Harrying of the North, over fifty years later, the Anglo-Norman chronicler Orderic Vitalis wrote (paraphrased): The King stopped at nothing to hunt his enemies. He cut down many people and destroyed homes and land. Nowhere else had he shown such cruelty. This made a real change. To his shame, William made no effort to control his fury, punishing the innocent with the guilty. He ordered that crops and herds, tools and food be burned to ashes. More than 100,000 people perished of starvation. I have often praised William in this book, but I can say nothing good about this brutal slaughter. God will punish him. The land was ravaged on either side of William's route north from the River Aire. His army destroyed crops and settlements and forced rebels into hiding. In the New Year of 1070 he split his army into smaller units and sent them out to burn, loot, and terrify. From the Humber to the Tees, William's men burnt whole villages and slaughtered the inhabitants. Food stores and livestock were destroyed so that anyone surviving the initial massacre would succumb to starvation over the winter.[20] Florence of Worcester writing in the 12th century said that: [King William] assembled an army, and hastened into Northumbria, giving way to his resentment; and spent the whole winter in laying waste the country, slaughtering the inhabitants, and inflicting every sort of evil, without cessation. ... so great a famine prevailed that men, compelled by hunger, devoured human flesh, that of horses, dogs, and cats, and whatever custom abhors; others sold themselves to perpetual slavery, so that they might in any way preserve their wretched existence. In 1086, Yorkshire still had large areas of waste territory. The Domesday Book entries indicate wasteas est or hoc est vast (\"it is wasted\") for estate after estate; in all a total of 60% of all holdings were waste. It states that 66% of all villages contained wasted manors. Even the prosperous areas of the county had lost 60% of its value compared to 1066. Only 25% of the population and plough teams remained with a reported loss of 80,000 oxen and 150,000 people.[25][26] The Domesday Book recorded a drastic decline in land values between 1066 and 1086, for Yorkshire, and between 1086 and the 12th century there was a corresponding drop in the value of the land for tax purposes.[27] The drop in value of Yorkshire estates between 1066 and 1086 according to the Domesday Book Independent archaeological evidence supports the massive destruction and displacement of people. The archaeologist Richard Ernest Muir wrote that there was evidence for the \"violent disruption [that] took place in Yorkshire in 1069–71, in the form of hoards of coins which were buried by the inhabitants.\"[25] B. K. Roberts in his book The Making of the English Village, suggests the reason that large numbers of villages have been laid out in regular pattern in Durham and Yorkshire was through a restructuring at a single point in time, as opposed to natural settlement growth. He goes on to say that it is highly unlikely that such plans could have resulted from piecemeal additions and must have been necessary after the Harrying of the North. The dating is thought to be secure as it is known that Norman lords used similar regular plans in founding new towns in the 'plantation' of rural settlements in other conquered parts of the British Isles.[29][30][31] However, although the Domesday Book records large numbers of manors in the north as waste, some historians have posited it was not possible for William's relatively small army to be responsible for such wide-scale devastation imputed to him, so perhaps raiding Danes[e] or Scots[f] may have contributed to some of the destruction. It has been variously argued that waste signified manorial re-organisation, some form of tax break, or merely a confession of ignorance by the Domesday commissioners when unable to determine details of population and other manorial resources.[35][27] According to Paul Dalton,[27] it was questionable whether the Conqueror had the time, manpower or good weather necessary to reduce the north to a desert. It was evident, from the chroniclers, that William did harry the north but as the bulk of William's troops, Dalton suggests, were guarding castles in southern England and Wales, and as William was only in the north for a maximum of three months, the amount of damage he could do was limited.[27] Mark Hagger[34] suggests that in the words of the Anglo-Saxon Chronicle, William's Harrying of the North was \"stern beyond measure\"[36] but should not be described as genocide as William was acting by the rules of his own time, not the present.[c][34]Vegetius, the Latin writer, wrote his treatise De Re Militari in the fourth century about Roman warfare, and Hagger posits that this still would have provided the basis for military thinking in the eleventh century.[34] Vegetius said, \"The main and principal point in war is to secure plenty of provisions and to destroy the enemy by famine\", so Hagger's conclusion is that the Harrying of the North was no worse than other similar conflicts of the time.[34][37] Other historians have questioned the figures supplied by Orderic Vitalis, who was born in 1075 and would have been writing Ecclesiastical History around 55 years after the event. The figure of 100,000 deaths was perhaps used in a rhetorical sense, as the estimated population for the whole of England, based on the 1086 Domesday returns, was about 2.25 million; thus, a figure of 100,000 represented ca. 4.5% of the entire population of the country at that time.[21][25][38] David Horspool concludes that despite the Harrying of the North being regarded with some \"shock\" in Northern England for some centuries after the event, the destruction may have been exaggerated and the number of dead not as high as previously thought.[21] Having effectively subdued the population, William carried out a complete replacement of Anglo-Saxon leaders with Norman ones in the North. The new aristocracy in England was predominantly of Norman extraction; however, one exception was that of Alan Rufus, a trusted Breton lord, who obtained in 1069–1071 a substantial fiefdom in North Yorkshire, which the Domesday Book calls \"the Hundred of the Land of Count Alan\", later known as Richmondshire.[42][43] Here Alan governed, as it were, his own principality: the only location held by the King in this area was Ainderby Steeple on its eastern edge, while Robert of Mortain[44] held one village on its southern fringe; the other Norman lords were excluded, whereas Alan retained the surviving Anglo-Danish lords or their heirs. Alan also exercised patronage in York, where he founded St Mary's Abbey in 1088. By 1086 Alan was one of the richest and most powerful men in England.[45] In Scotland, Malcolm married the Ætheling's sister, Margaret, in 1071.[10] Edgar sought Malcolm's assistance in his struggle against William.[8] The marriage of Malcolm to Edgar's sister profoundly affected the history of both England and Scotland. The influence of Margaret and her sons brought about the Anglicisation of the Lowlands and provided the Scottish king with an excuse for forays into England, which he could claim were to redress the wrongs against his brother-in-law.[46] The formal link between the royal house of Scotland and Wessex was a threat to William, who marched up to Scotland in 1072 to confront the Scottish king. The two kings negotiated the Treaty of Abernethy (1072), through which, according to the Anglo Saxon Chronicle, Malcolm became William's vassal; among the other provisions was the expulsion of Edgar Ætheling from the Scottish court.[47][48] Edgar finally submitted to William in 1074. William's hold on the crown was then theoretically uncontested.[49] In 1080 Walcher, the Bishop of Durham, was murdered by the local Northumbrians. In response, William sent his half-brother Odo, Earl of Kent[g] north with an army to harry the Northumbrian countryside. Odo destroyed much land north of the Tees, from York to Durham, and stole valuable items from Durham monastery, including a rare sapphire-encrusted crozier. Many of the Northumbrian nobility were driven into exile.[50] Richmond Castle from across the River Swale After the conquest of 1066 the Normans used the church as an agent of colonisation with most of the wealthy churches in England passing into the hands of clerics from north west France. There had been no monasteries north of Burton upon Trent in 1066 but post harrying several monasteries were built including Fountains Abbey which became one of the largest and richest. With concern that Yorkshire could be attacked or invaded by the Scots or Vikings coupled with the threat of further revolts, the Normans reorganised the defences in the area and installed men chosen for their abilities to hold on to whatever they got. They increased the number of motte-and-bailey castles they built and in 1071 work commenced above the River Swale, to build a castle at a site now known as 'Richmond'. The name 'Richmond' is derived from the Norman French meaning 'strong hill'. The Honour of Richmond, controlled by Alan Rufus, served to defend the routes out of Scotland into the Vale of York. The central Vale of York was protected by the castles of Pontefract, Wakefield, Conisbrough and Tickhill. While the Lordship of Holderness was reorganised to protect against invaders from the North sea.[51][52][53] As a result of the depopulation, Norman landowners sought settlers to work in the fields. Evidence suggests that such barons were willing to rent lands to any men not obviously disloyal. Unlike the Vikings in the centuries before, Normans did not settle wholesale in the shire, but only occupied the upper ranks of society. This allowed an Anglo-Scandinavian culture to survive beneath Norman rule. Evidence for continuity can be seen in the retention of many cultural traits. Many personal names of a pre-conquest character appear in charters that date from the 11th to the 13th century. The vigorous Northern literary tradition in the Middle English period and its distinctive dialect also suggest the survival of an Anglo-Scandinavian population. The relative scarcity of Norman place-names implies that the new settlers came in only at the top rank. Domesday Book shows that at this level, however, Norman takeover in Yorkshire was virtually complete.[54] From the Norman point of view, the Harrying of the North was a successful strategy, as large areas, including Cheshire, Shropshire, Derbyshire and Staffordshire were devastated, and the Domesday Book confirms this, although in those counties it was not as complete as in Yorkshire. The object of the harrying was to prevent further revolts in Mercia and Northumbria; however, it did not prevent rebellions elsewhere.[55] See also Notes ^ abBefore 1086 the area described as Eurvivscrire (Yorkshire) in the Domesday book contained Amounderness, Cartmel, Furness, Kendall , parts of Copeland, Lonsdale and Cravenshire (modern Lancashire north of the Ribble and parts of Cumberland and Westmorland)[1] ^The area north of Yorkshire was not conquered by William I; it was his successor, William Rufus who took control of what is now Cumbria, Cumberland and Westmorland in 1092.[4] ^ abFor a modern definition of Genocide and an opinion on whether the Harrying of the North would class as genocide see Moses 2008, pp. 5, 28 ^For an analysis of the medieval chroniclers' view of the Harrying of the North, see S. J. Speights, \"Violence and the creation of socio-political order in post conquest Yorkshire\", in Halsalls. Violence and Society in the Early Medieval West (Chapter 8) ^According to Florence of Worcester, \"[William] sent messengers to the Danish earl and promised to pay him secretly a large sum of money, and to grant permission for the Danish army to forage freely along the sea coast, on condition that the Danes would depart without fighting when the winter was over.\"[32] ^Symeon of Durham said that the Scots under Malcolm III \"made sad havoc in the province of Northumbria; and to convey from thence very many men and women captive to Scotland.\"[33] He also described how the Scots ran through old people with their pikes and hurled babies into the air and caught them on the points of their lances.[34] The Anglo-Saxon Chronicle MS E for 1079 says that Malcolm \"ravaged Northumberland as far as the Tyne and killed many hundreds of people and took home much money and people into captivity\". ^Odo, Earl of Kent was also Bishop of Bayeaux. His personal seal depicts him as bishop on one face carrying a crozier. On the other face he is depicted on horseback with a sword and shield. Under church rules he was not supposed to be armed however it seems that he was not adverse to flouting church regulations. He was one of Williams most ruthless commanders who had a reputation for amassing wealth and power. His unfettered ambition brought him into conflict with the Archbishop of Canterbury in 1076 when he faced the Trial of Penenden Heath and finally with William himself in 1082. Odo was tried on a variety of crimes, he was eventually found guilty of treason and imprisoned.[50]"}
{"url": "https://en.wikipedia.org/wiki/Classification_of_indigenous_peoples_of_the_Americas", "text": "The classification of the Indigenous peoples of the Americas is based upon cultural regions, geography, and linguistics. Anthropologists have named various cultural regions, with fluid boundaries, that are generally agreed upon with some variation. These cultural regions are broadly based upon the locations of the Indigenous peoples of the Americas from early European and African contact beginning in the late 15th century. When Indigenous peoples have been forcibly removed by nation-states, they retain their original geographic classification. Some groups span multiple cultural regions. In the United States and Canada, ethnographers commonly classify Indigenous peoples into ten geographical regions with shared cultural traits, called cultural areas.[1]Greenland is part of the Arctic region. Some scholars combine the Plateau and Great Basin regions into the Intermontane West, some separate Prairie peoples from Great Plains peoples, while some separate Great Lakes tribes from the Northeastern Woodlands. Nota bene: The California cultural area does not exactly conform to the state of California's boundaries, and many tribes on the eastern border with Nevada are classified as Great Basin tribes and some tribes on the Oregon border are classified as Plateau tribes.[57] The Colombia and Venezuela culture area includes most of Colombia and Venezuela. Southern Colombia is in the Andean culture area, as are some peoples of central and northeastern Colombia, who are surrounded by peoples of the Colombia and Venezuela culture. Eastern Venezuela is in the Guianas culture area, and southeastern Colombia and southwestern Venezuela are in the Amazonia culture area.[66] ^ abWendy Tymchuk, Senior Technical Editor (2008). \"Learn about Y-DNA Haplogroup Q\". Genebase Systems. Archived from the original(Verbal tutorial possible) on 2010-06-22. Retrieved 2009-11-21. Haplogroups are defined by unique mutation events such as single nucleotide polymorphisms, or SNPs. These SNPs mark the branch of a haplogroup, and indicate that all descendants of that haplogroup at one time shared a common ancestor. The Y-DNA SNP mutations were passed from father to son over thousands of years. Over time, additional SNPs occur within a haplogroup, leading to new lineages. These new lineages are considered subclades of the haplogroup. Each time a new mutation occurs, there is a new branch in the haplogroup, and therefore a new subclade. Haplogroup Q, possibly the youngest of the 20 Y-chromosome haplogroups, originated with the SNP mutation M242 in a man from Haplogroup P that likely lived in Siberia approximately 15,000 to 20,000 years before present{{cite web}}: |author= has generic name (help) ^Juliette Saillard; Peter Forster; Niels Lynnerup; Hans-Jürgen Bandelt; Søren Nørby (2000). \"mtDNA Variation among Greenland Eskimos. The Edge of the Beringian Expansion\". Laboratory of Biological Anthropology, Institute of Forensic Medicine, University of Copenhagen, Copenhagen, McDonald Institute for Archaeological Research, University of Cambridge, Cambridge, University of Hamburg, Hamburg. Archived from the original on 2011-08-11. Retrieved 2009-11-22. The relatively lower coalescence time of the entire haplogroup A2 including the shared sub-arctic branches A2b (Siberians and Inuit) and A2a (Eskimos and Na-Dené) is probably due to secondary expansions of haplogroup A2 from the Beringia area, which would have averaged the overall internal variation of haplogroup A2 in North America. ^A. Torroni; T. G. Schurr; C. C. Yang; EJE. Szathmary; R. C. Williams; M. S. Schanfield; G. A. Troup; W. C. Knowler; D. N. Lawrence; K. M. Weiss; D. C. Wallace (January 1992). \"Native American Mitochondrial DNA Analysis Indicates That the Amerind and the Nadene Populations Were Founded by Two Independent Migrations\". Center for Genetics and Molecular Medicine and Departments of Biochemistry and Anthropology, Emory University School of Medicine, Atlanta, Georgia. 130 (1). Genetics Society of America: 153–62. Archived from the original on 2009-02-20. Retrieved 2009-11-28. The divergence time for the Nadene portion of the HaeIII np 663 lineage was about 6,000–10,000 years. Hence, the ancestral Nadene migrated from Asia independently and considerably more recently than the progenitors of the Amerinds"}
{"url": "https://web.archive.org/web/20151208100739/http://www.comingsoon.net/movies/news/401669-first-look-at-leonardo-dicaprio-in-the-revenant", "text": "These crawls are part of an effort to archive pages as they are created and archive the pages that they refer to. That way, as the pages that are referenced are changed or taken from the web, a link to the version that was live when the page was written will be preserved. Then the Internet Archive hopes that references to these archived pages will be put in place of a link that would be otherwise be broken, or a companion link to allow people to see what was originally intended by a page's authors. This is a collection of web page captures from links added to, or changed on, Wikipedia pages. The idea is to bring a reliability to Wikipedia outlinks so that if the pages referenced by Wikipedia articles are changed, or go away, a reader can permanently find what was originally referred to. First Look at Leonardo DiCaprio in The Revenant Through April, Leonardo DiCaprio will be shooting in the wilds of Calgary amid the Canadian Rockies playing a fur trapper hunting the men who left him for dead in The Revenant, the latest from Academy Award-nominated director Alejandro González Iñárritu (Birdman). 20th Century Fox has provided us with the first photos from the film which you can check out below! “He’s a brave, incredible actor. I’m so surprised about how good he is,”Iñárritu says of DiCaprio while talking to Entertainment Weekly. “I think there’s a profound understanding of humanity that I can see through his eyes.” It has turned into an arduous shoot, due to the remoteness of many locations and cinematographer Emmanuel Lubezki’s penchant for shooting with only natural light, meaning the crew can only shoot for a few hours each day. Says Iñárritu, “It’s a very experimental thing that we’re doing here…I’m now addicted to doing things that can fail horribly or maybe that can give us a surprise. We are all into it.” DiCaprio will play Hugh Glass, a 19th century fur trapper who is mauled by a grizzly bear, left for dead and then robbed. When he survives against all odds, he sets out on a treacherous journey to exact revenge on his betrayers in this captivating and inspiring story based on the Michael Punke novel, “The Revenant: A Novel of Revenge.” The Revenant co-stars Tom Hardy, Will Poulter and Domhnall Gleeson, and will receive a limited release on December 25, 2015."}
{"url": "https://en.wikipedia.org/wiki/John_Weiss_Forney", "text": "He was born at Lancaster, Pennsylvania and at the age of 16 entered the printing office of the Lancaster Journal. Four years later he purchased the Lancaster Intelligencer, and in 1840 he became proprietor of the Journal and combined the two papers under the name of the Intelligencer and Journal. In 1845 PresidentJames K. Polk appointed him deputy surveyor of the port of Philadelphia, and he disposed of the Intelligencer and Journal, and purchased a half interest in the Pennsylvanian, a Democratic paper of great influence, which under his editorial control attained a national importance. From 1851 to 1855 he was Clerk of the United States House of Representatives, and, while continuing to write for the Pennsylvanian, he edited the Union, the organ of the Northern Democrats. While Clerk, it became Forney's duty to preside during a protracted struggle for the speakership in 1855, which resulted in the election of Nathaniel P. Banks. His tact as presiding officer won the applause of all parties. In 1855 he headed the Pennsylvania delegation to the Democratic National Convention at Cincinnati, and was instrumental in securing the nomination of Pennsylvania's candidate, James Buchanan. He conducted Buchanan's successful campaign for the presidency, and Buchanan would have given him a cabinet office if the appointment had been more popular in the South. In January 1857, Buchanan's influence was not strong enough to win Forney a seat in the United States Senate, which went instead to Simon Cameron. In August 1857, Forney established The Philadelphia Press, an independent Democratic newspaper. At first a Douglas Democrat and a supporter of Buchanan, upon the adoption of the Lecompton Constitution in the latter days of the Buchanan administration, he declined to support the Buchanan administration's effort to secure the admission of Kansas on that basis, and joined the Republican Party. He contributed to the organization of the Republican Party and its early successes. From 1859 to 1861, he was a second time clerk of the House, and he published in Washington, D.C. the Sunday Morning Chronicle, which in 1862 was changed to a daily, and was throughout the Civil War looked upon as the organ of the Lincoln administration. In 1861, he became Secretary of the United States Senate. Among the events of his secretariat may be remembered that he was the first to read aloud, in a joint session of Congress, George Washington's Farewell Address, a reading that became traditional after 1888. 'In January 1862, with the Constitution endangered by civil war, a thousand citizens of Philadelphia petitioned Congress to commemorate the forthcoming 130th anniversary of George Washington’s birth by providing that “the Farewell Address of Washington be read aloud on the morning of that day in one or the other of the Houses of Congress.” Both houses agreed and assembled in the House of Representatives’ chamber on February 22, 1862, where Secretary of the Senate John W. Forney “rendered ‘The Farewell Address’ very effectively,” as one observer recalled.'[1] On the death of Lincoln, Forney supported Andrew Johnson for a short time, but afterward became one of the foremost in the struggle which resulted in the president's impeachment. In 1868, no longer Secretary of the Senate, he disposed of his interest in the Chronicle and returned to Philadelphia where in 1871 he was made collector of the port by President Ulysses S. Grant. He held the office for one year, and during that time perfected the system of direct transportation of imports in bond without appraisement and examination at the port of original entry. He was an earnest promoter of the Centennial Exposition and visited Europe in its interest in 1875. In 1877 he sold the Press and established a weekly, the Progress, which he edited until his death. Progress continued to be published by the Forney Publishing Company after his death.[2] In 1880 he left the Republican Party and supported Winfield Scott Hancock for the presidency. He is buried in West Laurel Hill Cemetery, Bala Cynwyd, Pennsylvania."}
{"url": "https://en.wikipedia.org/wiki/Wasco-Wishram", "text": "The Wishram and Wasco are Plateau tribes that are closely related and share many cultural aspects of the Northwest Coast tribes. They lived along the banks of the Columbia River, near The Dalles. The Dalles was a prime trading location, and the tribes benefited from a vast trade network. United States military expansion in the 1800s brought European diseases, which took a great toll on the Wasco and Wishram populations. Both tribes were forced by the United States in 1855 to sign treaties ceding the majority of their lands. These treaties established the Warm Springs Reservation.[1] Wasco comes from the word Wacq!ó, meaning \"cup\" or \"small bowl,\" the name of a distinctive bowl-shaped rock near the tribe's primary historic village. They traditionally lived on the south bank of the Columbia River. In 1822, their population was estimated to be 900.[2] They were divided into three subtribes: the Dalles Wasco or Wasco proper (a.k.a. the Ki-gal-twal-la) on the south side of the Columbia River near The Dalles in Wasco County), the Hood River Wasco (on the Hood River or Dog River to its mouth into the Columbia River; Lewis and Clark grouped them with the White Salmon River Band and named them Smock-Shop Band of Chil-luck-kit-te-quaw, but they were two separate groups: White Salmon River Band in Washington and Hood River Band in Oregon, called Ninuhltidih (Curtis) or Kwikwulit (Mooney) and the Cascades Indians or Watlala (downstream from the other Wasco groups, two groups, one on each side of the Columbia River; the Oregon group were called Gahlawaihih [Curtis]). The Watlala, whose dialect is the most divergent dialect of the Wasco, may have been a separate tribe though identified as Wasco since 1830. The Wishram are known as the Tlakluit and Echeloot. They traditionally settled in permanent villages along the north banks of the Columbia River. In the 1700s, the estimated Wishram population was 1,500. In 1962 only 10 Wishrams were counted on the Washington census.[1] Their main summer and winter village on the Columbia River, Washington, was Wishram village, referred to as Nixlúidix by its residents. It is considered the largest prehistoric Chinook village site. The site is now part of Columbia Hills State Park. Located near Five Mile Rapids, the village was located at the far eastern reach of Chinookan lands. The village and the name for its people as ″Wishram″ comes from the neighboring Sahaptin-speaking tribes, which called the village Wɨ́šx̣am/Wɨ́šx̣aa - ″Spearfish″, and its people therefore Wɨ́šx̣amma - ″Wishram people″.[3] The 1855 treaties signed by the Wasco-Wishram provide for the tribes to fish \"at all ... usual and accustomed stations in common with the citizens of the United States...\" Between 1938 and 1956, the Bonneville Dam, Grand Coulee Dam, and The Dalles Dam all wreaked havoc upon native fisheries. The government paid money to the tribes to compensate the loss of fish; however, that provided no compensation for the cultural and religious importance that fishing for salmon and steelhead held for the tribe. In 1974 a landmark court case confirmed the rights of Northwest Coast tribes to fish as they have historically done.[1] The Wasco-Wishram language is part of the Upper Chinookan or Kiksht division of the Chinookan language family, itself a branch of the proposed Penutian language family.[1] Currently, five elders from the Warm Springs Reservation are fluent speakers. The tribe has a language program to revive its use among tribal members of all ages.[5] Both tribes are known for their intricate wood carving, beadwork, and basketry.[6] Wasco-Tlingit artist Pat Courtney Gold takes traditional Wasco-Wishram designs and weaves them into contemporary baskets.[7] ^Bernstein, Bruce and Gerald McMaster. First American Art: The Charles and Valerie Diker Collection of American Indian Art. Washington, DC: Smithsonian National Museum of the American Indian, 2004: 41,71, 152-3, 219. ISBN978-0-295-98403-2."}
{"url": "https://web.archive.org/web/20210928075946/http://www.eliotrausch.com/about", "text": "Eliot Rausch’s work reflects his long-standing pursuit of finding value and transcendence in the mundane. Eliot Rausch is an interdisciplinary storyteller. A native Angeleno who graduated CSULB with a BFA. A young editor and documentarian who launched an expansive international film career on the heels of a docu-series examining human frailty. Last Minutes with Oden won the Documentary Award and Grand Prize at the Vimeo Festival, Time Magazine honored it as one of the most important moments of 2010. Working with brands ranging from The Red Cross, LA28, Veteran Affairs of America, to Google and Apple, Eliot has garnished other noteworthy accolades. His short film “Find Your Understanding,” created for Expedia, was awarded “Most Tear-Jerking Viral Ad of the Year” by Ad Age and one of TED's 10 Best Ads of the Year. While developing personal work surrounding Alzheimer’s and ALS, Eliot won a Cannes Lion and Clio award for a series honoring those living with multiple sclerosis. His partnership with the Department of Justice and Futures without Violence helped launch a multi-year “Changing Minds” film campaign, that addressed children’s exposure to violence and childhood trauma. Alongside Alejandro González Iñárritu, he directed the documentary \"A World Unseen,” a complimentary piece to “The Revenant,” and an exposé on climate change. In recent years Eliot has created films and communal gatherings intended to collectively explore modernities struggles with human commodification and the puer aeternus syndrome."}
{"url": "https://en.wikipedia.org/wiki/Haida_people", "text": "Haida history begins with the arrival of the primordial ancestresses of the Haida matrilineages in Haida Gwaii some 14,000 to 19,000 years ago. These include SGuuluu Jaad (Foam Woman), Jiila Kuns (Creek Woman), and KalGa Jaad (Ice woman).[8] The Haida canon of oral histories and archaeological findings agree that Haida ancestors lived alongside glaciers and were present at the time of the arrival of the first tree, a lodgepole pine, on Haida Gwaii.[9] For thousands of years since Haida have participated in a rigorous coast-wide legal system called Potlatch. After the Island's wide arrival of red cedar some 7,500 years ago Haida society transformed to centre around the coastal \"tree of life\". Massive carved cedar monuments and cedar big houses became widespread throughout Haida Gwaii. The first recorded contact between the Haida and Europeans was in July 1774 with Spanish explorer Juan Pérez, who was sailing north on an expedition to find and claim new territory for Spain. For two days in a row, the Santiago sat off the shore of Haida Gwaii waiting for the currents to settle down enough to allow them to dock and set foot on land. While they waited, several canoes of Haida sailed out to greet them, and ultimately to trade with Pérez and his men. After two days of poor conditions, however, the Santiago was ultimately unable to dock and they were forced to depart without having set foot on Haida Gwaii.[10] The Haida conducted regular trade with Russian, Spanish, British, and American maritime fur traders and whalers. According to sailing records, they diligently maintained strong trade relationships with Westerners, coastal people, and among themselves.[11] Trade for sea-otter pelts was initiated by British Captain George Dixon with the Haida in 1787. The Haida did well for themselves in this industry and until the mid-1800s they were at the centre of the profitable China sea-otter trade. Although they had gone on expeditions as far as Washington State, at first they had minimal confrontations with Europeans. Between 1780 and 1830, the Haida came into conflict with European and American traders. Among the dozens of ships the tribe captured were the Eleanor and the Susan Sturgis. The tribe made use of European weapons they acquired, using cannons and canoe-mounted swivel guns.[7] British colonial authorities formally annexed Haida Gwaii in 1853 by establishing the Colony of the Queen Charlotte Islands. It was later integrated into the Colony of British Columbia in 1858. Colonial authorities backed their claims using gunboat diplomacy, both in Haida Gwaii and more broadly throughout northeastern Pacific coastal indigenous title territories. Also in 1857, the USS Massachusetts was sent from Seattle to nearby Port Gamble, where indigenous raiding parties made up of Haida (from territory claimed by the British) and Tongass (from territory claimed by the Russians) had been attacking and enslaving the Coast Salish people there. When the Haida and Tongass (sea lion tribe Tlingit) warriors refused to acknowledge American jurisdiction and to hand over those among them who had attacked the Puget Sound communities, a battle ensued in which 26 natives and one government soldier were killed. In the aftermath of this, Colonel Isaac Ebey, a U.S. military officer and the first settler on Whidbey Island, was shot and beheaded on 11 August 1857 by a small Tlingit group from Kake, Alaska, in retaliation for the killing of a respected Kake chief in the raid the year before. Ebey's scalp was purchased from the Kake by an American trader in 1860.[12][13][14][15] The 1862 Pacific Northwest smallpox epidemic began in March 1862 when a steamship called Brother Jonathan arrived in Victoria from San Francisco containing a passenger infected with smallpox, 1862 March 26.[16] At the time thousands of indigenous people lived in villages outside the walls of Fort Victoria. The disease broke out amongst Tsimshian people in their community near Fort Victoria. This quickly spread into a pandemic. European public health standards at the time were well practiced and adhered-to official health standards, including vaccinations and victims isolation. Instead, as the disease spread, Victoria Police burned some one dozen homes, deliberately displacing 200 Haida on 1862 May 13. They went on to burn some 40–50 more indigenous villages the following day.[17] First Nations from further north had been camping periodically outside the city limits of Victoria to take advantage of trade, and at the time of the epidemic numbered almost 2000, many of whom were Haida. The colonial government made no effort to vaccinate the First Nations in the region nor to quarantine anyone infected. In June 1862, the encampments were forcibly cleared by police, and 20 canoes of Haidas, many of whom were likely already infected with smallpox, were forced back to Haida Gwaii, escorted by gunboats HMS Grappler and HMS Forward.[18] Those infected did not make it home, according to the plans of the colonial governments, and passed on at Bones Bay near Alert Bay. [19] Later on a group of copper miners travelled from Bella Coola aboard the Leonede under command of Captain McAlmond.[20] The boat took 12 passengers 1862 December. One of these passengers carried smallpox to Haida Gwaii. This might not have been a disaster should the infected miner have stayed in isolation at the mining site on Sk_'in G_aadll, or Skincuttle Island. Instead the disease was spread throughout Haida Gwaii. The disease quickly spread throughout Haida Gwaii, devastating entire villages and families, and creating an influx of refugees. The pre-epidemic population of Haida Gwaii was estimated to be 6,607, but was reduced to 829 in 1881.[21] The only two remaining villages were Massett and Skidegate. The population collapse caused by the epidemic weakened Haida sovereignty and power, ultimately paving the way for colonization. In 1885, the Haida potlatch (Haida: waahlgahl) was outlawed under the Potlatch Ban. The elimination of the potlatch system destroyed financial relationships and seriously interrupted the cultural heritage of coastal people. As the islands were Christianized, many cultural works such as totem posts were destroyed or taken to museums around the world. This significantly undermined Haida's self-knowledge and further diminished morale.[citation needed] The government began forcibly sending some Haida children to residential schools as early as 1911. Haida children were sent as far away as Alberta to live among English-speaking families where they were to be assimilated into the dominant culture. In 1911, Canada and British Columbia rejected a Haida offer whereby in exchange for full rights of British citizenship Haidas would formally join the Dominion of Canada. In November 1985, members of the Haida nation protested the ongoing logging of old-growth forests on Haida Gwaii, establishing a blockade to prevent the logging of Lyell Island by Western Forest Products. A standoff between protesters, police and loggers lasted two weeks, during which 72 Haidas were arrested. Images of elders being arrested gained media traction, which raised awareness and support for the Haida across Canada. In 1987, the governments of Canada and British Columbia signed the South Moresby Agreement, establishing the Gwaii Haanas National Park, which is cooperatively managed by the Canadian government and the Haida Nation. Haida drummers and singers greet guests on the shores of Ḵay Linagaay, a millennia-old village in Haida Gwaii. In December 2009, the government of British Columbia officially renamed the archipelago from Queen Charlotte Islands to Haida Gwaii. The Haida Nation asserts Haida title over all of Haida Gwaii and is pursuing negotiations with the provincial and federal governments. Haida authorities continue to pass legislation and manage human activities in Haida Gwaii, which includes making formal agreements with the Canadian communities established on the islands. Haida efforts are largely directed at the protection of land and water and functioning ecosystems and this is expressed in the protected status for nearly 70% of the million-hectare archipelago. The protected status applies to the landscape and water as well as smaller culturally significant areas. They have also forced a reduction of large-scale industrial activity and the careful regulation of access to resources. In British Columbia, the term \"Haida Nation\" often refers to the Haida people as a whole however, it also refers to their government, the Council of the Haida Nation. All people of Haida ancestry are entitled to Haida citizenship, including the Kaigani, who as Alaskans are also part of the Central Council Tlingit Haida Indian Tribes of Alaska government.[22][23] The Haida language is considered to be an isolate.[24] In the late 19th and early 20th centuries, Haida was de facto banned with the introduction of residential schools and the enforcement of the use of English language. Haida language revitalization projects began in the 1970s and continue to this day. It is estimated that there are only 3 or 4 dozen Haida-speaking people with almost all of them being the age of 70 or older. Haida wait for their Heiltsuk hosts to welcome them to sing and dance at a peace potlatch in Waglisla. Haida host Potlatches which were intricate economic and social-political processes that include acquisition of incorporeal wealth like names and the circulation of property in the form of gifts. They are often held when a citizen wishes to commemorate an event of importance. For example, deaths of a loved one, marriages, and other civil proceedings. The more important potlatches take years to prepare and can continue for days. Haida society continues to produce a robust and highly stylized art form, a leading component of Northwest Coast art. While artists frequently have expressed this in large wooden carvings (totem poles), Chilkat weaving, or ornate jewellery, in the 21st century, younger people are also making art in a popular expression such as Haida manga. The Haida also created \"notions of wealth\", and Jenness credits them with the introduction of the totem pole (Haida: ǥyaagang) and the bentwood box.[7] Missionaries regarded the carved poles as graven images rather than representations of the family histories that wove Haida society together. Chiefly families showed their histories by erecting totems outside their homes, or on house posts forming the building. Transformation masks were worn ceremonially, used by dancers and represented or illustrated the connection between various spirits. The masks usually depicted an animal transforming into another animal or a spiritual or mythical being. Masks were representations of the souls of the mask owner's family waiting in the afterlife to be reborn. Masks worn during ceremonial dances were designed with strings to open the mask, transforming the spiritual animal into a carving of the ancestor underneath. There was also an emphasis on the idea of metamorphosis and reincarnation. With the banning of potlatches by the Canadian government in 1885, many masks were confiscated. Masks and many other objects are considered sacred and designed only for specific people to see. It was unknown who the wearer of the mask was as each mask was made for each individual's soul and spirit animal. Due to the confiscation of the masks and the sacred meaning to each individual who wore the mask, it is unknown if the masks in museums are truly meant to be seen or if they are an aspect of European colonialism and the rejection of Haida religious and spiritual traditions. In 2018, the first feature-length Haida-language film, The Edge of the Knife (Haida: SG̲aawaay Ḵʹuuna), was released, with an all-Haida cast. The actors learned Haida for their performances in the film, with a two-week training camp followed by lessons throughout the five weeks of filming. Haida artist Gwaai Edenshaw and Tsilhqot'in filmmaker Helen Haig-Brown directed, with Edenshaw and his brother being co-screenwriters, with Graham Richard and Leonie Sandercock.[25] Christopher Auchter, the nephew of Michael Nicoll Yahgulanaas, has created a number of Haida centered films.[26] In 2017 he directed the animated film \"The Mountain of SGaana\" inspired by Haida mythology.[27] In spring 2020, \"Now Is the Time\", a documentary by Haida Film maker Christopher Auchter, was selected to screen at Sundance Film Festival.[27] The Haida nation was split between two moieties, the Raven and the Eagle. Marriages between two people from the same moiety were prohibited. Due to this any children that were born after the marriage would officially become part of the moiety that the mother had come from. Each group provided its members with entitlement to a vast range of economic resources such as fishing spots, hunting or collecting areas, and housing sites. Each group also had rights to their own myths and legends, dances, songs, and music. Eagles and Ravens were very important to the Haida families as they would identify with one or the other and this would signify what side on the village they would reside on. The family would also own their own property, had specific areas for food gathering. These categories of Eagles and Ravens divided them on an even larger scale, specifying their land, history, and customs. The Haida social system changed significantly by the end of the nineteenth century. At this point a majority of the Haida had taken nuclear family forms, and members of families belonging in the same moiety (Ravens and Eagles) were permitted to marry each other. The roles of the family varied between men and women. Men were responsible for all of the hunting and fishing, building homes and carving canoes and totem poles. The women's responsibilities were to stay close to home doing a majority of their work on the land. Women were responsible for all of the chores in relation to the keeping of the home. Women were also in charge of curing cedarwood to use for weaving and making clothes. It was also the duty of the women to gather berries and dig for shellfish and clams. Once a boy hit puberty, his uncles on his mothers' side would educate him on his family history and how to behave now that he was a man. It was believed that a special diet would increase his abilities. For example, duck tongues helped him hold his breath under water, whereas blue jay tongues helped him to be a strong climber. The aunts on the father's side of a young Haida woman would teach her about her duties to her tribe once she first began to menstruate. The young woman would go to a secluded space in her family home. They believed that by making her sleep on a stone pillow and only allowing her to eat and drink small amounts she would become tougher. Although not commonly practiced today, it was once customary for young boys and girls entering puberty to embark on vision quests. These quests would send them out alone for days. They would travel through the forests, in hopes of finding a spirit to guide them through their lives. It was believed that boys and girls who were destined for greatness could find unique spirit guides. A successful vision quest was celebrated by the wearing of masks, face paints, and costumes. [citation needed] Haida beliefs are varied and diverse. Modern Haida ascribe to a wide variety of faiths including Protestantism, Catholicism, and Bahá'i. Nihilist, atheist, agnostic, and absurdist perspectives also attend the nation's post-colonial context. Pre-colonial beliefs, however, may still be most popular, and potlatch maintains its elevated situation in Haida society. Many Haida believe in an ultimate being called Ne-kilst-lass, spelt Nang Kilsdlaas in Skidegate dialect, which can manifest through the form and antics of a Raven. Ne-kilst-lass revealed the world and was an active player in the creation of life. While Ne-kilst-lass has a generous inclination, they also includes a darker, indulgent, and trickster quality. [citation needed] Nang Kilsldaas is merely one of many dozens of supernatural beings who personify a wide variety of forces, objects, places, and phenomena. A few of the most prevalent include K_ing.gii, a deity who presides over the seas; X_yuu, the northeast wind; and Sin SG_aanuwee, a cosmological \"super-being\" that encompasses all others. Prior to contact with Europeans, other Indigenous communities regarded the Haida as aggressive warriors and made attempts to avoid sea battles with them. There is some archeological evidence that the Northwest coast tribes, to which the Haida belong, engaged in warfare as early as 2200 BC, although it was not a regular occurrence in the archeological record until the subsequent millennium. Though the Haida were more likely to participate in sea battles, it was not uncommon for them to engage in hand-to-hand combat or long-range attacks. Hostilities were not always violent, often ritualized and some resulting in Peace Treaties still in force hundreds of years later.[28] Analyses of skeletal injuries dating from the late Archaic period and early Formative period show that Northwest coast nations, particularly in the North where most Haida communities were situated, began more frequently engaging in battles of some sort from 1800 BC to AD 500, though the number of battles is unknown.[28] This rise in the incidence of battles during the Middle Pacific period also correlates with the erection of the first defensive fortifications in Haida communities.[28] These fortifications continued to be in use during the 18th century as evidenced by Captain James Cook's discovery of one such hilltop fortification in a Haida village. Numerous other sightings of such fortifications were recorded by other European explorers during this century.[29] There were multiple motivations for the Haida people to engage in warfare. Various accounts explain that the Haida went to battle more for revenge than anything else, and would acquire slaves from their enemies in the process.[4] According to the anthropologist Margaret Blackman, warfare on Haida Gwaii was primarily motivated by revenge. Many Northwest coast legends tell of Haida communities raiding and fighting with neighbouring communities because of insults or other disputes.[30] Other causes included conflicts over property, territory, resources, trade routes and even women. However, a battle between a Haida community and another often did not have simply one cause. In fact, many battles were the result of decades old disputes.[31] The Haida, like several other Northwest coast Indigenous communities, engaged in slave raiding as slaves were highly sought after for their use as labor as well as bodyguards and warriors.[6] During the 19th century, the Haida fought physically with other Indigenous communities to ensure domination of the fur trade with European merchants.[32] Haida groups also had feuds with these European merchants that could last years. In 1789, some Haidas were accused of stealing items from Captain Kendrick, most of which included drying linen. Kendrick seized two Haida chiefs and threatened to kill them via cannon-fire if they did not return the stolen items. Though the Haida community complied at the time, less than two years later 100 to 200 of its people attacked the same ship.[33] The missionary William Collison describes having seen a Haida fleet of around forty canoes.[34] However, he does not provide the number of warriors in these canoes, and there are no other known accounts that describe the number of warriors in a war party. The structure of a Haida war party generally followed that of the community itself, the only difference being that the chief took the lead during battles; otherwise his title was more or less meaningless.[5]Medicine men were often brought along raids or before battles to \"destroy the souls of enemies\" and ensure victory.[35] Battles between a group of Haida warriors and another community sometimes resulted in the annihilation of either one or both of the groups involved.[36][better source needed] Villages would be burned down during a battle which was a common practice during Northwest coast battles.[36][better source needed] The Haida burned their warriors who died in battles, though it is not known if this act was done after each battle or only after battles in which they were victorious.[37] The Haida believed that fallen warriors went to the House of Sun, which was considered a highly honorable death. For this reason, a specially made military suit was prepared for chiefs if they fell in battle. The slaves belonging to the chiefs who died in battle were burned with them.[37] The Haida used the bow and arrow until it was replaced by firearms acquired from Europeans in the 19th century, but other traditional weapons were still preferred.[38] The weapons that the Haida used were often multi-functional; they were used not only in battle, but during other activities as well. For instance, daggers were very common and almost always the weapon of choice for hand-to-hand combat, and were also used during hunting and to create other tools. One medicine man's dagger that Alexander Mackenzie came across during his exploration of Haida Gwaii, was used both for fights and to hold the medicine man's hair up.[39] Another dagger that Mackenzie obtained from a Haida village was said to be connected to a Haida legend; many daggers had individual histories which made them unique from one another.[39] The Haida wore rod-and-slat armour. This meant greaves for the thighs and lower back and slats (a long strip of wood) in the side pieces to allow for more flexibility during movement. They wore elk hide tunics under their armor and wooden helmets. Arrows could not penetrate this armor, and Russian explorers found that bullets could only penetrate the armor if shot from a distance of less than 20 feet. The Haida rarely used shields because of their developed armor.[40] ^ abGreen, Jonathan S. (1915). Journal of a tour on the north west coast of America in the year 1829, containing a description of a part of Oregon, California and the north west coast and the numbers, manners and customs of the native tribes. New York city: Reprinted for C. F. Heartman, p. 45. ^Collison, W. H.; Lillard, Charles (1981). In the wake of the war canoe: a stirring record of forty years' successful labour, peril, and adventure amongst the savage Indian tribes of the Pacific coast, and the piratical head-hunting Haida of the Queen Charlotte Islands, British Columbia. Victoria, B.C: Sono Nis, p. 138. ^Collison, W. H.; Dillard, Charles (1981). In the wake of the war canoe: a stirring record of forty years' successful labour, peril, and adventure amongst the savage Indian tribes of the Pacific coast, and the piratical head-hunting Haida of the Queen Charlotte Islands, British Columbia. Victoria, B.C: Sono Nis, p. 89. ^ abGreen, Jonathan S. (1915). Journal of a tour on the north west coast of America in the year 1829, containing a description of a part of Oregon, California and the north west coast and the numbers, manners and customs of the native tribes. New York city: Reprinted for C. F. Hartman, p. 47."}
{"url": "https://en.wikipedia.org/wiki/Hopewell_Culture_National_Historical_Park", "text": "In 2008, the Department of the Interior included Hopewell Culture National Historical Park as part of the Hopewell Ceremonial Earthworks, one of 14 sites on its tentative list from which the United States makes nominations for the UNESCO World Heritage Sites.[2]UNESCO inscribed the Hopewell Ceremonial Earthworks as a World Heritage Site on September 19, 2023, comprising the park and related sites. From about 200 BC to AD 500, the Ohio River Valley was a central area of the prehistoric Hopewell culture. The term Hopewell (taken from the land owner who owned the land where one of the mound complexes was located) culture is applied to a broad network of beliefs and practices among different Native American peoples who inhabited a large portion of eastern North America. The culture is characterized by its construction of enclosures made of earthen walls, often built in geometric patterns, and mounds of various shapes. Visible remnants of Hopewell culture are concentrated in the Scioto River valley near present-day Chillicothe, Ohio. The most striking Hopewell sites contain earthworks in the form of squares, circles, and other geometric shapes. Many of these sites were built to a monumental scale, with earthen walls up to 12 feet (3.7 m) high outlining geometric figures more than 1,000 feet (300 m) across. Conical and loaf-shaped earthen mounds up to 30 feet (9.1 m) high are often found in association with the geometric earthworks. The people who built them had a detailed knowledge of the local soils, and they combined different types to provide the most stability to the works. It required the organized labor of thousands of man hours, as people carried the earth in handwoven baskets. Entry sign at Hopewell Culture National Historical ParkMound City Site Mound City, located on Ohio Highway 104 approximately 4 miles (6.4 km) north of Chillicothe along the Scioto River, is a group of 23 earthen mounds constructed by the Hopewell culture. Each mound within the group covered the remains of a charnel house. After the Hopewell people cremated the dead, they burned the charnel house. They constructed a mound over the remains. They also placed artifacts, such as copper figures, mica, projectile points, shells, and pipes in the mounds. In 1992, Mound City Group was renamed and expanded as Hopewell Culture National Historical Park. Its definition included remnants of four other nearby earthwork and mound systems. Two Ross County sites are within a few miles of Mound City and open to the public. Seip Earthworks is located 17 miles (27 km) west of Chillicothe on U.S. Route 50. Hopewell Mound Group is the site of the 1891 excavation on the land of Mordecai Hopewell (for whom the culture is named). Hopeton Earthworks located across the Scioto River from Mound City and High Bank Works, which is closed to the public. The national park contains nationally significant archaeological resources, including large earthwork and mound complexes. These provide insight into the sophisticated and complex social, ceremonial, political, and economic life of the Hopewell people.[6] The park visitor center features an orientation film, book sales area, and self-guided tours."}
{"url": "https://web.archive.org/web/20150905111600/http://www.boxofficemojo.com/alltime/weekends/month/?mo=01&p=.htm", "text": "The Internet Archive discovers and captures web pages through many different web crawls. At any given time several distinct crawls are running, some for months, and some every day or longer. View the web archive through the Wayback Machine. CHART NOTES * Opening Weekend equals Fri-Sat-Sun. Movies that do not have a 3-day opening gross are not included on this chart. ^ Total Grosses do not include additional releases, if any. ** First weekend of wide release."}
{"url": "https://en.wikipedia.org/wiki/Muisca_economy", "text": "Early Spanish chronicler Pedro Simón as many contemporaneous writers had particular views of the Muisca economy. Modern anthropologists have revised many of the exploitational ideas of the Spanish colonisersMuisca mummies were carried on the backs of the guecha warriors to impress their enemiesThe Muisca Confederation was the confederation of Muisca rulers predating the current departments of Cundinamarca and BoyacáThe flat Bogotá savanna is the result of the Pleistocene Lake Humboldt. The fertile lacustrine soils mixed with volcanic ashes proved very advantageous to the Muisca agriculture The Muisca economy was self-sufficient regarding the basic supplies, thanks to the advanced technologies of the agriculture on raised terraces by the people. The system of trade was well established providing both the higher social classes and the general population abundances of gold, feathers, marine snails, coca, yopo and other luxury goods. Markets were held every four to eight days in various settlements throughout the Muisca Confederation and special markets were organised around festivities where merchants from far outside the Andes were trading their goods with the Muisca. Apart from agriculture, the Muisca were well developed in the production of different crafts, using the raw materials traded with surrounding indigenous peoples. Famous are the golden and tumbaga objects made by the Muisca. Cotton mantles, cloths and nets were made by the Muisca women and traded for valuable goods, tropical fruits and small cotton cloths were used as money. The Muisca were unique in South America for having real coins of gold, called tejuelos. Mining was an important source of income for the Muisca, who were called \"The Salt People\" because of their salt mines in Zipaquirá, Nemocón and Tausa. Like their western neighbours, the Muzo -who were called \"The Emerald People\"- they mined emeralds in their territories, mainly in Somondoco. Carbon was found throughout the region of the Muisca in Eocene sediments and used for the fires for cooking and the production of salt and golden ornaments. In the times before the Spanish conquest of the Muisca, the central part of present-day Colombia; the Eastern Ranges of the Colombian Andes was inhabited by the Muisca who were organised in a loose confederation of rulers. The central authorities of Bacatá in the south and Hunza in the north were called zipa and zaque respectively. Other rulers were the iraca priest in sacred City of the Sun Sugamuxi, the Tundama of Tundama and various other caciques (chiefs). The Muisca spoke Chibcha, in their own language called Muysccubun; \"language of the people\". The Muisca, different from the other three great civilizations of the Americas; the Maya, Aztecs, and Inca, did not build grand stone architecture, yet their settlements were relatively small and consisted of bohíos; circular houses of wood and clay, organised around a central market square with the house of the cacique in the centre. Roads were present to connect the settlements with each other and with the surrounding indigenous groups, of which the Guane and Lache to the north, the Panche and Muzo to the west and Guayupe, Achagua and Tegua to the east were the most important. The Muisca mummified the most respected members of their community and the mummies were not buried, yet displayed in their temples, in natural locations such as caves and even carried on their backs during warfare to impress their enemies. Their art is the most famous remnant of their culture, as living spaces, temples and other existing structures have been destroyed by the Spanish who colonised the Muisca territories. A primary example of their fine goldworking is the Muisca raft, together with more objects made of gold, tumbaga, ceramics and cotton displayed in the Museo del Oro in Bogotá, the ancient capital of the southern Muisca. Accounts of the Spanish conquistadores show the Muisca had a highly advanced and specialised economy based on a variety of sources of income. The main foundation of their economy was the agricultural development using raised terraces on the fertile plains and valleys of the Altiplano Cundiboyacense. The caciques did not control the production directly although surpluses were distributed among them.[1] Excavations at the archaeological siteEl Infiernito did not provide evidence of a power structure based on economical differences.[2] Social complexity and advanced status of economies are often measured based on the specialisation in craft production. The specialised crafts form an economical advantage and sign of social prestige over competing communities. This has been theorised in the case of the Muisca economy, yet certain research restricted to the Bogotá area has found little evidence to support that thesis. Explanations for the lack of archaeological evidence on wealth differences and relations between higher social classes and wealth have been given in the form of methodological issues, ethnohistorical exaggerations by the Spanish looking for gold and sampling issues.[3] The biased views of the Spanish on the Muisca economy and other characteristics of the Muisca society have been noted by various scholars and in recent years a re-examination of those primary accounts has been conducted, among others by Jorge Gamboa Mendoza.[4][5] All the 16th century Spanish chroniclers agree upon the trading advantage the Muisca had. One of them was Juan de los Barrios who wrote that the Muisca men were traders (hipa in the Chibcha language) and extremely able in such matters; \"The Muisca were so sharp in their dealings that no other Indian could equal them in matters of such dazzling ingenuity\".[6] The early Spanish writers have reported that the Muisca paid tribute to other caciques. It has been suggested, for instance by Carl Henrik Langebaek, that those \"tributes\" were a misinterpretation of the Spanish. The Chibcha verb \"to give, to present\" was zebquisca and the word for \"to give\" was zequasca, zemnisca or zequitusuca.[7] Agriculture was the main source of income for the Muisca who were generally self-sustaining due to the fertility of the soils of the Altiplano, especially on the Bogotá savanna. The fertility originates from the lake deposits, the result of the PleistoceneLake Humboldt that existed until around 30,000 years BP and which remnants are still visible today; the various lakes and wetlands (humedales) of the Altiplano. Other prehistorical and historical lakes were present in the other valleys of the high plateau; the Ubaté-Chiquinquirá Valley, Iraka Valley and the Tenza Valley. When the lakes dried up, they left leveled fertile soils which were used by the Muisca to cultivate a large variety of crops, mainly maize, tubers, beans. quinoa and potatoes.[8][9] The fertility of the Bogotá savanna was enhanced by the deposition of Neogene volcanic ashes.[10] Fruits were cultivated in Somondoco and Subachoque.[11] The highest population density was related to the richest agricultural lands, mainly on the Bogotá savanna.[12] To ensure a subsistence economy, the Muisca irrigated their lands and varied their cultivation over different climatic zones. The geography of the area allowed for micro-ecological regions providing farmlands on the fertile plains and in higher altitude terrains such as mountain slopes. Quinoa and potatoes (Solanum tuberosum) were cultivated on the highest altitudes, maize and coca in the temperate regions and yuca, arracacha, pineapples, tobacco and cotton in the low-lying valleys with a warmer climate.[13][14] Additionally, Cucurbita maxima, Oxalis tuberosa (oca), peppers and Ullucus tuberosus were cultivated by the Muisca.[15] The surplus of the agricultural production was available for trading on the many markets throughout the Muisca territories.[9] The Muisca obtained most of their meat and fish by hunting and fishing. The many rivers and lakes on the Altiplano provided rich resources in fish, especially the lakes Fúquene and Tota. Hunting and fishing were activities performed by the Muisca men, while the Muisca women cared for the sowing, cloth production and ceramics elaboration.[19] Domestication of guinea pigs started already in the Herrera Period around 500 BCE. Evidence for this has been discovered at Tequendama IV among other sites.[20] Salt was mined in Nemocón, Tausa and Zipaquirá, giving the Muisca the name \"The Salt People\" Emeralds were exploited using pits dug next to the formation. Because the emeralds from Somondoco were in sedimentary rocks, they would wash clean into the pits during the rainy season The territories of the Muisca contained rich mineral resources of various kinds. Salt was mainly extracted in Zipaquirá, Nemocón and Tausa with minor mining activity in Sesquilé, Gámeza, and Guachetá.[21]Emeralds were mined in Somondoco, Coscuez and Ubalá.[12]Carbon exploitation was executed in Sugamuxi, Tópaga and Gámeza. Copper mining took place in Gachantivá, Moniquirá and Sumapaz. Gold and silver deposits were not common in the Muisca area and mostly obtained through trade.[22] The mining of emeralds was conducted using coas, long thick wooden poles. The people dug holes during the rainy seasons next to the emerald-containing rocks and the emeralds from sedimentary rocks would wash into the holes that dried up and provided the clean emeralds. Emeralds from veins in metamorphic rocks were excavated using sharp poles.[22] Carbon was a common resource in the Muisca territory and was found mainly in the Bogotá and Guaduas Formations. The process of exploitation was similar to the emeralds, using pointed wooden sticks.[22] Apart from agriculture and mining, production was an important economic activity of the Muisca. Raw materials for the production of golden and tumbaga objects, cotton cloths and ceramics were mostly traded with neighbouring indigenous groups, or the result of extraction within the Muisca Confederation, such as clays from the many rivers on the Altiplano. The ceramics of the Muisca were elaborated by the women. They were used for cooking, preparation of chicha and as trading materials with other indigenous groups The Muisca were notable for their ceramics production and major production was located close to rivers and lakes. The surroundings of Lake Fúquene formed a principal place for ceramics production, especially in Ráquira and Tinjacá. The Spanish called the people from this region \"Pottery People\".[23] Other important clay and ceramic producing settlements were Soacha, Cogua, Guatavita, Gachancipá and Tocancipá on the Bogotá savanna and Tutazá, Ráquira, Sutamarchán (Boyacá) and Guasca and Suesca to the north of the flat plains.[24][25] The production of pottery was the task assigned to the Muisca women who produced various ceramics such as anthropomorphic vases, cups and mugs, the typical bowls called múcura, pans, the large pots for salt extraction (gacha) and jars with two, four or six holds. The pots were decorated with colourful paints and stylilised serpent or frog figures.[24] The Muisca were famous for the fine goldworking, here seen in different figures Most famous were the Muisca for their goldworking. The majority of historical artefacts in the world are made of gold and tumbaga, a copper-gold-silver alloy. Gold was not common in the Muisca territories as a primary resource and was obtained through trade. The primary site for goldworking was Guatavita, close to the sacred lake which shares its name, Lake Guatavita. A range of objects was made of the precious minerals; crowns, nose rings, pectorals, earrings, diadems, tunjos (small anthropomorphic or zoomorphic offer pieces), brooches, scepters, coins (tejuelo) and tools.[19] To produce their objects, the people used melting pots, torches and ovens. The tumbaga was poured into heated stone moulds filled with beeswax to elaborate the desired figures. The heat would melt the wax and leave space for the gold to replace it. The advanced techniques produced highly stylilised figures.[19] Cotton was an important raw product for the Muisca, grown in the lower altitude areas. The women wove mantles, bags, small cloths serving as money and nets from it The weaving was performed by the women and used the cotton cultivated in the warmer climates, traded for salt or ceramics. Wooden spindles and clay rolls were used to perform the weaving of braided or tied cloths and mantles. Also nets were made of cotton. Needles were made of gold or bone. The cloths were painted black, red and other colours with clay rollers and pencils. As paint the Muisca women used indigo, woodlice (purple), saffron (gold), plants of the acanthus family and Bocconia frutescens (orange) and other natural inks.[24][26] Early chronicler Juan de Castellanos noted that the Muisca were \"more traders than fighters\".[27] Trading was performed using salt, small cotton cloths and larger mantles and ceramics as barter trade.[28] Also flat dishes made of gold were used as coins. These tejuelo were plain round disks of 1 centimetre (0.39 in), 4 centimetres (1.6 in) or 5 centimetres (2.0 in) diameter.[14][28][29]Tejuelo have been found in Guayatá in the Tenza Valley. This monetary system using coins was unique among the South American indigenous peoples.[30] Every four days markets were active in Bacatá, Hunza, Zipaquirá and Turmequé.[31][32] Other important market settlements were Chocontá, Pacho, Tocancipá, Funza and Somondoco.[14][31][32] According to Pedro Simón, the Muisca held markets every eight days.[27]Sorocotá, along the Suárez River was a major market town for trade with the Guane where gold from Girón and the Carare River area was traded for emeralds from Somondoco. Also tropical fruits that didn't grow on the high plateau in the Andes were sold here. The town of La Tora, present-day Barrancabermeja, was important for trade with the Caribbean coast and the major source for the highly regarded marine snail shells, elaborated with gold by the Tairona.[29] Trade with the lowland people of the Llanos Orientales happened along trade routes across the Eastern Ranges. The crossings over rivers were made with ropes. Products as yopo, bee wax and honey, cotton, fish and fruits were traded with the Llanos peoples Guayupe, Achagua and Tegua.[33] Also the precious colourful feathers of exotic birds, used for the Muisca crowns were traded with the Llanos, that provided animal skins such as jaguars for the hats of the caciques as well. While archaeological evidence suggests the trade was mainly inside the Muisca terrain, the low preservation degree of certain objects may well have biased that conclusion.[34] The cotton, important for the weaving of mantles both for clothing on the relatively cold Altiplano, came from northern and eastern regions. The northern circle of trade was centered around Sugamuxi and Tundama and the eastern trade dominated by the markets in Teusacá, Chocontá and Suesca.[25] Coca trade concentrated in the north around Motavita and Chitagoto as well as Soatá.[5] The merchants from Paipa would travel the 80 kilometres (50 mi) from the city to Soatá to buy coca which was sold again on the market in Tunja, 100 kilometres (62 mi) to the south.[35] Soon after the arrival of the Spanish, a system of encomenderos was installed in the New Kingdom of Granada, as Colombia was called after 1537, where the caciques of the Muisca settlements were forced to pay tributes to the Spanish every six months.[36] The previously self-sustaining economy was quickly transformed into intensive agriculture and mining that created a change in the landscape and culture of the Muisca.[37] The indigenous inhabitants were forced to work the farmlands and mines for the Spanish, who imported slaves from Africa in addition to the Muisca labour.[38] The European settlers used the Muisca economy, where gold was exchanged for cotton, salt, emeralds, mantles and other products to avoid paying the quinta real tax to the Spanish crown, which was based on gold. Where the first settlers required the Muisca chiefs to pay their tributes to the new reign in gold, later payments were done using the replacement products that were then changed to gold at the markets of Pamplona and Mariquita.[39] In 1558, 20 years after the victory of the Spanish conquistadores over the Muisca, a letter to the Spanish crown revealed that more than 11,000 pesos were lost per year in evaded tax payments due to the system of trade via other products than gold.[40] Golden sea snail in the collection of the Museo del Oro in Bogotá. The Muisca obtained the precious sea snails from the Tairona at markets to the north of their territories, e.g. in Barrancabermeja Remaining of the Muisca economy in the present are the many markets throughout central Colombia, the emerald mining (Colombia is the most productive country worldwide of the green beryl gemstone, producing 70–90% of their finest quality ones)[41] and the elaboration of cloths and pottery. Collection of Muisca economical products are displayed in the famous Gold Museum in Bogotá, the Archaeology museum of Sogamoso, the Colombian National Museum and other smaller museums on the Altiplano.[42]"}
{"url": "https://www.hollywoodreporter.com/movies/movie-news/south-korea-box-office-revenant-856647/", "text": "The Revenant opened to an impressive $5.7 million at the South Korean box office, one of the top 10 openings of all time for a Fox title, not accounting for inflation. Directed by Alejandro Gonzalez Inarritu and starring Leonardo DiCaprio, the movie commanded 35.2 percent of the market after nabbing 12 Oscar nominations. Korean fans and media have largely spotlighted whether DiCaprio will win his first Oscar. Coming in second place was a domestic film, Mood of the Day. The Showbox/Mediaplex film took 14.1 percent of the revenue to earn a total $2.4 million during its first week in theaters — an impressive box-office performance among recent local romantic comedies. Mood of the Day features popular actor YooYeon-seok as a womanizer who tries to woo a conservative woman, played by in-demand actress Moon Chae-won. Related Stories Pixar’s The Good Dinosaur finished at No. 3, falling two spots after topping the Korean box office last week. The film, handled by Walt Disney Company Korea, brought in 12.4 percent of ticket sales over the weekend to amass a gross total of almost $6 million. The Himalayas, another domestic film, fell to fourth place, down one from the previous week. The mountaineering epic handled by CJ Entertainment brought in 9.3 percent of the weekend sales revenue that added to its cumulative earning of $47.9 million. Its blockbuster performance — the film topped charts for three weeks, from Dec. 18 until Jan.3 — had been accountable for the underperformance of Star Wars: The Force Awakens. The Inside Men: The Original, a second Showbox/Mediaplex title among the top-ranking films, finished at No. 5. This was three places down from last week’s No. 2. The director’s cut version of the political drama The Inside Men accounted for 8.2 percent of the share, contributing to its total gross of $12.5 million."}
{"url": "https://en.wikipedia.org/wiki/Srebrenica_massacre", "text": "In 2004, in a unanimous ruling on the case of Prosecutor v. Krstić, the Appeals Chamber of the ICTY, located in The Hague, ruled that the massacre of the enclave's male inhabitants constituted genocide, a crime under international law.[23] The ruling was also upheld by the International Court of Justice (ICJ) in 2007.[24] The forcible transfer and abuse of between 25,000 and 30,000 Bosniak Muslim women, children and elderly which accompanied the massacre was found to constitute genocide, when accompanied with the killings and separation of the men.[25][26] In 2013, 2014, and again in 2019, the Dutch state was found liable in the Dutch supreme court and in the Hague district court of failing to do enough to prevent more than 300 of the deaths.[27][28][29][30][31] In April 2013, Serbian PresidentTomislav Nikolić apologised for \"the crime\" of Srebrenica, but refused to call it genocide.[32] Following the declaration of independence, Bosnian Serb forces, supported by the Serbian government of Slobodan Milošević and the Yugoslav People's Army (JNA), attacked the Republic of Bosnia and Herzegovina in order to secure and unify the territory under Serb control, and to create an ethnically homogenous Serb state of Republika Srpska.[35] In the subsequent struggle for territorial control, the non-Serb populations from areas under Serbian control, especially the Bosniak population in Eastern Bosnia, near the Serbian borders, were subject to ethnic cleansing.[36] Srebrenica, and the surrounding Central Podrinje region, had immense strategic importance to the Bosnian Serb leadership, as it was the bridge to two disconnected parts of the envisioned ethnic state of Republika Srpska.[37] Capturing Srebrenica and eliminating its Muslim population would also undermine the viability of the Bosnian Muslim state.[37] In 1991, 73% of the population in Srebrenica were Bosnian Muslims and 25% were Bosnian Serb.[38] Tensions between the Bosnian Muslim and the Bosnian Serb in Srebrenica intensified in the early 1990s, as the local Bosnian Serb population began to be provided with weapons and military equipment distributed by the Serb paramilitary groups, the Yugoslav People's Army (the \"JNA\") and Serb Democratic Party (the \"SDS\").[38]: 35 By April 1992, Srebrenica had become increasingly isolated by the Serb forces. On 17 April 1992, the Bosnian Muslim population of Srebrenica was given a 24-hour ultimatum to surrender all weapons and leave town. In April 1992, Srebrenica was briefly captured by the Bosnian Serbs, and then was retaken by Bosnian Muslims on 8 May 1992. Nonetheless, the Bosnian Muslims remained surrounded by Serb forces, and was cut off from outlying areas. Between April 1992 and March 1993, the Naser Orić trial judgment described the situation in Srebrenica as follows:[38]: 47 Between April 1992 and March 1993, the town of Srebrenica and the villages in the area held by Bosniak were constantly subjected to Serb military assaults, including artillery attacks, sniper fire, as well as occasional bombing from aircraft. Each onslaught followed a similar pattern. Serb soldiers and paramilitaries surrounded a Bosnian Muslim village or hamlet, called upon the population to surrender their weapons, and then began with indiscriminate shelling and shooting. In most cases, they then entered the village or hamlet, expelled or killed the population, who offered no significant resistance, and destroyed their homes. During this period, Srebrenica was subjected to indiscriminate shelling from all directions on a daily basis. Potočari in particular was a daily target for Serb artillery and infantry because it was a sensitive point in the defence line around Srebrenica. Other Bosnian Muslim settlements were routinely attacked as well. All this resulted in a great number of refugees and casualties. During the first three months of war, from April to June 1992, the Bosnian Serb forces, with support from the JNA, destroyed 296 predominantly Bosniak villages in the region around Srebrenica, forcibly uprooted some 70,000 Bosniaks from their homes and systematically massacred at least 3,166 Bosniaks (documented deaths) including many women, children and elderly.[39] Serb military and paramilitary forces from the area and neighbouring parts of eastern Bosnia and Herzegovina and Serbia gained control of Srebrenica for several weeks in early 1992, killing and expelling Bosniak civilians. In May 1992, Bosnian government forces under the leadership of Orić recaptured the town. Over the remainder of 1992, offensives by Bosnian government forces from Srebrenica increased the area under their control, and by January 1993 they had linked with Bosniak-held Žepa to the south and Cerska to the west. At this time, the Srebrenica enclave had reached its peak size of 900 square kilometres (350 square miles), although it was never linked to the main area of Bosnian-government controlled land in the west and remained, in the words of the ICTY, \"a vulnerable island amid Serb-controlled territory\".[42] During this time, Army of the Republic of Bosnia and Herzegovina (ARBiH) forces under the command of Naser Orić used Srebrenica as a staging ground to attack neighboring Serb villages inflicting many casualties.[43][44] On one occasion in 1993, the militarized Serb village of Kravica was attacked by ARBiH and resulted in numerous Serb civilian casualties. The resistance to the Serb siege of Srebrenica by the ARBiH under the command of Naser Orić were seen as a catalyst for what occurred in Srebrenica in 1995. Serbs started persecuting Bosniaks in 1992. Serbian propaganda deemed Bosniak resistance to Serb attacks as a ground for revenge. According to General Philippe Morillon's testimony at the session of the ICTY 12 February 2004: JUDGE ROBINSON: Are you saying, then, General, that what happened in 1995 was a direct reaction to what Naser Oric did to the Serbs two years before? THE WITNESS: [Interpretation] Yes. Yes, Your Honour. I am convinced of that. This doesn't mean to pardon or diminish the responsibility of the people who committed that crime, but I am convinced of that, yes.[45] Over the next few months, the Serb military captured the villages of Konjević Polje and Cerska, severing the link between Srebrenica and Žepa and reducing the size of the Srebrenica enclave to 150 square kilometres. Bosniak residents of the outlying areas converged on the town of Srebrenica and its population swelled to between 50,000 and 60,000 people, which was about ten times Srebrenica's pre-war population.[46] General Philippe Morillon of France, Commander of the United Nations Protection Force (UNPROFOR), visited Srebrenica in March 1993. By then, the town was overcrowded and siege conditions prevailed. There was almost no running water as the advancing Serb forces had destroyed the town's water supplies; people relied on makeshift generators for electricity. Food, medicine and other essentials were extremely scarce. The conditions rendered Srebrenica a slow death camp.[46] Before leaving, General Morillon told the panicked residents of Srebrenica at a public gathering that the town was under the protection of the UN and that he would never abandon them. During March and April 1993 several thousand Bosniaks were evacuated from Srebrenica under the auspices of the UN High Commissioner for Refugees (UNHCR). The evacuations were opposed by the Bosnian government in Sarajevo as contributing to the ethnic cleansing of predominantly Bosniak territory. The Serb authorities remained intent on capturing the enclave. On 13 April 1993, the Serbs told the UNHCR representatives that they would attack the town within two days unless the Bosniaks surrendered and agreed to be evacuated.[47] With the failure to demilitarize and lack of supplies getting into the city, Naser Orić consolidated his power and controlled the black market. Orić's men began hoarding food, fuel, cigarettes and embezzled money sent by foreign aid agencies to support Muslim orphans.[48] Basic necessities were out of reach for many of the people in Srebrenica due to Orić's actions. UN officials were beginning to lose patience with the ARBiH in Srebrenica and saw them as \"criminal gang leaders, pimps and black marketeers\".[49] A former Serb soldier of the \"Red Berets\" unit described the tactics used to starve and kill the besieged population of Srebrenica: It was almost like a game, a cat-and-mouse hunt. But of course we greatly outnumbered the Muslims, so in almost all cases, we were the hunters and they were the prey. We needed them to surrender, but how do you get someone to surrender in a war like this? You starve them to death. So very quickly we realised that it wasn't really weapons being smuggled into Srebrenica that we should worry about, but food. They were truly starving in there, so they would send people out to steal cattle or gather crops, and our job was to find and kill them... No prisoners. Well, yes, if we thought they had useful information, we might keep them alive until we got it out of them, but in the end, no prisoners... The local people became quite indignant, so sometimes we would keep someone alive to hand over to them [to kill] just to keep them happy.[50] When British journalist Tony Birtley visited the besieged Srebrenica in March 1993, he took footage of Bosniak civilians starving to death.[51] The judgment of the Hague Tribunal in the case of Naser Orić found that: Bosnian Serb forces controlling the access roads were not allowing international humanitarian aid—most importantly, food and medicine—to reach Srebrenica. As a consequence, there was a constant and serious shortage of food causing starvation to peak in the winter of 1992/1993. Numerous people died or were in an extremely emaciated state due to malnutrition. Bosnian Muslim fighters and their families, however, were provided with food rations from existing storage facilities. The most disadvantaged group among the Bosnian Muslims was that of the refugees, who usually lived on the streets and without shelter, in freezing temperatures. Only in November and December 1992, did two UN convoys with humanitarian aid reach the enclave, and this despite Bosnian Serb obstruction.\"[52] In April 1995, UNPROFOR became the name used for the Bosnia and Herzegovina regional command of the now-renamed United Nations Peace Forces (UNPF).[53] The 2011 report Srebrenica: a 'safe' area says that \"On 12 June 1995 a new command was created under UNPF\",[53] with \"12,500 British, French and Dutch troops equipped with tanks and high calibre artillery in order to increase the effectiveness and the credibility of the peacekeeping operation\".[53] The report states: In the UNPROFOR chain of command, Dutchbat occupied the fourth tier, with the sector commanders occupying the third tier. The fourth tier primarily had an operational task. Within this structure, Dutchbat was expected to operate as an independent unit with its own logistic arrangements. Dutchbat was dependent on the UNPROFOR organization to some extent for crucial supplies such as fuel. For the rest, it was expected to obtain its supplies from the Netherlands. From an organizational point of view, the battalion had two lifelines: UNPROFOR and the Royal Netherlands Army. Dutchbat had been assigned responsibility for the Srebrenica Safe Area. Neither UNPROFOR nor Bosnia-Hercegovina paid much attention to Srebrenica, however. Srebrenica was situated in eastern Bosnia and Herzegovina, which was geographically and mentally far removed from Sarajevo and Zagreb. The rest of the world was focused on the fight for Sarajevo and the peace process. As a Safe Area, Srebrenica only occasionally managed to attract the attention of the world press or the UN Security Council. That is why the Dutch troops there remained of secondary importance, in operational and logistic terms, for so long; and why the importance of the enclave in the battle for domination between the Bosnian Serbs and Bosnian Muslims failed to be recognised for so long.[54] On 18 April 1993, the first group of United Nations Protection Force (\"UNPROFOR\") troops arrived in Srebrenica. The UNPROFOR deployed Canadian troops to protect Srebrenica as one of the five newly established UN \"safe areas\".[46] UNPROFOR presence prevented all-out assault on the safe area, although occasional skirmishes and mortar attacks continued.[46] On 8 May 1993 agreement was reached for demilitarization of Srebrenica. According to UN reports, \"General [Sefer] Halilović and General [Ratko] Mladić agreed on measures covering the whole of the Srebrenica enclave and the adjacent enclave of Žepa. Under the terms of the new agreement, Bosniak forces within the enclave would hand over their weapons, ammunition and mines to UNPROFOR, after which Serb \"heavy weapons and units that constituted a menace to the demilitarised zones which will have been established in Žepa and Srebrenica will be withdrawn.\" Unlike the earlier agreement, the agreement of 8 May stated specifically that Srebrenica was to be considered a \"demilitarised zone\", as referred to in article 60 of the Protocol Additional to the Geneva Conventions of 12 August 1949, and relating to the Protection of Victims of International Armed Conflicts (Protocol I).\" From the outset, both parties to the conflict violated the \"safe area\" agreement, although a two year period of relative stability followed the establishment of the enclave.[56]Lieutenant colonelThom Karremans (the Dutchbat Commander) testified to the ICTY that his personnel were prevented from returning to the enclave by Serb forces and that equipment and ammunition were also prevented from getting in.[57] Bosniaks in Srebrenica complained of attacks by Serb soldiers, while to the Serbs it appeared that Bosnian government forces in Srebrenica were using the \"safe area\" as a convenient base from which to launch counter-offensives against the Army of the Republika Srpska (VRS) and that UNPROFOR was failing to take any action to prevent it.[57]: 24 General Sefer Halilović admitted that ARBiH helicopters had flown in violation of the no-fly zone and that he had personally dispatched eight helicopters with ammunition for the 28th Division.[57]: 24 Between 1,000 and 2,000 soldiers from three of the VRS Drina Corps Brigades were deployed around the enclave, equipped with tanks, armoured vehicles, artillery and mortars. The 28th Mountain Division of the Army of the Republic of Bosnia and Herzegovina (ARBiH) remaining in the enclave was neither well organised nor equipped, and lacked a firm command structure and communications system. Some of its soldiers carried old hunting rifles or no weapons at all, and few had proper uniforms. A Security Council mission led by Diego Arria arrived in Srebrenica on 25 April 1993 and, in their subsequent report to the UN, condemned the Serbs for perpetrating \"a slow-motion process of genocide.\"[58] The mission then stated that \"Serb forces must withdraw to points from which they cannot attack, harass or terrorise the town. UNPROFOR should be in a position to determine the related parameters. The mission believes, as does UNPROFOR, that the actual 4.5 km (3 mi) by 0.5 km (530 yd) decided as a safe area should be greatly expanded.\" Specific instructions from United Nations Headquarters in New York stated that UNPROFOR should not be too zealous in searching for Bosniak weapons and, later, that the Serbs should withdraw their heavy weapons before the Bosniaks gave up their weapons. The Serbs never did withdraw their heavy weapons.[58] Multiple attempts to demilitarise the ARBiH and force the withdrawal of the VRS proved futile. The ARBiH hid the majority of their heavy weapons, modern equipment and ammunition in the surrounding forest and only handed over disused and old weaponry.[13] On the other hand, given the failure to disarm the ARBiH, the VRS refused to withdraw from the front lines given the intelligence they had regarding hidden weaponry.[13] In March 1994, UNPROFOR sent 600 Royal Dutch Army soldiers (\"Dutchbat\") to replace the Canadian troops. By March 1995, Serb forces controlled all territory surrounding Srebrenica, preventing even UN access to the supply road. Humanitarian aid decreased significantly and living conditions in Srebrenica quickly deteriorated.[46] UNPROFOR presence prevented all-out assault on the safe area, although occasional skirmishes and mortar attacks continued.[46] The Dutchbat alerted UNPROFOR command to the dire conditions in Srebrenica, but UNPROFOR declined to send humanitarian relief or military support.[46] Early 1995: the situation in the Srebrenica \"safe area\" deteriorates[edit] By early 1995, fewer and fewer supply convoys were making it through to the enclave. The situation in Srebrenica and in other enclaves had deteriorated into lawless violence as prostitution among young Muslim girls, theft and black marketeering proliferated.[59] The already meager resources of the civilian population dwindled further, and even the UN forces started running dangerously low on food, medicine, ammunition and fuel, eventually being forced to start patrolling the enclave on foot. Dutch soldiers who left the area on leave were not allowed to return,[58] and their number dropped from 600 to 400 men. In March and April, the Dutch soldiers noticed a build-up of Serb forces near two of the observation posts, \"OP Romeo\" and \"OP Quebec\". In March 1995, Radovan Karadžić, President of the Republika Srpska (RS), despite pressure from the international community to end the war and ongoing efforts to negotiate a peace agreement, issued a directive to the VRS concerning the long-term strategy of the VRS forces in the enclave. The directive, known as \"Directive 7\", specified that the VRS was to: complete the physical separation of Srebrenica from Žepa as soon as possible, preventing even communication between individuals in the two enclaves. By planned and well-thought-out combat operations, create an unbearable situation of total insecurity with no hope of further survival or life for the inhabitants of Srebrenica.[60] By mid-1995, the humanitarian situation of the Bosniak civilians and military personnel in the enclave was catastrophic. In May, following orders, Orić and his staff left the enclave by helicopter to Tuzla, leaving senior officers in command of the 28th Division. In late June and early July, the 28th Division issued a series of reports including urgent pleas for the humanitarian corridor to the enclave to be reopened. When this failed, Bosniak civilians began dying from starvation. On Friday, 7 July the mayor of Srebrenica reported that eight residents had died of starvation.[61] On 4 June 1995, UNPROFOR commander Bernard Janvier, a Frenchman, secretly met with Ratko Mladić to obtain the release of hostages, many of whom were French. Mladić demanded of Janvier that there would be no more air strikes.[62] In the weeks leading up to the assault on Srebrenica by the VRS, ARBiH forces were ordered to carry out diversion and disruption attacks on the VRS by the high command.[63] On one particular occasion on the evening of 25–26 June, ARBiH forces attacked VRS units on the Sarajevo-Zvornik road, inflicting high casualties and looting VRS stockpiles.[63] The Serb offensive against Srebrenica began in earnest on 6 July 1995. The VRS, with 2,000 soldiers, were outnumbered by the defenders and did not expect the assault to be an easy victory.[63] In the following days, the five UNPROFOR observation posts in the southern part of the enclave fell one by one in the face of the Bosnian Serb advance. Some of the Dutch soldiers retreated into the enclave after their posts were attacked, but the crews of the other observation posts surrendered into Serb custody. Simultaneously, the defending Bosnian forces numbering 6,000 came under heavy fire and were pushed back towards the town. Once the southern perimeter began to collapse, about 4,000 Bosniak residents who had been living in a Swedish housing complex for refugees nearby fled north into the town of Srebrenica. Dutch soldiers reported that the advancing Serbs were \"cleansing\" the houses in the southern part of the enclave.[64] A Dutch YPR-765 similar to the ones used at Srebrenica On 8 July, a Dutch YPR-765 armoured vehicle took fire from the Serbs and withdrew. A group of Bosniaks demanded that the armoured vehicle stay to defend them, and established a makeshift barricade to prevent its retreat. As the armoured vehicle continued to withdraw, a Bosniak farmer who was manning the barricade threw a hand grenade onto it and subsequently killed Dutch soldier Raviv van Renssen.[65] Late on 9 July 1995, emboldened by early successes and little resistance from the largely demilitarised Bosniaks as well as the absence of any significant reaction from the international community, President Karadžić issued a new order authorising the 1,500-strong[66] VRS Drina Corps to capture the town of Srebrenica.[64] The following morning, 10 July 1995, Lieutenant Colonel Karremans made urgent requests for air support from North Atlantic Treaty Organization (NATO) to defend Srebrenica as crowds filled the streets, some of whom carried weapons. VRS tanks were approaching the town, and NATO airstrikes on these began on the afternoon of 11 July 1995. NATO bombers attempted to attack VRS artillery locations outside the town, but poor visibility forced NATO to cancel this operation. Further NATO air attacks were cancelled after VRS threats to bomb the UN's Potočari compound, to kill Dutch and French military hostages and to attack surrounding locations where 20,000 to 30,000 civilian refugees were situated.[64] 30 Dutchbat were taken hostage by Mladic's troops.[46] Late in the afternoon of 11 July, General Mladić, accompanied by General Živanović (then-Commander of the Drina Corps), General Krstić (then-Deputy Commander and Chief of Staff of the Drina Corps) and other VRS officers, took a triumphant walk through the deserted streets of the town of Srebrenica.[64] In the evening,[67] Lieutenant Colonel Karremans was filmed drinking a toast with General Mladić during the bungled negotiations on the fate of the civilian population grouped in Potočari.[14][68] The two highest ranking Serb politicians from Bosnia and Herzegovina, Karadžić and Momčilo Krajišnik, both indicted for genocide, were warned by VRS commander Mladić (found guilty of genocide at a UN tribunal in 2017) that their plans could not be realized without committing genocide. Mladić said: People are not little stones, or keys in someone's pocket, that can be moved from one place to another just like that.... Therefore, we cannot precisely arrange for only Serbs to stay in one part of the country while removing others painlessly. I do not know how Mr. Krajišnik and Mr. Karadžić will explain that to the world. That is genocide.[69] Headquarters in Potočari for soldiers under United Nations command; \"Dutchbat\" had 370[14] soldiers in Srebrenica during the massacre. The building was a disused battery factory. By the evening of 11 July 1995, approximately 20,000 to 25,000 Bosniak refugees from Srebrenica were gathered in Potočari, seeking protection within the UNPROFOR Dutchbat headquarters. Several thousand had pressed inside the compound itself, while the rest were spread throughout the neighbouring factories and fields. Although the vast majority were women, children, elderly or disabled, 63 witnesses estimated that there were at least 300 men inside the perimeter of the UNPROFOR compound and between 600 and 900 men in the crowd outside.[70] Conditions in Potočari included \"little food or water available\" and sweltering heat. One UNPROFOR Dutchbat officer described the scene as follows: They were panicked, they were scared, and they were pressing each other against the soldiers, my soldiers, the UN soldiers that tried to calm them. People that fell were trampled on. It was a chaotic situation.[70] On 12 July, the United Nations Security Council, in Resolution 1004, expressed concern at the humanitarian situation in Potočari, which also condemned the offensive by Bosnian Serb forces and demanded immediate withdrawal. On 13 July, the Dutch forces expelled five Bosniak refugees from the United Nations compound despite knowing that men outside the compound were being killed and abused.[71] On 12 July 1995, as the day wore on, the refugees in the compound could see VRS members setting houses and haystacks on fire. Throughout the afternoon, Serb soldiers mingled in the crowd and summary executions of men occurred.[70] In the late morning of 12 July, a witness saw a pile of 20 to 30 bodies heaped up behind the Transport Building in Potočari, alongside a tractor-like machine. Another testified that he saw a soldier slay a child with a knife in the middle of a crowd of expellees. He also said that he saw Serb soldiers execute more than a hundred Bosniak Muslim men in the area behind the Zinc Factory and then load their bodies onto a truck, although the number and nature of the murders stand in contrast to other evidence in the Trial Record, which indicates that the killings in Potočari were sporadic in nature. Soldiers were picking people out of the crowd and taking them away. A witness recounted how three brothers — one merely a child and the others in their teens — were taken out in the night. When the boys' mother went looking for them, she found them stark naked and with their throats slit.[70][72] That night, a Dutchbat medical orderly witnessed two Serb soldiers raping a young woman.[72] One survivor described the murder of a baby and the rape of women occurring in the close vicinity of Dutch UN peacekeepers who did nothing to prevent it. According to the survivor, a Serb told a mother to make her child stop crying, and when it continued to cry, he took it and slit its throat, after which he laughed.[73] Stories about rapes and killings spread through the crowd and the terror in the camp escalated.[70] Several individuals were so terrified that they committed suicide by hanging themselves.[72] One of the survivors, Zarfa Turković, described the horrors of rapes as follows: \"Two [Serb soldiers] took her legs and raised them up in the air, while the third began raping her. Four of them were taking turns on her. People were silent, no one moved. She was screaming and yelling and begging them to stop. They put a rag into her mouth, and then we just heard silent sobs....\"[74][75] From the morning of 12 July, Serb forces began gathering men and boys from the refugee population in Potočari and holding them in separate locations, and as the refugees began boarding the buses headed north towards Bosniak-held territory, Serb soldiers separated out men of military age who were trying to clamber aboard. Occasionally, younger and older men were stopped as well (some as young as 14 or 15).[76][77][78] These men were taken to a building in Potočari referred to as the \"White House\". As early as the evening of 12 July 1995, Major Franken of the Dutchbat heard that no men were arriving with the women and children at their destination in Kladanj.[70] At this time, the UNHCR Director of Operations, Peter Walsh, was dispatched to Srebrenica by the UNHCR Chief of Mission, Damaso Feci, to evaluate what emergency aid could be provided rapidly. Peter Walsh and his team arrived at Gostilj, just outside Srebrenica, in the early afternoon only to be turned away by VRS forces. Despite claiming freedom of movement rights, the UNHCR team was not allowed to proceed and was forced to head back north to Bijelina. Throughout this time, Peter Walsh relayed reports back to UNHCR in Zagreb about the unfolding situation including witnessing the enforced movement and abuse of Muslim men and boys and the sound of summary executions taking place.[citation needed] On 13 July 1995, Dutchbat troops witnessed definite signs that the Serb soldiers were murdering some of the Bosniak men who had been separated. For example, Corporal Vaasen saw two soldiers take a man behind the \"White House\", heard a shot and saw the two soldiers reappear alone. Another Dutchbat officer saw Serb soldiers murder an unarmed man with a single gunshot to the head and heard gunshots 20–40 times an hour throughout the afternoon. When the Dutchbat soldiers told Colonel Joseph Kingori, a United Nations Military Observer (UNMO) in the Srebrenica area, that men were being taken behind the \"White House\" and not coming back, Colonel Kingori went to investigate. He heard gunshots as he approached but was stopped by Serb soldiers before he could find out what was going on.[70] Some of the executions were carried out at night under arc lights, and bulldozers then pushed the bodies into mass graves.[79] According to evidence collected from Bosniaks by French policeman Jean-René Ruez, some were buried alive; he also heard testimony describing Serb forces killing and torturing refugees at will, streets littered with corpses, people committing suicide to avoid having their noses, lips and ears chopped off, and adults being forced to watch the soldiers kill their children.[79] Thousands of women and girls suffered rape and sexual abuse and other forms of torture. According to the testimony of Zumra Šehomerovic: The Serbs began at a certain point to take girls and young women out of the group of refugees. They were raped. The rapes often took place under the eyes of others and sometimes even under the eyes of the children of the mother. A Dutch soldier stood by and he simply looked around with a Walkman on his head. He did not react at all to what was happening. It did not happen just before my eyes, for I saw that personally, but also before the eyes of us all. The Dutch soldiers walked around everywhere. It is impossible that they did not see it. There was a woman with a small baby a few months old. A Chetnik told the mother that the child must stop crying. When the child did not stop crying, he snatched the child away and cut its throat. Then he laughed. There was a Dutch soldier there who was watching. He did not react at all. I saw yet more frightful things. For example, there was a girl, she must have been about nine years old. At a certain moment some Chetniks recommended to her brother that he rape the girl. He did not do it and I also think that he could not have done it for he was still just a child. Then they murdered that young boy. I have personally seen all that. I really want to emphasize that all this happened in the immediate vicinity of the base. In the same way I also saw other people who were murdered. Some of them had their throats cut. Others were beheaded.[80] Testimony of Ramiza Gurdić: I saw how a young boy of about ten was killed by Serbs in Dutch uniform. This happened in front of my own eyes. The mother sat on the ground and her young son sat beside her. The young boy was placed on his mother's lap. The young boy was killed. His head was cut off. The body remained on the lap of the mother. The Serbian soldier placed the head of the young boy on his knife and showed it to everyone. … I saw how a pregnant woman was slaughtered. There were Serbs who stabbed her in the stomach, cut her open and took two small children out of her stomach and then beat them to death on the ground. I saw this with my own eyes.[81] Testimony of Kada Hotić: There was a young woman with a baby on the way to the bus. The baby cried and a Serbian soldier told her that she had to make sure that the baby was quiet. Then the soldier took the child from the mother and cut its throat. I do not know whether Dutchbat soldiers saw that. … There was a sort of fence on the left-hand side of the road to Potocari. I heard then a young woman screaming very close by (4 or 5 meters away). I then heard another woman beg: \"Leave her, she is only nine years old.\" The screaming suddenly stopped. I was so in shock that I could scarcely move. … The rumour later quickly circulated that a nine year old girl had been raped.[82] That night, a Dutchbat medical orderly came across two Serb soldiers raping a young woman: [W]e saw two Serb soldiers, one of them was standing guard and the other one was lying on the girl, with his pants off. And we saw a girl lying on the ground, on some kind of mattress. There was blood on the mattress, even she was covered with blood. She had bruises on her legs. There was even blood coming down her legs. She was in total shock. She went totally crazy. Bosnian Muslim refugees nearby could see the rape, but could do nothing about it because of Serb soldiers standing nearby. Other people heard women screaming, or saw women being dragged away. Several individuals were so terrified that they committed suicide by hanging themselves. Throughout the night and early the next morning, stories about the rapes and killings spread through the crowd and the terror in the camp escalated. Screams, gunshots and other frightening noises were audible throughout the night and no one could sleep. Soldiers were picking people out of the crowd and taking them away: some returned; others did not. Witness T recounted how three brothers—one merely a child and the others in their teens—were taken out in the night. When the boys' mother went looking for them, she found them with their throats slit.[83] As a result of exhaustive UN negotiations with Serb troops, around 25,000 Srebrenica women were forcibly transferred to Bosniak-controlled territory. Some buses apparently never reached safety. According to a witness account given by Kadir Habibović, who hid himself on one of the first buses from the base in Potočari to Kladanj, he saw at least one vehicle full of Bosniak women being driven away from Bosnian government-held territory.[84] Map of military operations during the Srebrenica massacre. The green arrow marks the route of the Bosniak column. On the evening of 11 July 1995, word spread through the Bosniak community that able-bodied men should take to the woods, form a column together with members of the ARBiH's 28th Division and attempt a breakthrough towards Bosnian government-held territory in the north.[85] The men believed they stood a better chance of surviving by trying to escape than if they let themselves fall into Serb hands.[86] Around 10 pm on 11 July the Division command, together with the municipal authorities, took the decision, on their own initiative, to form a column and leave the safe area in an attempt to reach government-controlled territory around Tuzla.[87] Dehydration made finding drinking water a major problem, along with lack of sleep and physical exhaustion—many were exhausted before setting out. There was little cohesion or sense of common purpose in the column.[88] Along the way, the column was shelled and ambushed. In severe mental distress, some of the refugees killed themselves. Others were induced to surrender. Survivors claimed they were attacked with a chemical agent that caused hallucinations, disorientation and strange behaviour.[89][90][91][92] Infiltrators in civilian clothing confused, attacked and killed refugees, including the wounded.[89][93] Many of those taken prisoner were killed on the spot.[94] Others were collected together before being taken to remote locations for mass execution. The attacks on the column broke it up into smaller segments. Only about one third of the men succeeded in crossing the asphalt road between Konjević Polje and Nova Kasaba. It was this group that eventually crossed Bosnian Serb lines to reach Bosnian government territory on and after 16 July. The vast majority of the victims of the massacre were members of the column who failed to complete the perilous journey.[citation needed] A second, smaller group of refugees (estimated at between 700 and 800) attempted to escape into Serbia via Mount Kvarac via Bratunac, or across the river Drina and via Bajina Bašta. It is not known how many were intercepted, arrested and killed on the way. A third group headed for Žepa, possibly having first tried to reach Tuzla. The estimates of the numbers involved vary widely, from 300 to around 850. In addition, small pockets of resistance apparently remained behind and engaged Serb forces. Almost all the 28th Division, 5,500 to 6,000 soldiers, not all armed, gathered in the village of Šušnjari, in the hills north of the town of Srebrenica, along with about 7,000 civilians. They included a very small number of women, not more than ten.[95] Others assembled in the nearby village of Jaglići.[96] At around midnight on 11 July 1995, the column started moving along the axis between Konjević Polje and Bratunac. The main column was preceded by a reconnaissance party of four scouts, approximately five kilometers ahead.[97] Members of the column walked one behind the other, following the paper trail laid down by a de-mining unit.[98] The column was led by a group of 50–100 of the best soldiers from each brigade, carrying the best available equipment. Elements of the 284th Brigade were followed by the 280th Brigade, with them the Chief of Staff Ramiz Bećirović. Civilians accompanied by other soldiers followed, and at the back was the independent battalion which was part of the 28th Division.[87] The command and armed men were at the front of the column, following the deminer unit.[98] Others in the column included the political leaders of the enclave, medical staff of the local hospital and the families of prominent persons in Srebrenica. A small number of women, children and elderly travelled with the column in the woods.[85][99] The column was between 12 and 15 kilometers long, about two and a half hours separating head from tail.[87] The breakout from the enclave and the attempt to reach Tuzla came as a surprise to the VRS and caused considerable confusion, as the VRS had expected the men to go to Potočari. Serb general Milan Gvero in a briefing referred to members of the column as \"hardened and violent criminals who will stop at nothing to prevent being taken prisoner and to enable their escape into Bosnian territory\".[100] The Drina Corps and the various brigades were ordered by the VRS Main Staff to assign all available manpower to the task of finding any Muslim groups observed, preventing them from crossing into Muslim territory, taking them prisoner and holding them in buildings that could be secured by small forces.[101] During the night, poor visibility, fear of mines and panic induced by artillery fire split the column in two.[102] On the afternoon of 12 July, the front section emerged from the woods and crossed the asphalt road from Konjević Polje and Nova Kasaba. Around 18.00 hours, the VRS Army located the main part of the column still in the hilly area around Kamenica (outside the village of Pobuđe). Around 20:00 hours this part of the column, led by the municipal authorities and the wounded, started descending Kamenica Hill towards the road. After a few dozen men had crossed, soldiers of the VRS Army arrived from the direction of Kravica in trucks and armoured vehicles including a white vehicle with UNPROFOR symbols, calling out for Bosniaks over the loudspeaker to surrender.[102] It was around this time that yellow smoke was observed, followed by observations of strange behaviour, including suicides, hallucinations and members of the column attacking one another.[89] Numerous survivors interviewed by Human Rights Watch claimed they were attacked with a chemical agent that caused hallucinations and disorientation.[90][91] (General Zdravko Tolimir was an advocate of the use of chemical weapons against the ArBiH.[92][103]) Heavy shooting and shelling began, which continued into the night. The armed members of the column returned fire and all scattered. Survivors describe a group of at least 1,000 engaged at close range by small arms. Hundreds appear to have been killed as they fled the open area and some were said to have killed themselves to escape capture. VRS Army and Ministry of Interior personnel persuaded members of the column to surrender by promising them protection and safe transportation towards Tuzla under UNPROFOR and Red Cross supervision. Appropriated UN and Red Cross equipment was used to deceive the refugees into believing the promises. Surrendering prisoners' personal belongings were confiscated and some were executed on the spot.[102] The rear of the column lost contact with the front and panic broke out. Many people remained in the Kamenica Hill area for a number of days, unable to move on with the escape route blocked by Serb forces. Thousands of Bosniaks surrendered or were captured. Some prisoners were ordered to summon friends and family members from the woods. There were reports of Serb forces using megaphones to call on the marchers to surrender, telling them that they would be exchanged for Serb soldiers held captive by Bosniak forces. It was at Kamenica that VRS personnel in civilian dress were reported to have infiltrated the column. Men from the rear of the column who survived this ordeal described it as a manhunt.[85] Close to Sandići, on the main road from Bratunac to Konjević Polje, one witness describes the Serbs forcing a Bosniak man to call other Bosniaks down from the mountains. Some 200 to 300 men, including the witness' brother, followed his instructions and descended to meet the VRS, presumably expecting some exchange of prisoners would take place. The witness hid behind a tree to see what would happen next. He watched as the men were lined up in seven ranks, each some forty metres in length, with their hands behind their heads; they were then mowed down by machine gun fire.[104] A small number of women, children and elderly people who had been part of the column were allowed to join the buses evacuating the women and children out of Potočari. Among them was Alma Delimustafić, a soldier of the 28th Brigade; at this time, Delimustafić was in her civilian clothes and was released.[105] The central section of the column managed to escape the shooting and reached Kamenica at about 11:00 hours and waited there for the wounded. Captain Ejub Golić and the Independent Battalion turned back towards Hajdučko Groblje to help the casualties. A number of survivors from the rear, who managed to escape crossed the asphalt roads to the north or the west of the area, had joined those in the central section of the column. The front third of the column, which had already left Kamenica Hill by the time the ambush occurred, headed for Mount Udrč (44°16′59″N19°3′6″E﻿ / ﻿44.28306°N 19.05167°E﻿ / 44.28306; 19.05167﻿ (Mount Udrc)); crossing the main asphalt road, they then forded the river Jadar. They reached the base of the mountain early on the morning of Thursday, 13 July and regrouped. At first, it was decided to send 300 ARBiH soldiers back in an attempt to break through the blockades. When reports came in that the central section of the column had nevertheless succeeded in crossing the road at Konjević Polje, this plan was abandoned. Approximately 1,000 additional men managed to reach Udrč that night.[106] From Udrč the marchers moved toward the River Drinjača and on to Mount Velja Glava, continuing through the night. Finding a Serb presence at Mount Velja Glava, where they arrived on Friday, 14 July, the column was forced to skirt the mountain and wait on its slopes before it was able to move on toward Liplje and Marčići. Arriving at Marčići in the evening of 14 July, the marchers were again ambushed near Snagovo by Serb forces equipped with anti-aircraft guns, artillery, and tanks.[107] According to Lieutenant Džemail Bećirović, the column managed to break through the ambush and, in so doing, captured a VRS officer, Major Zoran Janković—providing the Army of Bosnia and Herzegovina with a significant bargaining counter. This prompted an attempt at negotiating a cessation in the fighting, but negotiations with local Serb forces failed.[108] The evening of 15 July saw the first radio contact between the 2nd Corps and the 28th Division, established using a walkie-talkie captured from the VRS. After initial distrust on the part of the 28th Division, the Šabić brothers were able to identify each other as they stood on either side of the VRS lines. Early in the morning, the column crossed the asphalt road linking Zvornik with Caparde and headed in the direction of Planinci, leaving a unit of some 100 to 200 armed marchers behind to wait for stragglers. [citation needed] The column reached Križevići later that day, and remained there while an attempt was made to negotiate with local Serb forces for safe passage through the Serb lines into Bosnian government controlled territory. The members of the column were advised to stay where they were, and to allow the Serb forces time to arrange for safe passage. It soon became apparent, though, that the small Serb force deployed in the area was only trying to gain time to organise a further attack on the marchers. In the area of Marčići – Crni Vrh, the VRS armed forces deployed 500 soldiers and policemen in order to stop the split part of the column (about 2,500 people), which was moving from Glodi towards Marčići. [citation needed] At this point, the column's leaders decided to form several small groups of between 100 and 200 persons and send these to reconnoiter the way ahead. Early in the afternoon, the 2nd Corps and the 28th Division of the ARBiH met each other in the village of Potočani. The presidium of Srebrenica were the first to reach Bosnian terrain. [citation needed] On the evening of 15 July a heavy hailstorm caused the Serb forces to take cover. The column's advance group took advantage of this to attack the Serb rear lines at Baljkovica. During the fighting, the main body of what remained of the column began to move from Krizevici. It reached the area of fighting at about 3 am on Sunday, 16 July.[citation needed] At approximately 05.00 hours on 16 July, the 2nd Corps made its first attempt to break through the VRS cordon from the Bosnian side. The objective was to force a breakthrough close to the hamlets of Parlog and Resnik. They were joined by Naser Orić and a number of his men. [citation needed] Around 8 am on the morning of 16 July parts of the 28th Division, with the 2nd Corps of the RBiH Army from Tuzla providing artillery support, attacked and breached VRS Army lines. There was fierce fighting across the general area of Baljkovica.[109] Captured heavy arms including two Praga self-propelled anti-aircraft guns were fired at the Serb front line and the column finally succeeded in breaking through to Bosnian government controlled territory and linking up with BiH units at between 1 pm and 2 pm on 16 July. [citation needed] Following radio negotiations between the 2nd Corps and the Zvornik Brigade, the Zvornik Brigade Command, which had lost three lines of trenches, agreed to open a corridor to allow \"evacuation\" of the column in return for the release of captured policemen and soldiers. The Baljkovica corridor was open from 14.00 to 17.00 hours.[110] After the corridor was closed between 17.00 and 18.00 hours the Zvornik Brigade Command reported that around 5000 civilians, with probably \"a certain number of soldiers\" with them had been let through, but \"all those who passed were unarmed\".[111] By 4 August or thereabouts, the ArBiH determined that 3,175 members of the 28th Division had managed to get through to Tuzla. 2,628 members of the Division, soldiers and officers, were considered certain to have been killed. The approximate number of individual members of the column killed was estimated at between 8,300 and 9,722.[112] Once the corridor had closed Serb forces recommenced hunting down parts of the column still in areas under their control. Around 2,000 refugees were reported to be hiding in the woods in the area of Pobuđe.[111] On 17 July 1995, \"searching the terrain\", the VRS Army captured a number of Bosniaks. Four children aged between 8 and 14 captured by the Bratunac Brigade were taken to the military barracks in Bratunac.[111][113] Brigade Commander Blagojević suggested that the Drina Corps' press unit should record this testimony on video.[113] On 18 July, after a soldier was killed \"trying to capture some persons during the search operation\", the Zvornik Brigade Command issued an order to execute prisoners in its zone of responsibility in order to avoid any risks associated with their capture. The order was presumed to have remained effective until it was countermanded on 21 July.[111] According to a 1998 qualitative study involving survivors of the column, many of the members of the column exhibited symptoms of hallucinations to varying degrees.[114] On a number of occasions, Bosniak men began attacking one another in an apparent fear that the other member in the column was a Serb soldier. Members of the column also reported seeing people speaking incoherently, running towards VRS lines in a fit of rage and committing suicide using firearms and hand grenades. Although there was no evidence to suggest what exactly caused the behavior, the study suggested that fatigue and stress may have induced these symptoms.[114] Although Serb forces had long been blamed for the massacre, it was not until June 2004—following the Srebrenica commission's preliminary report—that Serb officials acknowledged that their security forces planned and carried out the mass killing. A Serb commission's final report on the 1995 Srebrenica massacre acknowledged that the mass murder of the men and boys was planned. The commission found that more than 7,800 were killed.[115][116][117] A concerted effort was made to capture all Bosniak men of military age.[118] In fact, those captured included many boys well below that age and elderly men several years above that age who remained in the enclave following the take-over of Srebrenica. These men and boys were targeted regardless of whether they chose to flee to Potočari or to join the Bosnian Muslim column. The operation to capture and detain the Bosnian Muslim men was well organised and comprehensive. The buses which transported the women and children were systematically searched for men.[118] The vast amount of planning and high-level coordination invested in killing thousands of men in a few days is apparent from the scale and the methodical nature in which the executions were carried out. The Army of Republika Srpska took the largest number of prisoners on 13 July, along the Bratunac-Konjević Polje road. It remains impossible to cite a precise figure, but witness statements describe the assembly points such as the field at Sandići, the agricultural warehouses in Kravica, the school in Konjević Polje, the football field in Nova Kasaba, the village of Lolići and the village school of Luke. Several thousand people were herded together in the field near Sandići and on the Nova Kasaba football pitch, where they were searched and put into smaller groups. In a video tape made by journalist Zoran Petrović, a Serb soldier states that at least 3,000 to 4,000 men had given themselves up on the road. By the late afternoon of 13 July, the total had risen to some 6,000 according to the intercepted radio communication; the following day, Major Franken of Dutchbat was given the same figure by Colonel Radislav Janković of the Serb army. Many of the prisoners had been seen in the locations described by passing convoys taking the women and children to Kladanj by bus, while various aerial photographs have since provided evidence to confirm this version of events.[99][118] One hour after the evacuation of the females from Potočari was completed, the Drina Corps staff diverted the buses to the areas in which the men were being held. Colonel Krsmanović, who on 12 July had arranged the buses for the evacuation, ordered the 700 men in Sandići to be collected, and the soldiers guarding them made them throw their possessions on a large heap and hand over anything of value. During the afternoon, the group in Sandići was visited by Mladić who told them that they would come to no harm, that they would be treated as prisoners of war, that they would be exchanged for other prisoners and that their families had been escorted to Tuzla in safety. Some of these men were placed on the transport to Bratunac and other locations, while some were marched on foot to the warehouses in Kravica. The men gathered on the soccer ground at Nova Kasaba were forced to hand over their personal belongings. They too received a personal visit from Mladić during the afternoon of 13 July; on this occasion, he announced that the Bosnian authorities in Tuzla did not want the men and that they were therefore to be taken to other locations. The men in Nova Kasaba were loaded onto buses and trucks and were taken to Bratunac or the other locations.[118] The Bosnian men who had been separated from the women, children and elderly in Potočari numbering approximately 1,000 were transported to Bratunac and subsequently joined by Bosnian men captured from the column.[119] Almost without exception, the thousands of Bosnian prisoners captured, following the take-over of Srebrenica, were executed. Some were killed individually or in small groups by the soldiers who captured them and some were killed in the places where they were temporarily detained. Most, however, were killed in carefully orchestrated mass executions, commencing on 13 July 1995 in the region just north of Srebrenica. The mass executions followed a well-established pattern. The men were first taken to empty schools or warehouses. After being detained there for some hours, they were loaded onto buses or trucks and taken to another site for execution. Usually, the execution fields were in isolated locations. The prisoners were unarmed and in many cases, steps had been taken to minimise resistance, such as blindfolding them, binding their wrists behind their backs with ligatures or removing their shoes. Once at the killing fields, the men were taken off the trucks in small groups, lined up and shot. Those who survived the initial round of shooting were individually shot with an extra round, though sometimes only after they had been left to suffer for a time.[118] The process of finding victim bodies in the Srebrenica region, often in mass graves, exhuming them and finally identifying them was relatively slow. Prior to midday on 13 July, seventeen men were transported by bus a short distance to a spot on the banks of the Jadar River where they were lined up and shot. One man, after being hit in the hip by a bullet, jumped into the river and managed to escape.[120] Skull of a victim of the July 1995 Srebrenica massacre. Exhumed mass grave outside the village of Potočari, Bosnia and Herzegovina. July 2007. The first large-scale mass executions began on the afternoon of 13 July 1995 in the valley of the River Cerska, to the west of Konjević Polje. One witness, hidden among trees, saw two or three trucks, followed by an armoured vehicle and an earthmoving machine proceeding towards Cerska. After that, he heard gunshots for half an hour and then saw the armoured vehicle going in the opposite direction, but not the earthmoving machine. Other witnesses report seeing a pool of blood alongside the road to Cerska that day. Muhamed Duraković, a UN translator, probably passed this execution site later that day. He reports seeing bodies tossed into a ditch alongside the road, with some men still alive.[121][122] Aerial photos and excavations later confirmed the presence of a mass grave near this location. Ammunition cartridges found at the scene reveal that the victims were lined up on one side of the road, whereupon their executioners shot from the other. The bodies—150 in number—were covered with earth where they lay. It could later be established that they had been killed by guns. All were men, between the ages of 14 and 50. All but three of the 150 were wearing civilian clothes. Many had their hands tied behind their backs. Nine could later be identified and were on the list of missing persons from Srebrenica.[121] Later on the afternoon of 13 July executions were also conducted in the largest of four warehouses (farm sheds) owned by the Agricultural Cooperative in Kravica. Between 1,000 and 1,500 men had been captured in fields near Sandići and detained in Sandići Meadow. They were brought to Kravica, either by bus or on foot, the distance being approximately one kilometer. A witness recalls seeing around 200 men, stripped to the waist and with their hands in the air, being forced to run in the direction of Kravica. An aerial photograph taken at 14:00 hours that afternoon shows two buses standing in front of the sheds.[123] At around 18:00 hours, when the men were all held in the warehouse, VRS soldiers threw in hand grenades and fired with various weapons, including rocket propelled grenades. The mass murder in Kravica seemed \"well organised and involved a substantial amount of planning, requiring the participation of the Drina Corps Command.\"[123] Supposedly, there was more killing in and around Kravica and Sandići. Even before the murders in the warehouse, some 200 or 300 men were formed up in ranks near Sandići and then were executed en masse with concentrated machine gun fire. At Kravica, it was claimed that some local men assisted the killings. Some victims were mutilated and killed with knives. The bodies were taken to Bratunac or simply dumped in the river that runs alongside the road. One witness stated that this all took place on 14 July. There were three survivors of the mass murder in the farm sheds at Kravica.[123] Armed guards shot at the men who tried to climb out the windows to escape the massacre. When the shooting stopped, the shed was full of bodies. Another survivor, who was only slightly wounded, reports: I was not even able to touch the floor, the concrete floor of the warehouse.... After the shooting, I felt a strange kind of heat, warmth, which was coming from the blood that covered the concrete floor and I was stepping on the dead people who were lying around. But there were even men (just men) who were still alive, who were only wounded and as soon as I would step on him, I would hear him cry, moan, because I was trying to move as fast as I could. I could tell that people had been completely disembodied and I could feel bones of the people that had been hit by those bursts of bullets or shells, I could feel their ribs crushing. Then I would get up again and continue....[70] When this witness climbed out of a window, he was seen by a guard who shot at him. He pretended to be dead and managed to escape the following morning. The other witness quoted above spent the night under a heap of bodies; the next morning, he watched as the soldiers examined the corpses for signs of life. The few survivors were forced to sing Serbian songs, and were then shot. Once the final victim had been killed, an excavator was driven in to shunt the bodies out of the shed; the asphalt outside was then hosed down with water. In September 1996, however, it was still possible to find the evidence.[123] Analyses of hair, blood and explosives residue collected at the Kravica Warehouse provide strong evidence of the killings. Experts determined the presence of bullet strikes, explosives residue, bullets and shell cases, as well as human blood, bones and tissue adhering to the walls and floors of the building. Forensic evidence presented by the ICTY Prosecutor established a link between the executions in Kravica and the 'primary' mass grave known as Glogova 2, in which the remains of 139 people were found. In the 'secondary' grave known as Zeleni Jadar 5 there were 145 bodies, a number of which were charred. Pieces of brick and window frame which were found in the Glogova 1 grave that was opened later also established a link with Kravica. Here, the remains of 191 victims were found.[123] As the buses crowded with Bosnian women, children and elderly made their way from Potočari to Kladanj, they were stopped at Tišća village, searched, and the Bosnian men and boys found on board were removed from the bus. The evidence reveals a well-organised operation in Tišća.[124] From the checkpoint, an officer directed the soldier escorting the witness towards a nearby school where many other prisoners were being held. At the school, a soldier on a field telephone appeared to be transmitting and receiving orders. Sometime around midnight, the witness was loaded onto a truck with 22 other men with their hands tied behind their backs. At one point the truck stopped and a soldier on the scene said: \"Not here. Take them up there, where they took people before.\" The truck reached another stopping point where the soldiers came around to the back of the truck and started shooting the prisoners. The survivor escaped by running away from the truck and hiding in a forest.[124] A large group of the prisoners who had been held overnight in Bratunac were bussed in a convoy of 30 vehicles to the Grbavci school in Orahovica early in the morning of 14 July 1995. When they got there, the school gym was already half-filled with prisoners who had been arriving since the early morning hours and within a few hours, the building was completely full. Survivors estimated that there were 2,000 to 2,500 men there, some of them very young and some quite elderly, although the ICTY Prosecution suggested this may have been an over-estimation and that the number of prisoners at this site was probably closer to 1,000. Some prisoners were taken outside and killed. At some point, a witness recalled, General Mladić arrived and told the men: \"Well, your government does not want you and I have to take care of you.\"[125] After being held in the gym for several hours, the men were led out in small groups to the execution fields that afternoon. Each prisoner was blindfolded and given a drink of water as he left the gym. The prisoners were then taken in trucks to the execution fields less than one kilometre away. The men were lined up and shot in the back; those who survived the initial shooting were killed with an extra shot. Two adjacent meadows were used; once one was full of bodies, the executioners moved to the other. While the executions were in progress, the survivors said, earth-moving equipment was digging the graves. A witness who survived the shootings by pretending to be dead, reported that General Mladić drove up in a red car and watched some of the executions.[125] The forensic evidence supports crucial aspects of the survivors' testimony. Both aerial photos show that the ground in Orahovac was disturbed between 5 and 27 July 1995 and again between 7 and 27 September 1995. Two primary mass graves were uncovered in the area and were named Lazete 1 and Lazete 2 by investigators.[125] The Lazete 1 gravesite was exhumed by the ICTY Prosecution between 13 July and 3 August 2000. All of the 130 individuals uncovered, for whom sex could be determined, were male; 138 blindfolds were uncovered in the grave. Identification material for 23 persons, listed as missing following the fall of Srebrenica, was located during the exhumations at this site. The gravesite Lazete 2 was partly exhumed by a joint team from the Office of the Prosecutor and Physicians for Human Rights between August and September 1996 and completed in 2000. All of the 243 victims associated with Lazete 2 were male and the experts determined that the vast majority died of gunshot injuries. In addition, 147 blindfolds were located.[125] Forensic analysis of soil/pollen samples, blindfolds, ligatures, shell cases and aerial images of creation/disturbance dates, further revealed that bodies from the Lazete 1 and 2 graves were removed and reburied at secondary graves named Hodžići Road 3, 4 and 5. Aerial images show that these secondary gravesites were created between 7 September and 2 October 1995 and all of them were exhumed in 1998.[125] Delegates of the International Association of Genocide Scholars (IAGS) examine an exhumed mass grave of victims of the July 1995 Srebrenica massacre, outside the village of Potočari, Bosnia and Herzegovina. July 2007. On 14 and 15 July 1995, another large group of prisoners numbering some 1,500 to 2,000 were taken from Bratunac to the school in Petkovići. The conditions under which these men were held at the Petkovići school were even worse than those in Grbavci. It was hot, overcrowded and there was no food or water. In the absence of anything else, some prisoners chose to drink their own urine. Every now and then, soldiers would enter the room and physically abuse prisoners, or would call them outside. A few of the prisoners contemplated an escape attempt, but others said it would be better to stay since the International Red Cross would be sure to monitor the situation and they could not all be killed.[126] The men were called outside in small groups. They were ordered to strip to the waist and to remove their shoes, whereupon their hands were tied behind their backs. During the night of 14 July, the men were taken by truck to the dam at Petkovići. Those who arrived later could see immediately what was happening there. A large number of bodies were strewn on the ground, their hands tied behind their backs. Small groups of five to ten men were taken out of the trucks, lined up and shot. Some begged for water but their pleas were ignored.[126] A survivor described his feelings of fear combined with thirst thus: I was really sorry that I would die thirsty, and I was trying to hide amongst the people as long as I could, like everybody else. I just wanted to live for another second or two. And when it was my turn, I jumped out with what I believe were four other people. I could feel the gravel beneath my feet. It hurt.... I was walking with my head bent down and I wasn't feeling anything.... And then I thought that I would die very fast, that I would not suffer. And I just thought that my mother would never know where I had ended up. This is what I was thinking as I was getting out of the truck. [As the soldiers walked around to kill the survivors of the first round of shooting] I was still very thirsty. But I was sort of between life and death. I didn't know whether I wanted to live or to die anymore. I decided not to call out for them to shoot and kill me, but I was sort of praying to God that they'd come and kill me.[70] After the soldiers had left, two survivors helped each other to untie their hands, and then crawled over the heap of bodies towards the woods, where they intended to hide. As dawn arrived, they could see the execution site where bulldozers were collecting the bodies. On the way to the execution site, one of the survivors had peeked out from under his blindfold and had seen that Mladić was also on his way to the scene.[70] Aerial photos confirmed that the earth near the Petkovići dam had been disturbed, and that it was disturbed yet again sometime between 7 and 27 September 1995. When the grave here was opened in April 1998, there seemed to be many bodies missing. Their removal had been accomplished with mechanical apparatus, causing considerable disturbance to the grave and its contents. At this time, the grave contained the remains of no more than 43 persons. Other bodies had been removed to a secondary grave, Liplje 2, prior to 2 October 1995. Here, the remains of at least 191 individuals were discovered.[70] On 14 July 1995, more prisoners from Bratunac were bussed northward to a school in the village of Pilica, north of Zvornik. As at other detention facilities, there was no food or water and several men died in the school gym from heat and dehydration. The men were held at the Pilica school for two nights. On 16 July 1995, following a now familiar pattern, the men were called out of the school and loaded onto buses with their hands tied behind their backs. They were then driven to the Branjevo Military Farm, where groups of 10 were lined up and shot.[127] Dražen Erdemović—who confessed killing at least 70 Bosniaks—was a member of the VRS 10th Sabotage Detachment (a Main Staff subordinate unit) and participated in mass executions. Erdemović appeared as a prosecution witness and testified: \"The men in front of us were ordered to turn their backs. When those men turned their backs to us, we shot at them. We were given orders to shoot.\"[128] On this point, one of the survivors recalls: When they shot, I threw myself on the ground... one man fell on my head. I think that he was killed on the spot. I could feel the hot blood pouring over me.... I could hear one man crying for help. He was begging them to kill him. And they simply said \"Let him suffer. We'll kill him later.\" Erdemović said that all but one of the victims wore civilian clothes and that, except for one person who tried to escape, they offered no resistance before being shot. Sometimes the executioners were particularly cruel. When some of the soldiers recognised acquaintances from Srebrenica, they beat and humiliated them before killing them. Erdemović had to persuade his fellow soldiers to stop using a machine gun for the killings; while it mortally wounded the prisoners it did not cause death immediately and prolonged their suffering.[128] Between 1,000 and 1,200 men were killed in the course of that day at this execution site.[130] Aerial photographs, taken on 17 July 1995 of an area around the Branjevo Military Farm, show a large number of bodies lying in the field near the farm, as well as traces of the excavator that collected the bodies from the field.[131] Erdemović testified that, at around 15:00 hours on 16 July 1995 after he and his fellow soldiers from the 10th Sabotage Detachment had finished executing the prisoners at the Branjevo Military Farm, they were told that there was a group of 500 Bosnian prisoners from Srebrenica trying to break out of a nearby Dom Kultura club. Erdemović and the other members of his unit refused to carry out any more killings. They were then told to attend a meeting with a Lieutenant Colonel at a café in Pilica. Erdemović and his fellow-soldiers travelled to the café as requested and, as they waited, they could hear shots and grenades being detonated. The sounds lasted for approximately 15–20 minutes after which a soldier from Bratunac entered the café to inform those present that \"everything was over\".[132] There were no survivors to explain exactly what had happened in the Dom Kultura.[132] The executions at the Dom Kultura were remarkable in that this was no remote spot but a location in the centre of town on the main road from Zvornik to Bijeljina.[133] Over a year later, it was still possible to find physical evidence of this crime. As in Kravica, many traces of blood, hair and body tissue were found in the building, with cartridges and shells littered throughout the two storeys.[134] It could also be established that explosives and machine guns had been used. Human remains and personal possessions were found under the stage, where blood had dripped down through the floorboards. Two of the three survivors of the executions at the Branjevo Military Farm were arrested by local Bosnian Serb police on 25 July and sent to the prisoner of war compound at Batkovici. One had been a member of the group separated from the women in Potočari on 13 July. The prisoners who were taken to Batkovici survived the ordeal.[135] and were later able to testify before the Tribunal.[136] Čančari Road 12 was the site of the re-interment of at least 174 bodies, moved here from the mass grave at the Branjevo Military Farm.[137] Only 43 were complete sets of remains, most of which established that death had taken place as the result of rifle fire. Of the 313 various body parts found, 145 displayed gunshot wounds of a severity likely to prove fatal.[138] The exact date of the executions at Kozluk is not known, although it can be narrowed down to the period of 14 to 17 July 1995. The most probable dates are 15 and 16 July, not least due to the geographic location of Kozluk, between Petkovići Dam and the Branjevo Military Farm. It therefore falls within the pattern of ever more northerly execution sites: Orahovac on 14 July, Petkovići Dam on 15 July, the Branjevo Military Farm and the Pilica Dom Kultura on 16 July.[139] Another indication is that a Zvornik Brigade excavator spent eight hours in Kozluk on 16 July and a truck belonging to the same brigade made two journeys between Orahovac and Kozluk that day. A bulldozer is known to have been active in Kozluk on 18 and 19 July.[140] Among Bosnian refugees in Germany, there were rumors of executions in Kozluk, during which the five hundred or so prisoners were forced to sing Serbian songs as they were being transported to the execution site. Although no survivors have since come forward, investigations in 1999 led to the discovery of a mass grave near Kozluk.[141] This proved to be the actual location of an execution as well, and lay alongside the Drina accessible only by driving through the barracks occupied by the Drina Wolves, a regular police unit of Republika Srpska. The grave was not dug specifically for the purpose: it had previously been a quarry and a landfill site. Investigators found many shards of green glass which the nearby 'Vitinka' bottling plant had dumped there. This facilitated the process of establishing links with the secondary graves along Čančari Road.[142] The grave at Kozluk had been partly cleared some time prior to 27 September 1995 but no fewer than 340 bodies were found there nonetheless.[143] In 237 cases, it was clear that they had died as the result of rifle fire: 83 by a single shot to the head, 76 by one shot through the torso region, 72 by multiple bullet wounds, five by wounds to the legs and one person by bullet wounds to the arm. The ages of the victims were between 8 and 85 years old. Some had been physically disabled, occasionally as the result of amputation. Many had clearly been tied and bound using strips of clothing or nylon thread.[142] Along the Čančari Road are twelve known mass graves, of which only two—Čančari Road 3 and 12—have been investigated in detail (as of 2000[update]).[144] Čančari Road 3 is known to have been a secondary grave linked to Kozluk, as shown by the glass fragments and labels from the Vitinka factory.[145] The remains of 158 victims were found here, of which 35 bodies were still more or less intact and indicated that most had been killed by gunfire.[146] The men who were found attempting to escape by the Bratunac-Konjević Polje road were told that the Geneva Convention would be observed if they gave themselves up.[148] In Bratunac, men were told that there were Serbian personnel standing by to escort them to Zagreb for an exchange of prisoners. The visible presence of UN uniforms and UN vehicles, stolen from Dutchbat, were intended to contribute to the feeling of reassurance. On 17 to 18 July, Serb soldiers captured about 150–200 Bosnians in the vicinity of Konjevic Polje and summarily executed about one half of them.[147] After the closure of the corridor at Baljkovica, several groups of stragglers nevertheless attempted to escape into Bosnian territory. Most were captured by VRS troops in the Nezuk–Baljkovica area and killed on the spot. In the vicinity of Nezuk, about 20 small groups surrendered to Bosnian Serb military forces. After the men surrendered, Bosnian Serb soldiers ordered them to line up and summarily executed them.[104][147] On 19 July, for example, a group of approximately 11 men was killed at Nezuk itself by units of the 16th Krajina Brigade, then operating under the direct command of the Zvornik Brigade. Reports reveal that a further 13 men, all ARBiH soldiers, were killed at Nezuk on 19 July.[149] The report of the march to Tuzla includes the account of an ARBiH soldier who witnessed several executions carried out by police that day. He survived because 30 ARBiH soldiers were needed for an exchange of prisoners following the ARBiH's capture of a VRS officer at Baljkovica. The soldier was himself exchanged late 1995; at that time, there were still 229 men from Srebrenica in the Batkovici prisoner of war camp, including two men who had been taken prisoner in 1994.[citation needed] At the same time, RS Ministry of the Interior forces conducting a search of the terrain from Kamenica as far as Snagovo killed eight Bosniaks.[150] Around 200 Muslims armed with automatic and hunting rifles were reported to be hiding near the old road near Snagovo.[150] During the morning, about 50 Bosniaks attacked the Zvornik Brigade line in the area of Pandurica, attempting to break through to Bosnian government territory.[150] The Zvornik Public Security Centre planned to surround and destroy these two groups the following day using all available forces.[151] According to ICTY indictments of Radovan Karadžić and Ratko Mladić, on 20 to 21 July 1995 near the village of Meces, VRS personnel, using megaphones, urged Bosniak men who had fled Srebrenica to surrender and assured them that they would be safe. Approximately 350 men responded to these entreaties and surrendered. The soldiers then took approximately 150 of them, instructed them to dig their own graves and summarily executed them.[152] ICMP's Podrinje Identification Project (PIP) was formed to deal with the identification primarily of victims of 1995 Srebrenica massacre. PIP includes a facility for storing, processing, and handling exhumed remains. Much of the remains are only fragments or commingled body fragments since they were recovered from secondary mass graves. The photo depicts one section of the refrigerated mortuary. During the days following the massacre, American spy planes overflew the area of Srebrenica, and took photos showing the ground in vast areas around the town had been removed, a sign of mass burials. On 22 July, the commanding officer of the Zvornik Brigade, Lieutenant Colonel Vinko Pandurević, requested the Drina Corps to set up a committee to oversee the exchange of prisoners. He also asked for instructions where the prisoners of war his unit had already captured should be taken and to whom they should be handed over. Approximately 50 wounded captives were taken to the Bratunac hospital. Another group of prisoners was taken to the Batkovići camp (near Bijeljina), and these were mostly exchanged later.[153] On 25 July, the Zvornik Brigade captured 25 more ARBiH soldiers who were taken directly to the camp at Batkovići, as were 34 ARBiH men captured the following day. Zvornik Brigade reports up until 31 July continue to describe the search for refugees and the capture of small groups of Bosniaks.[154] A number of Bosniaks managed to cross over the River Drina into Serbia at Ljubovija and Bajina Bašta. 38 of them were returned to RS. Some were taken to the Batkovići camp, where they were exchanged. The fate of the majority has not been established.[153] Some of those attempting to cross the Drina drowned.[153] By 17 July 1995, 201 Bosniak soldiers had arrived in Žepa, exhausted and many with light wounds.[153] By 28 July another 500 had arrived in Žepa from Srebrenica.[153][155] After 19 July 1995, small Bosniak groups were hiding in the woods for days and months, trying to reach Tuzla.[153] Numerous refugees found themselves cut off for some time in the area around Mount Udrc.[156][157] They did not know what to do next or where to go; they managed to stay alive by eating vegetables and snails.[156][157] The MT Udrc had become to a place for ambushing marchers, and the Bosnian Serbs swept through this area too, and according to one survivor they killed many people there.[156][157] Meanwhile, the VRS had commenced the process of clearing the bodies from around Srebrenica, Žepa, Kamenica and Snagovo. Work parties and municipal services were deployed to help.[157][158] In Srebrenica, the refuse that had littered the streets since the departure of the people was collected and burnt, the town disinfected and deloused.[157][158] Many people in the part of the column which had not succeeded in passing Kamenica did not wish to give themselves up and decided to turn back towards Žepa.[159] Others remained where they were, splitting up into smaller groups of no more than ten.[160] Some wandered around for months, either alone or groups of two, four or six men.[160] Once Žepa had succumbed to the Serb pressure, they had to move on once more, either trying to reach Tuzla or crossing the River Drina into Serbia.[161] Zvornik 7 The most famous group of seven men wandered about in occupied territory for the entire winter. On 10 May 1996, after nine months on the run and over six months after the end of the war, they were discovered in a quarry by American IFOR soldiers. They immediately turned over to the patrol; they were searched and their weapons (two pistols and three hand grenades) were confiscated. The men said that they had been in hiding in the immediate vicinity of Srebrenica since the fall of the enclave. They did not look like soldiers and the Americans decided that this was a matter for the police.[162] The operations officer of this American unit ordered that a Serb patrol should be escorted into the quarry whereupon the men would be handed over to the Serbs. The prisoners said they were initially tortured after the transfer, but later were treated relatively well. In April 1997 the local court in Republika Srpska convicted the group, known as the Zvornik 7, for illegal possession of firearms and three of them for the murder of four Serbian woodsmen. When announcing the verdict the presenter of the TV of Republika Srpska described them as the group of Muslim terrorists from Srebrenica who last year massacred Serb civilians.[163] The trial was widely condemned by the international community as \"a flagrant miscarriage of justice\",[164][165] and the conviction was later quashed for 'procedural reasons' following pressure from the international community. In 1999, the three remaining defendants in the Zvornik 7 case were swapped for three Serbs serving 15 years each in a Bosnian prison. From approximately 1 August 1995 to 1 November 1995, there was an organised effort to remove the bodies from primary mass gravesites and transport them to secondary and tertiary gravesites.[166] In the ICTY court case \"Prosecutor v. Blagojević and Jokić\", the trial chamber found that this reburial effort was an attempt to conceal evidence of the mass murders.[167] The trial chamber found that the cover up operation was ordered by the VRS Main Staff and subsequently carried out by members of the Bratunac and Zvornik Brigades.[167] The cover-up operation has had a direct impact on the recovery and identification of the remains. The removal and reburial of the bodies have caused them to become dismembered and co-mingled, making it difficult for forensic investigators to positively identify the remains.[168] For example, in one specific case, the remains of one person were found in two different locations, 30 km apart.[169] In addition to the ligatures and blindfolds found at the mass graves, the effort to hide the bodies has been seen as evidence of the organised nature of the massacres and the non-combatant status of the victims, since had the victims died in normal combat operations, there would be no need to hide their remains.[168][170] According to Agence France Presse (AFP), a dozen Greek volunteers fought alongside the Serbs at Srebrenica.[171] They were members of the Greek Volunteer Guard (ΕΕΦ), or GVG, a contingent of Greek paramilitaries formed at the request of Ratko Mladić as an integral part of the Drina Corps. The Greek volunteers were motivated by the desire to support their \"Orthodox brothers\" in battle.[172] They raised the Greek flag at Srebrenica after the fall of the town at Mladić's request, to honour \"the brave Greeks fighting on our side.\"[173] Radovan Karadžić subsequently decorated four of them.[174][175][176][177] In 2009, Stavros Vitalis [el] announced that the volunteers were suing the writer Takis Michas for libel over allegations in his book Unholy Alliance, in which Michas described aspects of the Greek state's tacit support for the Serbs during the Bosnian War. Insisting that the volunteers had simply taken part in what he described as the \"re-occupation\" of the town, Vitalis acknowledged that he himself was present with senior Serb officers in \"all operations\" for Srebrenica's re-occupation by the Serbs.[180][181] Michas said that the volunteers were treated like heroes and at no point did Greek justice contact them to investigate their knowledge of potential crimes to assist the work of the ICTY at The Hague.[182] On 16 November 1995 Radovan Karadžić, \"President of the Republika Srpska\" and Ratko Mladić, Commander of the VRS, were indicted by the ICTY for their alleged direct responsibility for the war crimes committed in July 1995 against the Bosnian Muslim population of Srebrenica.[58] In 1999, UN Secretary-General Kofi Annan submitted his report on the Fall of Srebrenica. In it, he acknowledged that the international community as a whole had to accept its share of responsibility for its response to the ethnic cleansing campaign that culminated in the murder of some 7,000 unarmed civilians from the town designated by the Security Council as a \"safe area.\"[58][183][184] The failure of Dutchbat to protect the Srebrenica enclave became a national trauma in the Netherlands and led to long-running discussions in the Netherlands.[185] In 1996, the Dutch government asked the Netherlands Institute for War Documentation to conduct research into the events before, during and after the fall of Srebrenica. The resulting report was published in 2002—Srebrenica: a 'safe' area.[186] It concluded that the Dutchbat mission was not well considered and well-nigh impossible. The NIOD report is cited often, however, the Institute for War and Peace Reporting labelled the report \"controversial\", as \"the sheer abundance of information makes it possible for anyone to pluck from it whatever they need to make their point\". One of the authors of the report claimed some of the sources were \"unreliable\", and were only used to support another author's argument.[187] As a result of the report, the Dutch government accepted partial political responsibility for the circumstances in which the massacre happened[188] and the second cabinet of Wim Kok resigned in 2002.[189][190] In September 2002, the Republika Srpska Office of Relations with the ICTY issued the \"Report about Case Srebrenica\". The document, authored by Darko Trifunović, was endorsed by many leading Bosnian Serb politicians. It concluded that 1,800 Bosnian Muslim soldiers died during fighting and a further 100 more died as a result of exhaustion. \"The number of Muslim soldiers killed by Bosnian Serbs out of personal revenge or lack of knowledge of international law is probably about 100...It is important to uncover the names of the perpetrators in order to accurately and unequivocally establish whether or not these were isolated instances.\" The report also examined the mass graves, claiming that they were made for hygiene reasons, question the legitimacy of the missing person lists and undermine a key witness' mental health and military history.[191] The International Crisis Group and the United Nations condemned the manipulation of their statements in this report.[192] On 30 September 2003, former US President Bill Clinton officially opened the Srebrenica Genocide memorial to honour the victims of the genocide. The total cost of the project was around $5.8 million. \"We must pay tribute to the innocent lives, many of them children who were snuffed out in what must be called genocidal madness\", Clinton said.[193][194] On 7 March 2003, the Human Rights Chamber for Bosnia and Herzegovina issued a decision which ordered the Republika Srpska, among other things, to conduct a full investigation into the Srebrenica events, and disclose the results at the latest on 7 September 2003.[195] The Chamber had no coercive power to implement the decision, especially because it ceased to exist in late 2003.[115] The RS then published two reports, on 3 June 2003 and 5 September 2003, which the Human Rights Chamber concluded did not fulfill the obligations of the RS.[115] On 15 October 2003, The High Representative, Paddy Ashdown, lamented that \"getting the truth from the [Bosnian Serb] government is like extracting rotten teeth\". Ashdown did, however, welcome a recommendation in the September report to form an independent commission to investigate the Srebrenica events and issue a report within six months.[196] The Srebrenica commission, officially titled the Commission for Investigation of the Events in and around Srebrenica between 10 and 19 July 1995, was established in December 2003, and submitted its final report[197] on 4 June 2004, and then an addendum[198] on 15 October 2004 after delayed information was supplied.[115][116] The report acknowledged that at least 7,000 men and boys were killed by Bosnian Serb forces, citing a provisional figure of 7,800.[199] In the report, because of \"limited time\" and to \"maximize resources\", the commission \"accepted the historical background and the facts stated in the second-instance judgment 'Prosecutor vs. Radislav Krstić', when the ICTY convicted the accused for 'assisting and supporting genocide' committed in Srebrenica\".[197] The findings of the commission remain generally disputed by Serb nationalists, who claim it was heavily pressured by the High Representative, given that an earlier RS government report which exonerated the Serbs was dismissed. Nevertheless, Dragan Čavić, the president of Republika Srpska, acknowledged in a televised address that Serb forces killed several thousand civilians in violation of the international law, and asserted that Srebrenica was a dark chapter in Serb history.[200] On 10 November 2004, the government of Republika Srpska issued an official apology. The statement came after a government review of the Srebrenica commission's report. \"The report makes it clear that enormous crimes were committed in the area of Srebrenica in July 1995. The Bosnian Serb Government shares the pain of the families of the Srebrenica victims, is truly sorry and apologises for the tragedy\", the Bosnian Serb government said.[117] After a request by Ashdown, the RS established a working group to implement the recommendations of the report by the Srebrenica commission. The group was to analyze the documentation in the report's confidential annexes and identify all the possible perpetrators who were officials in the institutions of the RS.[201] A report on 1 April 2005 identified 892 such persons still employed by the RS, and the information was provided to the State Prosecutor of Bosnia and Herzegovina with the understanding that the names would not be made public until official proceedings had been opened.[201] On 4 October 2005, the working group said they had identified 25,083 people who were involved in the massacre, including 19,473 members of various Bosnian Serb armed forces that actively gave orders or directly took part in the massacre.[202] On 1 June 2005, video evidence was introduced at the Slobodan Milošević trial to testify to the involvement of members of police units from Serbia in the Srebrenica massacre.[203] The video, the only undestroyed copy of twenty and previously available for rental in the Serbian town of Šid, was obtained and submitted to the ICTY by Nataša Kandić, director of the Belgrade-based Humanitarian Law Center.[204] The video footage (starting about 2hr 35 min. into the proceedings) shows an Orthodox priest blessing several members of a Serbian unit known as the \"Scorpions.\" Later these soldiers are shown visibly abusing civilians physically. They were later identified as four minors as young as 16 and two men in their early twenties. The footage then shows the execution of four of the civilians and shows them lying dead in the field. At this point the cameraman expresses disappointment that the camera's battery is almost out. The soldiers then ordered the two remaining captives to take the four dead bodies into a nearby barn, where they were also killed upon completing this task.[203][204] The video caused public outrage in Serbia. In the days following its showing, the Serbian government arrested some of the former soldiers identified on the video. The event was extensively covered by the newspaper Danas and radio and television station B92. Nura Alispahić, mother of the 16-year-old Azmir Alispahić, saw the execution of her son on television.[205] She said that she was already aware of her son's death and said she had been told that his body was burned following the execution; his remains were among those buried in Potočari in 2003.[206][207] The executions took place on 16/17 July, in Trnovo, about 30 minutes from the Scorpions' base near Sarajevo.[204] On 10 April 2007, a special war crimes court in Belgrade convicted four former members of the Scorpions of war crimes, treating the killings as an isolated war crime unrelated to the Srebrenica genocide and ignoring the allegations that the Scorpions were acting under the authority of the Serbian Interior Ministry, MUP.[208] ...the policies of aggression and ethnic cleansing as implemented by Serb forces in Bosnia and Herzegovina from 1992 to 1995 with the direct support of Serbian regime of Slobodan Milošević and its followers ultimately led to the displacement of more than 2,000,000 people, an estimated 200,000 killed, tens of thousands raped or otherwise tortured and abused, and the innocent civilians of Sarajevo and other urban centres repeatedly subjected to shelling and sniper attacks; meet the terms defining the crime of genocide in Article 2 of the Convention on the Prevention and Punishment of the Crime of Genocide, created in Paris on 9 December 1948, and entered into force on 12 January 1951.[210] On 6 July 2005, Missouri passed a resolution recognising the Srebrenica Genocide.[211] On 11 July 2005, St. Louis issued a proclamation declaring 11 July Srebrenica Remembrance Day there.[212] On 6 July 2005, Bosnian Serb police found two powerful bombs at the memorial site just days ahead of a ceremony to mark the massacre's 10th anniversary, when 580 identified victims were to be buried during the ceremony and more than 50,000 people, including international politicians and diplomats, were expected to attend. The bombs would have caused widespread loss of life and injury had they exploded.[213][214] In his message to the 10th anniversary commemoration at Potočari, the UN Secretary-General at the time, Kofi Annan, paid tribute to the victims of \"a terrible crime – the worst on European soil since the Second World War\", on a date \"marked as a grim reminder of man's inhumanity to man\". He said that the \"first duty of the international community was to uncover and confront the full truth about what happened, a hard truth for those who serve the United Nations, because great nations failed to respond adequately. There should have been stronger military forces in place, and a stronger will to use them\".[215] A boy at a grave during the 2006 funeral of genocide victims Annan added that while the blame lay first and foremost with those who had planned and carried out the massacre, the UN also bore its share of responsibility, having made serious errors of judgement, \"rooted in a philosophy of impartiality and non-violence which, however admirable, was unsuited to the conflict in Bosnia; because of that the tragedy of Srebrenica would haunt the UN's history forever.\"[215] He called on all Bosnians to search for truth and reconciliation and committed the UN to helping the people of Bosnia and Herzegovina to \"secure a peaceful, prosperous future among the family of nations.\"[183] By 2006, 42 mass graves have been uncovered around Srebrenica and the specialists believe there are 22 more mass graves. The victims identified number 2,070 while body parts in more than 7,000 bags still await identification.[216] On 11 August 2006 over 1,000 body parts were exhumed from one of Srebrenica mass graves located in Kamenica.[217] On 24 August 2006, the Sarajevo daily newspaper Oslobođenje started publishing a list of 892 Bosnian Serbs who had allegedly participated in the Srebrenica massacre and who were believed to be still employed by government and municipal institutions. The names of these individuals were listed among 28,000 Bosnian Serbs reported to have taken part in the massacre by the official Republika Srpska report on Srebrenica. The list had been withheld from publication with the report by the chief prosecutor of the Bosnian War Crimes Chamber, Marinko Jurčević who claimed that \"publishing this information might jeopardise the ongoing investigations\".[218][219] In December 2006, the Dutch government awarded the Dutch UN peacekeepers who served in Srebrenica an insignia because they believed they \"deserved recognition for their behaviour in difficult circumstances\", also noting the limited mandate and the ill-equipped nature of the mission. However, survivors and relatives of the victims condemned the move, calling it a \"humiliating decision\" and responded with protest rallies in The Hague, Assen (where the ceremony took place) and Bosnia's capital Sarajevo.[220] Women at the monument for victims of the July 1995 Srebrenica Massacre. At the annual memorial ceremony in Potočari, Bosnia and Herzegovina, 11 July 2007 On 31 May 2007, former Bosnian Serb general Zdravko Tolimir was apprehended by police from Serbia and the Bosnian Serb republic, turned over to NATO forces at the Banja Luka airport where he was read the ICTY indictment and formally arrested. On 1 June 2007 NATO forces conveyed him to Rotterdam where he was turned over to the ICTY in The Hague. As of July 2010[update], Tolimir was being tried by the ICTY on charges of genocide, conspiracy to commit genocide, extermination, persecution and forcible transfer. The indictment accuses Tolimir of participating in the \"joint criminal enterprise to remove the Muslim population\" from Srebrenica as well as the enclave of Zepa.[221][222] On 12 December 2012, Tolimir was convicted of genocide and sentenced to life imprisonment.[223] Radovan Karadžić, with similar charges as Zdravko Tolimir, was arrested in Belgrade on 21 July 2008 (after 13 years on the run) and brought before Belgrade's War Crimes Court.[224] He was transferred to the ICTY on 30 July 2008.[225] As of July 2010[update] Karadžić was being tried at the ICTY on 11 charges of genocide, war crimes and crimes against humanity.[226][227] In 2016, he was found guilty of 10 of the 11 charges in total, and sentenced to 40 years' imprisonment.[228][229] In 2019, an appeal he had filed against his conviction was rejected, and the sentence was increased to life imprisonment.[230] On 15 January 2009, the Parliament of the European Union voted with overwhelming majority of 556 votes in favour, 9 against and 22 abstentions on a resolution calling for the recognition of 11 July as a day for EU commemoration of the Srebrenica genocide.[231] Bosnian Serb politicians rejected the resolution, stating that such a commemoration is unacceptable to the Republika Srpska.[232] In late March 2010, Serbian Parliament passed a resolution condemning the Srebrenica massacre and apologizing for Serbia not doing more to prevent the tragedy. The motion was passed by a narrow margin with 127 out of 250 MPs voting in favor, with 173 legislators present during the vote. The Socialist Party of Serbia, formerly under Slobodan Milošević and now under new leadership, voted in favor of adopting the resolution. Opposition parties, in turn, expressed their discontent with the resolution claiming its text was \"shameful\" for Serbia, either claiming the wording was too strong or too weak.[233] Some relatives of Bosniak victims were also unhappy with the apology, as it did not use the word 'genocide', but rather pointed at the Bosnian Genocide case ruling of the International Court of Justice.[234] Serbian president, Boris Tadić, said that the declaration is the highest expression of patriotism and that it represents distancing from crimes.[235]Sulejman Tihić, former Bosniak member of the Presidency of Bosnia and Herzegovina and the current President of the House of Peoples of Bosnia and Herzegovina stated that now Bosnia and Herzegovina must adopt a similar resolution condemning crimes against Serbs and Croats.[236] On 25 April 2013, President Tomislav Nikolić apologised for the massacre: \"I kneel and ask for forgiveness for Serbia for the crime committed in Srebrenica. I apologise for the crimes committed by any individual in the name of our state and our people.\"[237] Bosniak mourners at the reburial ceremony for an exhumed victim of the Srebrenica massacre On 21 April 2010, the government of Milorad Dodik, the prime minister of Republika Srpska, initiated a revision of the 2004 report saying that the numbers of killed were exaggerated and the report was manipulated by a former peace envoy.[238] The Office of the High Representative responded by saying: \"The Republika Srpska government should reconsider its conclusions and align itself with the facts and legal requirements and act accordingly, rather than inflicting emotional distress on the survivors, torture history and denigrate the public image of the country\".[239] On 12 July 2010, at the 15th anniversary of the massacre, Milorad Dodik said that he acknowledged the killings that happened on the site, but did not regard what happened at Srebrenica as genocide.[240] On 26 May 2011, Ratko Mladić was arrested in Lazarevo, Serbia, on similar charges to Zdravko Tolimir and Radovan Karadžić after remaining at large for nearly 16 years sheltered initially by Serbian and Bosnian Serb security forces and later by family. His capture was considered to be one of the pre-conditions for Serbia being awarded candidate status for European Union membership. On 31 May 2011 he was extradited to The Hague, where he was processed at the detention center that holds suspects for the ICTY. His trial formally began in The Hague on 16 May 2012, and on 22 November 2017 he was sentenced to life in prison by the ICTY after being found guilty of 10 charges: one of genocide, five of crimes against humanity and four of violations of the laws or customs of war. He was cleared of one count of genocide. As the top military officer with command responsibility, Mladić was deemed by the ICTY to be responsible for both the Srebrenica massacre and the siege of Sarajevo. In a judgement dated 6 September 2013, Supreme Court of the Netherlands, the Netherlands as a state was found responsible for the death of the three of the murdered men. The court also found that it was the government of the Netherlands which had \"effective control\" over its troops.[185] The rationale for finding that the Netherlands exercised \"effective control\" over Dutchbat was given as Art. 8 of the Articles on State Responsibility, which it defines as \"factual control over specific conduct.\"[241] The ruling also meant that relatives of these victims of the genocide are able to pursue the government of the Netherlands for compensation.[242] As of July 2020[update] the International Commission on Missing Persons (ICMP) has identified 6,993 persons missing from the July 1995 fall of Srebrenica, mostly through analysing DNA profiles extracted from exhumed human remains and matching them to the DNA profiles obtained from blood samples donated by relatives of the missing. The organization estimates that the total number of deaths was just over 8,000.[246] The Bosnian Book of the Dead, documented 8,331 victims who were killed in July 1995 as part of the Srebrenica massacre. The figure includes civilians and 1,416 soldiers who were taken prisoners of war. Of the 8,331, 5,113 were from Srebrenica, 1,766 from Bratunac, 900 from Vlasenica, 437 from Zvornik and 115 from Rogatica/Žepa.[247] Two officers of the Army of the Republika Srpska have been convicted by the Tribunal for their involvement in the Srebrenica genocide, Radislav Krstić and Vidoje Blagojević. General Krstić, who led the assault on Srebrenica alongside Ratko Mladić, was convicted by the tribunal of aiding and abetting genocide and received a sentence of 35 years imprisonment. Colonel Blagojević received a sentence of 18 years imprisonment for crimes against humanity. Krstić was the first European to be convicted on a charge of genocide by an international tribunal since the Nuremberg trials,[249] and only the third person ever to have been convicted by an international tribunal under the 1948 Convention on the Prevention and Punishment of the Crime of Genocide. The ICTY's final ruling in the case against Krstić judicially recognized the Srebrenica massacre as an act of genocide: By seeking to eliminate a part of the Bosnian Muslims, the Bosnian Serb forces committed genocide. They targeted for extinction the 40,000 Bosnian Muslims living in Srebrenica, a group which was emblematic of the Bosnian Muslims in general. They stripped all the male Muslim prisoners, military and civilian, elderly and young, of their personal belongings and identification, and deliberately and methodically killed them solely on the basis of their identity.[23] Slobodan Milošević was accused of genocide or complicity in genocide in territories within Bosnia and Herzegovina, including Srebrenica,[250] but he died on 11 March 2006 during his ICTY trial and so no verdict was returned. The prosecution proved that genocide was committed in Srebrenica and that General Radislav Krstić, among others, was personally responsible for that. On 10 June 2010, seven senior Serb military and police officers, Vujadin Popović, Ljubiša Beara, Drago Nikolić, Ljubomir Borovčanin, Vinko Pandurević, Radivoje Miletić and Milan Gvero, were found guilty of various crimes ranging from genocide to murder and deportation. Popović and Beara were found guilty of genocide, extermination, murder, and persecution over the genocide, and were sentenced to life in prison. Nikolić was found guilty of aiding and abetting genocide, extermination, murder, and persecution and received 35 years in prison. Borovčanin was convicted of aiding and abetting extermination, murder, persecution, forcible transfer, murder as a crime against humanity and as a violation of the laws of customs of war, and was sentenced to 17 years in prison. Miletić was found guilty of murder by majority, persecution, and inhumane acts, specifically forcible transfer, and received 19 years in prison. Gvero was found guilty of persecution and inhumane acts and sentenced to five years in prison, but was acquitted of one count of murder and one count of deportation. Pandurević was found guilty of aiding and abetting murder, persecution and inhumane acts, but was acquitted of charges of genocide, extermination and deportation, and sentenced to 13 years in prison.[252][253] On 10 September 2010, after the prosecution filed an appeal, Vujadin Popović, Ljubiša Beara, Drago Nikolić, Vinko Pandurević, Radivoje Miletić and Milan Gvero could face more charges or longer sentences.[254] Radovan Karadžić (left), former president of Republika Srpska, and Ratko Mladić (right), former Chief of Staff of the Army of the Republika Srpska. Both were found guilty of genocide, war crimes and crimes against humanity, and sentenced to life imprisonment. In 2011, the former Chief of the General staff of the Yugoslav Army, Momčilo Perišić, was sentenced to 27 years in prison for aiding and abetting murder because he provided salaries, ammunition, staff and fuel to the VRS officers.[255] The judges, however, ruled that Perišić did not have effective control over Mladić and other VRS officers involved in the crimes. According to the Trial Chamber, the evidence proved Perišić's inability to impose binding orders on Mladić.[256] On 31 May 2007, Zdravko Tolimir, a long time fugitive and a former general in the Army of the Republika Srpska indicted by the Prosecutor of the ICTY on genocide charges in the 1992–95 Bosnia war was arrested by Serbian and Bosnian police.[257] Tolimir—\"Chemical Zdravko\"—is infamous for requesting the use of chemical weapons and proposing military strikes against refugees at Zepa.[258] Ratko Mladić's deputy in charge of intelligence and security and a key commander at the time of Srebrenica, Tolimir is also believed to be one of the organisers of the support network protecting Mladić and helping him elude justice.[259] Tolimir's trial began on 26 February 2010; he chose to represent himself.[260] Radovan Karadžić and Ratko Mladić were indicted by the ICTY for genocide and complicity in genocide in several municipalities within Bosnia and Herzegovina, including Srebrenica. Karadžić was captured in Serbia on 21 July 2008 and Mladić, on 26 May 2011.[261] Karadžić was convicted of genocide in the area of Srebrenica in 1995 and of persecution, extermination, murder, deportation, inhumane acts (forcible transfer), terror, unlawful attacks on civilians and hostage-taking in other areas of Bosnia and Herzegovina. He was sentenced to life in prison.[262] The Tribunal found Mladić guilty of 10 out of 11 counts of war crimes, crimes against humanity, and genocide, and sentenced him to life in prison.[263] ...the Trial Chamber considered evidence that, following the fall of Srebrenica in July 1995, Slobodan Medić (Boca) was ordered to transport Muslims, including six Muslim men and boys, to various locations, to be killed. The Trial Chamber found proven beyond reasonable doubt that the Scorpions, acting upon the orders of Slobodan Medić (Boca), killed the six Muslim men and boys in the rural area at Godinjske Bare.[266] In addition, the Srebrenica massacre was the core issue of the landmark court case Bosnian Genocide case at the International Court of Justice through which Bosnia and Herzegovina accused Serbia and Montenegro of genocide. The ICJ presented its judgement on 26 February 2007, which concurred with ICTY's recognition of the Srebrenica massacre as genocide.[24] It cleared Serbia of direct involvement in genocide during the Bosnian war,[267] but ruled that Belgrade did breach international law by failing to prevent the 1995 Srebrenica genocide, and for failing to try or transfer the persons accused of genocide to the ICTY, in order to comply with its obligations under Articles I and VI of the Genocide Convention, in particular in respect of General Ratko Mladić.[268][269][270] Citing national security, Serbia obtained permission from the ICTY to keep parts of its military archives out of the public eye during its trial of Slobodan Milošević, which may have decisively affected the ICJ's judgement in the lawsuit brought against Serbia by Bosnia-Herzegovina, as the archives were hence not on the ICTY's public record – although the ICJ could have, but did not, subpoena the documents themselves.[271] Chief prosecutor's office, OTP, rejects allegations that there was a deal with Belgrade to conceal documents from the ICJ Bosnia genocide case.[272] On 10 April 2007, a Serbian war crimes court sentenced four members of the Scorpions paramilitary group to a total of 58 years in prison for the execution of six Bosniaks during the Srebrenica massacre of July 1995.[273] On 11 June 2007, the ICTY transferred Milorad Trbic (former Chief of Security of the Zvornik Brigade of the Army of Republika Srpska) to Sarajevo to stand trial for genocide for his actions in and around Srebrenica before the War Crimes Chamber (Section I for War Crimes of the Criminal Division of the Court of Bosnia and Herzegovina; henceforth: the Court).[274] Milorad Trbic – \"[Is] charged with Genocide pursuant to Article 171 of the Criminal Code of Bosnia and Herzegovina (CC BiH). … The trial commenced on 8 November 2007, and the Prosecutor is currently presenting his evidence.\"[275] The \"Mitrović and others case (\"Kravice\")\" was an important trial before the Court of Bosnia and Herzegovina. The accused \"according to the indictment, in the period from 10 to 19 July 1995, as knowing participants in a joint criminal enterprise, the accused committed the criminal offence of genocide. This crime was allegedly committed as part of widespread and systematic attack against the Bosniak population inside the UN protected area of Srebrenica carried out by the Republika Srpska Army (RSA) and the RS Ministry of Interior, with a common plan to annihilate in part a group of Bosniak people.\"[276] On 29 July 2008, after a two-year trial, the Court found seven men guilty of genocide for their role in the Srebrenica massacre including the deaths of 1000 Bosniak men in a single day.[277][278] The court found that Bosniak men trying to escape from Srebrenica had been told they would be kept safe if they surrendered. Instead, they were transported to an agricultural co-operative in the village of Kravica, and later executed en masse.[277][278] On 20 April 2010, Croatia arrested Franc Kos, a member of the 10th commando detachment of the Army of the Republika Srpska, over genocide charges for the Srebrenica massacre. Bosnia and Herzegovina has an international warrant out for his arrest.[279] He is currently awaiting trial.[280] On 29 April 2010, the United States extradited Marko Boškić on suspicions of having committed genocide.[281] He later pleaded guilty.[280] On 18 January 2011, Israel arrested Aleksandar Cvetković, a veteran of the Bosnian Serb Army, after the Bosnian government filed an extradition request. Cvetković had moved to Israel in 2006 and secured citizenship through marriage to an Israeli. He was accused of having personally taken part in the executions of more than 800 men and boys, and initiated use of machine guns to speed up the killings.[282][283][284] On 1 August 2011, a Jerusalem court approved Cvetković's extradition,[285] and an appeal was denied in November 2012.[286] Božidar Kuvelja, a former Bosnian Serb policeman, was arrested in Čajniče, Bosnia and Herzegovina.[287][288] Survivors and victims' relatives have sought to establish the responsibility of the State of the Netherlands and the United Nations for what happened at Srebrenica in civil law actions brought before The Hague District Court in the Netherlands. In one case, 11 plaintiffs including the organisation \"Mothers of the Enclaves of Srebrenica and Žepa\",[17][300] asked the court, to rule that the UN and the State of the Netherlands breached their obligation to prevent genocide, as laid down in the Genocide Convention and hold them jointly liable to pay compensation to the plaintiffs.[301] On 10 July 2008, the court ruled that it had no jurisdiction against the UN, plaintiffs have appealed the judgement in relation to UN immunity.[302] Another action was brought by a former UN interpreter Hasan Nuhanović and the family of Rizo Mustafić [bs], an electrician employed by the UN at Srebrenica. They claimed that Dutch troops in the peacekeeping contingent responsible for security in the UN-protected zone allowed VRS troops to kill their relatives, Nuhanović's brother, father and mother[303] and Mustafić.[304] They argued that the Dutch Government had de facto operational command of the Dutch battalion in accordance with the Dutch Constitution (Article 97(2)), which grants the government superior command (oppergezag) over military forces.[304] On 10 September 2008, the district court dismissed these claims and held that the State could not be held responsible because the Dutchbat peacekeepers were operating in Bosnia under a United Nations mandate and operational command and control over the troops had been transferred to the UN command.[305] On 5 July 2011, the court of appeal reversed this decision and held that the State was responsible for, and indeed actively coordinated, the evacuation once Srebrenica fell, and therefore was responsible for the decision to dismiss Nuhanović's brother and Mustafić from the Dutchbat compound. The court further held that this decision was wrong, because the Dutch soldiers should have known that they were in great danger of being tortured or killed. Both claimants are therefore eligible for compensation.[306] On 6 September 2013, the Supreme Court dismissed a Government appeal,[307] a judgment that the Government accepted.[308] On 16 July 2014, a Dutch court held the Netherlands liable for the killings of more than 300 Bosniaks at Srebrenica; the same court ruled that the Netherlands was not liable for the other deaths in Srebrenica.[29] The decision was upheld by The Hague Appeals Court on 27 June 2017.[309][310] On 19 July 2019 the Dutch Supreme court ruled the Dutch state was liable for 10% for the 300 Bosnian men expelled from the compound.[311][312] In response to the suggestion that the Bosniak forces in Srebrenica made no adequate attempt to defend the town, the Report of the Secretary-General pursuant to General Assembly resolution 53/35—The Fall of Srebrenica,[58] delivered to the 54th session of the United Nations General Assembly on 15 November 1999 states: 476. Concerning the accusation that the Bosniaks did not do enough to defend Srebrenica, military experts consulted in connection with this report were largely in agreement that the Bosniaks could not have defended Srebrenica for long in the face of a concerted attack supported by armour and artillery. (...) 478. Many have accused the Bosniak forces of withdrawing from the enclave as the Serb forces advanced on the day of its fall. However, it must be remembered that on the eve of the final Serb assault the Dutchbat Commander urged the Bosniaks to withdraw from defensive positions south of Srebrenica town—the direction from which the Serbs were advancing. He did so because he believed that NATO aircraft would soon be launching widespread air strikes against the advancing Serbs. 479. A third accusation levelled at the Bosniak defenders of Srebrenica is that they provoked the Serb offensive by attacking out of that safe area. Even though this accusation is often repeated by international sources, there is no credible evidence to support it. Dutchbat personnel on the ground at the time assessed that the few \"raids\" the Bosniaks mounted out of Srebrenica were of little or no military significance. These raids were often organised in order to gather food, as the Serbs had refused access for humanitarian convoys into the enclave. Even Serb sources approached in the context of this report acknowledged that the Bosniak forces in Srebrenica posed no significant military threat to them. The biggest attack the Bosniaks launched out of Srebrenica during the more than two years during which it was designated a safe area appears to have been the raid on the village of Višnjica, on 26 June 1995, in which several houses were burned, up to four Serbs were killed and approximately 100 sheep were stolen. In contrast, the Serbs overran the enclave two weeks later, driving tens of thousands from their homes, and summarily executing thousands of men and boys. The Serbs repeatedly exaggerated the extent of the raids out of Srebrenica as a pretext for the prosecution of a central war aim: to create a geographically contiguous and ethnically pure territory along the Drina, while freeing their troops to fight in other parts of the country. The extent to which this pretext was accepted at face value by international actors and observers reflected the prism of \"moral equivalency\" through which the conflict in Bosnia was viewed by too many for too long.[313] It is agreed by all sides that Serbs suffered a number of casualties during military forays led by Naser Orić. The controversy over the nature and number of the casualties came to a head in 2005, the 10th anniversary of the massacre.[314] According to Human Rights Watch, the ultra-nationalist Serbian Radical Party \"launched an aggressive campaign to prove that Muslims had committed crimes against thousands of Serbs in the area\" which \"was intended to diminish the significance of the July 1995 crime.\"[314] A press briefing by the ICTY Office of the Prosecutor (OTP) dated 6 July 2005 noted that the number of Serb deaths in the region alleged by the Serbian authorities had increased from 1,400 to 3,500, a figure the OTP stated \"[does] not reflect the reality.\"[315] The briefing cited previous accounts: The Republika Srpska's Commission for War Crimes gave the number of Serb victims in the municipalities of Bratunac, Srebrenica and Skelani as 995; 520 in Bratunac and 475 in Srebrenica. The Chronicle of Our Graves by Milivoje Ivanišević, president of the Belgrade Centre for Investigating Crimes Committed against the Serbs, estimates the number of people killed at around 1,200. The accuracy of these numbers is challenged: the OTP noted that although Ivanišević's book estimated that around 1,200 Serbs were killed, personal details were only available for 624 victims.[315] The validity of labeling some of the casualties as \"victims\" is also challenged:[315] studies have found a significant majority of military casualties compared to civilian casualties.[316] This is in line with the nature of the conflict—Serb casualties died in raids by Bosniak forces on outlying villages used as military outposts for attacks on Srebrenica[317] (many of which had been ethnically cleansed of their Bosniak majority population in 1992).[318] For example, the village of Kravica was attacked by Bosniak forces on Orthodox Christmas Day, 7 January 1993. Some Serb sources such as Ivanišević allege that the village's 353 inhabitants were \"virtually completely destroyed\".[315] In fact, the VRS' own internal records state that 46 Serbs died in the Kravica attack: 35 soldiers and 11 civilians,[319] while the ICTY Prosecutor's Office's investigation of casualties on 7 and 8 January in Kravica and the surrounding villages found that 43 people were killed, of whom 13 were obviously civilians.[320] Nevertheless, the event continues to be cited by Serb sources as the key example of crimes committed by Bosniak forces around Srebrenica.[314] As for the destruction and casualties in the villages of Kravica, Šiljković, Bjelovac, Fakovići and Sikirić, the judgement states that the prosecution failed to present convincing evidence that the Bosnian forces were responsible for them, because the Serb forces used artillery in the fighting in those villages. In the case of the village of Bjelovac, Serbs even used warplanes.[321] The most up-to-date analysis of Serb casualties in the region comes from the Sarajevo-based Research and Documentation Centre, a non-partisan institution with a multiethnic staff, whose data have been collected, processed, checked, compared and evaluated by international team of experts.[316][322][323] The RDC's extensive review of casualty data found that Serb casualties in the Bratunac municipality amounted to 119 civilians and 424 soldiers. It also established that although the 383 Serb victims buried in the Bratunac military cemetery are presented as casualties of ARBiH units from Srebrenica, 139 (more than one third of the total) had fought and died elsewhere in Bosnia and Herzegovina.[316] Serb sources maintain that casualties and losses during the period prior to the creation of the safe area gave rise to Serb demands for revenge against the Bosniaks based in Srebrenica. The ARBiH raids are presented as a key motivating factor for the July 1995 genocide.[324] This view is echoed by international sources including the 2002 report commissioned by the Dutch government on events leading to the fall of Srebrenica (the NIOD report).[325] In Balkan Genocides: Holocaust and Ethnic Cleansing in the Twentieth Century, scholar Paul Mojzes notes that a great deal of animosity towards the men of Srebrenica stems from the period of May 1992 to January 1993 where forces under Orić's leadership attacked and destroyed scores of Serbian villages. Evidence indicated that Serbs had been tortured and mutilated and others were burned alive when their houses were torched.[326] The efforts to explain the Srebrenica massacre as motivated by revenge have been dismissed as bad faith attempts to justify the genocide.[327] The ICTY Outreach Programme notes that the claim that Bosnian Serb forces killed the prisoners from Srebrenica in revenge for crimes committed by Bosnian Muslim forces against Serbs in the villages around Srebrenica provides no defence under international law and soldiers, certainly experienced officers, would be aware of the fact. To offer revenge as a justification for crimes is to attack the rule of law, and civilization itself, and nor does revenge provide moral justification for killing people simply because they share the same ethnicity as others who perpetrated crimes. The methodical planning and mobilization of the substantial resources involved required orders to be given at a high command level. The VRS had a plan to kill the Bosnian Muslim prisoners, as Bosnian Serb war criminal Dragan Obrenović confirmed.[328] The UN Secretary-General's report on the fall of Srebrenica said: \"Even though this accusation is often repeated by international sources, there is no credible evidence to support it... The Serbs repeatedly exaggerated the extent of the raids out of Srebrenica as a pretext for the prosecution of a central war aim: to create a geographically contiguous and ethnically pure territory along the Drina.\"[313] Claim that the planning of mass executions in Srebrenica defies military logic[edit] During Radislav Krstić's trial before the ICTY, the prosecution's military advisor, Richard Butler, pointed out that by carrying out a mass execution, the Serb Army deprived themselves of an extremely valuable bargaining counter. Butler suggested that they would have had far more to gain had they taken the men in Potočari as prisoners of war, under the supervision of the International Red Cross (ICRC) and the UN troops still in the area. It might then have been possible to enter into some sort of exchange deal or they might have been able to force political concessions. Based on this reasoning, the ensuing mass murder defied military explanation.[329] On 13 July, \"Dutchbat\" expelled[why?] five Bosniak refugees from the United Nations compound. Later proceedings in Dutch courts have established legal liability of The Dutch State for the deaths of those expelled.[citation needed] In 2013 the Dutch Supreme Court held the Netherlands responsible for the death of 3 Bosnian men who were expelled from a compound held by Dutchbat, on 13 July 1995.[71] On 19 July 2019. The Dutch Supreme court ruled that Dutchbat was not liable for the fall of Srebrenica and not liable for the massacre. The Dutch state was liable for 10% for the 300 Bosnian man expelled from the compound.[311] Claims of problems in the UNPROFOR chain of command above \"Dutchbat\"[edit] In 2005 an unnamed officer on Haukland's multinational staff at Tuzla in 1995, disputed the claim by Haukland and the Norwegian Chief of Defence, Arne Solli, that the attack on Srebrenica was a surprise.[332] The officer said \"We knew early on that the Serbs were amassing their forces around Srebrenica. At the end of June, Haukland informed the headquarters at Sarajevo again and again in regards to this\".[332] In 2006 it was reported that Haukland regularly informed Sollie about the conditions within Haukland's sector, and when Haukland departed Bosnia on his vacation to Norway, they travelled on the same airplane.[334] The 2002 report Srebrenica: a 'safe' area, did not assign any blame to Haukland for the massacre. In March 2010, John Sheehan, NATO's Supreme Allied Commander Atlantic from 1994 to 1997, told a US Senate hearing that the Dutch had \"declared a peace dividend and made a conscious effort to socialise their military – that includes the unionisation of their militaries, it includes open homosexuality\", claiming that gay soldiers could result in events like Srebrenica.[335] He claimed that his opinion was shared by the leadership of the Dutch armed forces, mentioning the name \"Hankman Berman\", who Sheehan added, had told him that the presence of gay soldiers at Srebrenica had sapped morale and contributed to the disaster.[336] General van den Breemen denied having said such a thing and called Sheehan's comments \"total nonsense\", Sheehan's remarks were also dismissed by the Dutch authorities as, \"disgraceful\" and \"unworthy of a soldier\".[337][338][339][340][341] Sheehan apologised to Dutch military officials on 29 March 2010, withdrawing his comments and blaming instead \"the rules of engagement...developed by a political system with conflicting priorities and an ambivalent understanding of how to use the military.\"[342] Criticism of the 1995 UN Special Representative for the former Yugoslavia[edit] In 2005, Professor Arne Johan Vetlesen said \"Thorvald Stoltenberg's co-responsibility in Srebrenica boils down to that he through three years as a top broker contributed to create a climate—diplomatic, politic and indirectly militarily—such that Mladic calculated correctly when he figured that he could do exactly what he wanted with Srebrenica's Muslim population\".[343] Scepticism about the massacre has ranged from challenging the judicial recognition of the killings as an act of genocide to the denial of a massacre having taken place. The finding of genocide by the ICJ and the ICTY, has been disputed on evidential and theoretical grounds. The number of the dead has been questioned as has the nature of their deaths. It has been alleged that considerably fewer than 8,000 were killed and/or that most of those killed died in battle rather than by execution. It has been claimed that the interpretation of \"genocide\" is refuted by the survival of the women and children.[345] During the Bosnian war, Slobodan Milošević had effective control of most Serbian media.[346][347] Following the end of the war, denial of Serbian responsibility for the killings in Srebrenica continued to be widespread among Serbians.[348] Questions were raised regarding Srebrenica[clarification needed] in the symposium Srebrenica 1995–2015: činjenice nedoumice, propaganda held by the Museum of Genocide Victims (Muzej žrtava genocida) and Strategic Culture Foundation (Fond strateške kulture) in Belgrade in 2017.[349] Sonja Biserko, president of the Helsinki Committee for Human Rights in Serbia, and Edina Bečirević, the Faculty of Criminalistics, Criminology and Security Studies of the University of Sarajevo have pointed to a culture of denial of the Srebrenica genocide in Serbian society, taking many forms and present in particular in political discourse, the media, the law and the educational system.[350] Milorad Dodik, president of Republika Srpska, has repeatedly insisted that the massacre cannot be labeled as genocide. Milorad Dodik, President of Republika Srpska, stated in an interview with the Belgrade newspaper Večernje novosti in April 2010 that \"we cannot and will never accept qualifying that event as a genocide\". Dodik disowned the 2004 Republika Srpska report acknowledging the scale of the killing and apologising to the relatives of the victims, alleging that the report had been adopted because of pressure from the international community. Without substantiating the figure, he claimed that the number of victims was 3,500 rather than the 7,000 accepted by the report, alleging that 500 listed victims were alive and over 250 people buried in the Potočari memorial centre died elsewhere.[351] In July 2010, on the 15th anniversary of the massacre, Dodik declared that he did not regard the killings at Srebrenica as genocide, and maintained that \"If a genocide happened then it was committed against Serb people of this region where women, children and the elderly were killed en masse\" (referring to eastern Bosnia).[240] In December 2010, Dodik condemned the Peace Implementation Council, an international community of 55 countries, for referring to the Srebrenica massacre as genocide.[352] In 2021, Dodik continued to claim that there had been no genocide and asserted on Bosnian Serb TV that coffins in the memorial cemetery were empty, with only names.[353] Tomislav Nikolić, President of Serbia, stated on 2 June 2012 that \"there was no genocide in Srebrenica. In Srebrenica, grave war crimes were committed by some Serbs who should be found, prosecuted and punished. ... It is very difficult to indict someone and prove before a court that an event qualifies as genocide.\"[354] La Nation, a bi-monthly Swiss newspaper, published a series of articles claiming that 2,000 soldiers were killed in the \"pseudo-massacre\" in Srebrenica. The Society for Threatened Peoples and Swiss Association Against Impunity filed a joint suit against La Nation for genocide denial. Swiss law prohibits genocide denial.[358] Swiss politician Donatello Poggi was convicted for racial discrimination after calling the Srebrenica genocide claims lies in articles published in 2012.[359] Lewis MacKenzie, former commander of the United Nations Protection Force (UNPROFOR) in Bosnia, in 2009 continued to claim that only around 2,000 men and boys were killed at Srebrenica and that the number killed had been exaggerated by a factor of 4. He further claimed that the bussing out of the women and children contradicted the notion of genocide, writing that the women would have been killed first if there had been any intent to destroy the group. MacKenzie expressed these opinions without reference to the arguments published by the ICTY Trial and Appeal Chambers in the Krstic case judgements several years earlier and subsequently confirmed by the ICJ.[360][361] Portuguese retired general Carlos Martins Branco published \"Was Srebrenica a Hoax? Eyewitness Account of a Former UN Military Observer in Bosnia\" in 1998, and his memoirs \"A Guerra nos Balcãs, jihadismo, geopolítica e desinformação\" (\"War in the Balkans, Jihadism, Geopolitics, and Disinformation\") in November 2016. He said \"Srebrenica was portrayed – and continues to be – as a premeditated massacre of innocent Muslim civilians. As a genocide! But was it really so? A more careful and informed assessment of those events leads me to doubt it\".[362] The Srebrenica Research Group, a group led by Edward S. Herman claimed in conclusions published in Srebrenica And the Politics of War Crimes (2005), \"The contention that as many as 8,000 Muslims were killed has no basis in available evidence and is essentially a political construct\".[363] The director of the Simon Wiesenthal Center office in Israel, Efraim Zuroff, said: \"As far as I know, what happened [in Srebrenica] does not [fit] the description or the definition of genocide. I think the decision to call it genocide was made for political reasons. Obviously a tragedy occurred, innocent people lost their lives and their memory should be preserved.\" Zuroff also called attempts to equate Srebrenica to the Holocaust \"horrible\" and \"absurd\", saying: \"I wish the Nazis moved aside Jewish women and children before their bloody rampage, instead of murdering them, but that, as we know, did not happen.\"[364] In March 2005, Miloš Milovanović, a former commander of the Serb paramilitary unit Serbian Guard who represents the Serbian Democratic Party in the Srebrenica Municipal Assembly said that \"the massacre is a lie; it is propaganda to paint a bad picture of the Serbian people. The Muslims are lying; they are manipulating the numbers; they are exaggerating what happened. Far more Serbs died at Srebrenica than Muslims.\"[365][366] VRS commander Ratko Škrbić denies that there was genocide committed in Srebrenica. He authored the book Srebrenička podvala, an analysis of the July 1995 events.[367] In October 2016, Mladen Grujičić, the first ethnically Serb Mayor of Srebrenica, questioned whether the massacre had been proven to be genocide.[368][369] International human rights and genocide scholar William Schabas, who holds a \"distinct interpretation of the crime of genocide\", has been accused of genocide denial in the context of Srebrenica and other instances of crimes against humanity.[370][371] In his 2009 book Genocide in International Law: The Crime of Crimes, in a summary of his opinions on the status of the atrocities committed in Srebrenica and throughout the Bosnian war, he explained that he did not believe that Srebrenica met the legal definition of genocide, stating that: \"Ethnic cleansing is also a warning sign of genocide to come. Genocide is the last resort of the frustrated ethnic cleanser.\"[372] Schabas has nevertheless accepted the final verdicts of international courts in their legal rulings that the massacre was a genocide, clarifying: \"I am not arguing with anybody about whether genocide took place in Srebrenica. That has been decided.\"[370] ^Niksic, Sabina (9 October 2017). \"Bosnian court acquits ex-Srebrenica commander of war crimes\". Associated Press. Serbs continue to claim the 1995 Srebrenica slaughter was an act of revenge by uncontrolled troops because they say that soldiers under Oric's command killed thousands of Serbs in the villages surrounding the eastern town ^Prosecutor v. Naser Orić, 81 (International Tribunal for the Prosecution of Persons Responsible for Serious Violations of International Humanitarian Law Committed in the Territory of Former Yugoslavia since 1991 30 June 2006), Text. ^ abcProsecutor v. Naser Orić, 89 (International Tribunal for the Prosecution of Persons Responsible for Serious Violations of International Humanitarian Law Committed in the Territory of Former Yugoslavia since 1991 30 June 2006), Text. ^Prosecutor v. Naser Orić, 110 (International Tribunal for the Prosecution of Persons Responsible for Serious Violations of International Humanitarian Law Committed in the Territory of Former Yugoslavia since 1991 30 June 2006), Text. ^Prosecutor v. Radislav Krstic, 9 (International Tribunal for the Prosecution of Persons Responsible for Serious Violations of International Humanitarian Law Committed in the Territory of Former Yugoslavia since 1991 19 April 2004), Text. ^ abcProsecutor v. Radislav Krstic, 22 (International Tribunal for the Prosecution of Persons Responsible for Serious Violations of International Humanitarian Law Committed in the Territory of Former Yugoslavia since 1991 2 August 2001), Text. ^ abMarlise Simons (6 September 2013). \"Dutch Peacekeepers Are Found Responsible for Deaths\". The New York Times. Retrieved 7 September 2013. Dutchbat soldiers knew that outside the compound men were being killed and abused, the court summary said, but the soldiers decided not to evacuate most refugees, including the three men, along with the battalion and instead sent them away on 13 July. ^ abHay, Alastair (1998). \"Surviving the Impossible: The Long March from Srebrenica. An Investigation of the Possible Use of Chemical Warfare Agents\". Medicine, Conflict and Survival. 14 (2): 120–155. doi:10.1080/13623699808409383. PMID9633268. ^\"Krstic Judgement\"(PDF). ICTY. 2 August 2001. Part II. FINDINGS OF FACT – B. THE ROLE OF THE DRINA CORPS IN THE SREBRENICA CRIMES – 5. Involvement of the Drina Corps in the Mass Executions – (a) The Morning of 13 July 1995: Jadar River Executions. ^ ab\"Krstic Judgement\"(PDF). ICTY. 2 August 2001. Part II. FINDINGS OF FACT – B. THE ROLE OF THE DRINA CORPS IN THE SREBRENICA CRIMES – 5. Involvement of the Drina Corps in the Mass Executions – (b) The Afternoon of 13 July 1995: Cerska Valley Executions. ^ abcde\"Krstic Judgement\"(PDF). ICTY. 2 August 2001. Part II. FINDINGS OF FACT – B. THE ROLE OF THE DRINA CORPS IN THE SREBRENICA CRIMES – 5. Involvement of the Drina Corps in the Mass Executions – (c) Late Afternoon of 13 July 1995: Kravica Warehouse. ^ ab\"Krstic Judgement\"(PDF). ICTY. 2 August 2001. Part II. FINDINGS OF FACT – B. THE ROLE OF THE DRINA CORPS IN THE SREBRENICA CRIMES – 5. Involvement of the Drina Corps in the Mass Executions – (d) 13–14 July 1995: Tišca. ^ abcde\"Krstic Judgement\"(PDF). ICTY. Part II. FINDINGS OF FACT – B. THE ROLE OF THE DRINA CORPS IN THE SREBRENICA CRIMES – 5. Involvement of the Drina Corps in the Mass Executions – (e) 14 July 1995: Grbavci School Detention Site and Orahovac Execution site. ^ ab\"Krstic Judgement\"(PDF). ICTY. 2 August 2001. Part II. FINDINGS OF FACT – B. THE ROLE OF THE DRINA CORPS IN THE SREBRENICA CRIMES – 5. Involvement of the Drina Corps in the Mass Executions – (f) 14–15 July 1995: Petkovci School Detention Site and Petkovci Dam Execution Site, para. 226, p. 81. ^The Court of Bosnia and Herzegovina – Trbic case: Charged with genocide pursuant to Article 171 of the Criminal Code of Bosnia and Herzegovina (CC BiH) in conjunction with the killing members of the group, causing serious bodily or mental harm to members of the group, deliberately inflicting on the group conditions of life calculated to bring about its physical destruction in whole or in part, imposing measures intended to prevent births within the group (X-KR-07/386 – Trbic MiloradArchived 27 December 2008 at the Wayback Machine) The Politics of the Srebrenica Massacre article argues that only some Bosniaks were executed, most died in battle, and some of the bodies in mass graves are actually Serbs, by Edward S. Herman, Znet, 7 July 2005 Staff Sense Tribunal, is a specialised project of Sense News Agency Sense based in International War Crimes Tribunal for the former Yugoslavia in The Hague. The focus of this project is regular coverage of the work of the ICTY, and the activities of ICJ (International Court of Justice) and ICC (International Criminal Court). Retrieved 16 March 2008"}
{"url": "https://en.wikipedia.org/w/index.php?title=Population_history_of_the_Indigenous_peoples_of_the_Americas&action=edit&section=11", "text": "By publishing changes, you agree to the Terms of Use, and you irrevocably agree to release your contribution under the CC BY-SA 4.0 License and the GFDL. You agree that a hyperlink or URL is sufficient attribution under the Creative Commons license."}
{"url": "https://en.wikipedia.org/wiki/Andean_music", "text": "Andean music is a group of styles of music from the Andes region in South America. Original chants and melodies come from the general area inhabited by Quechuas (originally from Peru, Bolivia, Ecuador, Chile), Aymaras (originally from Bolivia), and other peoples who lived roughly in the area of the Inca Empire prior to European contact. This early music then was fused with music elements. It includes folklore music of parts of Peru, Bolivia, and Ecuador. Andean music is popular to different degrees across South America, having its core public in rural areas and among indigenous populations. The Nueva Canción movement of the 1970s revived the genre across South America and brought it to places where it was unknown or forgotten. The panflute is among the most emblematic instruments of Andean music. The panpipes group include the sikú (or zampoña) and Antara. These are ancient indigenous instruments that vary in size, tuning, and style. Instruments in this group are constructed from aquatic reeds found in many lakes in the Andean region of South America. The sikú has two rows of canes and are tuned in either pentatonic or diatonic scales. Some modern single-row panpipes modeled after the native antara are capable of playing full scales, while traditional sikús are played using two rows of canes wrapped together. It is still commonplace for two performers to share a melody while playing the larger style of sikú called the toyo. This style of voicing with notes interspersed between two musicians is called playing in hocket and is still in use today in many of the huaynos traditional songs and contemporary Andean music. Quenas (notched-end flutes) remain popular and are traditionally made out of the same aquatic canes as the sikús, although PVC pipe is sometimes used due to its resistance to heat, cold and humidity. Generally, quenas are played only during the dry season, while vertical flutes, either pinkillos or tarkas, are played during the wet season. Tarkas are constructed from local Andean hardwood sources. Marching bands dominated by drums and panpipes are commonplace today and are used to celebrate weddings, carnivals and other holidays. The twentieth century saw drastic changes in Andean society and culture. Bolivia, for example, saw a nationalistic revolution in 1952, leading to increased rights and social awareness for natives. The new government established a folklore department in the Bolivian Ministry of Education and radio stations began broadcasting in Aymara and Quechua. The 1970s was a decade in which Andean music saw its biggest growth. Different groups sprang out of the different villages throughout the Andes Region. Peru, Ecuador, Chile, Bolivia, south of Colombia, and northwest Argentina. Many musicians made their way to the big cities forming different bands and groups. One of the most legendary was Los Kjarkas, from Bolivia, singing and composing songs that became huge hits in Bolivia and would later become Andean standards. They would later take Andean music to the rest of the world. Harawi - Ancient traditional musical genre and also indigenous lyric poetry. Harawi was widespread in the Inca Empire and now is especially common in countries that were part of it: Peru, Bolivia, Ecuador, partially Chile. Typically, harawi is a moody, soulful slow and melodic song or tune played on the quena. Originally from the Caribbean coast of Colombia, cumbia later spread through much of Latin America. In Peru, it developed into a style colloquially known as chicha, which has become a popular style in the Andean region, especially among the lower socioeconomic strata of the society including Quechua and Aymara populations.[citation needed] Several Andean music genres have also borrowed elements originally introduced by the Peruvian cumbia such as electric bass guitars, electronic percussion and little from the original cumbia rhythm.[citation needed] Andean music has served as a major source of inspiration for the neo-folkloric Nueva canción movement that began in the 1960s, Nueva canción musicians both interpreted old songs and created new pieces that are now considered Andean music. Some Nueva canción musicians such as Los Jaivas would fuse Andean music with psychedelic and progressive rock."}
{"url": "https://en.wikipedia.org/wiki/Al-Jawazi_massacre", "text": "The al-Jawazi massacre (Arabic: مذبحة الجوازي) is a name given to a massacre committed against the Arab tribe of al-Jawazi of Banu Sulaym in the city of Benghazi in Cyrenaica in eastern Ottoman Tripolitania (now Libya), on 5 September 1816,[1] in which over 10,000 members of the tribe were killed.[2] The site of the massacre was a Turkish castle in the city. It was done in retaliation for the revolt that broke out against the rule of the Karamanli dynasty and their refusal to pay the imposed taxes. The pasha Yusuf Karamanli claimed to pacify the al-Jawazi tribe by inviting 45 of its notables and sheikhs, accompanied with hundreds of other members of the tribe, to the castle for the purpose of requesting peace with them. As soon as they sat down, the Pasha's guards and janissaries attacked them and slaughtered them; while members of the tribe who were stationed outside the walls of the castle were attacked and large numbers of them were massacred, including women, old men, and children, killing over 10,000 of them.[3] The tribe members who survived fled Tripolitania and left for neighbouring countries such as Egypt, Tunis, and Algiers.[1]"}
{"url": "https://www.theverge.com/2015/12/24/10663146/hollywood-s-christmas-is-being-ruined-by-unprecedented-leaks", "text": "Hollywood’s Christmas is being ruined by unprecedented leaks Tarantino’s The Hateful Eight and Alejandro González Iñárritu's The Revenant starring Leonardo DiCaprio have already leaked online ahead of their Christmas Day openings. Two blockbuster films joined by a list of titles leaked this week including Creed, Legend, In the Heart of the Sea, Joy, Steve Jobs, Concussion, Spotlight, The Danish Girl, and Bridge of Spies. A list that continues to grow by the hour. Already, this is shaping up to be one of the worst cases of piracy to ever hit Hollywood. All the leaks are of exceptional quality for early releases with \"DVDScr\" in the title suggesting DVD screeners — DVDs issued to critics and awards voters — as the source content. The leaks began on December 20th with a note saying The Hateful Eight was just one of 40 screeners that the so-called CM8 \"scene group\" would be releasing. \"Will do them all one after each other,\" read the footnote, \"started with the hottest title of the year, the rest will follow.\" All but The Revenant have been leaked under the name \"Hive-CM8.\" Screengrab from leaked version of Creed. Yesterday the FBI tracked the source of the Hateful Eight leak to a DVD screener sent to Andrew Kosove, co-CEO of Alcon Entertainment, according to The Hollywood Reporter. Kosove says he never saw the DVD, whose delivery was accepted by an office assistant. The screener was sent to Kosove for awards consideration and identified by invisible watermarks embedded in the playback. Screengrab from leaked version of The Revenant. Leaks of Hollywood films are not new. However, these typically occur after general release and the quality is usually limited to that of a handheld HD camera until the movies are available via legitimate channels. In 2009, a rough cut forX-Men Origins: Wolverineleaked weeks before the finished version hit theaters. Last year The Expendables 3 leaked online nearly a month before its official release. The movie was downloaded by more than 10 million people causing it to lose a claimed $250 million in revenue. TorrentFreak estimates that The Hateful Eight has already been shared more than a million times. A number that will undoubtedly take money off the table for Weinstein Co, the film’s distributor, and \"stir up anti-piracy sentiment in Hollywood like never before.\""}
{"url": "https://en.wikipedia.org/wiki/Cahinnio", "text": "In July 1687, Father Anastasius Douay, a French priest, visited a Cahinnio village near present-day Arkadelphia, Arkansas.[3] In the 1680s, French explorer Henri Joutel traveled with the La Salle expedition, to Cahinnio territory. He wrote that they presented his expedition with two loaves of corn bread, describing it as \"the finest and the best we had so far seen; they seemed to have been baked in an oven, and yet we not noticed any among them.\"[4] Joutel noted that corn was an important food staple among the Cahinnio, as were beans and sunflower seeds.[5] Additionally he recorded that the Cahinnio used deer hide for pouches and bearskins for rugs.[6] The Cahinnio were known for their superior bows, which they made from Osage orange wood.[7] During the 18th century, the Cahinnio moved northwest, possibly due to new sources of salt and horses.[8] They settled along the southern bank of the Ouachita River.[1] By 1763, they moved to the upper Arkansas River. In 1771, the Cahinnio and several neighboring tribes signed a peace treaty with the French.[9] Ultimately, they assimilated into other Kadohadacho tribes by the 19th century. They are enrolled in the Caddo Nation of Oklahoma today."}
{"url": "https://en.wikipedia.org/wiki/Las_Palmas_complex", "text": "The Las Palmas complex is an archaeological pattern recognized primarily on the basis of mortuary customs in the Cape region of Baja California Sur, Mexico. The complex is focused on the occurrence in caves or rockshelters of secondary human burials containing bones painted with red ochre. The skulls in such burials tend to be extremely long-headed (hyperdolichocephalic), leading to suggestions that makers of the Las Palmas complex (identified with the historically known Pericú) might represent either a genetically isolated remnant of a very early wave of immigrants into the Americas or later trans-Pacific migrants. Other elements in the material inventory of the Las Palmas complex include stone grinding basins, atlatls, lark's-head netting, coiled basketry, and sewn palm-bark containers."}
{"url": "https://en.wikipedia.org/wiki/Francisco_de_Montejo", "text": "Francisco de Montejo was born about 1473 to a family of lesser Spanish nobility in Salamanca, Spain. He never documented his parentage during his lifetime but his father was probably Juan de Montejo. His mother is unknown but her surname may have been Téllez.[1] He had a brother, Juan, who served with him in the New World and a sister, Maria, whose son Francisco de Montejo would become an important conquistador in Yucatán.[2] In 1513, Montejo joined an expedition being organized in Seville under the leadership of Pedrarias Davila who had received a royal appointment to govern Castilla de Oro, a new Spanish colony in Central America. Montejo was sent on ahead to Santo Domingo to recruit additional men for the colony. Later, Pedrarius sent him on an unsuccessful expedition to the region that would later become Nueva Granada.[2] Montejo became disillusioned with opportunities under Pedrarius and left for Cuba where he participated under Diego Velázquez de Cuéllar in the conquest of Cuba. The conquest was almost complete when Montejo arrived but he gained the favor of Velázquez and was rewarded with encomiendas and extensive grants of land.[2] In 1518, when Francisco Hernández de Córdoba reported his discovery of new lands in the west, Montejo joined Juan de Grijalva's expedition to explore the coast of Yucatán. He invested his own money to help outfit the small fleet and was designated captain of one of the four ships. When they reached the Mexican coastline, Montejo became the first Spaniard to step ashore in the Aztec Empire and establish friendly relations with the Indians he encountered. [2] On his return to Cuba, Montejo joined the Hernán Cortés expedition in an attempt to seize control of the newly-discovered lands. He became one of Cortés's most important lieutenants,[3] serving as captain of one ship and its company of soldiers. He fought in the bloody campaign in Tobasco and was then sent north with two small ships to find a suitable site for a permanent town. The site he identified became Villa Rica de la Vera Cruz and Montejo was appointed one of its first alcaldes (chief town administrators). Throughout this early phase of the conquest, Cortés showed high regard for Montejo.[4][5]: 27 In July 1519, Montejo was sent to the Spanish Court to provide news of the conquest and defend Cortés's authority against claims by Velázquez. He also presented the emperor with a rich treasure of gold, silver and jewels that had already been seized in New Spain[6] Montejo successfully represented Cortés in Castile until 1522 when he returned to the new City of Mexico, established on the ruins of the old Aztec capital. Cortés rewarded him handsomely with the assignment of encomiendas, notably the rich and populous town of Azcapotzalco.[4][7] By this time, Montejo was a wealthy and prestigious conquistador. In addition to his holdings in New Spain, he held encomiendas in Cuba and properties in Salamanca, Spain. For a couple years Montejo appeared ready to settle in New Spain. He built a luxurious home in Mexico City and developed his haciendas and mines. However, in 1524 he was again sent to Spain to defend Cortés against charges that he had become too powerful and independent of the Crown's best interests. Montejo was successful in his advocacy and also won special favor at Court for tact and obvious talents.[8] In 1525 in Seville Montejo married Beatriz de Herrera, the wealthy widow of conquistador Alonso Esquivel. In 1526 he was awarded his own coat of arms.[7] After settling affairs for Cortés, Montejo began to search for his next opportunity. The conquest by Cortés had set the standard for success and other ambitious conquistadors wanted to emulate him. Montejo thought that Yucatán provided such an opportunity. The coastline was dotted with towns and the interior was rumored to contain wealthy civilizations and spectacular cities.[8] In 1526 Montejo traveled to Grenada to petition the Spanish king, Charles V, for permission to conquer and colonize Yucatán. His request was supported by Pánfilo de Narváez, another veteran conquistador. Montejo argued persuasively that Yucatán would serve as a center of trade for the region and enrich the Crown. His formal petition was submitted on November 19, 1526 and quickly approved by Charles and the Council of the Indies on December 8. Montejo was appointed Adelantado of Yucatán, governor and captain general of the new province, and authorized to conquer, settle, and govern at his own expense. His titles and offices were to be held for life. The Crown would not bear any expenses except to pay him a salary of 250,000 maravedis for the offices of governor and captain general. However, other valuable rights, privileges, and exemptions were granted. His contract also stipulated that the principal objective of the Crown was to bring the indigenous peoples of the New World into the Catholic Church. The appropriate treatment of the Indians was carefully spelled out and Montejo was warned to adhere to the spirit and letter of these instructions.[9] Montejo returned to Seville to organize his expedition. His contract stipulated that the conquest must begin within one year (by December 8, 1527) so he had to move quickly. He raised 28,000 castellanos, a significant amount of money at the time, acquired and outfitted four ships and recruited 250 men in addition the crews for the ships. Alonso Dávila was named his principal lieutenant. Careful to heed royal instructions, three clergymen were included to address the religious needs of the conquerors and the conquered. Montejo and his fleet set sail from Seville towards the end of June, 1526.[10] The crossing was uneventful and the fleet stopped first at Hispaniola where supplies were purchased, additional men were recruited and enough horses were procured to bring their small cavalry up to fifty strong. They soon set off again and reached Cozumel in late September. From there Montejo crossed the channel to the mainland and established their first town, Salamanca, named after Montejo's hometown. The site was poorly chosen; the climate was hot and humid and freshwater was scarce. At first the local natives were willing to provide food but Spanish demands quickly turned their attitude from cooperation to resentment and outright hostility. Oviedo later reported that \"all [the Spaniards] fell ill and many died.\"[11] He returned to Yucatán in 1528, and attempted to conquer it from its east coast at Tulum and Chetumal, but was driven back by fierce resistance from the Maya living along this coast. In 1530, he decided to try conquering Yucatán from the west, and began by pacifying what is today the modern Mexican state of Tabasco. He continued this attempted conquest of western Yucatán from 1531 until 1535, when his forces were driven from Yucatán despite some prior successes. In 1533, Montejo received a royal decree giving him permission to conquer Puerto Caballos and Naco in Honduras. This put him in conflict with Pedro de Alvarado, who had received a similar decree in 1532 and later declared in 1536 that he had conquered and pacified the province of Honduras. Alvarado continued to serve as the Governor of Honduras until 1540, although he was recalled to Spain in 1537. In 1540, the Spanish King awarded the Governorship of Honduras to Montejo, and he traveled to Gracias a Dios to install an administration loyal to him. It would fall to Montejo's eponymous son, nicknamed \"El Mozo\" (born 1508, died 1565), to conquer Yucatán. He founded the city of San Francisco de Campeche in 1540, and Mérida in 1542.[12] In 1546, the elder Montejo assumed the title of Governor and Captain General of Yucatán. However, by 1550, complaints about Montejo caused him to be recalled to Spain, where he died in 1553. Montejo was survived by his son, \"El Mozo,\" and a daughter, Catalina Montejo y Herrera."}
{"url": "https://en.wikipedia.org/wiki/Amazonas_before_the_Inca_Empire", "text": "The present-day Department of Amazonas in Peru, occupying part of the western Amazon basin, carries evidence of human cultures predating the Inca Empire. The presence of the Chachapoya culture and the Wari culture in architectural excavations allow for evidence of multiple civilized presences previous to the conquest of the area by the Incan Empire.[1][2] The Wari (Huari) were present and occupants of the southern region of the modern day Amazonas Department of Peru, from CE 500 to CE 1000, around 500 years prior to the founding of the Inca Empire. Similar to the Inca Empire, the Huari also expanded across more territories than their other pre-Inca neighbors to the South, the Moche and Chimú. The Huari are theorized to be the pioneers of terrace agriculture, a farming system suited to the topographical terrain of Peru that later would revolutionize European agriculture.[1] The architectural remains of the Huari that date their presence in the state of Peru, pre-Inca establishment, include the Wari Complex (Huari Complex), a site of 988 acres thought to home up to 40,000 people. Near the city of Ayacucho, the site includes subterranean galleries, mausoleums of human remains, and astronomical tables.[2]"}
{"url": "https://en.wikipedia.org/wiki/Racism_in_South_America", "text": "The continent of South America is culturally and racially diverse. This article examines by country and region the current and historical trends in race relations and racism within South America. Racism of various forms is to be found worldwide.[1] Racism is widely condemned throughout the world, with 170 states signatories of the International Convention on the Elimination of All Forms of Racial Discrimination by August 8, 2006.[2] In different countries, the forms that racism takes may be different for historic, cultural, religious, economic or demographic reasons. Bolivia is composed of many cultures, including the Aymara, the Quechua, and the Guarani.[4] \"Pure\" native American people are in general deemed inferior by mestizos and people of European origin. The economic difficulties of the population, the education level of all groups, the economic level of the natives, and the predominant prejudice inherited from colonial times mainly in urban areas aggravates the treatment. The situation has worsened in the last year. The elites, formed mainly by people of foreign origin in the Eastern region, claimed autonomy as a result of the probable redistribution of land, which would go from the more privileged people to the less privileged people[5] (specifically, the Guarani natives and other indigenous people). On October 10, 2010, the Law Against Racism and All Forms of Discrimination (Spanish: Ley 045 Contra el Racismo y Toda Forma de Discriminación; commonly known as the Law Against Racism) was passed by the Plurinational Legislative Assembly of Bolivia as Law 045.[6] This law intends to combat racism and discrimination, but as of February 2014, no convictions had been recorded.[7] Due to this lack of convictions, the legislation has been widely criticized by the Bolivian media as being a dead letter.[8] In the immediate aftermath of Dom Pedro’s abdication in 1831, the poor people of color, including slaves, staged anti-Portuguese riots in the streets of Brazil's larger cities.[10] Racism in Brazil has long been characterized by a belief in racial democracy. An ideology stating that racial prejudice is not a significant factor in Brazilian society, and that racism is not an obstacle to employment, education, and social mobility the way some believe it is in other countries. This theory has come under fire in recent years by researchers who say that racism is very much a factor in the country's social life. Despite the majority of the country's population being of mixed (Pardo), African, or indigenous heritage, depictions of non-European Brazilians on the programming of most national television networks is scarce and typically relegated for musicians/their shows. In the case of telenovellas, Brazilians of darker skin tone are typically depicted as housekeepers or in positions of lower socioeconomic standing. This is a reflection of the economic inequality among races in Brazil, with mixed (Pardo), African and indigenous population constituting the majority of the poor. In addition, the national wealth and income concentrated in the white families. In a sign that some Brazilian universities have come to see racism as an obstacle to higher education, several of them have created positive action programs aimed at increasing the admission of Afro-Brazilians and members of the native population.[11] Guyana's racial tensions originate in the colonial period in it. Africans were brought to Guyana as slaves and were put to work in sugar and cotton plantations, whereas Indians were brought to Guyana as indentured servants and took the place of Africans that worked on plantations. These historical encounters led to discriminatory stereotyping. For example, Africans were viewed as strong, but lazy. The Indians were viewed as hardworking, but greedy. These groups of people were both used as labor for British colonists, however they both had different stereotypes given to them which affected how one race viewed the other.[15] Racial tensions in Guyana started to divide more when it came to politics. After the British left and Guyana was freed, the government in Guyana was completely split. When people ran for president it became more of a racial issue, where Indians favored other Indians, which were called the People’s Progressive Party (PPP), and the Afro-Guyanese people favored their own kind of people as well and became their own party called the People’s National Congress (PNC). This split happened during president Cheddi Jagan ruling time.[citation needed] Racial tensions continued to escalate in the 1900s [clarification needed] when Afro-Guyanese people would preach “Africa for Africans.” It did empower the Afro-Guyanese group of people; however, it did divide the country even more. The Indo-Guyanese people also began showing more pride in being Indian, and the women would begin wearing Indian garb. Whenever one race would try and boost themselves, the other race would follow, turning this into a competition in Guyana.[citation needed] Today Guyana is extremely divided, and if you go to one group of Guyanese people, they would tend to bash the other group of Guyanese people. A study by Monroe College mentions that Indo-Guyanese people, and Afro-Guyanese people seek protection, and vice versa with Afro-Guyanese group of people. When the Venezuelan War of Independence started, the Spanish enlisted the Llaneros, playing on their dislike of the criollos of the independence movement. During this time, José Tomás Boves led an army of llaneros which routinely killed white Venezuelans. After several more years of war, the country achieved independence from Spain in 1821.[16] In Venezuela, like other South American countries, economic inequality often breaks along ethnic and racial lines.[17] A 2013 Swedish academic study stated that Venezuela was the most racist country in the Americas,[17] followed by the Dominican Republic.[17]"}
{"url": "https://en.wikipedia.org/wiki/History_of_Native_Americans_in_the_United_States", "text": "The history of Native Americans in the United States began before the founding of the country, tens of thousands of years ago with the settlement of the Americas by the Paleo-Indians. Anthropologists and archeologists have identified and studied a wide variety of cultures that existed during this era. Their subsequent contact with Europeans had a profound impact on their history afterwards. The Paleo-Indian or Lithic stage lasted from the first arrival of people in the Americas until about 5000/3000 BCE (in North America). Three major migrations occurred, as traced by linguistic and genetic data; the early Paleoamericans soon spread throughout the Americas, diversifying into many hundreds of culturally distinct nations and tribes.[6][7] By 8000 BCE the North American climate was very similar to today's.[8] A study published in 2012 gives genetic backing to the 1986 theory put forward by linguist Joseph Greenberg that the Americas must have been populated in three waves, based on language differences.[9][10] The Clovis culture, a megafauna hunting culture, is primarily identified by use of fluted spear points. Artifacts from this culture were first excavated in 1932 near Clovis, New Mexico. The Clovis culture ranged over much of North America and also appeared in South America. The culture is identified by the distinctive Clovis point, a flaked flint spear-point with a notched flute, by which it was inserted into a shaft. Dating of Clovis materials has been by association with animal bones and by the use of carbon dating methods. Recent reexaminations of Clovis materials using improved carbon-dating methods produced results of 11,050 and 10,800 radiocarbon years BP (roughly 9100 to 8850 BCE). Numerous Paleoindian cultures occupied North America, with some arrayed around the Great Plains and Great Lakes of the modern United States of America and Canada, as well as adjacent areas to the West and Southwest. According to the oral histories of many of the indigenous peoples of the Americas, they have been living on this continent since their genesis, described by a wide range of traditional creation stories. Other tribes have stories that recount migrations across long tracts of land and a great river believed to be the Mississippi River.[11] Genetic and linguistic data connect the indigenous people of this continent with ancient northeast Asians. Archeological and linguistic data has enabled scholars to discover some of the migrations within the Americas. The Folsom tradition was characterized by the use of Folsom points as projectile tips and activities known from kill sites, where slaughter and butchering of bison took place. Folsom tools were left behind between 9000 BCE and 8000 BCE.[12] Na-Dené-speaking peoples entered North America starting around 8000 BCE, reaching the Pacific Northwest by 5000 BCE,[13] and from there migrating along the Pacific Coast and into the interior. Linguists, anthropologists, and archeologists believe their ancestors constituted a separate migration into North America, later than the first Paleo-Indians. They migrated into Alaska and northern Canada, south along the Pacific Coast, into the interior of Canada, and south to the Great Plains and the American Southwest. They were the earliest ancestors of the Athabascan-speaking peoples, including the present-day and historical Navajo and Apache. They constructed large multi-family dwellings in their villages, which were used seasonally. People did not live there year-round, but for the summer to hunt and fish, and to gather food supplies for the winter.[14] The Archaic period lasted until about 1000 BCE. A major culture of the Archaic stage was the Mound builders, who stretched from the Great Lakes to the Mississippi and Ohio rivers. Since the 1990s, archeologists have explored and dated eleven Middle Archaic sites in present-day Louisiana and Florida at which early cultures built complexes with multiple earthworkmounds; they were societies of hunter-gatherers rather than the settled agriculturalists believed necessary according to the theory of Neolithic Revolution to sustain such large villages over long periods. Native American cultures are not included in characterizations of advanced Stone Age cultures as \"Neolithic,\" which is a category that more often includes only the cultures in Eurasia, Africa, and other regions. Watson Brake Aerial Illustration The prime example is Watson Brake in northern Louisiana, whose 11-mound complex is dated to 3500 BCE, making it the oldest dated site in the Americas for such complex construction. It is nearly 2,000 years older than the Poverty Point site. Construction of the mounds went on for 500 years until was abandoned about 2800 BCE, probably due to changing environmental conditions.[15] Poverty Point is a 1 square mile (2.6 km2) complex of six major earthwork concentric rings, with additional platform mounds at the site. Artifacts show the people traded with other Native Americans located from Georgia to the Great Lakes region. This is one among numerous mound sites of complex indigenous cultures throughout the Mississippi and Ohio valleys. They were one of several succeeding cultures often referred to as mound builders. The Post-Archaic stage includes the Formative, Classic and Post-Classic stages in Willey and Phillipp's scheme. The Formative stage lasted from 1000 BCE until about 500 CE, the Classic from about 500 CE to 1200 CE, while the Post-Classic refers to 1200 CE until the present day. It also includes the Woodland period of North Americanpre-Columbian, whose culture refers to the time period from roughly 1000 BCE to 1000 CE in the eastern part of North America. The Hopewell tradition is the term for the common aspects of the Woodland period culture that flourished along rivers in the Eastern Woodlands from 200 BCE to 500 CE.[17] The Hopewell tradition was not a single culture or society, but a widely dispersed set of related populations, who were connected by a common network of trade routes,[18] known as the Hopewell Exchange System. At its greatest extent, the Hopewell exchange system ran from the Southeastern Woodlands into the northern shores of Lake Ontario. Within this area, societies participated in a high degree of exchange; most activities were conducted along the waterways that served as their major transportation routes. The Hopewell exchange system traded materials from all over North America. A map showing the extent of the Coles Creek cultural period and some important sites. The Mississippian culture which extended throughout the Ohio and Mississippi valleys and built sites throughout the Southeast created the largest earthworks in North America north of Mexico, most notably at Cahokia, on a tributary of the Mississippi River in present-day Illinois. The ten-story Monks Mound at Cahokia has a larger perimeter than the Pyramid of the Sun at Teotihuacan, and roughly the same as the Great Pyramid of Egypt. The 6 square miles (16 km2) city complex was based on the culture's cosmology; it included more than 100 mounds, positioned to support their sophisticated knowledge of astronomy, and built with knowledge of varying soil types. The society began building at this site about 950 CE, and reached its peak population in 1,250 CE of 20,000–30,000 people, which was not equalled by any city in the present-day United States until after 1800. Kincaid[19] c. 1050–1400 CE,[20] is one of the largest settlements of the Mississippian culture, it was located at the southern tip of present-day U.S. state of Illinois. Kincaid Mounds has been notable for both its significant role in native North American prehistory and for the central role the site has played in the development of modern archaeological techniques. The site had at least 11 substructure platform mounds (ranking fifth for mound-culture pyramids). Artifacts from the settlement link its major habitation and the construction of the mounds to the Mississippian period, but it was also occupied earlier during the Woodland period. The Hohokam culture was centered along American Southwest.[24] The early Hohokam founded a series of small villages along the middle Gila River. They raised corn, squash, and beans. The communities were near good arable land, with dry farming common in the earlier years of this period.[24] They were known for their pottery, using the paddle-and-anvil technique. The Classical period of the culture saw the rise in architecture and ceramics. Buildings were grouped into walled compounds, as well as earthen platform mounds. Platform mounds were built along river as well as irrigation canal systems, suggesting these sites were administrative centers allocating water and coordinating canal labor. Polychrome pottery appeared, and inhumation burial replaced cremation. The trade included that of shells and other exotics. Social and climatic factors led to a decline and abandonment of the area after 1400 CE. The Ancestral Puebloan culture covered present-day Four Corners region of the United States, comprising southern Utah, northern Arizona, northwestern New Mexico, and southwestern Colorado.[25] It is believed that the Ancestral Puebloans developed, at least in part, from the Oshara tradition, who developed from the Picosa culture. They lived in a range of structures that included small family pit houses, larger clan type structures, grand pueblos, and cliff sited dwellings. The Ancestral Puebloans possessed a complex network that stretched across the Colorado Plateau linking hundreds of communities and population centers. The culture is perhaps best known for the stone and earth dwellings built along cliff walls, particularly during the Pueblo II and Pueblo III eras. The Iroquois League of Nations or \"People of the Long House\", based in present-day upstate and western New York, had a confederacy model from the mid-15th century. It has been suggested that their culture contributed to political thinking during the development of the later United States government. Their system of affiliation was a kind of federation, different from the strong, centralized European monarchies.[26][27] Long-distance trading did not prevent warfare and displacement among the indigenous peoples, and their oral histories tell of numerous migrations to the historic territories where Europeans encountered them. The Iroquois invaded and attacked tribes in the Ohio River area of present-day Kentucky and claimed the hunting grounds. Historians have placed these events as occurring as early as the 13th century, or in the 17th century Beaver Wars.[28] Through warfare, the Iroquois drove several tribes to migrate west to what became known as their historically traditional lands west of the Mississippi River. Tribes originating in the Ohio Valley who moved west included the Osage, Kaw, Ponca and Omaha people. By the mid-17th century, they had resettled in their historical lands in present-day Kansas, Nebraska, Arkansas and Oklahoma. The Osage warred with Caddo-speaking Native Americans, displacing them in turn by the mid-18th century and dominating their new historical territories.[28] From the 16th through the 19th centuries, the population of Native Americans declined in the following ways: epidemic diseases brought from Europe; violence and warfare, such as the Indian Wars[30] at the hands of European explorers and colonists; displacement from their lands including forced marches such as the Trail of tears resulted in many deaths as did enslavement; continued tribal internal warfare,;[31] and a high rate of intermarriage also led to a reduction in the numbers of Native Americans.[32][33] Most mainstream scholars believe that, among the various contributing factors, epidemicdisease was the overwhelming cause of the population decline of the American natives because of their lack of immunity to new diseases brought from Europe.[34][35][36] With the rapid declines of some populations and continuing rivalries among their nations, Native Americans sometimes re-organized to form new cultural groups, such as the Seminoles of Florida in the 19th century and the Mission Indians of Alta California. Some scholars characterize the treatment of Native Americans by the US as genocide or genocidal whilst others dispute this characterization.[30][37][38] Estimating the number of Native Americans living in what is today the United States of America before the arrival of the European explorers and settlers has been the subject of much debate. While it is difficult to determine exactly how many Natives lived in North America before Columbus,[39] estimates range from a low of 2.1 million (Ubelaker 1976) to 7 million people (Russell Thornton) to a high of 18 million (Dobyns 1983).[38] A low estimate of around 1 million was first posited by the anthropologist James Mooney in the 1890s, by calculating population density of each culture area based on its carrying capacity. In 1965, the American anthropologistHenry F. Dobyns published studies estimating the original population to have been 10 to 12 million. By 1983, he increased his estimates to 18 million.[37][40][41] Historian David Henige criticized higher estimates such as those of Dobyns', writing that many population figures are the result of arbitrary formulas selectively applied to numbers from unreliable historical sources.[42] By 1800, the Native population of the present-day United States had declined to approximately 600,000, and only 250,000 Native Americans remained in the 1890s.[43] Chickenpox and measles, endemic but rarely fatal among Europeans (long after being introduced from Asia), often proved deadly to Native Americans. Smallpoxepidemics often immediately followed European exploration and sometimes destroyed entire village populations. While precise figures are difficult to determine, some historians estimate that at least 30% (and sometimes 50% to 70%) of some Native populations died after first contact due to Eurasian smallpox.[44][45] One element of the Columbian exchange suggests explorers from the Christopher Columbus expedition contracted syphilis from indigenous peoples and carried it back to Europe, where it spread widely.[46] Other researchers believe that the disease existed in Europe and Asia before Columbus and his men returned from exposure to indigenous peoples of the Americas, but that they brought back a more virulent form. In the 100 years following the arrival of the Spanish to the Americas, large disease epidemics depopulated large parts of the Eastern Woodlands in the 15th century.[47] In 1618–1619, smallpox killed 90% of the Native Americans in the area of the Massachusetts Bay.[48] Historians believe many Mohawk in present-day New York became infected after contact with children of Dutch traders in Albany in 1634. The disease swept through Mohawk villages, reaching the Onondaga at Lake Ontario by 1636, and the lands of the western Iroquois by 1679, as it was carried by Mohawk and other Native Americans who traveled the trading routes.[49] The high rate of fatalities caused breakdowns in Native American societies and disrupted generational exchange of culture. After European explorers reached the West Coast in the 1770s, smallpox rapidly killed at least 30% of Northwest Coast Native Americans. For the next 80 to 100 years, smallpox and other diseases devastated native populations in the region.[51]Puget Sound area populations, once estimated as high as 37,000 people, were reduced to only 9,000 survivors by the time settlers arrived en masse in the mid-19th century.[52] The Spanish missions in California did not have a large effect on the overall population of Native Americans because the small number of missions was concentrated in a small area along the southern and central coast. The numbers of indigenes decreased more rapidly after California ceased to be a Spanish colony, especially during the second half of the 19th century and the beginning of the 20th (see chart on the right). Smallpox epidemics in 1780–1782 and 1837–1838 brought devastation and drastic depopulation among the Plains Indians.[53][54] By 1832, the federal government established a smallpox vaccination program for Native Americans (The Indian Vaccination Act of 1832). It was the first federal program created to address a health problem of Native Americans.[55][56] With the meeting of two worlds, animals, insects, and plants were carried from one to the other, both deliberately and by chance, in what is called the Columbian exchange. Sheep, pigs, horses, and cattle were all Old World animals that were introduced to contemporary Native Americans who never knew such animals.[57] In the 16th century, Spaniards and other Europeans brought horses to Mexico. Some of the horses escaped and began to breed and increase their numbers in the wild. The early American horse had been game for the earliest humans on the continent. It was hunted to extinction about 7000 BCE, just after the end of the last glacial period.[citation needed] Native Americans benefited from the reintroduction of horses, as they adopted the use of the animals, they began to change their cultures in substantial ways, especially by extending their nomadic ranges for hunting. The reintroduction of the horse to North America had a profound impact on Native American culture of the Great Plains. The tribes trained and used horses to ride and to carry packs or pull travois. The people fully incorporated the use of horses into their societies and expanded their territories. They used horses to carry goods for exchange with neighboring tribes, to hunt game, especially bison, and to conduct wars and horse raids. One of the first major contacts, in what would be called the American Deep South, occurred when the conquistador Juan Ponce de León landed in La Florida in April 1513. There he encountered the Timucuan and Ais peoples.[58] De León returned in 1521 in an attempt at colonization, but after fierce resistance from the Calusa people, the attempt was abandoned. He was later followed by other Spanish explorers, such as Pánfilo de Narváez in 1528 and Hernando de Soto in 1539. In 1536, a group of four Spanish explorers and one enslaved black Moorish man, found themselves stranded on the coast of what is now Texas.[59] The group was led by Álvar Núñez Cabeza de Vaca, and for a time they were held in semi captivity by the coastal natives.[60] The enslaved Moor, whose name was Esterban, later became a scout who had encounters with the Zunis.[60] Rumors of the fabled Seven Cities of Gold being located in the northern area of New Spain began to emerge amongst the Spaniards. And in 1540 Francisco Vázquez de Coronado, using the information gained by the scouting expeditions of Esterban and Fray Marcos, set out to conquer Cíbola.[59] Coronado and his band of over one thousand found no cities of gold. What the conquistadors did encounter was Hawikuh, a Zuni town. There the Zuni people, having never seen horses or a band of this size before, were frightened. Although Coronado had been explicitly instructed not to harm the natives, when the Zuni refused his insistence for food and supplies, Coronado ordered an attack on the town.[58] Through the mid 17th century the Beaver Wars were fought over the fur trade between the Iroquois and the Hurons, the northern Algonquians, and their French allies. During the war the Iroquois destroyed several large tribal confederacies—including the Huron, Neutral, Erie, Susquehannock, and Shawnee, and became dominant in the region and enlarged their territory. King Philip's War, also called Metacom's War or Metacom's Rebellion, was an armed conflict between Native American inhabitants of present-day southern New England and English colonists and their Native American allies from 1675 to 1676. It continued in northern New England (primarily on the Maine frontier) even after King Philip was killed, until a treaty was signed at Casco Bay in April 1678.[citation needed] According to a combined estimate of loss of life in Schultz and Tougias' King Philip's War, The History and Legacy of America's Forgotten Conflict (based on sources from the Department of Defense, the Bureau of Census, and the work of Colonial historian Francis Jennings), 800 out of 52,000 English colonists of New England (1 out of every 65) and 3,000 out of 20,000 natives (3 out of every 20) lost their lives due to the war, which makes it proportionately one of the bloodiest and costliest in the history of America.[citation needed] More than half of New England's 90 towns were assaulted by Native American warriors. One in ten soldiers on both sides were wounded or killed.[61] The war is named after the main leader of the Native American side, Metacomet (also known as Metacom or Pometacom) who was known to the English as King Philip. He was the last Massasoit (Great Leader) of the Pokanoket Tribe/Pokanoket Federation and Wampanoag Nation. Upon their loss to the Colonists, many managed to flee to the North to continue their fight against the British (Massachusetts Bay Colony) by joining with the Abenaki Tribes and Wabanaki Federation.[citation needed] Between 1754 and 1763, many Native American tribes were involved in the French and Indian War/Seven Years' War. Those involved in the fur trade in the northern areas tended to ally with French forces against British colonial militias. Native Americans fought on both sides of the conflict. The greater number of tribes fought with the French in the hopes of checking British expansion. The British had made fewer allies, but it was joined by some tribes that wanted to prove assimilation and loyalty in support of treaties to preserve their territories. They were often disappointed when such treaties were later overturned. The tribes had their own purposes, using their alliances with the European powers to battle traditional Native enemies. Native American culture began to have an influence on European thought in this period. Some Europeans considered Native American societies to be representative of a golden age known to them only in folk history.[62] The political theorist Jean Jacques Rousseau wrote that the idea of freedom and democratic ideals was born in the Americas because \"it was only in America\" that Europeans from 1500 to 1776 knew of societies that were \"truly free.\"[62] Natural freedom is the only object of the policy of the [Native Americans]; with this freedom do nature and climate rule alone amongst them ... [Native Americans] maintain their freedom and find abundant nourishment... [and are] people who live without laws, without police, without religion. However, leading historians of the period note that historic evidence is lacking to support such an interpretation. Gordon Wood wrote, \"The English colonists did not need the Indians to tell them about federalism or self-government. The New England Confederation was organized as early as 1643.\"[66] The historian Jack Rakove, a specialist in early American history, in 2005 noted that the voluminous documentation of the Constitutional proceedings \"contain no significant reference to Iroquois.\"[66] Secondly, he notes: \"All the key political concepts that were the stuff of American political discourse before the Revolution and after, had obvious European antecedents and referents: bicameralism, separation of powers, confederations, and the like.\"[66] American Indians have played a central role in shaping the history of the nation, and they are deeply woven into the social fabric of much of American life.... During the last three decades of the 20th century, scholars of ethnohistory, of the \"new Indian history,\" and of Native American studies forcefully demonstrated that to understand American history and the American experience, one must include American Indians. Yamacraw Creek Native Americans meet with the Trustee of the colony of Georgia in England, July 1734. The painting shows a Native American boy (in a blue coat) and woman (in a red dress) in European clothing. During the American Revolution, the newly proclaimed United States competed with the British for the allegiance of Native American nations east of the Mississippi River. Most Native Americans who joined the struggle sided with the British, based both on their trading relationships and hopes that colonial defeat would result in a halt to further colonial expansion onto Native American land. Many native communities were divided over which side to support in the war and others wanted to remain neutral. Seeking out treaties with the Indigenous inhabitants soon became a very pressing matter. It was during the American Revolution that the newly forming United States would sign its first treaty as a nation with the Indigenous inhabitants. In a bid to gain ground near the British stronghold of Detroit the Continental Congress reached out to the Leni Lenape, also known as the Delawares, to form an alliance. Understanding a treaty would be the best way to secure this alliance, in 1778 The Treaty with The Delawares was signed by representatives from the Congress and the Lenape.[68] For the Iroquois Confederacy, based in New York, the American Revolution resulted in civil war. The only Iroquois tribes to ally with the colonials were the Oneida and Tuscarora. Frontier warfare during the American Revolution was particularly brutal, with noncombatants suffering greatly. Both sides committed numerous atrocities and destroyed villages and food supplies to reduce the ability of people to fight, as in frequent raids in the Mohawk Valley and central New York. The largest of these expeditions was the Sullivan Expedition of 1779, in which American colonial troops destroyed more than 40 Iroquois villages in order to neutralize Iroquois raids. The expedition failed to have the desired effect: Native American activity became even more determined.[69] The British made peace with the Americans in the Treaty of Paris, through which they ceded vast Native American territories to the United States without informing or consulting with the Native Americans. Within the Peace Treaty of Paris of 1783, no mention of Indigenous peoples or their rights were made.[70] The United States initially treated the Native Americans who had fought as allies with the British as a conquered people who had lost their lands. Although most members of the Iroquois tribes went to Canada with the Loyalists, others tried to stay in New York and western territories to maintain their lands. The state of New York made a separate treaty with Iroquois nations and put up for sale 5,000,000 acres (20,000 km2) of land that had previously been their territories. The state established small reservations in western New York for the remnant peoples. The Indians presented a reverse image of European civilization which helped America establish a national identity that was neither savage nor civilized. — Charles Sanford, The Quest for Paradise: Europe and the American Moral Imagination[71] The United States was eager to expand, to develop farming and settlements in new areas, and to satisfy land hunger of settlers from New England and new immigrants. The belief and inaccurate presumption was that the land was not settled and existed in a state of nature and therefore was free to be settled by citizens of the newly formed United States.[72] In the years after the American Revolution, the newly formed nation set about acquiring lands in the Northwest Territory through a multitude of treaties with Native nations. The coercive tactics used to obtain these treaties often left the Native Nations with the option to sell the land or face war.[58] The states and settlers were frequently at odds with this policy.[73] Congress passed the Northwest Ordinance in 1787, which was conceived to allow for the United States to sell lands inhabited by the Native nations to settlers willing to move into that area.[60] During this time, what came to be called The Northwest Indian War also began, led by the Native nations of the Ohio country trying to repulse American settlers and halt the seizure of land by the Continental Congress. Leaders such as Little Turtle and Blue Jacket lead the allied tribes of the Miamis and Shawnees,[75] who were among the tribes that had been disregarded during the signing of the Peace Treaty of Paris.[76] European nations sent Native Americans (sometimes against their will) to the Old World as objects of curiosity. They often entertained royalty and were sometimes prey to commercial purposes. Christianization of Native Americans was a charted purpose for some European colonies. Whereas it hath at this time become peculiarly necessary to warn the citizens of the United States against a violation of the treaties.... I do by these presents require, all officers of the United States, as well civil as military, and all other citizens and inhabitants thereof, to govern themselves according to the treaties and act aforesaid, as they will answer the contrary at their peril. United States policy toward Native Americans had continued to evolve after the American Revolution. George Washington and Henry Knox believed that Native Americans were equals but that their society was inferior. Washington formulated a policy to encourage the \"civilizing\" process.[78] Washington had a six-point plan for civilization which included: impartial justice toward Native Americans regulated buying of Native American lands promotion of commerce promotion of experiments to civilize or improve Native American society Robert Remini, a historian, wrote that \"once the Indians adopted the practice of private property, built homes, farmed, educated their children, and embraced Christianity, these Native Americans would win acceptance from white Americans.\"[80] The United States appointed agents, like Benjamin Hawkins, to live among the Native Americans and to teach them how to live like whites.[81] How different would be the sensation of a philosophic mind to reflect that instead of exterminating a part of the human race by our modes of population that we had persevered through all difficulties and at last had imparted our Knowledge of cultivating and the arts, to the Aboriginals of the Country by which the source of future life and happiness had been preserved and extended. But it has been conceived to be impracticable to civilize the Indians of North America — This opinion is probably more convenient than just. Benjamin Hawkins, seen here on his plantation, teaches Creek Native Americans how to use European technology. Painted in 1805. In the late 18th century, reformers starting with Washington and Knox,[82] supported educating native children and adults, in efforts to \"civilize\" or otherwise assimilate Native Americans to the larger society (as opposed to relegating them to reservations). The Civilization Fund Act of 1819 promoted this civilization policy by providing funding to societies (mostly religious) who worked on Native American improvement. I rejoice, brothers, to hear you propose to become cultivators of the earth for the maintenance of your families. Be assured you will support them better and with less labor, by raising stock and bread, and by spinning and weaving clothes, than by hunting. A little land cultivated, and a little labor, will procure more provisions than the most successful hunt; and a woman will clothe more by spinning and weaving, than a man by hunting. Compared with you, we are but as of yesterday in this land. Yet see how much more we have multiplied by industry, and the exercise of that reason which you possess in common with us. Follow then our example, brethren, and we will aid you with great pleasure... — President Thomas Jefferson, Brothers of the Choctaw Nation, December 17, 1803[83] The end of the 18th century also saw the revival of spirituality among the Iroquois society and other nations of the eastern seaboard. After years of war and uncertainty, despair and demoralization led some within these communities to turn to alcohol.[58] In 1799, the Seneca warrior Handsome Lake, who suffered from depression and alcoholism himself, received a spiritual vision.[84] This vision led Handsome Lake to travel among the Seneca as a religious prophet. He preached about a revival of the traditional ceremonies of the Haudenosaunee nations and a renouncement of drinking.[84] This movement, which also carried some elements of Christianity, came to be known as Gaiwiio, or Good Word.[85] Tecumseh was the Shawnee leader of Tecumseh's War who attempted to organize an alliance of Native American tribes throughout North America.[86] As American expansion continued, Native Americans resisted settlers' encroachment in several regions of the new nation (and in unorganized territories), from the Northwest to the Southeast, and then in the West, as settlers encountered the tribes of the Great Plains. \"The Indian wars under the government of the United States have been more than 40 in number. They have cost the lives of about 19,000 white men, women and children, including those killed in individual combats, and the lives of about 30,000 Indians. The actual number of killed and wounded Indians must be very much higher than the given... Fifty percent additional would be a safe estimate...\"[89] In July 1845, the New York newspaper editor John L. O’Sullivan coined the phrase, \"Manifest Destiny,\" as the \"design of Providence\" supporting the territorial expansion of the United States.[90]Manifest Destiny had serious consequences for Native Americans, since continental expansion for the United States took place at the cost of their occupied land. Manifest Destiny was a justification for expansion and westward movement, or, in some interpretations, an ideology or doctrine that helped to promote the progress of civilization. Advocates of Manifest Destiny believed that expansion was not only good, but that it was obvious and certain. The term was first used primarily by Jacksonian Democrats in the 1840s to promote the annexation of much of what is now the Western United States (the Oregon Territory, the Texas Annexation, and the Mexican Cession). What a prodigious growth this English race, especially the American branch of it, is having! How soon will it subdue and occupy all the wild parts of this continent and of the islands adjacent. No prophecy, however seemingly extravagant, as to future achievements in this way [is] likely to equal the reality. In 1851, delegates from the federal government and upwards of ten thousand Indigenous peoples, consisting of various Plains tribes including the Sioux, Cheyenne and Crow among many others, assembled. They gathered for the purpose of signing the Treaty of Fort Laramie which would set the definitive boundaries of the tribal territories, and tribes were to agree to leave travelers through the territory unharmed.[92] In 1853 members of the tribes from the southern Plains such as the Comanches, Kiowas, and Kiowa Apaches signed treaties similar to the Fort Laramie Treaty of 1851.[92] In the years following the 1851 treaty, tracks were laid for the Union Pacific Railroad and gold was discovered in Montana and Colorado.[93] These factors amongst others led to increased traffic through tribal land which in turn disrupted the game animals that were necessary for the Plains’ nations survival.[58] Conflicts between the U.S. Army, settlers and Native Americans continued; however, in 1864 after the massacre of a Cheyenne village along the banks of Sand Cheek, war between the U.S. and the tribes of the Great Plains was inevitable.[94] Chief Red Cloud and other Sioux Warriors After a decade of wars between the U.S. and the tribes of the Great Plains, including Red Cloud's War in 1866, the federal government again called for a treaty. In 1868 the Peace Treaty of Fort Laramie was signed, with one of the terms of the treaty being that the Sioux would settle on the Black hills Reservation in Dakota Territory.[58] In 1874, gold was discovered within the Black Hills, land which is to this day most sacred to the Sioux. The Black Hills were at this time also the center of the Sioux Nation, the federal government offered six million dollars for the land, but Sioux leaders refused to sell. (In the Hands) By 1877 the Black Hills were confiscated, and the land that had once been the Sioux Nation was further divided into six smaller reservations.[95] The age of manifest destiny, which came to be associated with extinguishing American Indian territorial claims and moving them to reservations, gained ground as the United States population explored and settled west of the Mississippi River. Although Indian Removal from the Southeast had been proposed by some as a humanitarian measure to ensure their survival away from Americans, conflicts of the 19th century led some European-Americans to regard the natives as \"savages\". The period of the Gold Rush was marked by the California genocide. Under U.S. sovereignty, the indigenous population plunged from approximately 150,000 in 1848 to 30,000 in 1870 and reached its nadir of 16,000 in 1900. Thousands of California Native Americans, including women and children, are documented to have been killed by non-Native Americans in this period. The dispossession and murder of California Native Americans was aided by institutions of the state of California, which encouraged indigenous peoples to be killed with impunity.[96][97] Ely Parker was a Union Civil War General who wrote the terms of surrender between the United States and the Confederate States of America.[98] Parker was one of two Native Americans to reach the rank of brigadier general during the Civil War. Many Native Americans served in the military during the Civil War, on both sides.[99] By fighting with the whites, Native Americans hoped to gain favor with the prevailing government by supporting the war effort.[99][100] General Ely S. Parker, a member of the Seneca tribe, transcribed the terms of the articles of surrender which General Robert E. Lee signed at Appomattox Court House on April 9, 1865. Gen. Parker, who served as Gen. Ulysses S. Grant's military secretary and was a trained attorney, was once rejected for Union military service because of his race. At Appomattox, Lee is said to have remarked to Parker, \"I am glad to see one real American here,\" to which Parker replied, \"We are all Americans.\"[99] General Stand Watie, a leader of the Cherokee Nation and Confederate Indian cavalry commander, was the last Confederate General to surrender his troops.[101] As many as 100,000 Native Americans relocated to the West as a result of this Indian Removal policy. In theory, relocation was supposed to be voluntary and many Native Americans did remain in the East. In practice, great pressure was put on Native American leaders to sign removal treaties. The most egregious violation of the stated intention of the removal policy took place under the Treaty of New Echota, which was signed by a dissident faction of Cherokees but not the principal chief. The following year, the Cherokee conceded to removal, but Georgia included their land in a lottery for European-American settlement before that. President Jackson used the military to gather and transport the Cherokee to the west, whose timing and lack of adequate supplies led to the deaths of an estimated 4,000 Cherokees on the Trail of Tears. About 17,000 Cherokees, along with approximately 2,000 enslaved blacks held by Cherokees, were taken by force migration to Indian Territory.[102] Tribes were generally located to reservations where they could more easily be separated from traditional life and pushed into European-American society. Some southern states additionally enacted laws in the 19th century forbidding non-Native American settlement on Native American lands, with the intention to prevent sympathetic white missionaries from aiding the scattered Native American resistance.[103] In 1817, the Cherokee became the first Native Americans recognized as U.S. citizens. Under Article 8 of the 1817 Cherokee treaty, \"Upwards of 300 Cherokees (Heads of Families) in the honest simplicity of their souls, made an election to become American citizens.\"[104][105] The next earliest recorded date of Native Americans' becoming U.S. citizens was in 1831, when some Mississippi Choctaw became citizens after the United States Congress ratified the Treaty of Dancing Rabbit Creek.[106][107][108][109] Article 22 sought to put a Choctaw representative in the U.S. House of Representatives.[106] Under article XIV of that treaty, any Choctaw who elected not to move with the Choctaw Nation could become an American citizen when he registered and if he stayed on designated lands for five years after treaty ratification. Through the years, Native Americans became U.S. citizens by: 1. Treaty provision (as with the Cherokee) 2. Registration and land allotment under the Dawes Act of February 8, 1887 3. Issuance of Patent in Fee simple 4. Adopting Habits of Civilized Life 5. Minor Children 6. Citizenship by Birth 7. Becoming Soldiers and Sailors in the U.S. Armed Forces 8. Marriage to a U.S. citizen 9. Special Act of Congress. In 1857, Chief Justice Roger B. Taney expressed the opinion of the court that since Native Americans were \"free and independent people,\" they could become U.S. citizens.[110][111] Taney asserted that Native Americans could be naturalized and join the \"political community\" of the United States.[111] [Native Americans], without doubt, like the subjects of any other foreign Government, be naturalized by the authority of Congress, and become citizens of a State, and of the United States; and if an individual should leave his nation or tribe, and take up his abode among the white population, he would be entitled to all the rights and privileges which would belong to an emigrant from any other foreign people. — Chief Justice Roger B. Taney, 1857, What was Taney thinking? American Indian Citizenship in the era of Dred Scott, Frederick E. Hoxie, April 2007.[111] After the American Civil War, the Civil Rights Act of 1866 states, \"that all persons born in the United States, and not subject to any foreign power, excluding Indians not taxed, are hereby declared to be citizens of the United States\".[112] This was affirmed by the ratification of the Fourteenth Amendment. But the concept of Native Americans as U.S. citizens fell out of favor among politicians at the time. Senator Jacob Howard of Michigan commented, “I am not yet prepared to pass a sweeping act of naturalization by which all the Indian savages, wild or tame, belonging to a tribal relation, are to become my fellow-citizens and go to the polls and vote with me\". (Congressional Globe, 1866, 2895)[113] In a Senate floor debate regarding the Fourteenth Amendment, James Rood Doolittle of Wisconsin stated, \" ... all those wild Indians to be citizens of the United States, the Great Republic of the world, whose citizenship should be a title as proud as that of king, and whose danger is that you may degrade that citizenship (Congressional Globe, 1866, 2892).\"[113] In 1871 Congress added a rider to the Indian Appropriations Act ending United States recognition of additional Native American tribes or independent nations, and prohibiting additional treaties. That hereafter no Indian nation or tribe within the territory of the United States shall be acknowledged or recognized as an independent nation, tribe, or power with whom the United States may contract by treaty: Provided, further, that nothing herein contained shall be construed to invalidate or impair the obligation of any treaty heretofore lawfully made and ratified with any such Indian nation or tribe. After the Indian wars in the late 19th century, the United States established Native American boarding schools, initially run primarily by or affiliated with Christian missionaries.[115] At this time American society thought that Native American children needed to be acculturated to the general society. The boarding school experience often proved traumatic to Native American children, who were forbidden to speak their native languages, taught Christianity and denied the right to practice their native religions, and in numerous other ways forced to abandon their Native American identities[116] and adopt European-American culture. Since the late 20th century, investigations have documented cases of sexual, physical and mental abuse occurring at such schools.[117][118] While problems were documented as early as the 1920s, some of the schools continued into the 1960s. Since the rise of self-determination for Native Americans, they have generally emphasized education of their children at schools near where they live. In addition, many federally recognized tribes have taken over operations of such schools and added programs of language retention and revival to strengthen their cultures. Beginning in the 1970s, tribes have also founded colleges at their reservations, controlled and operated by Native Americans, to educate their young for jobs as well as to pass on their cultures. On August 29, 1911 Ishi, generally considered to have been the last Native American to live most of his life without contact with European American culture, was discovered near Oroville, California after a forest fire drove him from nearby mountains. He was the last of his tribe, the rest having been massacred by a party of White \"Indian fighters\" in 1865 when he was a boy. After being jailed in protective custody, Ishi was released to anthropologists led by Alfred L. Kroeber at the University of California. They studied his Southern Yahi language and culture, and provided him a home until his death from tuberculosis five years later.[119][120][121] On June 2, 1924, U.S. Republican President Calvin Coolidge signed the Indian Citizenship Act, which made citizens of the United States of all Native Americans born in the United States and its territories and who were not already citizens. Prior to passage of the act, nearly two-thirds of Native Americans were already U.S. citizens.[122] American Indians today have all the rights guaranteed in the U.S. Constitution, can vote in elections, and run for political office. There has been controversy over how much the federal government has jurisdiction over tribal affairs, sovereignty, and cultural practices.[123] Be it enacted by the Senate and House of Representatives of the United States of America in Congress assembled, That all noncitizen Native Americans born within the territorial limits of the United States be, and they are hereby, declared to be citizens of the United States: Provided, That the granting of such citizenship shall not in any manner impair or otherwise affect the right of any Native American to tribal or other property. Some 44,000 Native Americans served in the United States military during World War II: at the time, one-third of all able-bodied Indian men from 18 to 50 years of age.[124] The entry of young men into the United States military during World War II has been described as the first large-scale exodus of indigenous peoples from the reservations. It involved more people than any migration since the removals from areas east of the Mississippi River of the early 19th century. The men's service with the U.S. military in the international conflict was a turning point in Native American history. The overwhelming majority of Native Americans welcomed the opportunity to serve; they had a voluntary enlistment rate that was 40% higher than those who were drafted. War Department officials said that if the entire population had enlisted in the same proportion as the Native Americans, the response would have rendered the draft unnecessary.[125] Their fellow soldiers often held them in high esteem, in part since the legend of the tough Native American warrior had become a part of the fabric of American historical legend. White servicemen sometimes showed a lighthearted respect toward Native American comrades by calling them \"chief\". Native American cultures were profoundly changed after their young men returned home, because of their wide contact with the world outside of the reservation system. \"The war\", said the U.S. Indian Commissioner in 1945, \"caused the greatest disruption of Native life since the beginning of the reservation era\", affecting the habits, views, and economic well-being of tribal members.[126] The most significant of these changes was the opportunity—as a result of wartime labor shortages—to find well-paying work in cities. After the war many Native Americans relocated to urban areas, particularly on the West Coast with the buildup of the defense industry. In the 1950s the federal government had a relocation policy encouraging them to do so because of economic opportunity in cities. But Native Americans struggled with discrimination and the great cultural changes in leaving their reservations behind. There were also losses as a result of the war. For instance, a total of 1,200 Pueblo men served in World War II; only about half came home alive. In addition many more Navajo served as Code talkers for the military in the Pacific. The code they made, although cryptologically very simple, was never cracked by the Japanese. Military service and urban residency contributed to the rise of American Indian activism, particularly after the 1960s and the occupation of Alcatraz Island (1969–1971) by a student Indian group from San Francisco. In the same period, the American Indian Movement (AIM) was founded in Minneapolis, and chapters were established throughout the country, where American Indians combined spiritual and political activism. Political protests gained national media attention and the sympathy of the American public. Indian activists from around the country joined them at Pine Ridge, and the occupation became a symbol of rising American Indian identity and power. Federal law enforcement officials and the national guard cordoned off the town, and the two sides had a standoff for 71 days. During much gunfire, one United States Marshal was wounded and paralyzed. In late April a Cherokee and local Lakota man were killed by gunfire; the Lakota elders ended the occupation to ensure no more lives were lost.[127] In June 1975, two FBI agents seeking to make an armed robbery arrest at Pine Ridge Reservation were wounded in a firefight, and killed at close range. The AIM activist Leonard Peltier was sentenced in 1976 to two consecutive terms of life in prison in the FBI deaths.[128] In 1968, the government enacted the Indian Civil Rights Act. This gave tribal members most of the protections against abuses by tribal governments that the Bill of Rights accords to all U.S. citizens with respect to the federal government.[129] In 1975 the U.S. government passed the Indian Self-Determination and Education Assistance Act, marking the culmination of 15 years of policy changes. It resulted from American Indian activism, the Civil Rights Movement, and community development aspects of President Lyndon Johnson's social programs of the 1960s. The Act recognized the right and need of Native Americans for self-determination. It marked the U.S. government's turn away from the 1950s policy of termination of the relationship between tribes and the government. The U.S. government encouraged Native Americans' efforts at self-government and determining their futures. Tribes have developed organizations to administer their own social, welfare and housing programs, for instance. Tribal self-determination has created tension with respect to the federal government's historic trust obligation to care for Indians, but the Bureau of Indian Affairs has never lived up to that responsibility.[130] By this time, tribes had already started to establish community schools to replace the BIA boarding schools. Led by the Navajo Nation in 1968, tribes started tribal colleges and universities, to build their own models of education on reservations, preserve and revive their cultures, and develop educated workforces. In 1994 the U.S. Congress passed legislation recognizing the tribal colleges as land-grant colleges, which provided opportunities for funding. Thirty-two tribal colleges in the United States belong to the American Indian Higher Education Consortium. By the early 21st century, tribal nations had also established numerous language revival programs in their schools. In addition, Native American activism has led major universities across the country to establish Native American studies programs and departments, increasing awareness of the strengths of Indian cultures, providing opportunities for academics, and deepening research on history and cultures in the United States. Native Americans have entered academia; journalism and media; politics at local, state and federal levels; and public service, for instance, influencing medical research and policy to identify issues related to American Indians. In 1981, Tim Giago founded the Lakota Times, an independent Native American newspaper, located at the Pine Ridge Reservation but not controlled by tribal government. He later founded the Native American Journalists Association. Other independent newspapers and media corporations have been developed, so that Native American journalists are contributing perspective on their own affairs and other policies and events. In 2004, Senator Sam Brownback (Republican of Kansas) introduced a joint resolution (Senate Joint Resolution 37) to \"offer an apology to all Native Peoples on behalf of the United States\" for past \"ill-conceived policies\" by the U.S. government regarding Indian Tribes.[131] President Barack Obama signed the historic apology into law in 2009, as Section 8113 of the 2010 defense appropriations bill.[132] After years of investigation and independent work by Native American journalists, in 2003 the U.S. government indicted suspects in the December 1975 murder of Anna Mae Aquash at the Pine Ridge Indian Reservation. A Mi'kmaq, Aquash was the highest-ranking woman activist in the American Indian Movement (AIM) at the time. She was killed several months after two FBI agents had been killed at the reservation. Many Lakota believe that she was killed by AIM on suspicion of having been an FBI informant, but she never worked for the FBI.[133]Arlo Looking Cloud was convicted in federal court in 2004. In 2007 the United States extradited AIM activist John Graham from Canada to stand trial for her murder.[134] He was also convicted and sentenced to life. The Indian Arts and Crafts Act of 1990 (P.L. 101–644) is a truth-in-advertising law that prohibits misrepresentation in marketing of American Indian or Alaska Native arts and crafts products within the United States, including dreamcatchers. It is illegal to offer or display for sale, or sell any art or craft product in a manner that falsely suggests it is Indian produced. Native American tribes and individuals began to file suits against the federal government over a range of issues, especially land claims and mismanagement of trust lands and fees. A number of longstanding cases were finally settled by the administration of President Barack Obama, who made a commitment to improve relations between the federal government and the tribes. Among these was Cobell v. Salazar, a class action suit settled in 2009, with Congress appropriating funds in 2010.[135] Another was Keepseagle v. , settled in April 2011. The $760 million settlement \"designated $680 million for Native American farmers who had faced discrimination from the U.S. Department of Agriculture over a period of several years in the past.[136] By 2012, \"the Justice and Interior departments had reached settlements totaling more than $1 billion with 41 tribes for claims of mismanagement.\"[135] The Navajo Nation gained the largest settlement with a single tribe, of $554 million.[135] It is the largest tribe in the United States. In 2013, under renewal of the Violence Against Women Act, the federal government strengthened protection of Native American women, as it established authority for tribes to prosecute non-Natives who commit crimes on Indian land.[135] Domestic and sexual abuse of Native American women has been a problem in many areas, but previous laws prevented arrest or prosecution by tribal police or courts of non-native abusive partners.[137][138] Native American migration to urban areas continued to grow: 70% of Native Americans lived in urban areas in 2012, up from 45% in 1970, and 8% in 1940. Urban areas with significant Native American populations include Rapid City, Minneapolis, Oklahoma City, Denver, Phoenix, Tucson, Seattle, Chicago, Houston, and New York City. Many have lived in poverty and struggled with discrimination. Racism, unemployment, drugs and gangs were common problems which Indian social service organizations, such as the Little Earth housing complex in Minneapolis, have attempted to address.[139] As of the 2020 census, the largest self-identified Native American group not combined with another race is Aztec, numbering 378,122 individuals. Though Aztecs are indigenous to Mexico and not the United States, they are nevertheless considered Native American people per census guidelines, which includes any indigenous people from the Americas.[140][141] ^Native American History and Cultures, \"Se tribes\". Archived from the original on 2006-09-10. Retrieved 2006-09-19. Susan Squires and John Kincheloe, syllabus for HIS 943A, Meredith College, 2005. Retrieved 2006-09-19. ^Mee, Charles L., Jr. The Genius of the People. New York: Harper & Row, 1987. p. 237. Note: John Rutledge of South Carolina is said to have read lengthy tracts of Iroquoian law to the other framers of the Constitution, beginning with the words, \"We, the people, to form a union, to establish peace, equity, and order...\""}
{"url": "https://en.wikipedia.org/wiki/Charles_F._Newcombe", "text": "Newcombe was born in Newcastle-upon-Tyne, England, as the eighth of fourteen children. His parents were William Lister Newcombe (1817–1908) and Eliza Jane (Rymer) (1816–1888), who were both from Yorkshire.[3] In 1884, Newcombe emigrated to the United States, establishing a general practice in Hood River, Oregon. He moved in 1885 with his family to Victoria, British Columbia. In 1889, he moved back to Victoria and worked at the \"Insane Asylum\" in New Westminster. His wife Marian died after the birth of their sixth child in 1891, leaving him with two daughters and four sons. Newcombe began to study the botany of North America and made many trips to Haida Gwaii (formerly the Queen Charlotte Islands) by boat. In the process he became very interested in the Haida and started to collect their artifacts to \"preserve\" them from, what was then thought to be, the impending demise of the native culture.[3] Newcombe and others were driven by the \"fear that 'pure' Northwest Coast cultures were disappearing through depopulation and assimilation\". In 1897, George Amos Dorsey traveled with him in an effort to collect Haida artifacts for the Field Columbian Museum in Chicago. Dorsey, an American, was known for his haste in his collecting. Dorsey asked Scottish guide James Deans to keep quiet about their activities in the area. Local missionary John Henry Keen lambasted Deans and his unidentified American collaborators for disrupting and desecrating the graves of the local natives in their hunt for Northwest Coast artifacts.[4] Newcombe also conducted biological and geographic research, such as on local (British Columbia) mollusks and paleontology. In 1913, he led a Commission studying the effect of sea lions on the salmon industry. In 1914, he prepared a report on the circumnavigation of Vancouver Island. Much of his work, including collection of plants, mollusks, fossils, aboriginal artifacts and information, was done with the help of his youngest surviving son, William Henry Arnold Newcombe (1884-1960). Newcombe died in 1924 in Victoria, British Columbia, after catching a cold (more likely pneumonia) on a sailing expedition.[3]"}
{"url": "https://en.wikipedia.org/wiki/D%27Albertis_Castle", "text": "View of the Albertis CastleSculpture of Christopher Columbus as a boy by Giulio Monteverde. D'Albertis Castle (Italian: Castello d'Albertis) is a historical residence in Genoa, north-western Italy. It was the home of sea captain Enrico Alberto d'Albertis and was donated to the city of Genoa on his death in 1932. It currently houses the Museo delle Culture del Mondo (Museum of World Cultures), inaugurated in 2004. D'Albertis designed the castle in the style of an architectural collage with a Gothic revival appearance inspired by palaces in Florence and castles of Aosta Valley. Erected between 1886 and 1892 under the supervision of Gothic RevivalistAlfredo D'Andrade, the castle is located on the site of a 13th-century fortified area, which had been reinforced in the 16th century. Alberto not only based his design on the city's foundation, he incorporated and preserved the foundations of the bastion and one of the turrets.[1] Inaugurated for the celebrations of 400 years of Columbus' discover of America, it was the first villa-castle built in Genoa.[2] From top of the hill of Monte Galletto (or Montegalletto), one of the hills in the district of Castelletto, the castle dominates Genoa with a view of the Ligurian Sea."}
