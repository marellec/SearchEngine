{"url": "https://en.m.wikipedia.org/wiki/List_of_common_misconceptions", "text": "List of common misconceptions Each entry on this list of common misconceptions is worded as a correction; the misconceptions themselves are implied rather than stated. These entries are concise summaries of the main subject articles, which can be consulted for more detail. Federal legal tender laws in the United States do not state that a private business, a person, or an organization must accept cash for payment, though it must be regarded as valid payment for debts when tendered to a creditor.[1] The common image of Santa Claus (Father Christmas) as a jolly old man in red robes was not created by The Coca-Cola Company as an advertising gimmick. Santa Claus had already taken this form in American popular culture and advertising by the late 19th century, long before Coca-Cola used his image in the 1930s.[8] PepsiCo never owned the \"6th most powerful navy\" in the world after a deal with the Soviet Union. In 1989, Pepsi acquired several decommissioned warships as part of a barter deal.[10][11] The oil tankers were leased out and the other ships sold for scrap.[12] A follow-on deal involved another 10 ships.[13] Searing does not seal moisture in meat; in fact, it causes it to lose some moisture. Meat is seared to brown it, to affect its color, flavor, and texture.[14] Twinkies, an American snack cake generally considered to be \"junk food\", have a shelf life of around 45 days, despite the common claim (usually facetious) that they remain edible for decades.[15][16] Twinkies, with only sorbic acid as an added preservative, normally remain on a store shelf for 7 to 10 days.[17][18] With the exception of some perishables, properly stored foods can safely be eaten past their \"expiration\" dates.[19][20] The vast majority of expiration dates in the United States are regulated by state governments and refer to food quality, not safety; the \"use by\" date represents the last day the manufacturer warrants the quality of their product. Hydrox is not a knock-off of Oreos. Hydrox, invented in 1908, predates Oreos by four years and outsold it until the 1950s, when Oreos raised prices and the name \"Hydrox\" became increasingly unappealing due to being said to sound like a laundry detergent brand, after similar new brands of the decade.[31][32][33] Spices were not used in the Middle Ages to mask the flavor of rotten meat before refrigeration. Spices were an expensive luxury item; those who could afford them could afford good meat, and there are no contemporaneous documents calling for spices to disguise the taste of bad meat.[43] Microwave ovens do not cook food from the inside out. 2.45 GHz microwaves can only penetrate approximately 1 centimeter (3⁄8 inch) into most foods. The inside portions of thicker foods are mainly heated by heat conducted from the outer portions.[54] Microwave ovens do not cause cancer, as microwave radiation is non-ionizing and therefore does not have the cancer risks associated with ionizing radiation such as X-rays. No studies have found that microwave radiation causes cancer, even with exposure levels far greater than normal radiation leakage.[55] Microwaving food does not reduce its nutritive value and may preserve it better than other cooking processes due to shorter cooking times.[56] Ronald Reagan was never seriously considered for the role of Rick Blaine in the 1942 film Casablanca, eventually played by Humphrey Bogart. An early studio press release mentioned Reagan, but the studio already knew that Reagan was unavailable because of his upcoming military service.[57] Indeed, the producer had always wanted Bogart for the part.[58] Irregardless is a word.[86][87]Nonstandard, slang, or colloquial terms used by English speakers are sometimes alleged not to be real words, despite appearing in numerous dictionaries. All words in English became accepted by being commonly used for a certain period of time; thus, there are many vernacular words currently not accepted as part of the standard language, or regarded as inappropriate in formal speech or writing, but the idea that they are not words is a misconception.[88] Other examples of words that are sometimes alleged not to be words include burglarize, licit,[89] and funnest[90] which appear in numerous dictionaries as English words.[91] The word crap did not originate as a back-formation of British plumber Thomas Crapper's aptronymous surname, nor does his name originate from the word crap.[98] The surname \"Crapper\" is a variant of \"Cropper\", which originally referred to someone who harvested crops.[99] The word crap ultimately comes from Medieval Latincrappa.[100] The word fuck did not originate in the Middle Ages as an acronym for either \"fornicating under consent of king\" or \"for unlawful carnal knowledge\", either as a sign posted above adulterers in the stocks, or as a sign on houses visible from the road during the Black Death. Nor did it originate as a corruption of \"pluck yew\" (an idiom falsely attributed to the English for drawing a longbow).[104] It is most likely derived from Middle Dutch or other Germanic languages, where it either meant \"to thrust\" or \"to copulate with\" (fokken in Middle Dutch), \"to copulate\", or \"to strike, push, copulate\" or \"penis\".[104][105] Either way, these variations would have been derived from the Indo-European root word -peuk, meaning \"to prick\".[104] The expression \"rule of thumb\" did not originate from an English law allowing a man to beat his wife with a stick no thicker than his thumb, and there is no evidence that such a law ever existed.[106] The false etymology has been broadly reported in media including Time magazine (1983), The Washington Post (1989) and CNN (1993).[107] The expression originates from the seventeenth century from various trades where quantities were measured by comparison to the width or length of a thumb.[108][109] The word the was never pronounced or spelled \"ye\" in Old or Middle English.[110] The confusion, seen in the common stock phrase \"ye olde\", derives from the use of the character thorn (þ), which in Middle English represented the sound now represented in Modern English by \"th\". Early printing presses often lacked types for the letter þ, meaning that \"þͤ\" () and \"þe\" were substituted with the visually similar \"yͤ\" and \"ye\", respectively.[111] The anti-Italian slur wop did not originate from an acronym for \"without papers\" or \"without passport\";[112] it is actually derived from the term guappo (roughly meaning thug or \"dandy\"), from Spanish guapo.[113] \"Xmas\", along with a modern Santa Claus, used on a Christmas postcard (1910) Xmas did not originate as a secular plan to \"take the Christ out of Christmas\".[114]X represents the Greek letter chi, the first letter of Χριστός (Christós), \"Christ\" in Greek,[115] as found in the chi-rho symbol ΧΡ since the 4th century. In English, \"X\" was first used as a scribal abbreviation for \"Christ\" in 1100; \"X'temmas\" is attested in 1551, and \"Xmas\" in 1721.[116] It is not necessary to wait 24 hours before filing a missing person report. When there is evidence of violence or of an unusual absence, it is important to start an investigation promptly.[117][118]Criminology experts say the first 72 hours in a missing person investigation are the most critical.[119] The US Armed Forces have generally forbidden military enlistment as a form of deferred adjudication (that is, an option for convicts to avoid jail time) since the 1980s. US Navy protocols discourage the practice, while the other four branches have specific regulations against it.[121] The United States does not require police officers to identify themselves as police in the case of a sting or other undercover work, and police officers may lie when engaged in such work.[122] Claiming entrapment as a defense instead focuses on whether the defendant was induced by undue pressure (such as threats) or deception from law enforcement to commit crimes they would not have otherwise committed.[123] Violent crime rates in the United States declined significantly between 1994 and 2003. Neither the Mafia nor other criminal organizations regularly use or have used cement shoes to drown their victims.[133] There are only two documented cases of this method being used in murders: one in 1964 and one in 2016 (although, in the former, the victim had concrete blocks tied to his legs rather than being enclosed in cement).[134] The French Army did use cement shoes on Algerians killed in death flights during the Algerian War.[135] In the United States, a defendant may not have their case dismissed simply because they were not read their Miranda rights at the time of their arrest. Miranda warnings cover the rights of a person when they are taken into custody and then interrogated by law enforcement.[136][137] If a person is not given a Miranda warning before the interrogation is conducted, statements made by them during the interrogation may not be admissible in a trial. The prosecution may still present other forms of evidence, or statements made during interrogations where the defendant was read their Miranda rights, to get a conviction.[138] Chewing gum is not punishable by caning in Singapore. Although importing and selling chewing gum has been illegal in Singapore since 1992, and corporal punishment is still an applicable penalty for certain offenses in the country, the two facts are unrelated; chewing gum-related offenses have always been only subject to fines, and the possession or consumption of chewing gum itself is not illegal.[139][140] No cases have been proven of strangers killing or permanently injuring children by intentionally hiding poisons, drugs, or sharp objects such as razor blades in candy during Halloween trick-or-treating.[144] However, in rare cases, adult family members have spread this story to cover up filicide or accidental deaths. Folklorists, scholars, and law enforcement experts say that the story that strangers put poison into candy and give that candy to trick-or-treating children has been \"thoroughly debunked\".[145][144] Mary Shelley's 1818 novel Frankenstein is named after the fictional scientist Victor Frankenstein, who created the sapient creature in the novel, not the creature itself, which is never named and is called Frankenstein's monster. However, as later adaptations started to refer to the monster itself as Frankenstein, this usage became well-established, and some no longer regard it as erroneous.[147][148] Listening to Mozart or classical music does not enhance intelligence (or IQ). A study from 1993 reported a short-term improvement in spatial reasoning.[160][161] However, the weight of subsequent evidence supports either a null effect or short-term effects related to increases in mood and arousal, with mixed results published after the initial report in Nature.[162][163][164][165] The Beatles' 1965 appearance at Shea Stadium was not the first time that a rock concert was played at a large, outdoor sports stadium in the U.S. Such venues were employed by Elvis Presley in the 1950s and the Beatles themselves in 1964.[174] Phil Collins did not write his 1981 hit \"In the Air Tonight\" about witnessing someone drowning and then confronting the person in the audience who let it happen. According to Collins himself, it was about his emotions when divorcing from his first wife.[176] Jesus was most likely not born on December 25, when his birth is traditionally celebrated as Christmas. It is more likely that his birth was in either the season of spring or perhaps summer. Although the Common Era ostensibly counts the years since the birth of Jesus,[178] it is unlikely that he was born in either AD 1 or 1 BC, as such a numbering system would imply. Modern historians estimate a date closer to between 6 BC and 4 BC.[179] The Bible does not say that exactly three magi came to visit the baby Jesus, nor that they were kings, or rode on camels, or that their names were Caspar, Melchior, and Balthazar, nor what color their skin was. Three magi are inferred because three gifts are described, but the Bible says only that there was more than one magus.[180][181][182][183][184][185] Paul the Apostle did not change his name from Saul. He was born a Jew, with Roman citizenship inherited from his father, and thus carried both a Hebrew and a Greco-Roman name from birth, as mentioned by Luke in Acts 13:9: \"...Saul, who also is called Paul...\".[188] Roman Catholic dogma does not say that the pope is either sinless or always infallible.[191] Catholic dogma since 1870 does state that a dogmatic teaching contained in divine revelation that is promulgated by the pope (deliberately, and under certain very specific circumstances; generally called ex cathedra) is free from error, although official invocation of papal infallibility is rare. Most theologians state that canonizations meet the requisites.[192] Otherwise, even when speaking in his official capacity, dogma does not hold that he is always free from error. Saint Augustine did not say \"God created hell for inquisitive people\".[198] He actually said: \"I do not give the answer that someone is said to have given (evading by a joke the force of the objection), 'He was preparing hell for those who pry into such deep subjects.' ... I do not answer in this way. I would rather respond, 'I do not know,' concerning what I do not know than say something for which a man inquiring about such profound matters is laughed at, while the one giving a false answer is praised.\"[199] So Augustine is saying that he would not say this and that he does not know the answer to the question. Most Muslim women do not wear a burqa (also transliterated as burka or burkha), which covers the body, head, and face, with a mesh grille to see through. Many Muslim women cover their hair and face (excluding the eyes) with a niqāb, or just their hair with a hijab[202] and many Muslim women wear neither face nor head coverings of any kind.[203] The word \"jihad\" does not always mean \"holy war\"; its literal meaning in Arabic is \"struggle\". While there is such a thing as \"jihad bil saif\", or jihad \"by the sword\",[206] it can be any spiritual or moral effort or struggle,[207][208] such as seeking knowledge, putting others before oneself, and inviting others to Islam.[209] The Quran does not promise martyrs 72 virgins in heaven. It does mention that virgin female companions,[210]houri, are given to all people, martyr or not, in heaven, but no number is specified. The source for the 72 virgins is a hadith in Sunan al-Tirmidhi by Imam Tirmidhi.[211][212] Hadiths are sayings and acts of Muhammad as reported by others, not part of the Quran itself.[213][211] The name golf is not an acronym for \"Gentlemen Only, Ladies Forbidden\".[217][218][219] It may have come from the Dutch word kolf or kolve, meaning \"club\",[218] or from the Scottish word goulf or gowf meaning \"to strike or cuff\".[217] The black belt in martial arts does not necessarily indicate expert level or mastery. It was introduced for judo in the 1880s to indicate competency at all of the basic techniques of the sport. Promotion beyond 1st dan (the first black belt rank) varies among different martial arts.[221] India did not withdraw from the 1950 FIFA World Cup because their squad played barefoot, which was against FIFA regulations.[224] In reality, India withdrew because the country's managing body, the All India Football Federation (AIFF), was insufficiently prepared for the team's participation and gave various reasons for withdrawing, including a lack of funding and prioritizing the Olympics.[225] There is no definitive proof that violent video games cause people to become violent. Some studies have found no link between aggression and violent video games,[226][227] and the popularity of gaming has coincided with a decrease in youth violence.[228][229] The moral panic surrounding video games in the 1980s through to the 2020s, alongside several studies and incidents of violence and legislation in many countries, likely contributed to proliferating this idea.[230][231] The Pyramids of Egypt were not constructed with slave labor. Archaeological evidence shows that the laborers were a combination of skilled workers and poor farmers working in the off-season with the participants paid in high-quality food and tax exemptions.[243][244][245] The idea that slaves were used originated with Herodotus, and the idea that they were Israelites arose centuries after the pyramids were constructed.[244][245][246] Galleys in ancient times were not commonly operated by chained slaves or prisoners, as depicted in films such as Ben Hur, but by paid laborers or soldiers,[247] with slaves used only in times of crisis, in some cases even gaining freedom after the crisis was averted. Ptolemaic Egypt was a possible exception.[248] Other types of vessel, such as merchant vessels (usually sailing vessels) were manned by slaves, sometimes even with slaves as ship's master.[249] The ancient Greeks did not use the word \"idiot\" (Ancient Greek: ἰδιώτης, romanized: idiṓtēs) to disparage people who did not take part in civic life or who did not vote. An ἰδιώτης was simply a private citizen as opposed to a government official. Later, the word came to mean any sort of non-expert or layman, then someone uneducated or ignorant, and much later to mean stupid or mentally deficient.[254] Julius Caesar was not born via caesarean section. Such a procedure would have been fatal to the mother at the time, and Caesar's mother was still alive when Caesar was 45 years old.[259][260] The name \"caesarean\" probably comes from the Latin verb caedere 'to cut'.[261] While modern life expectancies are much higher than those in the Middle Ages and earlier,[263] adults in the Middle Ages did not die in their 30s or 40s on average. That was the life expectancy at birth, which was skewed by high infant and adolescent mortality. The life expectancy among adults was much higher;[264] a 21-year-old man in medieval England, for example, could expect to live to the age of 64.[265][264] However, in various places and eras, life expectancy was noticeably lower, as in medieval London, where 90% of people in general died before the age of 45[266] and one study estimated that 36 percent of men and 56 percent of women in medieval urban areas passed before the age of 35.[267] Monks in this time period often died in their 20s or 30s.[267] In the tale of King Canute and the tide, the king did not command the tide to reverse in a fit of delusional arrogance.[272] According to the story, his intent was to prove a point to members of his privy council that no man is all-powerful, and that all people must bend to forces beyond their control, such as the tides. Marco Polo did not import pasta from China,[273] a misconception that originated with the Macaroni Journal, published by an association of food industries to promote the use of pasta in the United States.[274] Marco Polo describes a food similar to \"lasagna\" in his Travels, but he uses a term with which he was already familiar. There is no evidence that iron maidens were used for torture, or even yet invented, in the Middle Ages. Instead they were pieced together in the 18th century from several artifacts found in museums, arsenals and the like to create spectacular objects intended for commercial exhibition.[275] Spiral staircases in castles were not designed in a clockwise direction to hinder right-handed attackers.[276][277] While clockwise spiral staircases are more common in castles than anti-clockwise, they were even more common in medieval structures without a military role, such as religious buildings.[278][276] The plate armor of European soldiers did not stop soldiers from moving around or necessitate a crane to get them into a saddle. They would routinely fight on foot and could mount and dismount without help.[279] However, armor used in tournaments in the late Middle Ages was significantly heavier than that used in warfare,[280] which may have contributed to this misconception. Whether chastity belts, devices designed to prevent women from having sexual intercourse, were invented in medieval times is disputed by modern historians. Most existing chastity belts are now thought to be deliberate fakes or anti-masturbatory devices from the 19th and early 20th centuries.[281] Medieval cartographers did not regularly write \"here be dragons\" on their maps. The only maps from this era that have the phrase inscribed on them are the Hunt-Lenox Globe and the Ostrich Egg Globe, next to a coast in Southeast Asia for both of them. Maps instead were more likely to have \"here are lions\" inscribed. Maps in this period did occasionally have illustrations of mythical beasts like dragons and sea serpents, as well as exotic animals like elephants, on them.[283] The Mexica people of the Aztec Empire did not mistake Hernán Cortés and his landing party for gods during Cortés' conquest of the empire. This notion came from Francisco López de Gómara, who never went to Mexico and concocted the myth while working for the retired Cortés in Spain years after the conquest.[288] Shah Jahan, the Indian Mughal Emperor who commissioned the Taj Mahal, did not cut off the hands of the rumored 40,000 workers or lead designers so as to not allow the construction of another monument more beautiful than the Taj Mahal. This is an urban myth that goes back to the 1960s.[289][290][291] The early settlers of the Plymouth Colony in North America usually did not wear all black, and their capotains (hats) were shorter and rounder than the widely depicted tall hat with a buckle on it. Instead, their fashion was based on that of the late Elizabethan era.[292] The traditional image was formed in the 19th century when buckles were a kind of emblem of quaintness.[293] (The Puritans, who also settled in Massachusetts near the same time, did frequently wear all black.)[294] The familiar story that Isaac Newton was inspired to research the nature of gravity when an apple fell on his head is almost certainly apocryphal. All Newton himself ever said was that the idea came to him as he sat \"in a contemplative mood\" and \"was occasioned by the fall of an apple\".[295] Marie Antoinette did not say \"let them eat cake\" when she heard that the French peasantry were starving due to a shortage of bread. The phrase was first published in Rousseau's Confessions, written when Marie Antoinette was only nine years old and not attributed to her, just to \"a great princess\". It was first attributed to her in 1843.[297] George Washington did not have wooden teeth. His dentures were made of lead, gold, hippopotamus ivory, the teeth of various animals, including horse and donkey teeth,[298][299] and human teeth, possibly bought from slaves or poor people.[300][301] The possible origin of this myth is that ivory teeth quickly became stained and may have had the appearance of wood to observers.[299] Napoleon Bonaparte was not especially short for a Frenchman of his time. He was the height of an average French male in 1800, but short for an aristocrat or officer.[306] After his death in 1821, the French emperor's height was recorded as 5 feet 2 inches in French feet, which in English measurements is 5 feet 7 inches (1.70 m).[307][308] Albert Einstein did not fail mathematics classes in school. Einstein remarked, \"I never failed in mathematics.... Before I was fifteen I had mastered differential and integral calculus.\"[312] Einstein did, however, fail his first entrance exam into the Swiss Federal Polytechnic School (ETH) in 1895, when he was two years younger than his fellow students, but scored exceedingly well in the mathematics and science sections, and then passed on his second attempt.[313] Grigori Rasputin was not assassinated by being fed cyanide-laced cakes and wine, shot multiple times, and then thrown into the Little Nevka river when he survived the former two. A contemporary autopsy reported that he was just killed with gunshots. A sensationalized account from the memoirs of co-conspirator PrinceFelix Yusupov is the only source of this story.[315][316][317] The Italian dictator Benito Mussolini did not \"make the trains run on time\". Much of the repair work had been performed before he and the Fascist Party came to power in 1922. Moreover, the Italian railways' supposed adherence to timetables was more propaganda than reality.[318] The Nazis did not use the term \"Nazi\" to refer to themselves. The full name of the Nazi Party was Nationalsozialistische Deutsche Arbeiterpartei (National Socialist German Workers' Party), and members referred to themselves as Nationalsozialisten (National Socialists) or Parteigenossen (party comrades). The term \"Nazi\" was in use prior to the rise of the Nazis as a colloquial and derogatory word for a backwards farmer or peasant. Opponents of the National Socialists abbreviated their name as \"Nazi\" for derogatory effect and the term was popularized by German exiles outside of Germany.[320] Although popularly known as the \"red telephone\", the Moscow–Washington hotline was never a telephone line, nor were red phones used. The first implementation of the hotline used teletype equipment, which was replaced by facsimile (fax) machines in 1988. Since 2008, the hotline has been a secure computer link over which the two countries exchange email.[327] Moreover, the hotline links the Kremlin to the Pentagon, not the White House.[328] The Alaska Purchase was generally viewed as positive or neutral in the United States, both among the public and the press. The opponents of the purchase who characterized it as \"Seward's Folly\", alluding to William H. Seward, the Secretary of State who negotiated it, represented a minority opinion at the time.[345][346] There is no evidence that Frederic Remington, on assignment to Cuba in 1897, telegraphed William Randolph Hearst: \"There will be no war. I wish to return,\" nor that Hearst responded: \"Please remain. You furnish the pictures, and I'll furnish the war\". The anecdote was originally included in a book by James Creelman and probably never happened.[350] Immigrants' last names were not Americanized (voluntarily, mistakenly, or otherwise) upon arrival at Ellis Island. Officials there kept no records other than checking ship manifests created at the point of origin, and there was simply no paperwork that would have let them recast surnames, let alone any law. At the time in New York, anyone could change the spelling of their name simply by using that new spelling.[352] These names are often referred to as an \"Ellis Island Special\". Prohibition did not make drinking alcohol illegal in the United States. The Eighteenth Amendment and the subsequent Volstead Act prohibited the production, sale, and transport of \"intoxicating liquors\" within the United States, but their possession and consumption were never outlawed.[353] U.S. Senator George Smathers never gave a speech to a rural audience describing his opponent, Claude Pepper, as an \"extrovert\" whose sister was a \"thespian\", in the apparent hope they would confuse them with similar-sounding words like \"pervert\" and \"lesbian\". Smathers offered US$10,000 to anyone who could prove he had made the speech; it was never claimed.[362] Rosa Parks was not sitting in the front (\"white\") section of the bus during the event that made her famous and incited the Montgomery bus boycott. Rather, she was sitting in the front of the back (\"colored\") section of the bus, where African Americans were expected to sit, and rejected an order from the driver to vacate her seat in favor of a white passenger when the \"white\" section of the bus had become full.[363] When Kitty Genovese was murdered outside her apartment in 1964, there were not 38 neighbors standing idly by and watching who failed to call the police until after she was dead, as was initially reported[367] to widespread public outrage that persisted for years and even became the basis of a theory in social psychology. In fact, witnesses only heard brief portions of the attack and did not realize what was occurring, and only six or seven actually saw anything. One witness, who had called the police, said when interviewed by officers at the scene, \"I didn't want to get involved\",[368] an attitude later attributed to all the neighbors.[369] While it was praised by one architectural magazine before it was built as \"the best high apartment of the year\", the Pruitt–Igoehousing project in St. Louis, Missouri, considered to epitomize the failures of urban renewal in American cities after it was demolished in the early 1970s, never won any awards for its design.[370] The architectural firm that designed the buildings did win an award for an earlier St. Louis project, which may have been confused with Pruitt–Igoe.[371] There is little contemporary documentary evidence for the notion that US Vietnam veterans were spat upon by anti-war protesters upon return to the United States. This belief was detailed in some biographical accounts and was later popularized by films such as Rambo.[372][373][374] Women did not burn their bras outside the Miss America contest in 1969 as a protest in support of women's liberation. They did symbolically throw bras in a trash can, along with other articles seen as emblematic of women's position in American society such as mops, make-up, and high-heeled shoes. The myth of bra burning came when a journalist hypothetically suggested that women may do so in the future, as men of the era burned their draft cards.[375] Black holes have the same gravitational effects as any other equal mass in their place. They will draw objects nearby towards them, just as any other celestial body does, except at very close distances to the black hole, comparable to its Schwarzschild radius.[394] If, for example, the Sun were replaced by a black hole of equal mass, the orbits of the planets would be essentially unaffected. A black hole can pull in a substantial inflow of surrounding matter, but only if the star from which it formed was already doing so.[395] Egg balancing is possible on every day of the year, not just the vernal equinox,[402] and there is no relationship between any astronomical phenomenon and the ability to balance an egg.[403] The Fisher Space Pen was not commissioned by NASA at a cost of millions of dollars, while the Soviets used pencils. It was independently developed by Paul C. Fisher, founder of the Fisher Pen Company, with $1 million of his own funds.[404] NASA tested and approved the pen for space use, then purchased 400 pens at $6 per pen.[405] The Soviet Union subsequently also purchased the Space Pen for its Soyuz spaceflights.[406] The Sun is not yellow; rather, it emits light across the full spectrum of visible colors, and this combined light appears white when outside of Earth's atmosphere. Earth's atmosphere scatters shorter wavelengths of light, particularly blues and violets, more than longer wavelengths like reds and yellows, and this scattering is why the Sun appears yellow during the day or orange or red during sunrise and sunset.[409][410] A satellite image of a section of the Great Wall of China, running diagonally from lower left to upper right (not to be confused with the much more prominent river running from upper left to lower right). The region pictured is 12 by 12 kilometers (7.5 mi × 7.5 mi). The Big Bang model does not fully explain the origin of the universe. It does not describe how energy, time, and space were caused, but rather it describes the emergence of the present universe from an ultra-dense and high-temperature initial state.[412] Bulls are not enraged by the color red, used in capes by professional bullfighters. Cattle are dichromats, so red does not stand out as a bright color. It is not the color of the cape, but the perceived threat by the bullfighter that incites it to charge.[414] Lemmings do not engage in mass suicidal dives off cliffs when migrating. The scenes of lemming suicides in the 1958 Disney documentary film White Wilderness, which popularized this idea, were completely fabricated. The lemmings in the film were actually purchased from Inuit children for 25 cents apiece, transported to the filming location in Canmore, Alberta, and repeatedly shoved off a nearby cliff by the filmmakers to create the illusion of a mass suicide.[415][416] The misconception itself is much older, dating back to at least the late 19th century, though its exact origins are uncertain.[417] Dogs do not consistently age seven times as quickly as humans. Aging in dogs varies widely depending on the breed; certain breeds, such as giant dog breeds and English bulldogs, have much shorter lifespans than average. Most dogs age consistently across all breeds in the first year of life, reaching adolescence[clarification needed] by one year old; smaller and medium-sized breeds begin to age more slowly in adulthood.[420] The phases of the Moon have no effect on the vocalizations of wolves, and wolves do not howl at the Moon.[421] Wolves howl to assemble the pack usually before and after hunts, to pass on an alarm particularly at a den site, to locate each other during a storm, while crossing unfamiliar territory, and to communicate across great distances.[422] There is no such thing as an \"alpha\" in a wolf pack. An early study that coined the term \"alpha wolf\" had only observed unrelated adult wolves living in captivity. In the wild, wolf packs operate like families: parents are in charge until the young grow up and start their own families, and younger wolves do not overthrow an \"alpha\" to become the new leader.[423][424] Bats are not blind. While about 70% of bat species, mainly in the microbat family, use echolocation to navigate, all bat species have eyes and are capable of sight. In addition, almost all bats in the megabat or fruit bat family cannot echolocate and have excellent night vision.[425] Sharks can get cancer. The misconception that sharks do not get cancer was spread by the 1992 book Sharks Don't Get Cancer, which was used to sell extracts of shark cartilage as cancer prevention treatments. Reports of carcinomas in sharks exist, and current data do not support any conclusions about the incidence of tumors in sharks.[429] Great white sharks do not mistake human divers for seals or other pinnipeds. When attacking pinnipeds, the shark surfaces quickly and attacks violently. In contrast, attacks on humans are slower and less violent: the shark charges at a normal pace, bites, and swims off. Great white sharks have efficient eyesight and color vision; the bite is not predatory, but rather for identification of an unfamiliar object.[430] Snake jaws cannot unhinge. The posterior end of the lower jaw bones contains a quadrate bone, allowing jaw extension. The anterior tips of the lower jaw bones are joined by a flexible ligament allowing them to bow outwards, increasing the mouth gape.[431][432] Porcupines do not shoot their quills. They can detach, and porcupines will deliberately back into attackers to impale them, but their quills do not project.[435][436][437] Mice do not have a special appetite for cheese, and will eat it only for lack of better options; they actually favor sweet, sugary foods. The myth may have come from the fact that before the advent of refrigeration, cheese was usually stored outside and was therefore an easy food for mice to reach.[438] There is no credible evidence that the candiru, a South American parasitic catfish, can swim up a human urethra if one urinates in the water in which it lives. The sole documented case of such an incident, written in 1997, has been heavily criticized upon peer review, and this phenomenon is now largely considered a myth.[439] Piranhas do not eat only meat but are omnivorous, and they only swim in schools to defend themselves from predators and not to attack. They very rarely attack humans, only when under stress and feeling threatened, and even then, bites typically only occur on hands and feet.[442] The hippopotamus does not produce pink milk, nor does it sweat blood. The skin secretions of the hippopotamus are red due to the presence of hipposudoric acid, a red pigment which acts as a natural sunscreen, and is neither sweat or blood. It does not affect the color of their milk, which is white or beige.[443] A human touching or handling eggs or baby birds will not cause the adult birds to abandon them.[447] The same is generally true for other animals having their young touched by humans as well, with the possible exception of rabbits (as rabbits will sometimes abandon their nest after an event they perceive as traumatizing).[448] The bold, powerful cry commonly associated with the bald eagle in popular culture is actually that of a red-tailed hawk. Bald eagle vocalizations are much softer and chirpier, and bear far more resemblance to the calls of gulls.[455][456] Ostriches do not stick their heads in the sand to hide from enemies or to sleep.[457] This misconception's origins are uncertain but it was probably popularized by Pliny the Elder (23–79 CE), who wrote that ostriches \"imagine, when they have thrust their head and neck into a bush, that the whole of their body is concealed\".[458] A duck's quack actually does echo,[459] although the echo may be difficult to hear for humans under some circumstances.[460] Despite this, a British panel show compiling interesting facts has been given the name Duck Quacks Don't Echo. The skin of a chameleon is not adapted solely for camouflage purposes, nor can a chameleon change its skin colour to match any background.[463] Rabbits are not specially partial to carrots. Their diet in the wild primarily consists of dark green vegetables such as grasses and clovers, and excessive carrot consumption is unhealthy for them due to containing high levels of sugar. This misconception originated from Bugs Bunny cartoons, whose carrot-chomping habit was meant as a reference to a minor character in It Happened One Night.[464][465][466] Houseflies have an average lifespan of 20 to 30 days, not 24 hours.[469] The misconception may arise from confusion with mayflies, which, in one species, have an adult lifespan of as little as 5 minutes.[470] The daddy longlegs spider (Pholcidae) is not the most venomous spider in the world. Their fangs are capable of piercing human skin, but the tiny amount of venom they carry causes only a mild burning sensation for a few seconds.[471] Other species such as harvestmen, crane flies, and male mosquitoes are also called daddy longlegs in some regional dialects, and share the misconception of being highly venomous but unable to pierce the skin of humans.[472][473] People do not swallow large numbers of spiders during sleep. A sleeping person makes noises that warn spiders of danger.[474][475] Most people also wake up from sleep when they have a spider on their face.[476] Earwigs are not known to purposely climb into external ear canals, though there have been anecdotal reports of earwigs being found in the ear.[482] The name may be a reference to the appearance of their hindwings, which are unique and distinctive among insects, and resemble a human ear when unfolded.[483][484] While certainly critical to the pollination of many plant species, European honey bees are not essential to human food production, despite claims that without their pollination, humanity would starve or die out \"within four years\".[485] In fact, many important crops need no insect pollination at all. The ten most important crops,[486] accounting for 60% of all human food energy,[487] all fall into this category. Ticks do not jump or fall from trees onto their hosts. Instead, they lie in wait to grasp and climb onto any passing host or otherwise trace down hosts via, for example, olfactory stimuli, the host's body heat, or carbon dioxide in the host's breath.[488][489] Poinsettias are not highly toxic to humans or cats. While it is true that they are mildly irritating to the skin or stomach,[501] and may sometimes cause diarrhea and vomiting if eaten, they rarely cause serious medical problems.[502] Sunflowers do not always point to the Sun. Flowering sunflowers face a fixed direction (often east) all day long, but do not necessarily face the Sun.[503] However, in an earlier developmental stage, before the appearance of flower heads, the immature buds do track the Sun (a phenomenon called heliotropism), and the fixed alignment of the mature flowers toward a certain direction is often the result.[504] The word theory in \"the theory of evolution\" does not imply scientific doubt regarding its validity; the concepts of theory and hypothesis have specific meanings in a scientific context. While theory in colloquial usage may denote a hunch or conjecture, a scientific theory is a set of principles that explains an observable phenomenon in natural terms.[508][509] \"Scientific fact and theory are not categorically separable\",[510] and evolution is a theory in the same sense as germ theory or the theory of gravitation.[511] The theory of evolution does not attempt to explain the origin of life[512] or the origin and development of the universe. The theory of evolution deals primarily with changes in successive generations over time after life has already originated.[513] The scientific model concerned with the origin of the first organisms from organic or inorganic molecules is known as abiogenesis, and the prevailing theory for explaining the early development of the universe is the Big Bang model. Mutations are not entirely random, nor do they occur at the same frequency everywhere in the genome. Certain regions of an organism's genome will be more or less likely to undergo mutation depending on the presence of DNA repair mechanisms and other mutation biases. For instance, in a study on Arabidopsis thaliana, biologically important regions of the plant's genome were found to be protected from mutations, and beneficial mutations were found to be more likely, i.e. mutation was \"non-random in a way that benefits the plant\".[526][527][528] Dimetrodon is often mistakenly called a dinosaur or considered to be a contemporary of dinosaurs in popular culture, but it became extinct some 40 million years before the first appearance of dinosaurs. Being a synapsid, Dimetrodon is actually more closely related to mammals than to dinosaurs, birds, lizards, or other diapsids.[539][540][541][542] Humans and aviandinosaurs currently coexist, but humans and non-avian dinosaurs did not coexist at any point.[544] The last of the non-avian dinosaurs died 66 million years ago in the course of the Cretaceous–Paleogene extinction event, whereas the earliest members of the genus Homo (humans) evolved between 2.3 and 2.4 million years ago. This places a 63-million-year expanse of time between the last non-avian dinosaurs and the earliest humans. Humans did coexist with woolly mammoths and saber-toothed cats: extinct mammals often erroneously depicted alongside non-avian dinosaurs.[545] Most diamonds are not formed from highly compressed coal. More than 99% of diamonds ever mined have formed in the conditions of extreme heat and pressure about 140 kilometers (87 mi) below the earth's surface. Coal is formed from prehistoric plants buried much closer to the surface, and is unlikely to migrate below 3.2 kilometers (2.0 mi) through common geological processes. Most diamonds that have been dated are older than the first land plants, and are therefore older than coal.[573] Diamonds are not infinitely hard, and are subject to wear and scratching: although they are the hardest known material on the Mohs Scale, they can be scratched by other diamonds[574] and worn down even by much softer materials, such as vinyl records.[575] Although the core of a wooden pencil is commonly referred to as \"lead\", wooden pencils do not contain the chemical element lead, nor have they ever contained it; \"black lead\" was formerly a name of graphite, which is commonly used for pencil leads.[577] The deep web is not primarily full of pornography, illegal drug trade websites, and stolen bank details. This information is primarily found in a small portion of the deep web known as the \"dark web\". Much of the deep web consists of academic libraries, databases, and anything that is not indexed by normal search engines.[580] Total population living in extreme poverty, by world region 1987 to 2015[587] The total number of people living in extremeabsolute poverty globally, by the widely used metric of $1.00/day (in 1990 U.S. dollars) has decreased over the last several decades, but most people surveyed in several countries incorrectly think it has increased or stayed the same.[588] However, this depends on the poverty line calculation used. For instance, if the metric used is instead one that prioritizes meeting a standard life expectancy that no longer significantly rises with additional consumption enabled by income, the number of individuals in poverty has risen by nearly 1 billion.[589][590] Human population growth is decreasing and the world population is expected to peak and then begin falling during the 21st century. Improvements in agricultural productivity and technology are expected to be able to meet anticipated increased demand for resources, making a global human overpopulation scenario unlikely.[591][592][593] For any given production set, there is not a set amount of labor input (a \"lump of labor\") to produce that output. This fallacy is commonly seen in Luddite and later, related movements as an argument either that automation causes permanent, structural unemployment, or that labor-limiting regulation can decrease unemployment. In fact, changes in capital allocation, efficiency, and economies of learning can change the amount of labor input for a given set of production.[594] Income is not a direct factor in determining credit score in the United States. Rather, credit score is affected by the amount of unused available credit, which is in turn affected by income.[595] Income is also considered when evaluating creditworthiness more generally. In the US, an increase in gross income will never reduce a taxpayer's post-tax earnings (net income) by putting them in a higher tax bracket. Tax brackets specify marginal tax rates: only income earned in the higher tax bracket is taxed at the higher rate.[597] An increase in gross income can reduce net income in a welfare cliff, however, when benefits are withdrawn when passing a certain income threshold.[598] Prevalence of the misconception varies by political party affiliation.[599] Constructing new housing decreases the cost of rent or of buying a home in both the immediate neighborhood and in the city as a whole. In real estate economics, \"supply skepticism\" leads many Americans to misunderstand the effect of increasing the supply of housing on housing costs. The misconception is unique to the housing market.[600][601] Earthquake strength (or magnitude) is not commonly measured using the Richter scale. Although the Richter scale was used historically to measure earthquake magnitude (although, notably, not earthquake damage), it was found in the 1970s that it does not reliably represent the magnitude of large earthquakes. It has therefore been largely replaced by the moment magnitude scale,[623] although very small earthquakes are still sometimes measured using the Richter scale.[624] Nevertheless, earthquake magnitude is still widely misattributed to the Richter scale.[625][626][627]Death rates from air pollution and accidents related to energy production, measured in deaths in the past per terawatt hours (TWh) Lightning can, and often does, strike the same place twice. Lightning in a thunderstorm is more likely to strike objects and spots that are more prominent or conductive. For instance, lightning strikes the Empire State Building in New York City on average 23 times per year.[628] Heat lightning does not exist as a distinct phenomenon. What is mistaken for \"heat lightning\" is usually ordinary lightning from storms too distant to hear the associated thunder.[629] The Earth's interior is not molten rock. This misconception may originate from a misunderstanding based on the fact that the Earth's mantle convects, and the incorrect assumption that only liquids and gases can convect. In fact, a solid with a large Rayleigh number can also convect, given enough time, which is what occurs in the solid mantle due to the very large thermal gradient across it.[634][635] There are small pockets of molten rock in the upper mantle, but these make up a tiny fraction of the mantle's volume.[636] The Earth's outer coreis liquid, but it is liquid metal, not rock.[637] The Amazon rainforest does not provide 20% of Earth's oxygen. This is a misinterpretation of a 2010 study which found that approximately 34% of photosynthesis by terrestrial plants occurs in tropical rainforests (so the Amazon rainforest would account for approximately half of this). Due to respiration by the resident organisms, all ecosystems (including the Amazon rainforest) have a net output of oxygen of approximately zero. The oxygen currently present in the atmosphere was accumulated over billions of years.[638] Rivers do not predominantly flow from north to south. Rivers flow downhill in all compass directions, often changing direction along their course.[640][641] Indeed, many major rivers flow northward, including the Nile, the Yenisey, the Ob, the Rhine, the Lena, and the Orinoco.[642][643] Waking up a sleepwalker does not harm them. Sleepwalkers may be confused or disoriented for a short time after awakening, but the health risks associated with sleepwalking are from injury or insomnia, not from being awakened.[645] Seizures cannot cause a person to swallow their own tongue,[646] and it is dangerous to attempt to place a foreign object into a convulsing person's mouth. Instead it is recommended to gently lay a convulsing person on their side to minimize the risk of asphyxiation.[647] Drowning is often inconspicuous to onlookers.[648] In most cases, the instinctive drowning response prevents the victim from waving or yelling (known as \"aquatic distress\"),[648] which are therefore not dependable signs of trouble; indeed, most drowning victims undergoing the response do not show prior evidence of distress.[649] Human blood in veins is not actually blue. Blood is red due to the presence of hemoglobin; deoxygenated blood (in veins) has a deep red color, and oxygenated blood (in arteries) has a light cherry-red color. Veins below the skin can appear blue or green due to subsurface scattering of light through the skin, and aspects of human color perception. Many medical diagrams also use blue to show veins, and red to show arteries, which contributes to this misconception.[650] Exposure to a vacuum, or experiencing all but the most extreme uncontrolled decompression, does not cause the body to explode or internal fluids to boil (although the fluids in the mouth and lungs will indeed boil at altitudes above the Armstrong limit); rather, it will lead to a loss of consciousness once the body has depleted the supply of oxygen in the blood, followed by death from hypoxia within minutes.[651] Exercise-induced delayed onset muscle soreness is not caused by lactic acid build-up. Muscular lactic acid levels return to normal levels within an hour after exercise; delayed onset muscle soreness is thought to be due to microtrauma from unaccustomed or strenuous exercise.[652] Cremated remains are not ashes in the usual sense. After the incineration is completed, the dry bone fragments are swept out of the retort and pulverized by a machine called a cremulator (essentially a high-capacity, high-speed blender) to process them into \"ashes\" or \"cremated remains\".[656] Half of body heat is not lost through the head, and covering the head is no more effective at preventing heat loss than covering any other portion of the body. Heat is lost from the body in proportion to the amount of exposed skin.[658][659] The head accounts for around 7–9% of the body's surface, and studies have shown that having one's head submerged in cold water only causes a person to lose 10% more heat overall.[660] This myth likely comes from a flawed United States military experiment in 1950, involving a prototype Arctic survival suit where the head was one of the few body parts left exposed.[661] The misconception was further perpetuated by a 1970 military field manual that claimed \"40–45%\" of heat is lost through the head, based on the 1950 study.[659][661] Adrenochrome is not harvested from living people and has no use as a recreational drug. Hunter S. Thompson conceived a fictional drug of the same name in his book Fear and Loathing in Las Vegas, apparently as a metaphor and unaware that a real substance by that name existed; it is Thompson's fictional adrenochrome, and not the real chemical compound, that is the source of numerous conspiracy theories revolving around human trafficking to harvest the fictional drug.[662][663] The use of cotton swabs (aka cotton buds or Q-Tips) in the ear canal has no associated medical benefits and poses definite medical risks.[665] The idea that a precise number of stages of grief exist is not supported in peer-reviewed research or objective clinical observation, let alone the five stages of grief model.[666] The model was originally based on uncredited work and originally applied to the terminally ill instead of the grieving or bereaved.[667] The common cold and the common flu are caused by viruses, not exposure to cold temperatures. However, low temperatures may somewhat weaken the immune system, and someone already infected with a cold or influenza virus but showing no symptoms can become symptomatic after they are exposed to low temperatures.[668][669] Viruses are more likely to spread during the winter for a variety of reasons such as dry air, less air circulation in homes, people spending more time indoors, and lower vitamin D levels in humans.[670][671][672] There is little to no evidence that any illnesses are curable through essential oils or aromatherapy. Fish oil has not been shown to cure dementia, though there is evidence to support the effectiveness of lemon oil as a way to reduce agitation in patients with dementia.[676] In those with the common cold, the color of the sputum or nasal secretion may vary from clear to yellow to green and does not indicate the class of agent causing the infection.[677] The color of the sputum is determined by immune cells fighting an infection in the nasal area.[678] Vitamin Cdoes not prevent or treat the common cold, although it may have a protective effect during intense cold-weather exercise. If taken daily, it may slightly reduce the duration and severity of colds, but it has no effect if taken after the cold starts.[679] In people with eczema, bathing does not dry the skin as long as a moisturizer is applied soon after. If moisturizer is not applied after bathing, then the evaporation of water from the skin can result in dryness.[682] There have never been any programs in the US that provide access to dialysis machines in exchange for pull tabs on beverage cans.[683] This rumor has existed since at least the 1970s, and usually cites the National Kidney Foundation as the organization offering the program. The Foundation itself has denied the rumor, noting that dialysis machines are primarily funded by Medicare.[684] High dietary protein intake is not associated with kidney disease in healthy people.[685] While significantly increased protein intake in the short-term is associated with changes in renal function, there is no evidence to suggest this effect persists in the long-term and results in kidney damage or disease.[686] Leprosy is not auto-degenerative as commonly supposed, meaning that it will not (on its own) cause body parts to be damaged or fall off.[688] Leprosy causes rashes to form and may degrade cartilage and, if untreated, inflame tissue. In addition, leprosy is only mildly contagious, partly because 95% of those infected with the mycobacteria that causes leprosy do not develop the disease.[689][688]Tzaraath, a Biblical disease that disfigures the skin is often identified as leprosy, and may be the source of many myths about the disease.[690] Rust does not cause tetanus infection. The Clostridium tetani bacterium is generally found in dirty environments. Since the same conditions that harbor tetanus bacteria also promote rusting of metal, many people associate rust with tetanus. C. tetani requires anoxic conditions to reproduce and these are found in the permeable layers of rust that form on oxygen-absorbing, unprotected ironwork.[691] Quarantine has never been a standard procedure for those with severe combined immunodeficiency, despite the condition's popular nickname (\"bubble boy syndrome\") and its portrayal in films. A bone marrow transplant in the earliest months of life is the standard course of treatment. The exceptional case of David Vetter, who lived much of his life encased in a sterile environment because he would not receive a transplant until age 12, was an inspirations for the \"bubble boy\" trope.[692] Gunnison, Colorado, did not avoid the 1918 flu pandemic by using protective sequestration. The implementation of protective sequestration did prevent the virus from spreading outside a single household after a single carrier came into the town while it was in effect, but it was not sustainable and had to be lifted in February 1919. A month later, the flu killed five residents and infected dozens of others.[693] Statements in medication package inserts listing the frequency of side effects describe how often the effect occurs after taking a drug, but are not making any assertion that there is a causal connection between taking the drug and the occurrence of the side effect. In other words, what is being reported on is correlation, not necessarily causation.[694] A dog's mouth is not significantly cleaner than a human's mouth. A dog's mouth contains almost as much bacteria as a human mouth.[695][696] Drinking milk or consuming other dairy products does not increase mucus production.[706] As a result, they do not need to be avoided by those with the flu or cold congestion. However, milk and saliva in one's mouth mix to create a thick liquid that can briefly coat the mouth and throat. The sensation that lingers may be mistaken for increased phlegm.[707] Drinking eight glasses (2–3 liters) of water a day is not needed to maintain health.[708] The amount of water needed varies by person, weight, diet, activity level, clothing, and the ambient heat and humidity. Water does not actually need to be drunk in pure form, and can be derived from liquids such as juices, tea, milk, soups, etc., and from foods including fruits and vegetables.[708][709] Drinking coffee and other caffeinated beverages does not cause dehydration for regular drinkers, although it can for occasional drinkers.[710][709] Eating less than an hour before swimming does not increase the risk of experiencing muscle cramps or drowning. One study shows a correlation between alcohol consumption and drowning, but not between eating and stomach cramps.[716] Spinach is not a particularly good source of dietary iron. While it does contain more iron than many vegetables such as asparagus, Swiss chard, kale, or arugula, it contains only about one-third to one-fifth of the iron in lima beans, chickpeas, apricots, or wheat germ. Additionally, the non-heme iron found in spinach and other vegetables is not as readily absorbed as the heme iron found in meats and fish.[729][730][731] Most cases of obesity are not related to slower resting metabolism. Resting metabolic rate does not vary much between people. Overweight people tend to underestimate the amount of food they eat, and underweight people tend to overestimate. In fact, overweight people tend to have faster metabolic rates due to the increased energy required by the larger body.[732] Alcoholic beverages do not make the entire body warmer.[734] Alcoholic drinks create the sensation of warmth because they cause blood vessels to dilate and stimulate nerve endings near the surface of the skin with an influx of warm blood. This can actually result in making the core body temperature lower, as it allows for easier heat exchange with a cold external environment.[735] Alcohol does not necessarily kill brain cells.[736] Alcohol can, however, lead indirectly to the death of brain cells in two ways. First, in chronic, heavy alcohol users whose brains have adapted to the effects of alcohol, abrupt ceasing following heavy use can cause excitotoxicity leading to cellular death in multiple areas of the brain.[737] Second, in alcoholics who get most of their daily calories from alcohol, a deficiency of thiamine can produce Korsakoff's syndrome, which is associated with serious brain damage.[738] The order in which different types of alcoholic beverages are consumed (\"Grape or grain but never the twain\" and \"Beer before liquor never sicker; liquor before beer in the clear\") does not affect intoxication or create adverse side effects.[739] Authentic absinthe has no hallucinogenic properties, and is no more dangerous than any other alcoholic beverage of equivalent proof.[740] This misconception stems from late-19th- and early-20th-century distillers who produced cheap knockoff versions of absinthe, which used copper salts to recreate the distinct green color of true absinthe, and some also reportedly adulterated cheap absinthe with poisonous antimony trichloride, reputed to enhance the louching effect.[741] Lack of a visible hymen is not a reliable indicator that a female has had penetrative sex,[743] because the tearing of the hymen may have been the result of some other event, and some women are born without one.[744][745] Traditional virginity tests, such as the \"two-finger\" test, are widely considered to be unscientific.[746][747][748] While pregnancies from sex between first cousins do carry a slightly elevated risk of birth defects, this risk is often exaggerated.[752] The risk is 5–6% (similar to that of a woman in her early 40s giving birth),[752][753] compared with a baseline risk of 3–4%.[753] The effects of inbreeding depression, while still relatively small compared to other factors (and thus difficult to control for in a scientific experiment), become more noticeable if isolated and maintained for several generations.[754] Having sex before a sporting event or contest is not physiologically detrimental to performance.[755] In fact it has been suggested that sex prior to sports activity can elevate male testosterone levels, which could potentially enhance performance for male athletes.[756] There is no definitive proof of the existence of the vaginal G-spot, and the general consensus is that no such spot exists on the female body.[757] A person's hair and fingernails do not continue to grow after death. Rather, the skin dries and shrinks away from the bases of hairs and nails, giving the appearance of growth.[764] Shaving does not cause terminal hair to grow back thicker or darker. This belief is thought to be due to the fact that hair that has never been cut has a tapered end, so after cutting, the base of the hair is blunt and appears thicker and feels coarser. That short hairs are less flexible than longer hairs contributes to this effect.[765] MC1R, the gene mostly responsible for red hair, is not becoming extinct, nor will the gene for blond hair do so, although both are recessivealleles. Redheads and blonds may become rarer but will not die out unless everyone who carries those alleles dies without passing their hair color genes on to their children.[766] Acne is mostly caused by genetics, and is not caused by a lack of hygiene or eating fatty foods, though certain medication or a carbohydrate-rich diet may worsen it.[767] Dandruff is not caused by poor hygiene, though infrequent hair-washing can make it more obvious. The exact causes of dandruff are uncertain, but they are believed to be mostly genetic and environmental factors.[768] James Watt did not invent the steam engine,[769] nor were his ideas on steam engine power inspired by a kettle lid pressured open by steam.[770] Watt improved upon the already commercially successful Newcomen atmospheric engine (invented in 1712) in the 1760s and 1770s, making certain improvements critical to its future usage, particularly the external condenser, increasing its efficiency, and later the mechanism for transforming reciprocating motion into rotary motion; his new steam engine later gained huge fame as a result.[771] Thomas Edison did not invent the light bulb.[778] He did, however, develop the first practical light bulb in 1880 (employing a carbonized bamboo filament), shortly prior to Joseph Swan, who invented an even more efficient bulb in 1881 (which used a cellulose filament). Henry Ford did not invent either the automobile or the assembly line. He did improve the assembly line process substantially, sometimes through his own engineering but more often through sponsoring the work of his employees, and he was the main person behind the introduction of the Model T, regarded as the first affordable automobile.[779]Karl Benz (co-founder of Mercedes-Benz) is credited with the invention of the first modern automobile,[780] and the assembly line has existed throughout history. The repeating decimal commonly written as 0.999... represents exactly the same quantity as the number one. Despite having the appearance of representing a smaller number, 0.999... is a symbol for the number 1 in exactly the same way that 0.333... is an equivalent notation for the number represented by the fraction 1⁄3.[794] The p-value is not the probability that the null hypothesis is true, or the probability that the alternative hypothesis is false; it is the probability of obtaining results at least as extreme as the results actually observed under the assumption that the null hypothesis was correct, which can indicate the incompatibility of results with the specific statistical model assumed in the null hypothesis.[795] This misconception, and similar ones like it, contributes to the common misuse of p-values in education and research.[795][796] An illustration of the (incorrect) equal-transit-time explanation of aerofoil lift The lift force is not generated by the air taking the same time to travel above and below an aircraft's wing.[798] This misconception, sometimes called the equal transit-time fallacy, is widespread among textbooks and non-technical reference books, and even appears in pilot training materials. In fact, the air moving over the top of an aerofoil generating lift is always moving much faster than the equal transit theory would imply,[798] as described in the incorrect and correct explanations of lift force. Blowing over a curved piece of paper does not demonstrate Bernoulli's principle. Although a common classroom experiment is often explained this way,[799] Bernoulli's principle only applies within a flow field, and the air above and below the paper is in different flow fields.[800] The paper rises because the air follows the curve of the paper and a curved streamline will develop pressure differences perpendicular to the airflow.[801][802] The Coriolis effect does not cause water to consistently drain from basins in a clockwise/counter-clockwise direction depending on the hemisphere. The common myth often refers to the draining action of flush toilets and bathtubs. In fact, rotation is determined by whatever minor rotation is initially present at the time the water starts to drain, as the magnitude of the coriolis acceleration is negligibly small compared to the inertial acceleration of flow within a typical basin.[803] A penny dropped from the Empire State Building would not kill a person or crack the sidewalk. A penny is too light and has too much air resistance to acquire enough speed to do much damage since it reaches terminal velocity after falling about 50 feet. Heavier or more aerodynamic objects could cause significant damage if dropped from that height.[806][807] Using a programmable thermostat's setback feature to limit heating or cooling in a temporarily unoccupied building does not waste as much energy as leaving the temperature constant. Using setback saves energy (5–15%) because heat transfer across the surface of the building is roughly proportional to the temperature difference between its inside and the outside.[808][809] It is not possible for a person to completely submerge in quicksand, as commonly depicted in fiction,[810] although sand entrapment in the nearshore of a body of water can be a drowning hazard as the tide rises.[811] True photographic memory (the ability to remember endless images, particularly pages or numbers, with such a high degree of precision that the image mimics a photo) has never been demonstrated to exist in any individual,[816] although a small number of young children have eidetic memory, where they can recall an object with high precision for a few minutes after it is no longer present.[817] Many people have claimed to have a photographic memory, but those people have been shown to have high precision memories as a result of mnemonic devices rather than a natural capacity for detailed memory encoding.[818] There are rare cases of individuals with exceptional memory, but none of them have a memory that mimics that of a camera. The phase of the Moon does not influence fertility, cause a fluctuation in crime, or affect the stock market. There is no correlation between the lunar cycle and human biology or behavior. However, the increased amount of illumination during the full moon may account for increased epileptic episodes, motorcycle accidents, or sleep disorders.[819] Dyslexia is not defined or diagnosed as mirror writing or reading letters or words backwards.[821][822] Mirror writing and reading letters or words backwards are behaviors seen in many children (dyslexic or not) as they learn to read and write.[821][822]Dyslexia is a neurodevelopmental disorder of people who have at least average intelligence and who have difficulty in reading and writing that is not otherwise explained by low intelligence.[823] Self-harm is not generally an attention-seeking behavior. People who engage in self-harm are typically very self-conscious of their wounds and scars and feel guilty about their behavior, leading them to go to great lengths to conceal it from others.[824] They may offer alternative explanations for their injuries, or conceal their scars with clothing.[825][826] There is no evidence that a chemical imbalance or neurotransmitter deficiency is the sole factor in depression and other mental disorders, but rather a combination of biological, psychological, and social factors.[827][828] Schizophrenia is characterized by continuous or relapsing episodes of psychosis. Major symptoms include hallucinations (typically hearing voices), delusions, paranoia, and disorganized thinking. Other symptoms include social withdrawal, decreased emotional expression, and apathy.[829] The term was coined from the Greek roots schizein and phrēn, \"to split\" and \"mind\", in reference to a \"splitting of mental functions\" seen in schizophrenia, not a splitting of the personality.[830] It does not involve split or multiple personalities—a split or multiple personality is dissociative identity disorder.[831] Broad generalizations are often made in popular psychology about certain brain functions being lateralized, or more predominant in one hemisphere than the other. These claims are often inaccurate or overstated.[832] The human brain, particularly the prefrontal cortex, does not reach \"full maturity\" at any particular age (e.g. 18, 21, or 25 years of age). Changes in structure and myelination of gray matter are recorded to continue with relative consistency all throughout adult life. Some mental abilities peak and begin to decline around high school graduation while others do not peak until much later (i.e. 40s or later).[833] Golgi-stained neurons in human hippocampal tissue. It is commonly believed that humans will not grow new brain cells, but research has shown that some neurons can reform in humans. Humans do not generate all of the brain cells they will ever have by the age of two years. Although this belief was held by medical experts until 1998, it is now understood that new neurons can be created after infancy in some parts of the brain into late adulthood.[834] People do not use only 10% of their brains.[835][836] While it is true that a small minority of neurons in the brain are actively firing at any one time, a healthy human will normally use most of their brain over the course of a day, and the inactive neurons are important as well. The idea that activating 100% of the brain would allow someone to achieve their maximum potential and/or gain various psychic abilities is common in folklore and fiction,[836][837][838] but doing so in real life would likely result in a fatal seizure.[839][840] This misconception was attributed to late 19th century leading thinker William James, who apparently used the expression only metaphorically.[837] Although Phineas Gage's brain injuries, caused by a several-foot-long tamping rod driven completely through his skull, caused him to become temporarily disabled, many fanciful descriptions of his aberrant behavior in later life are without factual basis or contradicted by known facts.[841] All different tastes can be detected on all parts of the tongue by taste buds,[844] with slightly increased sensitivities in different locations depending on the person; the tongue map showing the contrary is fallacious.[845] Swallowing gasoline does not generally require special emergency treatment, as long as it goes into the stomach and not the lungs, and inducing vomiting can make it worse.[854] A chloroform-soaked rag cannot instantly incapacitate a person.[855] It takes at least five minutes of inhaling an item soaked in chloroform to render a person unconscious. Most criminal cases involving chloroform also involve another drug being co-administered, such as alcohol or diazepam, or the victim being found to have been complicit in its administration. The misconception that chloroform can be used as an incapacitating agent[856] has been popularized by crime fiction authors. Toilet waste is never intentionally jettisoned from an aircraft. All waste is collected in tanks and emptied into toilet waste vehicles.[862]Blue ice is caused by accidental leakage from the waste tank. Passenger train toilets, on the other hand, have indeed historically flushed onto the tracks; modern trains in most developed countries usually have retention tanks on board and therefore do not dispose of waste in such a manner. Automotive batteries stored on a concrete floor do not discharge any faster than they would on other surfaces,[863] in spite of worry among Americans that concrete harms batteries.[864] Early batteries with porous, leaky cases may have been susceptible to moisture from floors, but for many years lead–acid car batteries have had impermeable polypropylene cases.[865] While most modern automotive batteries are sealed, and do not leak battery acid when properly stored and maintained,[866] the sulfuric acid in them can leak out and stain, etch, or corrode concrete floors if their cases crack or tip over or their vent-holes are breached by floods.[867] ^Myre G (February 28, 2018). \"A Brief History Of The AR-15\". National Public Radio. Archived from the original on May 13, 2023. Retrieved November 20, 2021. AR\" comes from the name of the gun's original manufacturer, ArmaLite, Inc. The letters stand for ArmaLite Rifle — and not for \"assault rifle\" or \"automatic rifle. ^Rhoads C (January 19, 2008). \"The Hydrox Cookie Is Dead, and Fans Won't Get Over It\". The Wall Street Journal. Retrieved July 6, 2022. In college, when friends ridiculed her for preferring the cheaper knock-off Hydrox to the real thing, she did some research. Among her findings: Hydrox was created in 1908 by what would later become Sunshine Biscuits Inc. That was four years before the National Biscuit Co. (later called Nabisco) came up with the similar Oreo. Oreo was the knock-off. The Hydrox name came from combining the words hydrogen and oxygen, which Sunshine executives thought evoked purity. Others thought it sounded more like a laundry detergent. ^Wheeling K (January 2021). \"A Brief History of Peanut Butter\". Smithsonian Magazine. Retrieved June 24, 2022. North Americans weren't the first to grind peanuts—the Inca beat us to it by a few hundred years—but peanut butter reappeared in the modern world because of an American, the doctor, nutritionist and cereal pioneer John Harvey Kellogg, who filed a patent for a proto-peanut butter in 1895. ^McElwain A (June 17, 2019). \"Did Tayto really invent cheese and onion crisps?\". Irish Times. Retrieved June 23, 2022. One of the oldest known published recipes for crisps is by William Kitchiner, an optician who doubled up as a Georgian-era celebrity chef. His book, A Cook's Oracle, published in 1817, was a big hit in the UK and a young America. Kitchiner's recipe – Potatoes fried in Slices or Shavings – calls for slivers of potato fried in \"lard or dripping\" and \"served with a very little salt sprinkled over them\". ^Burhans D (2008). \"Creation Myths\". Crunch!: A History of the Great American Potato Chip. University of Wisconsin Press. pp. 17–20. ISBN978-0-299-22770-8. ^Di Placido D (July 19, 2017). \"The Evolution Of The Zombie\". Forbes. Retrieved July 3, 2022. George A. Romero's first zombie film Night of the Living Dead is credited with popularizing the zombie, though it never actually uses that word. The \"ghouls\" in the film are mindless flesh-eaters that have little in common with the Haitian zombie other than rising from the grave. ^ abEschner K (October 31, 2017). \"Zombie Movies Are Never Really About Zombies\". Smithsonian Magazine. Retrieved July 3, 2022. In the 1960s and 70s, filmmaker George Romero brought the zombie film into the mainstream with Night of the Living Dead and Dawn of the Dead. The first of these was technically about \"ghouls.\" Romero didn't start calling them \"zombies\" until his second film. But his now-iconic films helped to erase enslaved people from zombie history. ^Sisterson D (March 28, 2017). \"Magic Wilderness: El Apóstol & Peludópolis\". Skwigly. Retrieved June 22, 2022. As we all know, Disney's Snow White and the Seven Dwarfs is usually cited as the first animated feature, but as most of us who read this site are no doubt aware, it wasn't. It was preceded by Lotte Reiniger's The Adventures of Prince Achmed, Ladislas Starevitch's The Tale of the Fox, and two features by the Argentinian animator Quirino Cristiani – all films which could scracely [sic] be more different from the Disney mode. ^Bendazzi G (2017). \"The First Feature Length Animated Film in History\". Twice the First: Quirino Cristiani and the Animated Feature Film. CRC Press. p. 36. ISBN978-1-351-37179-7. On the other hand, the movie was not widely successful, and appealed to a small portion of the population. It was strictly for a Buenos Aires audience: nobody in the provinces even saw it because it was not distributed there. And likewise, given the subject, it was not possible to export the film to other nations, not even to a close cousin similar to Uruguay. ^US941960A, Smith, George Albert, \"Kinematograph apparatus for the production of colored pictures\", issued 1909-11-30 ^ a. Geoffrey K. Pullum's explanation in Language Log: The list of snow-referring roots to stick [suffixes] on isn't that long [in the Eskimoan language group]: qani- for a snowflake, apu- for snow considered as stuff lying on the ground and covering things up, a root meaning \"slush\", a root meaning \"blizzard\", a root meaning \"drift\", and a few others—very roughly the same number of roots as in English. Nonetheless, the number of distinct words you can derive from them is not 50, or 150, or 1500, or a million, but simply unbounded. Only stamina sets a limit. b. The seven most common English words for snow are snow, hail, sleet, ice, icicle, slush, and snowflake. English also has the related word glacier and the four common skiing terms pack, powder, crud, and crust, so one can say that at least 12 distinct words for snow exist in English. ^David Robson, New Scientist 2896, December 18 2012, Are there really 50 Eskimo words for snow?, \"Yet Igor Krupnik, an anthropologist at the Smithsonian Arctic Studies Center in Washington DC believes that Boas was careful to include only words representing meaningful distinctions. Taking the same care with their own work, Krupnik and others have now charted the vocabulary of about 10 Inuit and Yupik dialects and conclude that there are indeed many more words for snow than in English (SIKU: Knowing Our Ice, 2010). Central Siberian Yupik has 40 such terms, whereas the Inuit dialect spoken in Nunavik, Quebec, has at least 53, including matsaaruti, wet snow that can be used to ice a sleigh's runners, and pukak, for the crystalline powder snow that looks like salt. For many of these dialects, the vocabulary associated with sea ice is even richer.\" ^ \"Irregardless originated in dialectal American speech in the early 20th century... The most frequently repeated remark about it is that \"there is no such word.\" There is such a word, however.\" Merriam Webster Dictionary \"Definition of IRREGARDLESS\". Archived from the original on May 8, 2014. Retrieved October 27, 2011. ^Churchwell S (June 23, 2019). \"For sale, baby shoes, never worn — the myth of Ernest Hemingway's short story\". The Times. Retrieved April 5, 2023. For a long time, legend held that this was one of the world's great short stories, by one of the world's great short-story writers: Ernest Hemingway. There were different versions of the myth . . . None of it is true. And for those who think the internet is a cesspool of lies, it is an interesting experiment to google those six words today. The top items, including a Wikipedia entry on the myth, debunk it as the urban legend it is. ^William Pryse-Phillips (2003). Companion to Clinical Neurology. Oxford University Press. ISBN0-19-515938-1., p. 611 defines the term as \"Slight and transient improvement in spational[sic] reasoning skills detected in normal subjects as a result of exposure to the music of Mozart, specifically his sonata for two pianos (K448).\" ^Maurice Hinson (2004). The Pianist's Dictionary. Indiana University Press. p. 114. ISBN978-0-253-21682-3. Retrieved October 2, 2010. This piece bears an erroneous nickname since the story long associated with this nickname presumes the pianist is supposed to play the piece in one minute. The word \"minute\" means small or little waltz. ^Kennedy S (December 2, 2005). \"Dragon Quest vs. America\". 1up. Archived from the original on July 28, 2012. Retrieved June 26, 2022. Predating Xbox 360 hysteria by years, several fans were mugged on their way home with their new prize, and the situation became so bad that it was brought before the Japanese Diet. Although tales of a law requiring Dragon Quest games only be released on the mornings of weekends or holidays are the stuff of urban legend, each new title is as highly anticipated as the launch of a new console. ^Walker M (August 19, 2012). \"Dragon Quest X Online: Mezameshi Itsutsu no Shuzoku\". Nintendo World Report. Retrieved June 26, 2022. Its Thursday release is unheard of for a Dragon Quest game, which are generally released over the weekend so people don't take work off in droves to play them. ^ abWatterson B (1997). \"The Era of Pyramid-builders\". The Egyptians. Blackwell. p. 63. Herodotus claimed that the Great Pyramid at Giza was built with the labour of 100,000 slaves working in three-monthly shifts, a charge that cannot be substantiated. Much of the non-skilled labour on the pyramids was undertaken by peasants working during the Inundation season when they could not farm their lands. In return for their services they were given rations of food, a welcome addition to the family diet. ^ a. Neer R (2012). Art and Archaeology of the Greek World. Thames and Hudson. p. 37. ISBN978-0-500-05166-5. \"...popular associations of the eruption with a legend of Atlantis should be dismissed...nor is there good evidence to suggest that the eruption...brought about the collapse of Minoan Crete b. Manning S (2012). \"Eruption of Thera/Santorini\". In Cline E (ed.). The Oxford Handbook of the Bronze Age Aegean. Oxford University Press. pp. 457–454. doi:10.1093/oxfordhb/9780199873609.013.0034. ISBN978-0-19-987360-9. Marinatos (1939) famously suggested that the eruption might even have caused the destruction of Minoan Crete (also Page 1970). Although this simple hypothesis has been negated by the findings of excavation and other research since the late 1960s... which demonstrate that the eruption occurred late in the Late Minoan IA ceramic period, whereas the destructions of the Cretan palaces and so on are some time subsequent (late in the following Late Minoan IB ceramic period) c. Brouwers J (2021). \"Did Atlantis Exist?\". Bad Ancient. Retrieved August 30, 2023. ^\"National Pasta Association\". Archived from the original on March 20, 2012. article FAQs section \"Who \"invented\" pasta?\"; \"The story that it was Marco Polo who imported noodles to Italy and thereby gave birth to the country's pasta culture is the most pervasive myth in the history of Italian food.\" (Dickie 2008, p. 48). ^Crabtree S (July 6, 1999). \"New Poll Gauges Americans' General Knowledge Levels\". Gallup News Service. Archived from the original on March 27, 2014. Retrieved January 13, 2011. Fifty-five percent say it commemorates the signing of the Declaration of Independence (this is a common misconception, and close to being accurate; July 4th is actually the date in 1776 when the Continental Congress approved the Declaration, which was officially signed on August 2nd.) Another 32 percent give a more general answer, saying that July 4th celebrates Independence Day. ^ a. Craig L, Young K (2008). \"Beyond White Pride: Identity, Meaning and Contradiction in the Canadian Skinhead Subculture*\". Canadian Review of Sociology/Revue Canadienne de Sociologie. 34 (2): 175–206. doi:10.1111/j.1755-618x.1997.tb00206.x. Retrieved July 2, 2022. b. Borgeson K, Valeri R (Fall 2005). \"Examining Differences in Skinhead Ideology and Culture Through an Analysis of Skinhead Websites\". Michigan Sociological Review. 19: 45–62. JSTOR40969104. c. Lambert C (November 12, 2017). \"'Black Skinhead': The politics of New Kanye\". Daily Dot. Retrieved July 2, 2022. \"Skinhead\" was a term originally used to describe a 1960s British working-class subculture that revolved around fashion and music and that would heavily inspire the punk rock scene. While it has harmless roots, the skinhead movement fell into polemic politics. Nowadays, it's popularly associated with neo-Nazis, despite having split demographics of far-right, far-left, and apolitical. ^Krause CA (December 17, 1978). \"Jonestown Is an Eerie Ghost Town Now\". Washington Post. Retrieved June 20, 2022. A pair of woman's eyelasses, a towel, a pair of shorts, packets of unopened Flavor-Aid lie scattered about waiting for the final cleanup that may one day return Jonestown to the tidy, if overcrowded, little community it once was. ^Gudger E (January 1930). \"On the alleged penetration of the human urethra by an Amazonian catfish called candiru with a review of the allied habits of other members of the family pygidiidae\". The American Journal of Surgery (Print). 8 (1). Elsevier Inc.: 170–188. doi:10.1016/S0002-9610(30)90912-9. ISSN0002-9610. ^\"Have You Heard the Calls from Cook County's 12 Frog and Toad Species?\". Forest Preserves of Cook County. May 25, 2022. Retrieved June 25, 2022. Here's a bonus fact: you might notice that none of these species says, \"ribbit.\" In fact, the \"ribbit\" call is unique to the Pacific tree frog, which lives along the Pacific coast, and, notably, in Hollywood, California, where the largest volume of early frog recordings took place. ^Bittel J (September 22, 2019). \"Think you know what bunnies and bears eat? Their diets may surprise you\". Washington Post. Archived from the original on September 26, 2019. In the wild, rabbits aren't in the habit of digging up root vegetables such as carrots, potatoes and beets. They much prefer wild greens, such as grasses and clover. In fact, carrots may actually be bad for rabbits, because although the vegetables are high in good nutrients, including beta carotene, they are also relatively high in sugar. This means that feeding a rabbit lots of carrots could lead to tooth decay or other health issues. ^Curtin C (February 2007). \"Fact or Fiction?: Glass Is a (Supercooled) Liquid\". Scientific American. Archived from the original on December 14, 2013. Glass, however, is actually neither a liquid—supercooled or otherwise—nor a solid. It is an amorphous solid—a state somewhere between those two states of matter. And yet glass's liquidlike properties are not enough to explain the thicker-bottomed windows, because glass atoms move too slowly for changes to be visible. ^Begley S (August 13, 2007). \"The Truth About Denial\". Newsweek. Archived from the original on October 21, 2007. (MSNBC single page version, archived 20 August 2007) \"If you think those who have long challenged the mainstream scientific findings about global warming recognize that the game is over, think again. ... outside Hollywood, Manhattan and other habitats of the chattering classes, the denial machine is running at full throttle—and continuing to shape both government policy and public opinion. Since the late 1980s, this well-coordinated, well-funded campaign by contrarian scientists, free-market think tanks and industry has created a paralyzing fog of doubt around climate change. Through advertisements, op-eds, lobbying and media attention, greenhouse doubters (they hate being called deniers) argued first that the world is not warming; measurements indicating otherwise are flawed, they said. Then they claimed that any warming is natural, not caused by human activities. Now they contend that the looming warming will be minuscule and harmless. 'They patterned what they did after the tobacco industry,' says former senator Tim Wirth.\" ^ a. \"Lightning Myths and Facts\". National Weather Service. Fact: Lightning often strikes the same place repeatedly, especially if it's a tall, pointy, isolated object. The Empire State Building is hit an average of 23 times a year b. \"Lightning Often Strikes Twice\". NASA Spinoff. Office of the Chief Technologist, NASA. Archived from the original on March 25, 2012. Retrieved June 23, 2010. c. WeatherBug Meteorologists (May 17, 2010). \"The Myths and Facts of Lightning\". WeatherBug. Earth Works. Archived from the original on July 11, 2010. Retrieved June 23, 2010. d. Tristan Simpson (April 29, 2022). \"Can lightning strike the same place twice?\". The Weather Network. Believe it or not, this long-held myth is far from the truth. While the odds of being struck by lightning are low, the chances of lightning striking the same place twice are high. Lightning can, and often will, hit the same spot multiple times. ^ a. \"Ask an Astrophysicist\". NASA. Archived from the original on June 4, 2012. If you don't try to hold your breath, exposure to space for half a minute or so is unlikely to produce permanent injury. Holding your breath is likely to damage your lungs, ... but theory predicts—and animal experiments confirm—that otherwise, exposure to vacuum causes no immediate injury. You do not explode. Your blood does not boil. You do not freeze. You do not instantly lose consciousness b. \"Exploding Body in Vacuum\". ABC Science. April 6, 2005. Archived from the original on June 4, 2012. ...will we humans explode in the full vacuum of space, as urban legends claim? The answer is that we won't explode, and if the exposure is short enough, we can even survive. ^Dresden D (March 12, 2020). \"How many ribs do humans have? Men, women, and anatomy\". Medical News Today. Retrieved June 5, 2022. Although many people might think that males have fewer ribs than females — most likely sparked by the biblical story of Adam and Eve — there is no factual evidence. ^Spellman, Frank R; Price-Bayer, Joni. (2010). In Defense of Science: Why Scientific Literacy Matters. The Scarecrow Press. p. 81. ISBN978-1-60590-735-2 \"There is no scientific evidence that crystal healing has any effect. It has been called a pseudoscience. Pleasant feelings or the apparent successes of crystal healing can be attributed to the placebo effect or cognitive bias—a believer wanting it to be true.\" The literature about Biodiversity and the GE food/feed consumption has sometimes resulted in animated debate regarding the suitability of the experimental designs, the choice of the statistical methods or the public accessibility of data. Such debate, even if positive and part of the natural process of review by the scientific community, has frequently been distorted by the media and often used politically and inappropriately in anti-GE crops campaigns. b. \"State of Food and Agriculture 2003–2004. Agricultural Biotechnology: Meeting the Needs of the Poor. Health and environmental impacts of transgenic crops\". Food and Agriculture Organization of the United Nations. Retrieved August 30, 2019. Currently available transgenic crops and foods derived from them have been judged safe to eat and the methods used to test their safety have been deemed appropriate. These conclusions represent the consensus of the scientific evidence surveyed by the ICSU (2003) and they are consistent with the views of the World Health Organization (WHO, 2002). These foods have been assessed for increased risks to human health by several national regulatory authorities (inter alia, Argentina, Brazil, Canada, China, the United Kingdom and the United States) using their national food safety procedures (ICSU). To date no verifiable untoward toxic or nutritionally deleterious effects resulting from the consumption of foods derived from genetically modified crops have been discovered anywhere in the world (GM Science Review Panel). Many millions of people have consumed foods derived from GM plants – mainly maize, soybean and oilseed rape – without any observed adverse effects (ICSU). c. Ronald P (May 1, 2011). \"Plant Genetics, Sustainable Agriculture and Global Food Security\". Genetics. 188 (1): 11–20. doi:10.1534/genetics.111.128553. PMC3120150. PMID21546547. There is broad scientific consensus that genetically engineered crops currently on the market are safe to eat. After 14 years of cultivation and a cumulative total of 2 billion acres planted, no adverse health or environmental effects have resulted from commercialization of genetically engineered crops (Board on Agriculture and Natural Resources, Committee on Environmental Impacts Associated with Commercialization of Transgenic Plants, National Research Council and Division on Earth and Life Studies 2002). Both the U.S. National Research Council and the Joint Research Centre (the European Union's scientific and technical research laboratory and an integral part of the European Commission) have concluded that there is a comprehensive body of knowledge that adequately addresses the food safety issue of genetically engineered crops (Committee on Identifying and Assessing Unintended Effects of Genetically Engineered Foods on Human Health and National Research Council 2004; European Commission Joint Research Centre 2008). These and other recent reports conclude that the processes of genetic engineering and conventional breeding are no different in terms of unintended consequences to human health and the environment (European Commission Directorate-General for Research and Innovation 2010). ^ See also: Domingo JL, Bordonaba JG (2011). \"A literature review on the safety assessment of genetically modified plants\"(PDF). Environment International. 37 (4): 734–742. Bibcode:2011EnInt..37..734D. doi:10.1016/j.envint.2011.01.003. PMID21296423. In spite of this, the number of studies specifically focused on safety assessment of GM plants is still limited. However, it is important to remark that for the first time, a certain equilibrium in the number of research groups suggesting, on the basis of their studies, that a number of varieties of GM products (mainly maize and soybeans) are as safe and nutritious as the respective conventional non-GM plant, and those raising still serious concerns, was observed. Moreover, it is worth mentioning that most of the studies demonstrating that GM foods are as nutritional and safe as those obtained by conventional breeding, have been performed by biotechnology companies or associates, which are also responsible of commercializing these GM plants. Anyhow, this represents a notable advance in comparison with the lack of studies published in recent years in scientific journals by those companies.Krimsky S (2015). \"An Illusory Consensus behind GMO Health Assessment\". Science, Technology, & Human Values. 40 (6): 883–914. doi:10.1177/0162243915598381. S2CID40855100. I began this article with the testimonials from respected scientists that there is literally no scientific controversy over the health effects of GMOs. My investigation into the scientific literature tells another story. And contrast: Panchin AY, Tuzhikov AI (January 14, 2016). \"Published GMO studies find no evidence of harm when corrected for multiple comparisons\". Critical Reviews in Biotechnology. 37 (2): 213–217. doi:10.3109/07388551.2015.1130684. ISSN0738-8551. PMID26767435. S2CID11786594. Here, we show that a number of articles some of which have strongly and negatively influenced the public opinion on GM crops and even provoked political actions, such as GMO embargo, share common flaws in the statistical evaluation of the data. Having accounted for these flaws, we conclude that the data presented in these articles does not provide any substantial evidence of GMO harm. The presented articles suggesting possible harm of GMOs received high public attention. However, despite their claims, they actually weaken the evidence for the harm and lack of substantial equivalency of studied GMOs. We emphasize that with over 1783 published articles on GMOs over the last 10 years it is expected that some of them should have reported undesired differences between GMOs and conventional crops even if no such differences exist in reality. and Yang Y, Chen B (2016). \"Governing GMOs in the USA: science, law and public health\". Journal of the Science of Food and Agriculture. 96 (4): 1851–1855. Bibcode:2016JSFA...96.1851Y. doi:10.1002/jsfa.7523. PMID26536836. It is therefore not surprising that efforts to require labeling and to ban GMOs have been a growing political issue in the USA (citing Domingo and Bordonaba, 2011). Overall, a broad scientific consensus holds that currently marketed GM food poses no greater risk than conventional food... Major national and international science and medical associations have stated that no adverse human health effects related to GMO food have been reported or substantiated in peer-reviewed literature to date. Despite various concerns, today, the American Association for the Advancement of Science, the World Health Organization, and many independent international science organizations agree that GMOs are just as safe as other foods. Compared with conventional breeding techniques, genetic engineering is far more precise and, in most cases, less likely to create an unexpected outcome. ^ a. \"Statement by the AAAS Board of Directors On Labeling of Genetically Modified Foods\"(PDF). American Association for the Advancement of Science. October 20, 2012. Retrieved August 30, 2019. The EU, for example, has invested more than €300 million in research on the biosafety of GMOs. Its recent report states: \"The main conclusion to be drawn from the efforts of more than 130 research projects, covering a period of more than 25 years of research and involving more than 500 independent research groups, is that biotechnology, and in particular GMOs, are not per se more risky than e.g. conventional plant breeding technologies.\" The World Health Organization, the American Medical Association, the U.S. National Academy of Sciences, the British Royal Society, and every other respected organization that has examined the evidence has come to the same conclusion: consuming foods containing ingredients derived from GM crops is no riskier than consuming the same foods containing ingredients from crop plants modified by conventional plant improvement techniques. b. Pinholster G (October 25, 2012). \"AAAS Board of Directors: Legally Mandating GM Food Labels Could \"Mislead and Falsely Alarm Consumers\"\"(PDF). American Association for the Advancement of Science. Retrieved August 30, 2019. c. European Commission. Directorate-General for Research (2010). A decade of EU-funded GMO research (2001–2010)(PDF). Directorate-General for Research and Innovation. Biotechnologies, Agriculture, Food. European Commission, European Union. doi:10.2777/97784. ISBN978-92-79-16344-9. Retrieved August 30, 2019. d. \"ISAAA Summary of AMA Report on Genetically Modified Crops and Foods\". ISAAA. January 2001. Retrieved August 30, 2019. A report issued by the scientific council of the American Medical Association (AMA) says that no long-term health effects have been detected from the use of transgenic crops and genetically modified foods, and that these foods are substantially equivalent to their conventional counterparts. e. \"Featured CSA Report: Genetically Modified Crops and Foods (I-00) Full Text\". American Medical Association. Archived from the original on June 10, 2001. Crops and foods produced using recombinant DNA techniques have been available for fewer than 10 years and no long-term effects have been detected to date. These foods are substantially equivalent to their conventional counterparts. f. \"Report 2 of the Council on Science and Public Health (A-12): Labeling of Bioengineered Foods\"(PDF). American Medical Association. 2012. Archived from the original(PDF) on September 7, 2012. Retrieved August 30, 2019. \"Bioengineered foods have been consumed for close to 20 years, and during that time, no overt consequences on human health have been reported and/or substantiated in the peer-reviewed literature\". g. \"Restrictions on Genetically Modified Organisms: United States. Public and Scholarly Opinion\". Library of Congress. June 30, 2015. Retrieved August 30, 2019. Several scientific organizations in the US have issued studies or statements regarding the safety of GMOs indicating that there is no evidence that GMOs present unique safety risks compared to conventionally bred products. These include the National Research Council, the American Association for the Advancement of Science, and the American Medical Association. Groups in the US opposed to GMOs include some environmental organizations, organic farming organizations, and consumer organizations. A substantial number of legal academics have criticized the US's approach to regulating GMOs. h. National Academies Of Sciences E, Division on Earth Life Studies, Board on Agriculture Natural Resources, Committee on Genetically Engineered Crops: Past Experience Future Prospects (2016). Genetically Engineered Crops: Experiences and Prospects. The National Academies of Sciences, Engineering, and Medicine (US). p. 149. doi:10.17226/23395. ISBN978-0-309-43738-7. PMID28230933. Retrieved August 30, 2019. Overall finding on purported adverse effects on human health of foods derived from GE crops: On the basis of detailed examination of comparisons of currently commercialized GE with non-GE foods in compositional analysis, acute and chronic animal toxicity tests, long-term data on health of livestock fed GE foods, and human epidemiological data, the committee found no differences that implicate a higher risk to human health from GE foods than from their non-GE counterparts. ^Fullerton-Smith J (2007). The Truth About Food. Bloomsbury. pp. 115–17. ISBN978-0-7475-8685-2. Most parents assume that children plus sugary foods equals raucous and uncontrollable behaviour. ... according to nutrition experts, the belief that children experience a 'sugar high' is a myth. ^ a. Jesse Galef (August 29, 2011). \"Lies and Debunked Legends about the Golden Ratio\". Archived from the original on April 27, 2014. Retrieved April 10, 2013. b. \"Two other beliefs about [the golden ratio] are often mentioned in magazines and books: that the ancient Greeks believed it was the proportion of the rectangle the eye finds most pleasing and that they accordingly incorporated the rectangle in many of their buildings, including the famous Parthenon. These two equally persistent beliefs are likewise assuredly false and, in any case, are completely without any evidence.\" Devlin K (2008). The Unfinished Game: Pascal, Fermat, and the Seventeenth-Century Letter that Made the World Modern. Basic Books. p. 35. ^ a. Donald E. Simanek. \"Fibonacci Flim-Flam\". Archived from the original on February 1, 2010. Retrieved April 9, 2013. b. Devlin K (May 2007). \"The Myth That Will Not Go Away\". Archived from the original on July 1, 2013. Retrieved April 10, 2013. Part of the process of becoming a mathematics writer is, it appears, learning that you cannot refer to the golden ratio without following the first mention by a phrase that goes something like 'which the ancient Greeks and others believed to have divine and mystical properties.' Almost as compulsive is the urge to add a second factoid along the lines of 'Leonardo Da Vinci believed that the human form displays the golden ratio.' There is not a shred of evidence to back up either claim, and every reason to assume they are both false. Yet both claims, along with various others in a similar vein, live on. ^a. \"This occurs because of Bernoulli's principle – fast-moving air has lower pressure than non-moving air\". Make Magazine. Archived from the original on January 3, 2013. Retrieved September 5, 2012. b. \"Paper Lift\". Physics Force. University of Minnesota. Archived from the original on November 12, 2020. Retrieved January 7, 2021. ... When the demonstrator holds the paper in front of his mouth and blows across the top, he is creating an area of faster-moving air. The slower-moving air under the paper now has higher pressure, thus pushing the paper up, towards the area of lower pressure.. c. \"Educational Packet\"(PDF). Tall Ships Festival: Channel Islands Harbor. Archived from the original(PDF) on December 3, 2013. Retrieved June 25, 2012. Bernoulli's Principle states that faster moving air has lower pressure... You can demonstrate Bernoulli's Principle by blowing over a piece of paper held horizontally across your lips.\" ^a. Craig GM. \"Physical Principles of Winged Flight\"(PDF). Archived from the original(PDF) on March 7, 2021. Retrieved September 5, 2012. If the lift in figure A were caused by \"Bernoulli principle,\" then the paper in figure B should droop further when air is blown beneath it. However, as shown, it raises when the upward pressure gradient in downward-curving flow adds to atmospheric pressure at the paper lower surface. b. Babinsky H (2003). \"How Do Wings Work\". Physics Education. 38 (6): 497–503. Bibcode:2003PhyEd..38..497B. doi:10.1088/0031-9120/38/6/001. S2CID1657792. Retrieved January 7, 2021. In fact, the pressure in the air blown out of the lungs is equal to that of the surrounding air... Blowing over a piece of paper does not demonstrate Bernoulli's equation. While it is true that a curved paper lifts when flow is applied on one side, this is not because air is moving at different speeds on the two sides... It is false to make a connection between the flow on the two sides of the paper using Bernoulli's equation. c. Eastwell P (2007). \"Bernoulli? Perhaps, but What About Viscosity?\"(PDF). The Science Education Review. 6 (1). Archived from the original(PDF) on March 18, 2018. Retrieved September 10, 2023. ...air does not have a reduced lateral pressure (or static pressure...) simply because it is caused to move, the static pressure of free air does not decrease as the speed of the air increases, it misunderstanding Bernoulli's principle to suggest that this is what it tells us, and the behavior of the curved paper is explained by other reasoning than Bernoulli's principle. ... An explanation based on Bernoulli's principle is not applicable to this situation, because this principle has nothing to say about the interaction of air masses having different speeds... Also, while Bernoulli's principle allows us to compare fluid speeds and pressures along a single streamline and... along two different streamlines that originate under identical fluid conditions, using Bernoulli's principle to compare the air above and below the curved paper in Figure 1 is nonsensical; in this case, there aren't any streamlines at all below the paper! d. Raskin J. \"Coanda Effect: Understanding Why Wings Work\". Make a strip of writing paper about 5 cm X 25 cm. Hold it in front of your lips so that it hangs out and down making a convex upward surface. When you blow across the top of the paper, it rises. Many books attribute this to the lowering of the air pressure on top solely to the Bernoulli effect. Now use your fingers to form the paper into a curve that it is slightly concave upward along its whole length and again blow along the top of this strip. The paper now bends downward...an often-cited experiment which is usually taken as demonstrating the common explanation of lift does not do so... e. Auerbach D (2000). \"Why Aircraft Fly\". European Journal of Physics. 21 (4): 289. Bibcode:2000EJPh...21..289A. doi:10.1088/0143-0807/21/4/302. S2CID250821727. The well-known demonstration of the phenomenon of lift by means of lifting a page cantilevered in one's hand by blowing horizontally along it is probably more a demonstration of the forces inherent in the Coanda effect than a demonstration of Bernoulli's law; for, here, an air jet issues from the mouth and attaches to a curved (and, in this case pliable) surface. The upper edge is a complicated vortex-laden mixing layer and the distant flow is quiescent, so that Bernoulli's law is hardly applicable. f. Smith NF (November 1972). \"Bernoulli and Newton in Fluid Mechanics\". The Physics Teacher. 10 (8): 451–455. Bibcode:1972PhTea..10..451S. doi:10.1119/1.2352317. Millions of children in science classes are being asked to blow over curved pieces of paper and observe that the paper \"lifts\"... They are then asked to believe that Bernoulli's theorem is responsible... Unfortunately, the \"dynamic lift\" involved...is not properly explained by Bernoulli's theorem. ^ a. Anderson DF, Eberhardt S (2000). Understanding Flight. McGraw Hill Professional. p. 229. ISBN978-0-07-138666-1 – via Google Books. Demonstrations\" of Bernoulli's principle are often given as demonstrations of the physics of lift. They are truly demonstrations of lift, but certainly not of Bernoulli's principle. b. Feil M. The Aeronautics File. As an example, take the misleading experiment most often used to \"demonstrate\" Bernoulli's principle. Hold a piece of paper so that it curves over your finger, then blow across the top. The paper will rise. However most people do not realize that the paper would NOT rise if it was flat, even though you are blowing air across the top of it at a furious rate. Bernoulli's principle does not apply directly in this case. This is because the air on the two sides of the paper did not start out from the same source. The air on the bottom is ambient air from the room, but the air on the top came from your mouth where you actually increased its speed without decreasing its pressure by forcing it out of your mouth. As a result the air on both sides of the flat paper actually has the same pressure, even though the air on the top is moving faster. The reason that a curved piece of paper does rise is that the air from your mouth speeds up even more as it follows the curve of the paper, which in turn lowers the pressure according to Bernoulli. ^ a. Colbeck SC (1995). \"Pressure melting and ice skating\". American Journal of Physics. 63 (10): 888. Bibcode:1995AmJPh..63..888C. doi:10.1119/1.18028. Pressure melting cannot be responsible for the low friction of ice. The pressure needed to reach the melting temperature is above the compressive failure stress...\" b. Kenneth Chang (February 21, 2006). \"Explaining Ice: The Answers Are Slippery\". The New York Times. According to the frequently cited — if incorrect — explanation of why ice is slippery under an ice skate, the pressure exerted along the blade lowers the melting temperature of the top layer of ice, the ice melts and the blade glides on a thin layer of water that refreezes to ice as soon as the blade passes... But the explanation fails... because the pressure-melting effect is small. c. Robert Rosenberg (December 2005). \"Why is Ice slippery?\"(PDF). Physics Today: 50–55. ^ abHandler SM, Fierson WM, Section on O, Council on Children with D, American Academy of O, American Association for Pediatric Ophthalmology and S, et al. (March 2011). \"Learning disabilities, dyslexia, and vision\". Pediatrics. 127 (3): e818–56. doi:10.1542/peds.2010-3670. PMID21357342. A common misconception is that dyslexia is a problem of letter or word reversals. Reversals of letters or words and mirror writing occur normally in early readers and writers. Children with dyslexia are not unusually prone to reversals. Although they do occur, reversal of letters or words, or mirror writing, is not included in the definition of dyslexia. ^ abRadford B (March–April 1999). \"The Ten-Percent Myth\". Skeptical Inquirer. ISSN0194-6730. Archived from the original on October 30, 2013. Retrieved April 15, 2009. It's the old myth heard time and again about how people use only ten percent of their brains ^ abBeyerstein BL (1999). \"Whence Cometh the Myth that We Only Use 10% of our Brains?\". In Sergio Della Sala (ed.). Mind Myths: Exploring Popular Assumptions About the Mind and Brain. Wiley. pp. 3–24. ISBN978-0-471-98303-3. ^ a. Eisenbud M, Gesell TF (1997). Environmental radioactivity: from natural, industrial, and military sources. Academic Press. pp. 171–172. ISBN978-0-12-235154-9. It is important to recognize that the potassium content of the body is under strict homeostatic control and is not influenced by variations in environmental levels. For this reason, the dose from 40K in the body is constant. b. U. S. Environmental Protection Agency (1999), Federal Guidance Report 13, page 16: \"For example, the ingestion coefficient risk for 40K would not be appropriate for an application to ingestion of 40K in conjunction with an elevated intake of natural potassium. This is because the biokinetic model for potassium used in this document represents the relatively slow removal of potassium (biological half-time 30 days) that is estimated to occur for typical intakes of potassium, whereas an elevated intake of potassium would result in excretion of a nearly equal mass of natural potassium, and hence of 40K, over a short period.\" c. Maggie Koerth-Baker (August 27, 2010). \"Bananas are radioactive—But they aren't a good way to explain radiation exposure\". Retrieved May 25, 2011.. Attributes the title statement to Geoff Meggitt, former UK Atomic Energy Authority. Gullotta DN (2017). \"On Richard Carrier's Doubts: A Response to Richard Carrier's On the Historicity of Jesus: Why We Might Have Reason for Doubt\". Journal for the Study of the Historical Jesus. 15 (2–3): 310–346. doi:10.1163/17455197-01502009. O'Conner PT, Kellerman S (2009). Origins of the Specious: Myths and Misconceptions of the English Language. New York: Random House. ISBN978-1-4000-6660-5. Smith FJ (January 1, 1979). \"Some aspects of the tritone and the semitritone in the Speculum Musicae: the non-emergence of the diabolus in musica\". Journal of Musicological Research. 3 (1–2): 63–74. doi:10.1080/01411897908574507. ISSN0141-1896."}
{"url": "https://en.m.wikipedia.org/wiki/ISBN_(identifier)", "text": "ISBN The International Standard Book Number (ISBN) is a numeric commercial bookidentifier that is intended to be unique.[a][b] Publishers purchase or receive ISBNs from an affiliate of the International ISBN Agency.[2] An ISBN is assigned to each separate edition and variation (except reprintings) of a publication. For example, an e-book, a paperback and a hardcover edition of the same book must each have a different ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007.[c] The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country. The initial ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the 9-digit SBN code can be converted to a 10-digit ISBN by prefixing it with a zero). Privately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns such books ISBNs on its own initiative.[4] History The Standard Book Number (SBN) is a commercial system using nine-digit code numbers to identify books. In 1965, British bookseller and stationers WHSmith announced plans to implement a standard numbering system for its books.[1] They hired consultants to work on their behalf, and the system was devised by Gordon Foster, emeritus professor of statistics at Trinity College Dublin.[5] The International Organization for Standardization (ISO) Technical Committee on Documentation sought to adapt the British SBN for international use. The ISBN identification format was conceived in 1967 in the United Kingdom by David Whitaker[6][7] (regarded as the \"Father of the ISBN\")[8] and in 1968 in the United States by Emery Koltay[6] (who later became director of the U.S. ISBN agency R. R. Bowker).[8][9][10] The 10-digit ISBN format was developed by the ISO and was published in 1970 as international standard ISO 2108.[1][6] The United Kingdom continued to use the nine-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[11] Total number of ISBN registrations. 2020 An SBN may be converted to an ISBN by prefixing the digit \"0\". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has \"SBN 340 01381 8\", where \"340\" indicates the publisher, \"01381\" is the serial number assigned by the publisher, and \"8\" is the check digit. By prefixing a zero, this can be converted to ISBN0-340-01381-8; the check digit does not need to be re-calculated. Some publishers, such as Ballantine Books, would sometimes use 12-digit SBNs where the last three digits indicated the price of the book;[12] for example, Woodstock Handmade Houses had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, ISBN: 0-345-24223-8),[13] and it cost US$5.95.[14] The United-States, with 3.9 million registered ISBNs in 2020, was by far the biggest user of the ISBN identifier in 2020, followed by the Republic of Korea (329,582), Germany (284,000), China (263,066), the UK (188,553) and Indonesia (144,793). Lifetime ISBNs registered in the United States are over 39 millions in 2020.[15] Overview A separate ISBN is assigned to each edition and variation (except reprintings) of a publication. For example, an ebook, audiobook, paperback, and hardcover edition of the same book must each have a different ISBN assigned to it.[16]: 12 The ISBN is thirteen digits long if assigned on or after 1 January 2007, and ten digits long if assigned before 2007.[c][3] An International Standard Book Number consists of four parts (if it is a 10-digit ISBN) or five parts (for a 13-digit ISBN). Section 5 of the International ISBN Agency's official user manual[16]: 11 describes the structure of the 13-digit ISBN, as follows: The parts of a 10-digit ISBN and the corresponding EAN‑13 and barcode. Note the different check digits in each. The part of the EAN‑13 labeled \"EAN\" is the Bookland country code. for a 13-digit ISBN, a prefix element – a GS1 prefix: so far 978 or 979 have been made available by GS1, the registration group element (language-sharing country group, individual country or territory),[d] A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.[e] Issuing process ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from the government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.[18] A full directory of ISBN agencies is available on the International ISBN Agency website.[19] A list for a few countries is given below: Registration group element The ISBN registration group element is a 1-to-5-digit number that is valid within a single prefix element (i.e. one of 978 or 979),[16]: 11 and can be separated between hyphens, such as \"978-1-...\". Registration groups have primarily been allocated within the 978 prefix element.[38] The single-digit registration groups within the 978-prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. Example 5-digit registration groups are 99936 and 99980, for Bhutan. The allocated registration groups are: 0–5, 600–631, 65, 7, 80–94, 950–989, 9910–9989, and 99901–99993.[39][40] Books published in rare languages typically have longer group elements.[41] Within the 979 prefix element, the registration group 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[42] The registration groups within prefix element 979 that have been assigned are 8 for the United States of America, 10 for France, 11 for the Republic of Korea, and 12 for Italy.[43] The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero to a 9-digit SBN creates a valid 10-digit ISBN. Registrant element The national ISBN agency assigns the registrant element (cf.Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not legally required to assign an ISBN, although most large bookstores only handle publications that have ISBNs assigned to them.[44][45][46] The International ISBN Agency maintains the details of over one million ISBN prefixes and publishers in the Global Register of Publishers.[47] This database is freely searchable over the internet. Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers. By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[48] Here are some sample ISBN-10 codes, illustrating block length variations. ISBN Country or area Publisher 99921-58-10-7 Qatar NCCAH, Doha 9971-5-0210-0 Singapore World Scientific 960-425-059-0 Greece Sigma Publications 80-902734-1-6 Czech Republic; Slovakia Taita Publishers 85-359-0277-5 Brazil Companhia das Letras 1-84356-028-3 English-speaking area Simon Wallenberg Press 0-684-84328-5 English-speaking area Scribner 0-8044-2957-X English-speaking area Frederick Ungar 0-85131-041-9 English-speaking area J. A. Allen & Co. 93-86954-21-4 English-speaking area Edupedia Publications Pvt Ltd. 0-943396-04-2 English-speaking area Willmann–Bell 0-9752298-0-X English-speaking area KT Publishing English language pattern English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[49] Check digits A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the 10-digit ISBN is an extension of that for SBNs, so the two systems are compatible; an SBN prefixed with a zero (the 10-digit ISBN) will give the same check digit as the SBN without the zero. The check digit is base eleven, and can be an integer between 0 and 9, or an 'X'. The system for 13-digit ISBNs is not compatible with SBNs and will, in general, give a different check digit from the corresponding 10-digit ISBN, so does not provide the same protection against transposition. This is because the 13-digit code was required to be compatible with the EAN format, and hence could not contain the letter 'X'. ISBN-10 check digits According to the 2001 edition of the International ISBN Agency's official user manual,[50] the ISBN-10 check digit (which is the last digit of the 10-digit ISBN) must range from 0 to 10 (the symbol 'X' is used for 10), and must be such that the sum of the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11. That is, if xi is the ith digit, then x10 must be chosen such that: The two most common errors in handling an ISBN (e.g. when typing it or writing it down) are a single altered digit or the transposition of adjacent digits. It can be proven mathematically that all pairs of valid ISBN-10s differ in at least two digits. It can also be proven that there are no pairs of valid ISBN-10s with eight identical digits and two transposed digits. (These proofs are true because the ISBN is less than eleven digits long and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e., if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error were to occur in the publishing house and remain undetected, the book would be issued with an invalid ISBN.[51] In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely). ISBN-10 check digit calculation Each of the first nine digits of the 10-digit ISBN – excluding the check digit itself – is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11. For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows: Adding 2 to 130 gives a multiple of 11 (because 132 = 12×11) – this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is ISBN 0-306-40615-2. If the value of x10{\\displaystyle x_{10}} required to satisfy this condition is 10, then an 'X' should be used. Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation, the calculation could result in a check digit value of 11 − 0 = 11, which is invalid. (Strictly speaking, the first \"modulo 11\" is not needed, but it may be considered to simplify the calculation.) For example, the check digit for the ISBN of 0-306-40615-? is calculated as follows: It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples: // Returns ISBN error syndrome, zero for a valid ISBN, non-zero for an invalid one.// digits[i] must be between 0 and 10.intCheckISBN(intconstdigits[10]){inti,s=0,t=0;for(i=0;i<10;++i){t+=digits[i];s+=t;}returns%11;} The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition. ISBN-13 check digit calculation Appendix 1 of the International ISBN Agency's official user manual[16]: 33 describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10. As ISBN-13 is a subset of EAN-13, the algorithm for calculating the check digit is exactly the same for both. The calculation of an ISBN-13 check digit begins with the first twelve digits of the 13-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero replaces a ten, so, in all cases, a single check digit results. For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows: This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3 × 6 + 1 × 1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3 × 1 + 1 × 6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0–9 to express the check digit. Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0). ISBN-10 to ISBN-13 conversion A 10-digit ISBN is converted to a 13-digit ISBN by prepending \"978\" to the ISBN-10 and recalculating the final checksum digit using the ISBN-13 algorithm. The reverse process can also be performed, but not for numbers commencing with a prefix other than 978, which have no 10-digit equivalent. Errors in usage Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[52] For example, ISBN0-590-76484-5 is shared by two books – Ninja gaiden: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic. Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase \"Cancelled ISBN\".[53] The International Union Library Catalog (a.k.a., WorldCatOCLC – Online Computer Library Center system) often indexes by invalid ISBNs, if the book is indexed in that way by a member library.[54] eISBN Only the term \"ISBN\" should be used; the terms \"eISBN\" and \"e-ISBN\" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic \"eISBN\" which encompasses all the e-book formats for a title.[55] EAN format used in barcodes, and upgrading The barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits called an EAN-5 for the currency and the recommended retail price.[56] For 10-digit ISBNs, the number \"978\", the Bookland \"country code\", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN-13 formula (modulo 10, 1× and 3× weighting on alternating digits). Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a 13-digit ISBN (ISBN-13). The process began on 1 January 2005 and was planned to conclude on 1 January 2007.[57] As of 2011[update], all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. The 10-digit ISMN codes differed visually as they began with an \"M\" letter; the bar code represents the \"M\" as a zero, and for checksum purposes it counted as a 3. All ISMNs are now thirteen digits commencing 979-0; 979-1 to 979-9 will be used by ISBN. Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the 10-digit ISBN check digit generally is not the same as the 13-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[58] Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPCbarcode system to full EAN-13, in 2005, eased migration to the ISBN in North America. Explanatory notes ^Occasionally, publishers erroneously assign an ISBN to more than one title – the first edition of The Ultimate Alphabet and The Ultimate Alphabet Workbook have the same ISBN, 0-8050-0076-3. Conversely, books are published with several ISBNs: A German second-language edition of Emil und die Detektive has the ISBNs 87-23-90157-8 (Denmark), 0-8219-1069-8 (United States), 91-21-15628-X (Sweden), 0-85048-548-7 (United Kingdom) and 3-12-675495-3 (Germany). ^In some cases, books sold only as sets share ISBNs. For example, the Vance Integral Edition used only two ISBNs for 44 books. ^ abPublishers were required to convert existing ISBNs from the 10-digit format to the 13-digit format (in their publication records) by 1 January 2007. For existing publications, the new 13-digit ISBN would only need to be added if (and when) a publication was reprinted. During the transition period, publishers were recommended to print both the 10-digit and 13-digit ISBNs on the verso of a publication's title page, but they were required to print only the 13-digit ISBN after 1 January 2007.[3] ^Some books have several codes in the first block: e.g. A. M. Yaglom's Correlation Theory..., published by Springer Verlag, has two ISBNs, 0-387-96331-6 and 3-540-96331-6. Though Springer's 387 and 540 codes are different for English (0) and German (3); the same item number 96331 produces the same check digit for both (6). Springer uses 431 as the publisher code for Japanese (4), and 4-431-96331-? also has a check digit of 6. Other Springer books in English have publisher code 817, and 0-817-96331-? would also have a check digit of 6. This suggests that special considerations were made for assigning Springer's publisher codes, as random assignments of different publisher codes would not be expected to lead by coincidence to the same check digit every time for the same item number. Finding publisher codes for English and German, say, with this effect would amount to solving a linear equation in modular arithmetic.[original research?] ^The International ISBN Agency's ISBN User's Manual says: \"The ten-digit number is divided into four parts of variable length, which must be separated clearly, by hyphens or spaces\", although omission of separators is permitted for internal data processing. If present, hyphens must be correctly placed.[17] The actual definition for hyphenation contains more than 220 different registration group elements with each one broken down into a few to several ranges for the length of the registrant element (more than 1,000 total). The document defining the ranges, listed by agency, is 29 pages. ^\"Independent Publishers\". Waterstones. Archived from the original on 9 January 2020. Retrieved 2 February 2020. Before submitting any titles to our central buying team for consideration, your book must have the following: An ISBN... ^\"How to obtain an ISBN\". Barnes & Noble. Archived from the original on 2 February 2020. Retrieved 2 February 2020. We use ISBNs to track inventory and sales information. All books Barnes & Noble transacts on must have an ISBN. ^\"Product ID (GTIN) requirements for Books\". Amazon.com. Archived from the original on 2 February 2020. Retrieved 2 February 2020. Effective June 1, 2017, you must provide an ISBN, EAN, or JAN to list a book in the Amazon catalog, regardless of the book's publication date."}
{"url": "https://en.m.wikipedia.org/wiki/Cretaceous%E2%80%93Paleogene_extinction_event", "text": "Complex Cretaceous–Paleogene clay layer (gray) in the Geulhemmergroeve tunnels near Geulhem, The Netherlands (finger is below the actual Cretaceous–Paleogene boundary); Wyoming rock with an intermediate claystone layer that contains 1,000 times more iridium than the upper and lower layers. Picture taken at the San Diego Natural History Museum; Rajgad Fort's Citadel, an eroded hill from the Deccan Traps, which are another hypothesized cause of the K–Pg extinction event. As originally proposed in 1980[9] by a team of scientists led by Luis Alvarez and his son Walter, it is now generally thought that the K–Pg extinction was caused by the impact of a massive asteroid 10 to 15 km (6 to 9 mi) wide,[10][11] 66 million years ago, which devastated the global environment, mainly through a lingering impact winter which halted photosynthesis in plants and plankton.[12][13] The impact hypothesis, also known as the Alvarez hypothesis, was bolstered by the discovery of the 180 km (112 mi) Chicxulub crater in the Gulf of Mexico's Yucatán Peninsula in the early 1990s,[14] which provided conclusive evidence that the K–Pg boundary clay represented debris from an asteroid impact.[8] The fact that the extinctions occurred simultaneously provides strong evidence that they were caused by the asteroid.[8] A 2016 drilling project into the Chicxulub peak ring confirmed that the peak ring comprised granite ejected within minutes from deep in the earth, but contained hardly any gypsum, the usual sulfate-containing sea floor rock in the region: the gypsum would have vaporized and dispersed as an aerosol into the atmosphere, causing longer-term effects on the climate and food chain. In October 2019, researchers reported that the event rapidly acidified the oceans, producing ecological collapse and, in this way as well, produced long-lasting effects on the climate, and accordingly was a key reason for the mass extinction at the end of the Cretaceous.[15][16] A wide range of terrestrial species perished in the K–Pg extinction, the best-known being the non-avian dinosaurs, along with many mammals, birds,[22] lizards,[23]insects,[24][25] plants, and all the pterosaurs.[26] In the oceans, the K–Pg extinction killed off plesiosaurs and mosasaurs and devastated teleost fish,[27]sharks, mollusks (especially ammonites, which became extinct), and many species of plankton. It is estimated that 75% or more of all species on Earth vanished.[28] However, the extinction also provided evolutionary opportunities: in its wake, many groups underwent remarkable adaptive radiation—sudden and prolific divergence into new forms and species within the disrupted and emptied ecological niches. Mammals in particular diversified in the Paleogene,[29] evolving new forms such as horses, whales, bats, and primates. The surviving group of dinosaurs were avians, a few species of ground and water fowl, which radiated into all modern species of birds.[30] Among other groups, teleost fish[31] and perhaps lizards[23] also radiated. The blue graph shows the apparent percentage (not the absolute number) of marine animalgenera becoming extinct during any given time interval. It does not represent all marine species, just those that are readily fossilized. The labels of the traditional \"Big Five\" extinction events and the more recently recognised Capitanian mass extinction event are clickable links; see Extinction event for more details. (source and image info) The K–Pg extinction event was severe, global, rapid, and selective, eliminating a vast number of species. Based on marine fossils, it is estimated that 75% or more of all species were made extinct.[28] Despite the event's severity, there was significant variability in the rate of extinction between and within different clades. Species that depended on photosynthesis declined or became extinct as atmospheric particles blocked sunlight and reduced the solar energy reaching the ground. This plant extinction caused a major reshuffling of the dominant plant groups.[33]Omnivores, insectivores, and carrion-eaters survived the extinction event, perhaps because of the increased availability of their food sources. Neither strictly herbivorous nor strictly carnivorousmammals seem to have survived. Rather, the surviving mammals and birds fed on insects, worms, and snails, which in turn fed on detritus (dead plant and animal matter).[34][35][36] In streamcommunities and lake ecosystems, few animal groups became extinct, including large forms like crocodyliforms and champsosaurs, because such communities rely less directly on food from living plants, and more on detritus washed in from the land, protecting them from extinction.[37][38] Modern crocodilians can live as scavengers and survive for months without food, and their young are small, grow slowly, and feed largely on invertebrates and dead organisms for their first few years. These characteristics have been linked to crocodilian survival at the end of the Cretaceous. Similar, but more complex patterns have been found in the oceans. Extinction was more severe among animals living in the water column than among animals living on or in the sea floor. Animals in the water column are almost entirely dependent on primary production from living phytoplankton, while animals on the ocean floor always or sometimes feed on detritus.[34]Coccolithophorids and mollusks (including ammonites, rudists, freshwater snails, and mussels), and those organisms whose food chain included these shell builders, became extinct or suffered heavy losses. For example, it is thought that ammonites were the principal food of mosasaurs, a group of giant marine reptiles that became extinct at the boundary.[39] The K–Pg extinction had a profound effect on the evolution of life on Earth. The elimination of dominant Cretaceous groups allowed other organisms to take their place, causing a remarkable amount of species diversification during the Paleogene Period.[29] After the K–Pg extinction event, biodiversity required substantial time to recover, despite the existence of abundant vacant ecological niches.[34] Evidence from the Salamanca Formation suggests that biotic recovery was more rapid in the Southern Hemisphere than in the Northern Hemisphere.[40] The mass extinction of marine plankton appears to have been abrupt and right at the K–Pg boundary.[41] The K–Pg boundary represents one of the most dramatic turnovers in the fossil record for various calcareousnanoplankton that formed the calcium deposits for which the Cretaceous is named. The turnover in this group is clearly marked at the species level.[42][43] Statistical analysis of marine losses at this time suggests that the decrease in diversity was caused more by a sharp increase in extinctions than by a decrease in speciation.[44] The K–Pg boundary record of dinoflagellates is not so well understood, mainly because only microbial cysts provide a fossil record, and not all dinoflagellate species have cyst-forming stages, which likely causes diversity to be underestimated.[34] Recent studies indicate that there were no major shifts in dinoflagellates through the boundary layer.[45] Radiolaria have left a geological record since at least the Ordovician times, and their mineral fossil skeletons can be tracked across the K–Pg boundary. There is no evidence of mass extinction of these organisms, and there is support for high productivity of these species in southern high latitudes as a result of cooling temperatures in the early Paleocene.[34] Approximately 46% of diatom species survived the transition from the Cretaceous to the Upper Paleocene, a significant turnover in species but not a catastrophic extinction.[34][46] The occurrence of planktonicforaminifera across the K–Pg boundary has been studied since the 1930s.[47] Research spurred by the possibility of an impact event at the K–Pg boundary resulted in numerous publications detailing planktonic foraminiferal extinction at the boundary;[34] there is ongoing debate between groups which think the evidence indicates substantial extinction of these species at the K–Pg boundary,[48] and those who think the evidence supports multiple extinctions and expansions through the boundary.[49][50] Numerous species of benthic foraminifera became extinct during the event, presumably because they depend on organic debris for nutrients, while biomass in the ocean is thought to have decreased. As the marine microbiota recovered, it is thought that increased speciation of benthic foraminifera resulted from the increase in food sources.[34] Phytoplankton recovery in the early Paleocene provided the food source to support large benthic foraminiferal assemblages, which are mainly detritus-feeding. Ultimate recovery of the benthic populations occurred over several stages lasting several hundred thousand years into the early Paleocene.[51][52] There is significant variation in the fossil record as to the extinction rate of marine invertebrates across the K–Pg boundary. The apparent rate is influenced by a lack of fossil records, rather than extinctions.[34] Ostracods, a class of small crustaceans that were prevalent in the upper Maastrichtian, left fossil deposits in a variety of locations. A review of these fossils shows that ostracod diversity was lower in the Paleocene than any other time in the Cenozoic. Current research cannot ascertain whether the extinctions occurred prior to, or during, the boundary interval.[53][54] Among decapods, extinction patterns were highly heterogeneous and cannot be neatly attributed to any particular factor. Decapods that inhabited the Western Interior Seaway were especially hard-hit, while other regions of the world's oceans were refugia that increased chances of survival into the Palaeocene.[55] Approximately 60% of late-Cretaceous Scleractiniacoral genera failed to cross the K–Pg boundary into the Paleocene. Further analysis of the coral extinctions shows that approximately 98% of colonial species, ones that inhabit warm, shallow tropical waters, became extinct. The solitary corals, which generally do not form reefs and inhabit colder and deeper (below the photic zone) areas of the ocean were less impacted by the K–Pg boundary. Colonial coral species rely upon symbiosis with photosynthetic algae, which collapsed due to the events surrounding the K–Pg boundary,[56][57] but the use of data from coral fossils to support K–Pg extinction and subsequent Paleocene recovery, must be weighed against the changes that occurred in coral ecosystems through the K–Pg boundary.[34] The numbers of cephalopod, echinoderm, and bivalve genera exhibited significant diminution after the K–Pg boundary. Entire groups of bivalves, including rudists (reef-building clams) and inoceramids (giant relatives of modern scallops), became extinct at the K–Pg boundary,[58][59] with the gradual extinction of most inoceramid bivalves beginning well before the K–Pg boundary.[60] Most species of brachiopods, a small phylum of marine invertebrates, survived the K–Pg extinction event and diversified during the early Paleocene.[34] Rudist bivalves from the Late Cretaceous of the Omani Mountains, United Arab Emirates. Scale bar is 10 mm. Except for nautiloids (represented by the modern order Nautilida) and coleoids (which had already diverged into modern octopodes, squids, and cuttlefish) all other species of the molluscan class Cephalopoda became extinct at the K–Pg boundary. These included the ecologically significant belemnoids, as well as the ammonoids, a group of highly diverse, numerous, and widely distributed shelled cephalopods.[61][62] The extinction of belemnites enabled surviving cephalopod clades to fill their niches.[63] Ammonite genera became extinct at or near the K–Pg boundary; there was a smaller and slower extinction of ammonite genera prior to the boundary associated with a late Cretaceous marine regression, and a small, gradual reduction in ammonite diversity occurred throughout the very late Cretaceous.[60] Researchers have pointed out that the reproductive strategy of the surviving nautiloids, which rely upon few and larger eggs, played a role in outsurviving their ammonoid counterparts through the extinction event. The ammonoids utilized a planktonic strategy of reproduction (numerous eggs and planktonic larvae), which would have been devastated by the K–Pg extinction event. Additional research has shown that subsequent to this elimination of ammonoids from the global biota, nautiloids began an evolutionary radiation into shell shapes and complexities theretofore known only from ammonoids.[61][62] Approximately 35% of echinoderm genera became extinct at the K–Pg boundary, although taxa that thrived in low-latitude, shallow-water environments during the late Cretaceous had the highest extinction rate. Mid-latitude, deep-water echinoderms were much less affected at the K–Pg boundary. The pattern of extinction points to habitat loss, specifically the drowning of carbonate platforms, the shallow-water reefs in existence at that time, by the extinction event.[64] There are fossil records of jawed fishes across the K–Pg boundary, which provide good evidence of extinction patterns of these classes of marine vertebrates. While the deep-sea realm was able to remain seemingly unaffected, there was an equal loss between the open marine apex predators and the durophagousdemersal feeders on the continental shelf. Within cartilaginous fish, approximately 7 out of the 41 families of neoselachians (modern sharks, skates, and rays) disappeared after this event and batoids (skates and rays) lost nearly all the identifiable species, while more than 90% of teleost fish (bony fish) families survived.[65][66] In the Maastrichtian age, 28 shark families and 13 batoid families thrived, of which 25 and 9, respectively, survived the K–T boundary event. Forty-seven of all neoselachian genera cross the K–T boundary, with 85% being sharks. Batoids display with 15%, a comparably low survival rate.[65][67] Among elasmobranchs, those species that inhabited higher latitudes and lived pelagic lifestyles were more likely to survive, whereas epibenthic lifestyles and durophagy were strongly associated with the likelihood of perishing during the extinction event.[68] There is evidence of a mass extinction of bony fishes at a fossil site immediately above the K–Pg boundary layer on Seymour Island near Antarctica, apparently precipitated by the K–Pg extinction event;[69] the marine and freshwater environments of fishes mitigated the environmental effects of the extinction event.[70] Teleost fish diversified explosively after the mass extinction, filling the niches left vacant by the extinction. Groups appearing in the Paleocene and Eocene epochs include billfish, tunas, eels, and flatfish.[31] Insect damage to the fossilized leaves of flowering plants from fourteen sites in North America was used as a proxy for insect diversity across the K–Pg boundary and analyzed to determine the rate of extinction. Researchers found that Cretaceous sites, prior to the extinction event, had rich plant and insect-feeding diversity. During the early Paleocene, flora were relatively diverse with little predation from insects, even 1.7 million years after the extinction event.[71][72] Studies of the size of the ichnotaxonNaktodemasis bowni, produced by either cicada nymphs or beetle larvae, over the course of the K-Pg transition show that the Lilliput effect occurred in terrestrial invertebrates thanks to the extinction event.[73] The extinction event produced major changes in Paleogene insect communities. Many groups of ants were present in the Cretaceous, but in the Eocene ants became dominant and diverse, with larger colonies. Butterflies diversified as well, perhaps to take the place of leaf-eating insects wiped out by the extinction. The advanced mound-building termites, Termitidae, also appear to have risen in importance.[74] Plant fossils illustrate the reduction in plant species across the K–Pg boundary. There is overwhelming evidence of global disruption of plant communities at the K–Pg boundary.[75][33] Extinctions are seen both in studies of fossil pollen, and fossil leaves.[26] In North America, the data suggests massive devastation and mass extinction of plants at the K–Pg boundary sections, although there were substantial megafloral changes before the boundary.[76] In North America, approximately 57% of plant species became extinct. In high southern hemisphere latitudes, such as New Zealand and Antarctica, the mass die-off of flora caused no significant turnover in species, but dramatic and short-term changes in the relative abundance of plant groups.[71][77] European flora was also less affected, most likely due to its distance from the site of the Chicxulub impact.[78] Another line of evidence of a major floral extinction is that the divergence rate of subviral pathogens of angiosperms sharply decreased, which indicates an enormous reduction in the number of flowering plants.[79] However, phylogenetic evidence shows no mass angiosperm extinction.[80] Due to the wholesale destruction of plants at the K–Pg boundary, there was a proliferation of saprotrophic organisms, such as fungi, that do not require photosynthesis and use nutrients from decaying vegetation. The dominance of fungal species lasted only a few years while the atmosphere cleared and plenty of organic matter to feed on was present. Once the atmosphere cleared photosynthetic organisms returned – initially ferns and other ground-level plants.[81] In some regions, the Paleocene recovery of plants began with recolonizations by fern species, represented as a fern spike in the geologic record; this same pattern of fern recolonization was observed after the 1980 Mount St. Helens eruption.[82] Just two species of fern appear to have dominated the landscape for centuries after the event.[83] In the sediments below the K–Pg boundary the dominant plant remains are angiosperm pollen grains, but the boundary layer contains little pollen and is dominated by fern spores.[84] More usual pollen levels gradually resume above the boundary layer. This is reminiscent of areas blighted by modern volcanic eruptions, where the recovery is led by ferns, which are later replaced by larger angiosperm plants.[85] In North American terrestrial sequences, the extinction event is best represented by the marked discrepancy between the rich and relatively abundant late-Maastrichtian pollen record and the post-boundary fern spike.[75] Polyploidy appears to have enhanced the ability of flowering plants to survive the extinction, probably because the additional copies of the genome such plants possessed allowed them to more readily adapt to the rapidly changing environmental conditions that followed the impact.[86] While it appears that many fungi were wiped out at the K-Pg boundary, it is worth noting that evidence has been found indicating that some fungal species thrived in the years after the extinction event. Microfossils from that period indicate a great increase in fungal spores, long before the resumption of plentiful fern spores in the recovery after the impact. Monoporisporites and hypha are almost exclusive microfossils for a short span during and after the iridium boundary. These saprophytes would not need sunlight, allowing them to survive during a period when the atmosphere was likely clogged with dust and sulfur aerosols.[81] The proliferation of fungi has occurred after several extinction events, including the Permian–Triassic extinction event, the largest known mass extinction in Earth's history, with up to 96% of all species suffering extinction.[89] There is limited evidence for extinction of amphibians at the K–Pg boundary. A study of fossil vertebrates across the K–Pg boundary in Montana concluded that no species of amphibian became extinct.[90] Yet there are several species of Maastrichtian amphibian, not included as part of this study, which are unknown from the Paleocene. These include the frog Theatonius lancensis[91] and the albanerpetontidAlbanerpeton galaktion;[92] therefore, some amphibians do seem to have become extinct at the boundary. The relatively low levels of extinction seen among amphibians probably reflect the low extinction rates seen in freshwater animals.[37] The choristoderes (a group of semi-aquatic diapsids of uncertain position) survived across the K–Pg boundary[34] subsequently becoming extinct in the Miocene.[93] The gharial-like choristodere genus Champsosaurus' palatal teeth suggest that there were dietary changes among the various species across the K–Pg event.[94] More than 80% of Cretaceous turtle species passed through the K–Pg boundary. All six turtle families in existence at the end of the Cretaceous survived into the Paleogene and are represented by living species.[95] The rhynchocephalians which were a globally distributed and diverse group of lepidosaurians during the early Mesozoic, had begun to decline by the mid-Cretaceous, although they remained successful in the Late Cretaceous of southern South America.[96] They are represented today by a single species, the tuatara (Sphenodon punctatus) found in New Zealand.[97] Outside of New Zealand, one rhynchocephalian is known to have crossed the K-Pg boundary, Kawasphenodon peligrensis, known from the earliest Paleocene (Danian) of Patagonia.[98] The order Squamata comprising lizards and snakes first diversified during the Jurassic and continued to diversify throughout the Cretaceous.[99] They are currently the most successful and diverse group of living reptiles, with more than 10,000 extant species. The only major group of terrestrial lizards to go extinct at the end of the Creteaceous were the polyglyphanodontians, a diverse group of mainly herbivorous lizards known predominantly from the Northern Hemisphere[100] The mosasaurs, a diverse group of large predatory marine reptiles, also became extinct. Fossil evidence indicates that squamates generally suffered very heavy losses in the K–Pg event, only recovering 10 million years after it. The extinction of Cretaceous lizards and snakes may have led to the evolution of modern groups such as iguanas, monitor lizards, and boas.[23] The diversification of crown group snakes has been linked to the biotic recovery in the aftermath of the K-Pg extinction event.[101] ∆44/42Ca values indicate that prior to the mass extinction, marine reptiles at the top of food webs were feeding on only one source of calcium, suggesting their populations exhibited heightened vulnerability to extinctions at the terminus of the Cretaceous.[102] Along with the aforementioned mosasaurs, plesiosaurs, represented by the families Elasmosauridae and Polycotylidae, became extinct during the event.[103][104][105][106] The ichthyosaurs had disappeared from fossil record tens of millions of year prior to the K-Pg extinction event.[107] Ten families of crocodilians or their close relatives are represented in the Maastrichtian fossil records, of which five died out prior to the K–Pg boundary.[108] Five families have both Maastrichtian and Paleocene fossil representatives. All of the surviving families of crocodyliforms inhabited freshwater and terrestrial environments—except for the Dyrosauridae, which lived in freshwater and marine locations. Approximately 50% of crocodyliform representatives survived across the K–Pg boundary, the only apparent trend being that no large crocodiles survived.[34] Crocodyliform survivability across the boundary may have resulted from their aquatic niche and ability to burrow, which reduced susceptibility to negative environmental effects at the boundary.[70] Jouve and colleagues suggested in 2008 that juvenile marine crocodyliforms lived in freshwater environments as do modern marine crocodile juveniles, which would have helped them survive where other marine reptiles became extinct; freshwater environments were not so strongly affected by the K–Pg extinction event as marine environments were.[109] Among the terrestrial clade Notosuchia, only the family Sebecidae survived; the exact reasons for this pattern are not known.[110] Sebecids were large terrestrial predators, are known from the Eocene of Europe, and would survive in South America into the Miocene.[111] Two families of pterosaurs, Azhdarchidae and Nyctosauridae, were definitely present in the Maastrichtian, and they likely became extinct at the K–Pg boundary. Several other pterosaur lineages may have been present during the Maastrichtian, such as the ornithocheirids, pteranodontids, a possible tapejarid, a possible thalassodromid and a basal toothed taxon of uncertain affinities, though they are represented by fragmentary remains that are difficult to assign to any given group.[112][113] While this was occurring, modern birds were undergoing diversification; traditionally it was thought that they replaced archaic birds and pterosaur groups, possibly due to direct competition, or they simply filled empty niches,[70][114][115] but there is no correlation between pterosaur and avian diversities that are conclusive to a competition hypothesis,[116] and small pterosaurs were present in the Late Cretaceous.[117] At least some niches previously held by birds were reclaimed by pterosaurs prior to the K–Pg event.[118] Tyrannosaurus was among the dinosaurs living on Earth before the extinction. Excluding a few controversial claims,[which?] scientists agree that all non-avian dinosaurs became extinct at the K–Pg boundary. The dinosaur fossil record has been interpreted to show both a decline in diversity and no decline in diversity during the last few million years of the Cretaceous, and it may be that the quality of the dinosaur fossil record is simply not good enough to permit researchers to distinguish between the options.[119] There is no evidence that late Maastrichtian non-avian dinosaurs could burrow, swim, or dive, which suggests they were unable to shelter themselves from the worst parts of any environmental stress that occurred at the K–Pg boundary. It is possible that small dinosaurs (other than birds) did survive, but they would have been deprived of food, as herbivorous dinosaurs would have found plant material scarce and carnivores would have quickly found prey in short supply.[70] The growing consensus about the endothermy of dinosaurs (see dinosaur physiology) helps to understand their full extinction in contrast with their close relatives, the crocodilians. Ectothermic (\"cold-blooded\") crocodiles have very limited needs for food (they can survive several months without eating), while endothermic (\"warm-blooded\") animals of similar size need much more food to sustain their faster metabolism. Thus, under the circumstances of food chain disruption previously mentioned, non-avian dinosaurs died out,[33] while some crocodiles survived. In this context, the survival of other endothermic animals, such as some birds and mammals, could be due, among other reasons, to their smaller needs for food, related to their small size at the extinction epoch.[120] Whether the extinction occurred gradually or suddenly has been debated, as both views have support from the fossil record. A highly informative sequence of dinosaur-bearing rocks from the K–Pg boundary is found in western North America, particularly the late Maastrichtian-age Hell Creek Formation of Montana.[121] Comparison with the older Judith River Formation (Montana) and Dinosaur Park Formation (Alberta), which both date from approximately 75 Ma, provides information on the changes in dinosaur populations over the last 10 million years of the Cretaceous. These fossil beds are geographically limited, covering only part of one continent.[119] The middle–late Campanian formations show a greater diversity of dinosaurs than any other single group of rocks. The late Maastrichtian rocks contain the largest members of several major clades: Tyrannosaurus, Ankylosaurus, Pachycephalosaurus, Triceratops, and Torosaurus, which suggests food was plentiful immediately prior to the extinction.[122] A study of 29 fossil sites in Catalan Pyrenees of Europe in 2010 supports the view that dinosaurs there had great diversity until the asteroid impact, with more than 100 living species.[123] More recent research indicates that this figure is obscured by taphonomic biases and the sparsity of the continental fossil record. The results of this study, which were based on estimated real global biodiversity, showed that between 628 and 1,078 non-avian dinosaur species were alive at the end of the Cretaceous and underwent sudden extinction after the Cretaceous–Paleogene extinction event.[124] Alternatively, interpretation based on the fossil-bearing rocks along the Red Deer River in Alberta, Canada, supports the gradual extinction of non-avian dinosaurs; during the last 10 million years of the Cretaceous layers there, the number of dinosaur species seems to have decreased from about 45 to approximately 12. Other scientists have made the same assessment following their research.[125] Several researchers support the existence of Paleocene non-avian dinosaurs. Evidence of this existence is based on the discovery of dinosaur remains in the Hell Creek Formation up to 1.3 m (4 ft 3.2 in) above and 40,000 years later than the K–Pg boundary.[126] Pollen samples recovered near a fossilized hadrosaurfemur recovered in the Ojo Alamo Sandstone at the San Juan River in Colorado, indicate that the animal lived during the Cenozoic, approximately 64.5 Ma (about 1 million years after the K–Pg extinction event). If their existence past the K–Pg boundary can be confirmed, these hadrosaurids would be considered a dead clade walking.[127] The scientific consensus is that these fossils were eroded from their original locations and then re-buried in much later sediments (also known as reworked fossils).[128] Most paleontologists regard birds as the only surviving dinosaurs (see Origin of birds). It is thought that all non-avian theropods became extinct, including then-flourishing groups such as enantiornithines and hesperornithiforms.[129] Several analyses of bird fossils show divergence of species prior to the K–Pg boundary, and that duck, chicken, and ratite bird relatives coexisted with non-avian dinosaurs.[130] Large collections of bird fossils representing a range of different species provide definitive evidence for the persistence of archaic birds to within 300,000 years of the K–Pg boundary. The absence of these birds in the Paleogene is evidence that a mass extinction of archaic birds took place there.[22] The most successful and dominant group of avialans, enantiornithes, were wiped out. Only a small fraction of ground and water-dwelling Cretaceous bird species survived the impact, giving rise to today's birds.[22][131] The only bird group known for certain to have survived the K–Pg boundary is the Aves.[22] Avians may have been able to survive the extinction as a result of their abilities to dive, swim, or seek shelter in water and marshlands. Many species of avians can build burrows, or nest in tree holes, or termite nests, all of which provided shelter from the environmental effects at the K–Pg boundary. Long-term survival past the boundary was assured as a result of filling ecological niches left empty by extinction of non-avian dinosaurs.[70] Based on molecular sequencing and fossil dating, many species of birds (the Neoaves group in particular) appeared to radiate after the K–Pg boundary.[30][132] The open niche space and relative scarcity of predators following the K-Pg extinction allowed for adaptive radiation of various avian groups. Ratites, for example, rapidly diversified in the early Paleogene and are believed to have convergently developed flightlessness at least three to six times, often fulfilling the niche space for large herbivores once occupied by non-avian dinosaurs.[30][133][134] Mammalian species began diversifying approximately 30 million years prior to the K–Pg boundary. Diversification of mammals stalled across the boundary.[135] All major Late Cretaceous mammalian lineages, including monotremes (egg-laying mammals), multituberculates, metatherians (which includes modern marsupials), eutherians (which includes modern placentals), meridiolestidans,[136] and gondwanatheres[137] survived the K–Pg extinction event, although they suffered losses. In particular, metatherians largely disappeared from North America, and the Asian deltatheroidans became extinct (aside from the lineage leading to Gurbanodelta).[138] In the Hell Creek beds of North America, at least half of the ten known multituberculate species and all eleven metatherians species are not found above the boundary.[119] Multituberculates in Europe and North America survived relatively unscathed and quickly bounced back in the Paleocene, but Asian forms were devastated, never again to represent a significant component of mammalian fauna.[139] A recent study indicates that metatherians suffered the heaviest losses at the K–Pg event, followed by multituberculates, while eutherians recovered the quickest.[140] K–Pg boundary mammalian species were generally small, comparable in size to rats; this small size would have helped them find shelter in protected environments. It is postulated that some early monotremes, marsupials, and placentals were semiaquatic or burrowing, as there are multiple mammalian lineages with such habits today. Any burrowing or semiaquatic mammal would have had additional protection from K–Pg boundary environmental stresses.[70] After the K–Pg extinction, mammals evolved to fill the niches left vacant by the dinosaurs.[141] Some research indicates that mammals did not explosively diversify across the K–Pg boundary, despite the ecological niches made available by the extinction of dinosaurs.[142] Several mammalian orders have been interpreted as diversifying immediately after the K–Pg boundary, including Chiroptera (bats) and Cetartiodactyla (a diverse group that today includes whales and dolphins and even-toed ungulates),[142] although recent research concludes that only marsupial orders diversified soon after the K–Pg boundary.[135] However, morphological diversification rates among eutherians after the extinction event were thrice those of before it.[143] Also significant, within the mammalian genera, new species were approximately 9.1% larger after the K–Pg boundary.[144] After about 700,000 years, some mammals had reached 50 kilos (110 pounds), a 100-fold increase over the weight of those which survived the extinction.[145] It is thought that body sizes of placental mammalian survivors evolutionarily increased first, allowing them to fill niches after the extinctions, with brain sizes increasing later in the Eocene.[146][147] The rapidity of the extinction is a controversial issue, because some theories about its causes imply a rapid extinction over a relatively short period (from a few years to a few thousand years), while others imply longer periods. The issue is difficult to resolve because of the Signor–Lipps effect, where the fossil record is so incomplete that most extinct species probably died out long after the most recent fossil that has been found.[155] Scientists have also found very few continuous beds of fossil-bearing rock that cover a time range from several million years before the K–Pg extinction to several million years after it.[34] The sedimentation rate and thickness of K–Pg clay from three sites suggest rapid extinction, perhaps over a period of less than 10,000 years.[156] At one site in the Denver Basin of Colorado, after the K–Pg boundary layer was deposited, the fern spike lasted approximately 1,000 years, and no more than 71,000 years; at the same location, the earliest appearance of Cenozoic mammals occurred after approximately 185,000 years, and no more than 570,000 years, \"indicating rapid rates of biotic extinction and initial recovery in the Denver Basin during this event.\"[157] Models presented at the annual meeting of the American Geophysical Union demonstrated that the period of global darkness following the Chicxulub impact would have persisted in the Hell Creek Formation nearly 2 years.[158] In 1980, a team of researchers consisting of Nobel Prize-winning physicist Luis Alvarez, his son, geologist Walter Alvarez, and chemists Frank Asaro and Helen Michel discovered that sedimentary layers found all over the world at the Cretaceous–Paleogene boundary contain a concentration of iridium many times greater than normal (30, 160, and 20 times in three sections originally studied). Iridium is extremely rare in Earth's crust because it is a siderophile element which mostly sank along with iron into Earth's core during planetary differentiation.[12] Instead, iridium is more common in comets and asteroids.[8] Because of this, the Alvarez team suggested that an asteroid struck the Earth at the time of the K–Pg boundary.[12] There were earlier speculations on the possibility of an impact event,[159] but this was the first hard evidence,[12] and since then, studies have continued to demonstrate elevated iridium levels in association with the K-Pg boundary.[7][6][5] This hypothesis was viewed as radical when first proposed, but additional evidence soon emerged. The boundary clay was found to be full of minute spherules of rock, crystallized from droplets of molten rock formed by the impact.[160]Shocked quartz[c] and other minerals were also identified in the K–Pg boundary.[161][162] The identification of giant tsunami beds along the Gulf Coast and the Caribbean provided more evidence,[163] and suggested that the impact might have occurred nearby, as did the discovery that the K–Pg boundary became thicker in the southern United States, with meter-thick beds of debris occurring in northern New Mexico.[26] Further research identified the giant Chicxulub crater, buried under Chicxulub on the coast of Yucatán, as the source of the K–Pg boundary clay. Identified in 1990[14] based on work by geophysicist Glen Penfield in 1978, the crater is oval, with an average diameter of roughly 180 km (110 mi), about the size calculated by the Alvarez team.[164] In March 2010, an international panel of 41 scientists reviewed 20 years of scientific literature and endorsed the asteroid hypothesis, specifically the Chicxulub impact, as the cause of the extinction, ruling out other theories such as massive volcanism. They had determined that a 10-to-15-kilometer (6 to 9 mi) asteroid hurtled into Earth at Chicxulub on Mexico's Yucatán Peninsula.[8] Additional evidence for the impact event is found at the Tanis site in southwestern North Dakota, United States.[165] Tanis is part of the heavily studied Hell Creek Formation, a group of rocks spanning four states in North America renowned for many significant fossil discoveries from the Upper Cretaceous and lower Paleocene.[166] Tanis is an extraordinary and unique site because it appears to record the events from the first minutes until a few hours after the impact of the giant Chicxulub asteroid in extreme detail.[167][168] Amber from the site has been reported to contain microtektites matching those of the Chicxulub impact event.[169] Some researchers question the interpretation of the findings at the site or are skeptical of the team leader, Robert DePalma, who had not yet received his Ph.D. in geology at the time of the discovery and whose commercial activities have been regarded with suspicion.[170] Furthermore, indirect evidence of an asteroid impact as the cause of the mass extinction comes from patterns of turnover in marine plankton.[171] In 2007, it was proposed that the impactor belonged to the Baptistina family of asteroids.[173] This link has been doubted, though not disproved, in part because of a lack of observations of the asteroid and its family.[174] It was reported in 2009 that 298 Baptistina does not share the chemical signature of the K–Pg impactor.[175] Further, a 2011 Wide-field Infrared Survey Explorer (WISE) study of reflected light from the asteroids of the family estimated their break-up at 80 Ma, giving them insufficient time to shift orbits and impact Earth by 66 Ma.[176] Artistic impression of the asteroid slamming into tropical, shallow seas of the sulfur-rich Yucatán Peninsula in what is today Southeast Mexico.[177] The aftermath of this immense asteroid collision, which occurred approximately 66 million years ago, is believed to have caused the mass extinction of non-avian dinosaurs and many other species on Earth.[177] The impact spewed hundreds of billions of tons of sulfur into the atmosphere, producing a worldwide blackout and freezing temperatures which persisted for at least a decade.[177] The scientific consensus is that the asteroid impact at the K–Pg boundary left megatsunami deposits and sediments around the area of the Caribbean Sea and Gulf of Mexico, from the colossal waves created by the impact.[180] These deposits have been identified in the La Popa basin in northeastern Mexico,[181] platform carbonates in northeastern Brazil,[182] in Atlantic deep-sea sediments,[183] and in the form of the thickest-known layer of graded sand deposits, around 100 m (330 ft), in the Chicxulub crater itself, directly above the shocked granite ejecta. The megatsunami has been estimated at more than 100 m (330 ft) tall, as the asteroid fell into relatively shallow seas; in deep seas it would have been 4.6 km (2.9 mi) tall.[184] Fossiliferous sedimentary rocks deposited during the K–Pg impact have been found in the Gulf of Mexico area, including tsunami wash deposits carrying remains of a mangrove-type ecosystem, indicating that water in the Gulf of Mexico sloshed back and forth repeatedly after the impact; dead fish left in these shallow waters were not disturbed by scavengers.[185][186][187][188][189] The re-entry of ejecta into Earth's atmosphere included a brief (hours-long) but intense pulse of infrared radiation, cooking exposed organisms.[70] This is debated, with opponents arguing that local ferocious fires, probably limited to North America, fall short of global firestorms. This is the \"Cretaceous–Paleogene firestorm debate\". A paper in 2013 by a prominent modeler of nuclear winter suggested that, based on the amount of soot in the global debris layer, the entire terrestrial biosphere might have burned, implying a global soot-cloud blocking out the sun and creating an impact winter effect.[178] If widespread fires occurred this would have exterminated the most vulnerable organisms that survived the period immediately after the impact.[190] Aside from the hypothesized fire effects on reduction of insolation, the impact would have created a humongous dust cloud that blocked sunlight for up to a year, inhibiting photosynthesis.[13][41] The asteroid hit an area of gypsum and anhydrite rock containing a large amount of combustible hydrocarbons and sulfur,[191] much of which was vaporized, thereby injecting sulfuric acidaerosols into the stratosphere, which might have reduced sunlight reaching the Earth's surface by more than 50%.[192] Fine silicate dust also contributed to the intense impact winter.[193] The climatic forcing of this impact winter was about 100 times more potent than that of the 1991 eruption of Mount Pinatubo.[194] According to models of the Hell Creek Formation, the onset of global darkness would have reached its maximum in only a few weeks and likely lasted upwards of 2 years.[158] Freezing temperatures probably lasted for at least three years.[179] At Brazos section, the sea surface temperature dropped as much as 7 °C (13 °F) for decades after the impact.[195] It would take at least ten years for such aerosols to dissipate, and would account for the extinction of plants and phytoplankton, and subsequently herbivores and their predators. Creatures whose food chains were based on detritus would have a reasonable chance of survival.[120][41] In 2016, a scientific drilling project obtained deep rock-core samples from the peak ring around the Chicxulub impact crater. The discoveries confirmed that the rock comprising the peak ring had been shocked by immense pressure and melted in just minutes from its usual state into its present form. Unlike sea-floor deposits, the peak ring was made of granite originating much deeper in the earth, which had been ejected to the surface by the impact. Gypsum is a sulfate-containing rock usually present in the shallow seabed of the region; it had been almost entirely removed, vaporized into the atmosphere. The impactor was large enough to create a 190-kilometer-wide (120 mi) peak ring, to melt, shock, and eject deep granite, to create colossal water movements, and to eject an immense quantity of vaporized rock and sulfates into the atmosphere, where they would have persisted for several years. This worldwide dispersal of dust and sulfates would have affected climate catastrophically, led to large temperature drops, and devastated the food chain.[196][197] The release of large quantities of sulphur aerosols into the atmosphere as a consequence of the impact would also have caused acid rain.[198][192] Oceans acidified as a result.[15][16] This decrease in ocean pH would kill many organisms that grow shells of calcium carbonate.[192] The heating of the atmosphere during the impact itself may have also generated nitric acid rain through the production of nitrogen oxides and their subsequent reaction with water vapour.[199][198] After the impact winter, the Earth entered a period of global warming as a result of the vapourisation of carbonates into carbon dioxide, whose long residence time in the atmosphere ensured significant warming would occur after more short-lived cooling gases dissipated.[200] Carbon monoxide concentrations also increased and caused particularly devastating global warming because of the consequent increases in tropospheric ozone and methane concentrations.[201] The impact's injection of water vapour into the atmosphere also produced major climatic perturbations.[202] The end-Cretaceous event is the only mass extinction definitively known to be associated with an impact, and other large extraterrestrial impacts, such as the Manicouagan Reservoir impact, do not coincide with any noticeable extinction events.[203] The river bed at the Moody Creek Mine, 7 Mile Creek / Waimatuku, Dunollie, New Zealand contains evidence of a devastating event on terrestrial plant communities at the Cretaceous–Paleogene boundary, confirming the severity and global nature of the event.[75] Other crater-like topographic features have also been proposed as impact craters formed in connection with Cretaceous–Paleogene extinction. This suggests the possibility of near-simultaneous multiple impacts, perhaps from a fragmented asteroidal object similar to the Shoemaker–Levy 9 impact with Jupiter. In addition to the 180 km (110 mi) Chicxulub crater, there is the 24 km (15 mi) Boltysh crater in Ukraine (65.17±0.64 Ma), the 20 km (12 mi) Silverpit crater in the North Sea (59.5±14.5 Ma) possibly formed by bolide impact, and the controversial and much larger 600 km (370 mi) Shiva crater. Any other craters that might have formed in the Tethys Ocean would since have been obscured by the northward tectonic drift of Africa and India.[204][205][206][207] The Deccan Traps, which erupted close to the boundary between the Mesozoic and Cenozoic,[208][209][210] have been cited as an alternate explanation for the mass extinction.[211][212] Before 2000, arguments that the Deccan Trapsflood basalts caused the extinction were usually linked to the view that the extinction was gradual, as the flood basalt events were thought to have started around 68 Mya and lasted more than 2 million years. The most recent evidence shows that the traps erupted over a period of only 800,000 years spanning the K–Pg boundary, and therefore may be responsible for the extinction and the delayed biotic recovery thereafter.[213] The Deccan Traps could have caused extinction through several mechanisms, including the release of dust and sulfuric aerosols into the air, which might have blocked sunlight and thereby reduced photosynthesis in plants.[214] In addition, the latest Cretaceous saw a rise in global temperatures;[215][216] Deccan Traps volcanism might have resulted in carbon dioxide emissions that increased the greenhouse effect when the dust and aerosols cleared from the atmosphere.[217][209] The increased carbon dioxide emissions also caused acid rain, evidenced by increased mercury deposition due to increased solubility of mercury compounds in more acidic water.[218] Evidence for extinctions caused by the Deccan Traps includes the reduction in diversity of marine life when the climate near the K–Pg boundary increased in temperature. The temperature increased about three to four degrees very rapidly between 65.4 and 65.2 million years ago, which is very near the time of the extinction event. Not only did the climate temperature increase, but the water temperature decreased, causing a drastic decrease in marine diversity.[219] Evidence from Tunisia indicates that marine life was deleteriously affected by a major period of increased warmth and humidity linked to a pulse of intense Deccan Traps activity.[220] Charophyte declines in the Songliao Basin, China before the asteroid impact have been concluded to be connected to climate changes caused by Deccan Traps activity.[221] In the years when the Deccan Traps hypothesis was linked to a slower extinction, Luis Alvarez (d. 1988) replied that paleontologists were being misled by sparse data. While his assertion was not initially well-received, later intensive field studies of fossil beds lent weight to his claim. Eventually, most paleontologists began to accept the idea that the mass extinctions at the end of the Cretaceous were largely or at least partly due to a massive Earth impact. Even Walter Alvarez acknowledged that other major changes might have contributed to the extinctions.[222] More recent arguments against the Deccan Traps as an extinction cause include that the timeline of Deccan Traps activity and pulses of climate change has been found by some studies to be asynchronous,[223] that palynological changes do not coincide with intervals of volcanism,[224] and that many sites show climatic stability during the latest Maastrichtian and no sign of major disruptions caused by volcanism.[225] Multiple modelling studies conclude that an impact event, not volcanism, fits best with available evidence of extinction patterns.[21][20][19] Combining these theories, some geophysical models suggest that the impact contributed to the Deccan Traps. These models, combined with high-precision radiometric dating, suggest that the Chicxulub impact could have triggered some of the largest Deccan eruptions, as well as eruptions at active volcano sites anywhere on Earth.[226][227] There is clear evidence that sea levels fell in the final stage of the Cretaceous by more than at any other time in the Mesozoic era. In some Maastrichtian stage rock layers from various parts of the world, the later layers are terrestrial; earlier layers represent shorelines and the earliest layers represent seabeds. These layers do not show the tilting and distortion associated with mountain building, therefore the likeliest explanation is a regression, a drop in sea level. There is no direct evidence for the cause of the regression, but the currently accepted explanation is that the mid-ocean ridges became less active and sank under their own weight.[34][228] A severe regression would have greatly reduced the continental shelf area, the most species-rich part of the sea, and therefore could have been enough to cause a marine mass extinction, but this change would not have caused the extinction of the ammonites. The regression would also have caused climate changes, partly by disrupting winds and ocean currents and partly by reducing the Earth's albedo and increasing global temperatures.[60] Marine regression also resulted in the loss of epeiric seas, such as the Western Interior Seaway of North America. The loss of these seas greatly altered habitats, removing coastal plains that ten million years before had been host to diverse communities such as are found in rocks of the Dinosaur Park Formation. Another consequence was an expansion of freshwater environments, since continental runoff now had longer distances to travel before reaching oceans. While this change was favorable to freshwater vertebrates, those that prefer marine environments, such as sharks, suffered.[119] Proponents of multiple causation view the suggested single causes as either too small to produce the vast scale of the extinction, or not likely to produce its observed taxonomic pattern. In a review article, J. David Archibald and David E. Fastovsky discussed a scenario combining three major postulated causes: volcanism, marine regression, and extraterrestrial impact. In this scenario, terrestrial and marine communities were stressed by the changes in, and loss of, habitats. Dinosaurs, as the largest vertebrates, were the first affected by environmental changes, and their diversity declined. At the same time, particulate materials from volcanism cooled and dried areas of the globe. Then an impact event occurred, causing collapses in photosynthesis-based food chains, both in the already-stressed terrestrial food chains and in the marine food chains.[119] Based on studies at Seymour Island in Antarctica, Sierra Petersen and colleagues argue that there were two separate extinction events near the Cretaceous–Paleogene boundary, with one correlating to Deccan Trap volcanism and one correlated with the Chicxulub impact. The team analyzed combined extinction patterns using a new clumped isotope temperature record from a hiatus-free, expanded K–Pg boundary section. They documented a 7.8±3.3 °C warming synchronous with the onset of Deccan Traps volcanism and a second, smaller warming at the time of meteorite impact. They suggested that local warming had been amplified due to the simultaneous disappearance of continental or sea ice. Intra-shell variability indicates a possible reduction in seasonality after Deccan eruptions began, continuing through the meteorite event. Species extinction at Seymour Island occurred in two pulses that coincide with the two observed warming events, directly linking the end-Cretaceous extinction at this site to both volcanic and meteorite events via climate change.[229] Speculative artist's rendering of a Thescelosaurus shortly after the K-Pg event ^The abbreviation is derived from the juxtaposition of K, the common abbreviation for the Cretaceous, which in turn originates from the correspondent German term Kreide, and Pg, which is the abbreviation for the Paleogene. ^Labandeira, C. C.; Johnson, K. R.; et al. (2002). \"Preliminary assessment of insect herbivory across the Cretaceous-Tertiary boundary: Major extinction and minimum rebound\". In Hartman, J.H.; Johnson, K.R.; Nichols, D.J. (eds.). The Hell Creek formation and the Cretaceous-Tertiary boundary in the northern Great Plains: An integrated continental record of the end of the Cretaceous. Geological Society of America. pp. 297–327. ISBN978-0-8137-2361-7. ^le Loeuff, J. (2012). \"Paleobiogeography and biodiversity of Late Maastrichtian dinosaurs: How many dinosaur species became extinct at the Cretaceous-Tertiary boundary?\". Bulletin de la Société Géologique de France. 183 (6): 547–559. doi:10.2113/gssgfbull.183.6.547."}
{"url": "https://en.m.wikipedia.org/wiki/Pliny_the_Elder", "text": "Among Pliny's greatest works was the twenty-volume Bella Germaniae (\"The History of the German Wars\"), which is no longer extant. Bella Germaniae, which began where Aufidius Bassus' Libri Belli Germanici (\"The War with the Germans\") left off, was used as a source by other prominent Roman historians, including Plutarch, Tacitus, and Suetonius. Tacitus may have used Bella Germaniae as the primary source for his work, De origine et situ Germanorum (\"On the Origin and Situation of the Germans\").[2] One of the Xanten Horse-Phalerae located in the British Museum, measuring 10.5 cm (4.1 in).[4] It bears an inscription formed from punched dots: PLINIO PRAEF EQ; i.e., Plinio praefecto equitum, \"Pliny prefect of cavalry\". It was perhaps issued to every man in Pliny's unit. The figure is the bust of the emperor. Pliny was the son of an equestrian Gaius Plinius Celer and his wife, Marcella. Neither the younger nor the elder Pliny mention the names. Their ultimate source is a fragmentary inscription (CIL V 1 3442) found in a field in Verona and recorded by the 16th-century Augustinian friar Onofrio Panvinio. The form is an elegy. The most commonly accepted reconstruction is Plinius Secundus augur ordered this to be made as a testament to his father [Ce]ler and his mother [Grania] Marcella The actual words are fragmentary. The reading of the inscription depends on the reconstruction,[5] but in all cases the names come through. Whether he was an augur and whether she was named Grania Marcella are less certain.[6]Jean Hardouin presents a statement from an unknown source that he claims was ancient, that Pliny was from Verona and that his parents were Celer and Marcella.[7] Hardouin also cites the conterraneity (see below) of Catullus.[5] How the inscription got to Verona is unknown, but it could have arrived by dispersal of property from Pliny the Younger's estate at Colle Plinio, north of Città di Castello, identified with certainty by his initials in the roof tiles. He kept statues of his ancestors there. Pliny the Elder was born at Como, not at Verona: it is only as a native of old Gallia Transpadana that he calls Catullus of Verona his conterraneus, or fellow-countryman, not his municeps, or fellow-townsman.[8][9] A statue of Pliny on the façade of the Como Cathedral celebrates him as a native son. He had a sister, Plinia, who married into the Caecilii and was the mother of his nephew, Pliny the Younger, whose letters describe his work and study regimen in detail. In one of his letters to Tacitus (avunculus meus), Pliny the Younger details how his uncle's breakfasts would be light and simple (levis et facilis) following the customs of our forefathers (veterum more interdiu). Pliny the Younger wanted to convey that Pliny the Elder was a \"good Roman\", which means that he maintained the customs of the great Roman forefathers. This statement would have pleased Tacitus. Two inscriptions identifying the hometown of Pliny the Younger as Como take precedence over the Verona theory. One (CIL V 5262) commemorates the younger's career as the imperial magistrate and details his considerable charitable and municipal expenses on behalf of the people of Como. Another (CIL V 5667) identifies his father Lucius' village as present-day Fecchio (tribe Oufentina), a hamlet of Cantù, near Como. Therefore, Plinia likely was a local girl and Pliny the Elder, her brother, was from Como.[10] Gaius was a member of the Pliniagens: the Insubric root Plina still persists, with rhotacism, in the local surname \"Prina\". He did not take his father's cognomen, Celer, but assumed his own, Secundus. As his adopted son took the same cognomen, Pliny founded a branch, the Plinii Secundi. The family was prosperous; Pliny the Younger's combined inherited estates made him so wealthy that he could found a school and a library, endow a fund to feed the women and children of Como, and own multiple estates around Rome and Lake Como, as well as enrich some of his friends as a personal favor. No earlier instances of the Plinii are known. In 59 BC, only about 82 years before Pliny's birth, Julius Caesar founded Novum Comum (reverting to Comum) as a colonia to secure the region against the Alpine tribes, whom he had been unable to defeat. He imported a population of 4,500 from other provinces to be placed in Comasco and 500 aristocratic Greeks to found Novum Comum itself.[11] The community was thus multi-ethnic and the Plinies could have come from anywhere. Whether any conclusions can be drawn from Pliny's preference for Greek words, or Julius Pokorny's derivation of the name from north Italic as \"bald\"[12] is a matter of speculative opinion. No record of any ethnic distinctions in Pliny's time is apparent—the population considered themselves to be Roman citizens. Pliny the Elder did not marry and had no children. In his will, he adopted his nephew, which entitled the latter to inherit the entire estate. The adoption is called a \"testamental adoption\" by writers on the topic[who?], who assert that it applied to the name change[what name change?] only, but Roman jurisprudence recognizes no such category. Pliny the Younger thus became the adopted son of Pliny the Elder after the latter's death.[13] For at least some of the time, however, Pliny the Elder resided in the same house in Misenum with his sister and nephew (whose husband and father, respectively, had died young); they were living there when Pliny the Elder decided to investigate the eruption of Mount Vesuvius, and was sidetracked by the need for rescue operations and a messenger from his friend asking for assistance. In AD 46, at about age 23, Pliny entered the army as a junior officer, as was the custom for young men of equestrian rank. Ronald Syme, Plinian scholar, reconstructs three periods at three ranks.[15][16] Pliny's interest in Roman literature attracted the attention and friendship of other men of letters in the higher ranks, with whom he formed lasting friendships. Later, these friendships assisted his entry into the upper echelons of the state; however, he was trusted for his knowledge and ability, as well. According to Syme, he began as a praefectus cohortis, a \"commander of a cohort\" (an infantry cohort, as junior officers began in the infantry), under Gnaeus Domitius Corbulo, himself a writer (whose works did not survive) in Germania Inferior. In AD 47, he took part in the Roman conquest of the Chauci and the construction of the canal between the rivers Maas and Rhine.[14] His description of the Roman ships anchored in the stream overnight having to ward off floating trees has the stamp of an eyewitness account.[17] Map of Castra Vetera, a large permanent base (castra stativa) of Germania Inferior, where Pliny spent the last of his 10-year term as a cavalry commander: The proximity of a naval base there means that he trained also in ships, as the Romans customarily trained all soldiers in all arms whenever possible. The location is on the lower Rhine River. At some uncertain date, Pliny was transferred to the command of Germania Superior under Publius Pomponius Secundus with a promotion to military tribune,[15] which was a staff position, with duties assigned by the district commander. Pomponius was a half-brother of Corbulo.[18] They had the same mother, Vistilia, a powerful matron of the Roman upper classes, who had seven children by six husbands, some of whom had imperial connections, including a future empress. Pliny's assignments are not clear, but he must have participated in the campaign against the Chatti of AD 50, at age 27, in his fourth year of service. Associated with the commander in the praetorium, he became a familiar and close friend of Pomponius, who also was a man of letters. At another uncertain date, Pliny was transferred back to Germania Inferior. Corbulo had moved on, assuming command in the east. This time, Pliny was promoted to praefectus alae, \"commander of a wing\", responsible for a cavalry battalion of about 480 men.[19] He spent the rest of his military service there. A decorative phalera, or piece of harness, with his name on it has been found at Castra Vetera, modern Xanten, then a large Roman army and naval base on the lower Rhine.[15] Pliny's last commander there, apparently neither a man of letters nor a close friend of his, was Pompeius Paullinus, governor of Germania Inferior AD 55–58.[20] Pliny relates that he personally knew Paulinus to have carried around 12,000 pounds of silver service on which to dine in a campaign against the Germans (a practice which would not have endeared him to the disciplined Pliny).[21] According to his nephew,[19] during this period, he wrote his first book (perhaps in winter quarters when more spare time was available), a work on the use of missiles on horseback, De Jaculatione Equestri (\"On the Use of the Dart by Cavalry\").[14] It has not survived, but in Natural History, he seems to reveal at least part of its content, using the movements of the horse to assist the javelin-man in throwing missiles while astride its back.[22] During this period, he also dreamed that the spirit of Drusus Nero begged him to save his memory from oblivion.[19] The dream prompted Pliny to begin forthwith a history of all the wars between the Romans and the Germans,[14] which he did not complete for some years. At the earliest time that Pliny could have left the service, Nero, the last of the Julio-Claudian dynasty, had been emperor for two years. He did not leave office until AD 68, when Pliny was 45 years old. During that time, Pliny did not hold any high office or work in the service of the state. In the subsequent Flavian dynasty, his services were in such demand that he had to give up his law practice, which suggests that he had been trying not to attract the attention of Nero, who was a dangerous acquaintance. Besides pleading law cases, Pliny wrote, researched, and studied. His second published work was The Life of Pomponius Secundus, a two-volume biography of his old commander, Pomponius Secundus.[19] Meanwhile, he was completing his monumental work, Bella Germaniae, the only authority expressly quoted in the first six books of the Annales of Tacitus,[14] and probably one of the principal authorities for the same author's Germania.[2] It disappeared in favor of the writings of Tacitus (which are far shorter), and, early in the fifth century, Symmachus had little hope of finding a copy.[25] Like Caligula, Nero seemed to grow gradually more insane as his reign progressed. Pliny devoted much of his time to writing on the comparatively safe subjects of grammar and rhetoric.[14] He published a three-book, six-volume educational manual on rhetoric, entitled Studiosus, \"The Student\". Pliny the Younger says of it: \"The orator is trained from his very cradle and perfected.\"[19] It was followed by eight books entitled Dubii sermonis[14] (Of Doubtful Phraseology). These are both now lost works. His nephew relates: \"He wrote this under Nero, in the last years of his reign, when every kind of literary pursuit which was in the least independent or elevated had been rendered dangerous by servitude.\" In 68, Nero no longer had any friends and supporters. He committed suicide, and the reign of terror was at an end, as was the interlude in Pliny's obligation to the state. At the end of AD 69, after a year of civil war consequent on the death of Nero, Vespasian, a successful general, became emperor. Like Pliny, he had come from the equestrian class, rising through the ranks of the army and public offices and defeating the other contenders for the highest office. His main tasks were to re-establish peace under imperial control and to place the economy on a sound footing. He needed in his administration all the loyalty and assistance he could find. Pliny, apparently trusted without question, perhaps (reading between the lines) recommended by Vespasian's son Titus, was put to work immediately and was kept in a continuous succession of the most distinguished procuratorships, according to Suetonius.[26] A procurator was generally a governor of an imperial province. The empire was perpetually short of, and was always seeking, officeholders for its numerous offices. Throughout the latter stages of Pliny's life, he maintained good relations with Emperor Vespasian. As is written in the first line of Pliny the Younger's Avunculus Meus: Before dawn he was going to Emperor Vespasian (for he also made use of the night), then he did the other duties assigned to him. In this passage, Pliny the Younger conveys to Tacitus that his uncle was ever the academic, always working. The word ibat (imperfect, \"he used to go\") gives a sense of repeated or customary action. In the subsequent text, he mentions again how most of his uncle's day was spent working, reading, and writing. He notes that Pliny \"was indeed a very ready sleeper, sometimes dropping off in the middle of his studies and then waking up again.\"[27] A definitive study of the procuratorships of Pliny was compiled by the classical scholar Friedrich Münzer, which was reasserted by Ronald Syme and became a standard reference point. Münzer hypothesized four procuratorships, of which two are certainly attested and two are probable but not certain. However, two does not satisfy Suetonius' description of a continuous succession.[28] Consequently, Plinian scholars present two to four procuratorships, the four comprising (i) Gallia Narbonensis in 70, (ii) Africa in 70–72, (iii) Hispania Tarraconensis in 72–74, and (iv) Gallia Belgica in 74–76. According to Syme, Pliny may have been \"successor to Valerius Paulinus\", procurator of Gallia Narbonensis (southeastern France), early in AD 70. He seems to have a \"familiarity with the provincia\", which, however, might otherwise be explained.[29] For example, he says[30] In the cultivation of the soil, the manners and civilization of the inhabitants, and the extent of its wealth, it is surpassed by none of the provinces, and, in short, might be more truthfully described as a part of Italy than as a province. Pliny certainly spent some time in the province of Africa, most likely as a procurator.[31] Among other events or features that he saw are the provoking of rubetae, poisonous toads (Bufonidae), by the Psylli;[32] the buildings made with molded earthen walls, \"superior in solidity to any cement;\"[33] and the unusual, fertile seaside oasis of Gabès (then Tacape), Tunisia, currently a World Heritage Site.[34] Syme assigns the African procuratorship to AD 70–72. The procuratorship of Hispania Tarraconensis was next. A statement by Pliny the Younger that his uncle was offered 400,000 sesterces for his manuscripts by Larcius Licinius while he (Pliny the Elder) was procurator of Hispania makes it the most certain of the three.[19] Pliny lists the peoples of \"Hither Hispania\", including population statistics and civic rights (modern Asturias and Gallaecia). He stops short of mentioning them all for fear of \"wearying the reader\".[35] As this is the only geographic region for which he gives this information, Syme hypothesizes that Pliny contributed to the census of Hither Hispania conducted in 73/74 by Vibius Crispus, legate from the Emperor, thus dating Pliny's procuratorship there.[36] During his stay in Hispania, he became familiar with the agriculture and especially the gold mines of the north and west of the country.[37] His descriptions of the various methods of mining appear to be eyewitness judging by the discussion of gold mining methods in his Natural History. He might have visited the mine excavated at Las Médulas. The last position of procurator, an uncertain one, was of Gallia Belgica, based on Pliny's familiarity with it. The capital of the province was Augusta Treverorum (Trier), named for the Treveri surrounding it. Pliny says that in \"the year but one before this\" a severe winter killed the first crops planted by the Treviri; they sowed again in March and had \"a most abundant harvest.\"[38] The problem is to identify \"this\", the year in which the passage was written. Using 77 as the date of composition Syme[39] arrives at AD 74–75 as the date of the procuratorship, when Pliny is presumed to have witnessed these events. The argument is based entirely on presumptions; nevertheless, this date is required to achieve Suetonius' continuity of procuratorships, if the one in Gallia Belgica occurred. Pliny was allowed home (Rome) at some time in AD 75–76. He was presumably at home for the first official release of Natural History in 77. Whether he was in Rome for the dedication of Vespasian's Temple of Peace in the Forum in 75, which was in essence a museum for display of art works plundered by Nero and formerly adorning the Domus Aurea, is uncertain, as is his possible command of the vigiles (night watchmen), a lesser post. No actual post is discernible for this period. On the bare circumstances, he was an official agent of the emperor in a quasiprivate capacity. Perhaps he was between posts. In any case, his appointment as commander of the imperial fleet at Misenum[40] took him there, where he resided with his sister and nephew. Vespasian died of disease on 23 June 79. Pliny outlived him by four months. During Nero's reign of terror, Pliny avoided working on any writing that would attract attention to himself. His works on oratory in the last years of Nero's reign (67–68) focused on form rather than on content. He began working on content again probably after Vespasian's rule began in AD 69, when the terror clearly was over and would not be resumed. It was to some degree reinstituted (and later cancelled by his son Titus) when Vespasian suppressed the philosophers at Rome, but not Pliny, who was not among them, representing, as he says, something new in Rome, an encyclopedist (certainly, a venerable tradition outside Italy).[41] In his next work, Bella Germaniae, Pliny completed the history which Aufidius Bassus left unfinished. Pliny's continuation of Bassus's History was one of the authorities followed by Suetonius and Plutarch.[14] Tacitus also cites Pliny as a source. He is mentioned concerning the loyalty of Burrus, commander of the Praetorian Guard, whom Nero removed for disloyalty.[42] Tacitus portrays parts of Pliny's view of the Pisonian conspiracy to kill Nero and make Piso emperor as \"absurd\"[43] and mentions that he could not decide whether Pliny's account or that of Messalla was more accurate concerning some of the details of the Year of the Four Emperors.[44] Evidently Pliny's extension of Bassus extended at least from the reign of Nero to that of Vespasian. Pliny seems to have known it was going to be controversial, as he deliberately reserved it for publication after his death:[14] It has been long completed and its accuracy confirmed; but I have determined to commit the charge of it to my heirs, lest I should have been suspected, during my lifetime, of having been unduly influenced by ambition. By this means I confer an obligation on those who occupy the same ground with myself; and also on posterity, who, I am aware, will contend with me, as I have done with my predecessors.[45] Pliny's last work, according to his nephew, was the Naturalis Historia (Natural History), an encyclopedia into which he collected much of the knowledge of his time.[19] Some historians consider this to be the first encyclopedia written.[46] It comprised 37 books. His sources were personal experience, his own prior works (such as the work on Germania), and extracts from other works. These extracts were collected in the following manner: One servant would read aloud, and another would write the extract as dictated by Pliny. He is said to have dictated extracts while taking a bath. In winter, he furnished the copier with gloves and long sleeves so his writing hand would not stiffen with cold (Pliny the Younger in avunculus meus). His extract collection finally reached about 160 volumes, which Larcius Licinius, the Praetorian legate of Hispania Tarraconensis, unsuccessfully offered to purchase for 400,000 sesterces.[19] That would have been in 73/74 (see above). Pliny bequeathed the extracts to his nephew. When composition of Natural History began is unknown. Since he was preoccupied with his other works under Nero and then had to finish the history of his times, he is unlikely to have begun before 70. The procuratorships offered the ideal opportunity for an encyclopedic frame of mind. The date of an overall composition cannot be assigned to any one year. The dates of different parts must be determined, if they can, by philological analysis (the post mortem of the scholars). The closest known event to a single publication date, that is, when the manuscript was probably released to the public for borrowing and copying, and was probably sent to the Flavians, is the date of the Dedication in the first of the 37 books. It is to the imperator Titus. As Titus and Vespasian had the same name, Titus Flavius Vespasianus, earlier writers hypothesized a dedication to Vespasian. Pliny's mention of a brother (Domitian) and joint offices with a father, calling that father \"great\", points certainly to Titus.[47] Pliny also says that Titus had been consul six times.[48] The first six consulships of Titus were in 70, 72, 74, 75, 76, and 77, all conjointly with Vespasian, and the seventh was in 79. This brings the date of the Dedication probably to 77. In that year, Vespasian was 68. He had been ruling conjointly with Titus for some years.[47] The title imperator does not indicate that Titus was sole emperor, but was awarded for a military victory, in this case that in Jerusalem in 70.[49] Aside from minor finishing touches, the work in 37 books was completed in AD 77.[50] That it was written entirely in 77 or that Pliny was finished with it then cannot be proved. Moreover, the dedication could have been written before publication, and it could have been published either privately or publicly earlier without the dedication. The only certain fact is that Pliny died in AD 79. Natural History is one of the largest single works to have survived from the Roman Empire and was intended to cover the entire field of ancient knowledge, based on the best authorities available to Pliny. He claims to be the only Roman ever to have undertaken such a work. It encompasses the fields of botany, zoology, astronomy, geology, and mineralogy, as well as the exploitation of those resources. It remains a standard work for the Roman period and the advances in technology and understanding of natural phenomena at the time. His discussions of some technical advances are the only sources for those inventions, such as hushing in mining technology or the use of water mills for crushing or grinding grain. Much of what he wrote about has been confirmed by archaeology. It is virtually the only work that describes the work of artists of the time, and is a reference work for the history of art. As such, Pliny's approach to describing the work of artists informed Lorenzo Ghiberti in writing his commentaries in the 15th century, and Giorgio Vasari, who wrote the celebrated Lives of the Most Excellent Painters, Sculptors, and Architects in 1550. Some historians consider Natural History to be the first encyclopedia ever written.[46] It was the earliest encyclopedia to survive. There were many ancient histories written before Pliny the Elder's Natural History, but scholars still recognize Natural History as an encyclopedia, setting it apart from the other ancient histories. Regardless of if it was first, it is certainly the most significant. Through Natural History, Pliny the Elder gives modern experts a view into meanings of various things from first century Rome in a way that no other surviving text does.[51] Each book of the Natural History covers a different topic, and the work is meant to cover every topic. Given the organization of the work, it is clear that it was meant to be a reference resource.[51] Even modern scholars will sometimes compare an unknown object mentioned in a different ancient text with the objects described by Pliny and make comparisons. Modern scholars are also able to use Natural History to understand the traditions, fantasies, and prejudices in Ancient Rome. Some people[who?] have said that certain prejudices that have been prevalent throughout western history (such as a stigma around menstruation) were spread by Natural History. The work became a model for all later encyclopedias in terms of the breadth of subject matter examined, the need to reference original authors, and a comprehensive index list of the contents. It is the only work by Pliny to have survived, and the last that he published, lacking a final revision at his sudden and unexpected death in the AD 79 eruption of Mount Vesuvius. As Pliny's vessel approached the shore near Herculaneum, cinders and pumice began to fall on it. The helmsman advised turning back, to which Pliny replied, \"Fortune favours the bold; steer to where Pomponianus is.\" Upon reaching Stabiae, they found SenatorPomponianus, but the same winds that brought them there prevented them from leaving. The group waited for the wind to abate, but they decided to leave later that evening for fear their houses would collapse. The group fled when a plume of hot toxic gases engulfed them. Pliny, a corpulent man who suffered from a chronic respiratory condition, possibly asthma, died from asphyxiation caused by the toxic gases, and was left behind. Upon the group's return three days later after the plume had dispersed, Pliny's body was found, with no apparent external injuries.[52] Twenty-seven years later, upon a request from Tacitus, Pliny the Younger provided an account (obtained from the survivors from Stabiae) of his uncle's death.[52][19][14] Suetonius wrote that Pliny approached the shore only from scientific interest and then asked a slave to kill him to avoid heat from the volcano.[53] In 1859, Jacob Bigelow, after summarizing the information about Pliny's death contained in Pliny the Younger's letter to Tacitus, concluded that Pliny had died from apoplexy (stroke) or heart disease.[54] In 1967, science historian Conway Zirkle similarly stated that \"there is widespread and persisting misinformation\" about Pliny's death. He suggested that despite his rescue attempt, Pliny never came within miles of Mount Vesuvius and no evidence has been found that shows he died from breathing in fumes, and like Bigelow, concluded that he died of a heart attack.[55] ^So also is the further speculation by Metello that she was the daughter of Titus, which suggests a possible connection with the Titii Pomponii on his mother's side, and a connection with the Caecilii (Celer was a cognomen used by that Gens) on his father's side: Metello, Manuel Arnao; João Carlos Metello de Nápoles (1998). Metellos de Portugal, Brasil e Roma: compilações genealógicas (in Portuguese). Lisboa: Edição Nova Arrancada. ISBN978-972-8369-18-7. ^\"XVI.2\". Natural History. Many is the time that these trees have struck our fleets with alarm, when the waves have driven them, almost purposely it would seem, against their prows as they stood at anchor in the night; and the men, destitute of all remedy and resource, have had to engage in naval combat with a forest of trees! ^\"XXXIII.50\". Natural History. to my own knowledge, Pompeius Paulinus... had with him, when serving with the army, and that, too, in a war against the most savage nations, a service of silver plate that weighed twelve thousand pounds! ^\"VIII.65\". Natural History. Those who have to use the javelin are well aware how the horse, by its exertions and the supple movements of its body, aids the rider in any difficulty he may have in throwing his weapon. ^\"XXV.76\". Natural History. I myself have seen the Psylli, in their exhibitions, irritate them by placing them upon flat vessels made red hot, their bite being fatal more instantaneously than the sting even of the asp. ^\"XXXIII.21\". Natural History. Asturia, Gallæcia, and Lusitania furnish in this manner, yearly, according to some authorities, twenty thousand pounds' weight of gold, the produce of Asturia forming the major part. Indeed, there is no part of the world that for centuries has maintained such a continuous fertility in gold."}
{"url": "https://en.m.wikipedia.org/wiki/Ouzo_effect", "text": "If then the concentration of ethanol is lowered by addition of more water the hydrophobic substance precipitates from the solution and forms an emulsion with the remaining ethanol-water-mixture. The tiny droplets of the substance in the emulsion scatter light and thus make the mixture appear white. In a water-rich ouzo mixture the droplet coalescence is dramatically slowed without mechanical agitation, dispersing agents, or surfactants. It forms a stable homogeneous fluid dispersion by liquid–liquid nucleation.[2] The size of the droplets when measured by small-angle neutron scattering was found to be on the order of a micron.[3] Using dynamic light scattering, Sitnikova et al.[1] showed that the droplets of oil in the emulsion grow by Ostwald ripening, and that droplets do not coalesce. The Ostwald ripening rate is observed to diminish with increasing ethanol concentrations until the droplets stabilize in size with an average diameter of 3microns. Based on thermodynamic considerations of the multi-component mixture, the emulsion derives its stability from trapping between the binodal and spinodal curves in the phase diagram.[3] However, the microscopic mechanisms responsible for the observed slowing of Ostwald ripening rates at increasing ethanol concentrations appear not fully understood. Video of water being poured into a solution of 151-proof Everclear and essential oils from grapefruit rinds. Emulsions have many commercial uses. A large range of prepared food products, detergents, and body-care products take the form of emulsions that are required to be stable over a long period of time. The ouzo effect is seen as a potential mechanism for generating surfactant-free emulsions without the need for high-shear stabilisation techniques that are costly in large-scale production processes. The creation of a variety of dispersions such as pseudolatexes, silicone emulsions, and biodegradable polymeric nanocapsules, have been synthesized using the ouzo effect, though as stated previously, the exact mechanism of this effect remains unclear.[4]Nanoparticles formed using the ouzo effect are thought to be kinetically stabilized as opposed to thermodynamically stabilized micelles formed using a surfactant due to the fast solidification of the polymer during the preparation process.[5]"}
{"url": "https://en.m.wikipedia.org/wiki/Chevrolet_Nova", "text": "Chevrolet Chevy II / Nova For the model sold in Latin America and based on Opel/Vauxhall Corsa, see Chevrolet Chevy. The Chevrolet Chevy II/Nova is a small automobile manufactured by Chevrolet, and produced in five generations for the 1962 through 1979, and 1985 through 1988 model years. Built on the X-body platform, the Nova was the top selling model in the Chevy II lineup through 1968. The Chevy II nameplate was dropped after 1968, with Nova becoming the nameplate for all of the 1969 through 1979 models. It was replaced by the 1980 Chevrolet Citation introduced in the spring of 1979. The Nova nameplate returned in 1985, produced through 1988 as a S-car based, NUMMI manufactured, subcompact based on the front wheel drive, Japan home-based Toyota Sprinter. Chevrolet designer Clare MacKichan recalled about creating the Chevy II: \"There was no time for experimentation or doodling around with new ideas from either the engineers or from us in design; And it had to be a basic-type car.\" The 1962 Chevy II rode a 110 in (2,794 mm) wheelbase, compared to 109.5 in (2,781 mm) for the Ford Falcon, at which Chevy's new compact was aimed. \"I think that was the quickest program we ever did at any time,\" he continued. \"We worked night and day on that car, and it didn't take very long to run it through our shop because we had a deadline.\" And that is what made the Chevy II one of the fastest new-car development programs in GM history – just 18 months after the designers got the green light, the first production Chevy II rolled off the Willow Run, Michigan, assembly line in August 1961, in time for its September 29 introduction. Unlike the Corvair, the 1962 Chevy II design team deliberately avoided any revolutionary features in concept or execution; their mission was to give Chevrolet buyers a simple, back-to-the-basics compact car. When he announced the Chevy II to the press, Chevrolet General Manager Ed Cole described the car as offering \"maximum functionalism with thrift.\" When the Chevy II was introduced, it was the second post-WWII American made car from the \"Big 3\", after the Pontiac Tempest (and the first Chevrolet since the 1928 Chevrolet National), to use a four-cylinder engine. There was a lot of debate within the Chevrolet organization over just what to call this new car, and the decision to go with \"Chevy II\" was a very late one. Among the finalists was Nova. It lost out because it didn't start with a \"C,\" but was selected as the name for the top-of-the-line series. Ultimately the Nova badge would replace Chevy II, but that wouldn't happen until 1969. In almost every way, the creators of the Chevy II used Falcon as a benchmark. The 1962 model range included sedans and wagons, as well as a two-door hardtop and a convertible. The only body styles it didn't offer which the Falcon did were a 2-door wagon/sedan delivery and coupe utility (the Ford Falcon Ranchero). After the rear-engineChevrolet Corvair was outsold by the conventional Ford Falcon in 1960, Chevrolet completed work on a more conventional compact car that would eventually become the Chevy II. The car was of semi-unibody construction having a bolt on front section joined to its unitized cabin and trunk rear section, available in two- and four-door sedan configurations as well as convertible and 4-door station wagon versions. The 1962 Chevy II came in three series and five body styles—the 100 Series, 300 Series and Nova 400 Series. A 200 series was also introduced, but was discontinued almost immediately.[2] The sportiest-looking of the lot was the US$2,475 ($24,930 in 2023 dollars [3]) Nova 400 convertible—23,741 were produced that year.[4] Available engines for the Chevy II in 1962 and 1963 included Chevrolet's inline-four engine of 153 cu in (2.5 L) and a new 194 cu in (3.2 L) Hi-Thriftstraight-six engine. All Chevy II engines featured overhead valves. A V8 engine was not available in 1962 and 1963. With no documentation proving it, the legend of a dealer installed V8 engine being in a 1962 or 1963 model year Chevy II is a myth. Refer to the GM Heritage Center 1963 Chevrolet Nova information available on the GM Heritage site.[5] In addition, that documentation does not list a V8 engine as a possible dealer installed option. In 1962 and 1963 the Nova option for the Chevy II was available in a convertible body style, and a two-door hardtop was available from 1962 to 1965, although the hardtop was dropped when the 1964 models were first introduced, but subsequently brought back to the line later in the model year. Like all Chevy two-door hardtops, the body style was marketed as the Sport Coupe. For 1963, the Chevy II Nova Super Sport was released, under RPO Z03.[6] It featured special emblems, instrument package, wheel covers, side moldings, bucket seats, and floor shifter, and was available only on the 400 series sport coupe and convertible.[6] Cost of the package was US$161.40, equal to $1,606.28 today.[7] As mentioned above, the Nova option could not officially have V8 engines at this time—the standard SS engine was the six-cylinder (this was also applicable to the Impala (and later the early Chevelle c. 1964–65) when the SS was a sport and appearance package)—but small-block V8 engine swaps were commonplace among enthusiasts. For 1964, sales were hit hard by the introduction of the new Chevelle,[8] and the Chevy II received its first factory V8 option, a 195 hp (145 kW) 283 cu in (4.6 L), as well as a 230 cu in (3.8 L) straight six.[9] The six-cylinder was all-new, replacing the older Stovebolt engine. Rival manufacturer Chrysler had earlier developed the Slant Six in their Plymouth Valiant, a Chevy II competitor, when the cars were introduced to the public in late 1959 as 1960 models. At introduction in the fall, the hardtop coupe was missing in the lineup, contributing to a loss of sales (as well as showroom appeal). Chevrolet subsequently reintroduced the Sport Coupe in the lineup later in the model year, and it remained available through 1967. 1965 Chevrolet Chevy II Nova 4-door sedan (with aftermarket wheels) The 1965 Chevrolet Chevy II and Nova were updated with cleaner front-end styling courtesy of a fresh full-width grille with new integrated headlight bezels. Parking lights moved down to the deep-section bumper, and sedans gained a new roofline. Taillight and backup lights were restyled, as was the rear cove. The 1965 Chevy II came in entry-level 100 form or as the posher Nova 400, each in three body styles. The Nova Super Sport came as a Sport Coupe only, and its production dipped to just 9,100 cars. Super Sports had a new brushed-chrome console with floor-mounted four-speed manual transmission or Powerglide automatic, but a column-mounted three-speed manual remained standard. Bucket seats wore textured vinyl trim, and the dashboard held ammeter, oil pressure, and temperature gauges. An expanded engine lineup gave customers six power choices of the six-cylinder or V-8 engines; the four-cylinder was available only in the 100. But, for Chevy II enthusiasts, 1965 is best remembered as the year the Chevy II became a muscle car. A 327 cu in (5.4 L) V8 was available with up to 300 hp (220 kW), suddenly putting Nova SS performance practically on a par with the GTO, 4-4-2, and 271 bhp Mustang 289s-at least in straight-line acceleration. Midyear also brought a more potent 283 with dual exhausts and 220 horsepower. The Chevelle Malibu SS continued to eat away at the Nova SS market: Out of 122,800 Chevy IIs built for 1965 (compared to 213,601 Falcons), only 9,100 were Super Sports. For 1965, Chevy II had the dubious distinction of being the only car in GM's lineup to suffer a sales decline. It is possible that some Chevy II sales were lost to the brand-new '65 Corvair, which addressed virtually all its 1960–64 problems, got rave reviews from automotive journals and featured sleek new (Z-body) styling along with a brand-new chassis. 1966 Chevy IIs introduced an extensive sharp-edged restyle based in part on the Super Nova concept car. In general, proportions were squared up but dimensions and features changed little. Highlights included a bold grille and semi-fastback roofline. \"Humped\" fenders in an angular rear end were reminiscent of larger 1966 Chevrolets, though the 1966 Chevy II and Nova had vertical taillights and single headlights. The lineup again started with Chevy II 100 and Chevy II Nova 400 models. For just $159 (equal to $1,493.13 today) more than a Nova 400, buyers could choose a Super Sport. Available only in a Sport Coupe, the SS was top of the line. The 194 cu in (3.18 L) inline-six was standard on the Super Sport, but any Chevy II (excluding four-cylinder) engine could be coupled with the SS. The SS was visually distinguished by wide rocker panels and a bright aluminum deck lid cove. It had a bright SS emblem on the grille and in the ribbed rear panel, and Super Sport script on the quarter panels. Wheel covers were inherited from the 1965 Malibu SS. Strato-bucket front seats were included, but a tachometer cost extra.[11] The ’66 Chevy II sales brochure clearly promoted the Super Sport as the “Chevrolet Chevy II Nova Super Sport,” but the name \"Nova\" was not used anywhere on the body. Front and rear emblems displayed \"Chevy II SS.\"[12] In 1967, Chevy II was still the name of the vehicle, but the Nova SS option package replaced all Chevy II badging with Nova SS badging. The 90 hp (67 kW) 153 cu in (2.51 L) inline-four engine was only offered in the base Chevy II 100 series models. Buyers could also order a 194 cu in (3.18 L) inline-six engine (std. in the SS), a 230 cu in (3.8 L) inline-six, a 195 hp (145 kW) or 220 hp (160 kW) 283 cu in (4.64 L) V-8, a 275 hp (205 kW) 327 cu in (5.36 L) V-8 and the top engine, a new Turbo-Fire 327 cu in (5.36 L) V-8 delivering 350 hp (260 kW). This engine was first seen in the Chevelle. This engine with the close-ratio four-speed manual transmission turned the normally mild Nova into a proper muscle car. The Powerglide automatic was not available with the 350 hp engine. The 1967 models received nothing more than a touch-up after a restyling for 1966. All Novas got a crosshatch pattern that filled the deck lid trim panel. The Nova officially was still called the Chevy II Nova and had overtaken the bottom-rung Chevy II 100 in sales. The Chevy II 100 lacked much in the way of trim or brightwork. 1967 models carried significant improvements in the area of safety equipment. A government-mandated energy-absorbing steering column and safety steering wheel, soft interior parts such as armrests and sun visors, recessed instrument panel knobs, front seat belt anchors and dual brake master cylinders, were included in all 1967 models. The 1967 Chevy II and its deluxe Nova rendition continued to attract compact-car shoppers, but the Chevrolet Camaro, introduced for 1967, took away some Nova sales. Available only in hardtop coupe form, the 1967 Chevrolet Nova SS got a new black-accented anodized aluminum grille. SS wheel covers were again inherited, this time from the 1965–66 Impala SS. The 1966 \"Chevy II SS\" badges were replaced with \"Nova SS\" emblems for the '67s. Nova versions started with the 194 cu in (3.18 L)in-line six engine but new was an optional 250 cu in (4.1 L) inline-six. Further powertrain options included a 195 hp (145 kW) 283 cu in (4.64 L) V-8 and, for $93 more, a 275 hp (205 kW) 327 cu in (5.36 L) V-8. Nova SS coupes had a console-mounted shift lever with their Powerglide automatic transmission or a four-speed manual. Other models had a column-mounted gearshift. Compared to the 1966 model year output, sales of the 1967 models dropped by more than a third to 106,500 (including 12,900 station wagons). About 10,100 Nova SS Chevrolets went to customers this year, 8,200 of them with V-8 engines. In the Chevy II 100 and regular Nova series, six-cylinder engines sold far better than V-8s. The 1968 models were fully-redesigned with an extensive restyle on a longer 111-inch wheelbase that gave Chevrolet's compacts a chassis that was just one inch shorter than that of the midsize Chevelle coupe. The station wagon and hardtop sport coupe were discontinued, the former in line with an industry trend which left AMC the only American maker of compact station wagons until Chrysler rejoined the market in 1976 (the 1966–70 Ford Falcon wagon was actually midsize, using a bodyshell identical to the Fairlane wagon's). One notable change was the front subframe assembly — as compared with Ford, Chrysler and AMC, in whose cars the entire front suspension was integrated with the bodyshell, a separate subframe housing the powertrain and front suspension (similar to the front part of the frame of GM's full-size, full-framed vehicles) replaced the earlier style. Although the front subframe design was unique for the Nova, the Camaro introduced a year earlier was the first to incorporate such a design; the redesigned Nova was pushed a year ahead to 1968 instead of 1969. The sales brochure claimed 15 powertrain choices for coupes and a dozen for sedans. Options included power brakes and steering, Four-Season or Comfort-Car air conditioning, rear shoulder belts, and head restraints. There were a few Chevrolet Novas built with the 194 ci (3.1 L), the same motor that had been used in the previous generations of the Chevy II. Sales of the 1968 Chevy II Nova fell by half. In 1969 Chevrolet dropped the Chevy II portion of its compact car's name; it was now known simply as the Chevrolet Nova. The 153 cu in (2.51 L) four-cylinderengine was offered between 1968 and 1970, then was dropped due to lack of interest (besides its other usage in the Jeep DJ-5A a.k.a. the Postal Jeep or a marine/industrial engine) and to clear the field for the Vega. Far more popular were the 230 cu in (3.8 L) six-cylinder and the base 307 cu in (5.03 L) V8, which replaced the 283 cu in (4.64 L) V8 offered in previous years. Several units were produced with the 327 cu in (5.36 L), 275 hp (205 kW), engine, four-barrel quadrajet carb and four-speed Saginaw transmission with a heavy-duty 12-bolt positraction rear as a \"towing option' package. At mid-year, a semi-automatic transmission based on the Powerglide called the Torque-Drive (RPO MB1) was introduced as a low-cost option (~$100 less than the Powerglide) for clutchless motoring. The Torque-Drive transmission was only offered with the four and six-cylinder engines. The two-speed Powerglide was still the only fully-automatic transmission available with most engines, as the more desirable three-speed Turbo-Hydramatic was only available with the largest V8 engines. The Nova Super Sport was transformed from a trim option to a performance package for 1968. One of the smallest muscle cars ever fielded by Detroit, the Nova SS now included a 295 hp (220 kW) 350 cu in (5.7 L) V8 engine along with a heavy-duty suspension and other performance hardware, priced at US$312.[14] Optional V8 engines included two versions of the big-block 396 cu in (6.5 L) rated at 350 bhp (350 PS; 260 kW); and 375 bhp (380 PS; 280 kW) at 5600 rpm and 415 lb⋅ft (563 N⋅m) at 3600 rpm of torque,[15] which went for US$348.[16] Both engines were offered with a choice of transmissions including the M-21 close-ratio four-speed manual, the heavy-duty M-22 \"Rock Crusher\" four-speed manual, or the three-speed Turbo-Hydramatic 400 automatic transmission. A total of 5,571 SS coupés were produced for 1968. Novas sported the SS badge until 1976.[17] Front disc brakes were optional on the 1968 Nova SS. For 1969 the Chevy II nameplate was retired, leaving the Nova nameplate.[18] The \"Chevy II by Chevrolet\" trunklid badge was replaced with \"Nova by Chevrolet\" and the \"Chevy II\" badge above the grille was replaced with the bowtie emblem and the 1969 model was promoted under the Nova model name in Chevrolet sales literature.[19] As with other 1969 GM vehicles, locking steering columns were incorporated into the Nova. Simulated air extractor/vents were added below the Nova script, which was relocated to the front fender behind the wheel-well instead of the rear quarter panel. The 350 cu in (5.7 L) V8 with four-barrel carburetor that came standard with the SS option was revised with a 5 hp (4 kW) increase to 300 hp (220 kW), while a two-barrel carbureted version of the 350 cu in (5.7 L) V8 rated at 255 hp (190 kW) was a new option on non-SS models. The SS option price remained US$312[20] A new Turbo-Hydramatic 350 three-speed automatic was made available for non-SS Novas with six-cylinder and V8 engines, although the older two-speed Powerglide continued to be available on the smaller-engined Novas. 1969 SS models were the first Nova SS models to have standard front disc brakes. 1970–1972 Chevrolet Nova four-door sedan The 1970 Nova was basically a carryover from 1969. The side marker and taillight lenses for the 1970 Nova were wider and positioned slightly differently. This was the final year for the SS396 (actually, a 402 cubic in. engine now). All other engines were carried over including the seldom-ordered four-cylinder which was in its final year.[21] The car finally became simply the Chevrolet Nova this year after two years of transitional nameplates (Chevy II Nova in 1968 and Chevrolet Chevy Nova in 1969). Out of 254,242 Novas sold for 1970, 19,558 were the SS 350 or SS 396 version. Approximately 177 Central Office Production Order (COPO) Novas were ordered, with 175 converted by Yenko Chevrolet. The other two were sold in Canada. The Nova was used in Trans-Am racing this year. Year 1971 Novas were similar to the previous year. The 396 cu in (6.49 L) engine was replaced with the 350 cu in (5.7 L) in the SS model. 1971 also saw the introduction of the Rally Nova, a trim level that only lasted two years (until it resurfaced as the Nova Rally in 1977). The Rally kit included black or white stripes that ran the length of the car and between the taillights in the back, a Rally Nova sticker on the driver's side of the hood, 6-slot 14X6\" Rally wheels, heavy duty suspension with mono-leaf or multi-leaf in the rear depending on optional equipment ordered., and a \"sport\" body colored driver's side mirror that was adjustable from the interior. The well-hyped Vega stole sales from the Nova this year, but the compact soon would enjoy a resurgence of popularity that would last deep into the 1970s. A mid-year production change was the front door hinges spot welded to the A-pillar and the door shell, a design shared with the Vega and later implemented by GM's subsequent light-duty trucks and vans which later was used with the S10, Astro van, and full-size trucks commencing with the GMT400 a decade later. The 250 cu in (4.1 L) six-cylinder engine was now the standard Nova engine with the demise of the 153 cu in (2.51 L) four-cylinder and 230 cu in (3.8 L) six-cylinder engines. The 307 cu in (5.03 L) and 350 cu in (5.7 L) V8s were carried over from 1970 and all engines featured lowered compression ratios to enable the use of unleaded gasoline as a result of a GM corporate mandate that took effect with the 1971 model year. After 1971, other GM divisions began rebadging the Nova as their new entry-level vehicle, such as the Pontiac Ventura II (once a trim option for full-size Pontiacs to 1970), Oldsmobile Omega and the Buick Apollo. This was considered to build brand loyalty with respective GM divisions although the company later fused their badge engineering with platform sharing to cut expenditures. The initials of the four model names spelled out the acronym NOVA (Nova, Omega, Ventura, Apollo). The 1973 introduction of the Omega and Apollo coincided with the subsequent oil crisis where sales of the X and H platform increased. The 1972 Nova received only minor trim changes. The Rally package option with heavy duty suspension returned and was a rather popular choice, with 33,319 sold. SuperSport equipment went on 12,309 coupes. Nova production moved to Norwood, Ohio, where it would be assembled alongside the Camaro. At mid-year, a sunroof option called the Sky Roof became available on two-door models. Also, the optional Strato bucket seats available on coupes switched from the previous low-back design with adjustable headrests to the high back units with built-in headrests introduced the previous year on Camaros and Vegas. Despite the lack of change, Nova had its best sales season in years, with the production of the 1972 models reaching 349,733. Of these, 139,769 had the six-cylinder engine. Mid 1971 saw the introduction of the Rally Nova[22](RPO-YF1) available with the Nova coupe (X27) model only. The Rally Nova option was basically an appearance option but did include heavy duty front and rear suspension (RPO-F40) which could be mono-leaf or multi-leaf in the rear depending on optional equipment ordered. The Rally Nova was a would-be Muscle Car. It had the look of a Muscle Car with stripes, black grill, left hand sport mirror and 14X6\" 6-slot Rally wheels but it could not be ordered with the SS-only 200 hp (150 kW) Turbo-Fire 350 V8 4V (RPO-L48) engine. This was done for two reasons, people who wanted the Muscle Car look could have it without paying for the more expensive Super Sport option. The other reason was because of insurance surcharges that applied to owners of \"real\" Muscle Cars. The SS was more expensive to insure because of the 200 hp Turbo-Fire 350 V8 engine. [23][24][25] The Rally Nova option included black or white tapered stripe decals that ran the length of the car with \"Rally Nova\" wording in the stripes toward the rear of both quarter panels. A stripe decal between the taillights on the back, a \"Rally Nova\" decal on the driver's side of the hood, 6-slot 14x6\" Rally wheels with special center caps, driver's side body colored remote adjustable Sport mirror, black painted grill with bright upper and lower horizontal bars, black accent headlight bezels, bright roof drip moldings and color-keyed floor carpeting.[26] Optional interior trim levels Custom (RPO-ZJ1) and Special (RPO-ZJ3) could be ordered. Custom Exterior (RPO-ZJ2/YF8) and Exterior Decor Package (RPO-ZJ5) were not available when the Rally Nova was ordered. This means bright rear panel trim plate, bright side window moldings, sill moldings, fender moldings, side molding and accent striping would have never come on a Rally Nova. The only allowed exterior options were a vinyl roof (RPO-C08), engine badges for 307 or 350 engines and for mid 1972 the newly available Sky Roof.[25][27] Retired race car driver and muscle car specialist Don Yenko of Yenko Chevrolet in Canonsburg, Pennsylvania, refitted a series of third-generation Novas, as well as Chevelles and Camaros for optimum performance to compete with the frontrunning Ford Mustangs, Plymouth Barracudas and Dodge Challengers. The specially redesigned Nova (sometimes known as the \"Yenko Supernova\") had a stronger body frame and suspension system to house the powerful and heavy 427cid (7.0 L) V8 engine that powered the Yenko Super Cars. Only 37 were known to be produced with an original selling price of $4,000.00. Today, only seven units are registered and known to exist. In 1970, emissions standards and fuel economy were taking a toll on muscle cars. To counter this, Yenko requested a high-output Chevy 350cid V8 in his special line of Novas, the same engine that the new Z-28 Camaro and LT1 Corvette shared. Additionally, the new \"Yenko Deuce\", as it was known, had extensive suspension, transmission, and rear axle upgrades along with some very lively stripes, badges, and interior decals. The 1973 model year introduced a hatchback body style based on the 2-door coupe. The front and rear of the Nova were restyled, following a government mandate for vehicles to be fitted with front bumpers capable of withstanding 5 mph (8 km/h) impacts and rear bumpers capable of absorbing 2.5 mph (4 km/h) impacts. To go along with the bigger bumpers, stylists gave the Nova a new grille with a loosely patterned crosshatch insert and parking lights located inboard of the headlights. In 1974, the rear bumper could absorb 5 mph impacts. Fuel tank capacity increased to 21 gallons, which required a redesigned trunk pan where a circular section was stamped to house the space-saver spare tire used on hatchback models. 1973 Chevrolet Nova SS An SS option remained available, but it was merely a $123 dress-up package that included a blackout grille and Rally wheels. It could be ordered with any of the Nova engines. 35,542 SS packages were installed, making 1973 the best-selling year for the option. A modified rear side window shape was also introduced, eliminating the vent windows on both two- and four-door models. A revised rear suspension was adapted from the second generation Camaro with multi-leaf springs replacing the mono-leaf springs used on Novas since the original 1962 model. By this time, six-cylinder and V8 engines were de rigueur for American compact cars, with the 307 cu in (5.03 L) and 350 cu in (5.7 L) V8s becoming fairly common. The 1973 Nova with a six-cylinder engine or 307 cu. in. (5.0 L) V8 were among the last Chevrolets to be offered with the two-speed Powerglide automatic transmission, which was in its final year. A dressy Custom series (which became a mid-level trim package in 1975) joined the Nova line and a Custom hatchback listed for $2,701 with a six-cylinder engine. That was $173 more than the six-cylinder base-model two-door hatchback. Air conditioning added $381. Every 1973 Chevrolet Nova got side guard door beams and additional sound insulation, as well as flow-through ventilation systems. A sunroof could be installed, and fold-down rear seats were available. For 1974, the Chevrolet Nova got a centered bow-tie grille emblem, as well as modified bumpers that added two inches to the length and helped cushion minor impacts. The Powerglide was replaced by a lightweight version of the three-speed Turbo-Hydramatic 350 ( THM 250 ) already offered with the 350 cu in (5.7 L) V8, which was the only V8 offered for 1974. Nova sales continued the surge they had enjoyed since 1972 and approached 400,000 cars for 1974. Six-cylinder Novas were the fastest gainers, as sales of V-8 Novas declined. These were the years of the first energy crisis as Middle Eastern countries cut back on oil exports. After waiting for hours in gas lines and fretting about the prospect of fuel rationing, thrifty compacts looked pretty good to plenty of Americans and it fit the bill. The 'Spirit of America' Nova was introduced in 1974. In anticipation of the US bicentennial in 1976, the limited edition Nova Coupes were painted white and featured blue and red accent stripes as well as red and blue interior carpets and fabrics. Oldsmobile and Buick entered the compact car market; both the Apollo and Omega debuted, using the same body styles from the Nova lineup. Additional options were included on these Nova-like models, such as lighting under the dashboard and in the glove compartment. Pontiac's final GTO of this era was based on a facelifted 1974 Ventura coupe, itself based on the Nova, but fitted with a shaker hood scoop from the Trans Am. Novas and all 1974 cars were fitted with a weight-sensitive relay within the front seat that prevented the vehicle from being started until the driver's seatbelt had been fastened, following a safety mandate from the NHTSA. Later, Congress repealed the mandate requiring this type of device, declaring that it infringed on a driver's freedom of choice, and allowed owners of 1974-model cars to have the seat belt interlock bypassed.[29] The devices were not included in future Nova models. Along with this controversial seat belt interlock, a new, more convenient \"inertial reel\" one-piece lap/shoulder safety belt assembly was standard for both front outboard passengers, along with a plastic clip attached to the headrest to guide the belt across the wearer's shoulder. The 1975 Chevrolet Nova was the most-changed Chevy car for that model year. \"Now it's beautiful,\" said the brochure of Nova's all-new sheet metal, \"refined along the lines of elegant European sedans.\" Chevrolet wisely maintained a visual kinship with the 1968–1974 design, and also retained Nova's efficiently sized 111-inch wheelbase. Front tread grew by an inch and a half, and the front stabilizer bar had a larger diameter. Novas now had standard front disc brakes and steel-belted radial tires. The front suspension and subframe assembly was similar to the one used in the second generation GM F-body cars (the Camaro and Pontiac Firebird), whereas the rear axle and suspension were carried over from the previous generation. Coupes, including the hatchback, had fixed side windows (or optional flip-out windows) - the first for a GM vehicle later optioned throughout the 1980s with its light duty trucks (S10, Astro/Safari, and GMT400 trucks to the K2XX series) and vertical vents on the B-pillar. All Novas now had cut-pile carpeting, formerly installed only in the Custom series. Speedometers had larger, easier-to-read graphics. Windshields offered greater glass area. Front-door armrests were redesigned with integral pull bars. The base model carried the inline six-cylinder 250 cu in (4.1 L), 105 hp (78 kW), three V8 engines (262 cu in (4.29 L), a 1975-only option, a 305 cu in (5.00 L) and a 350 cu in (5.7 L)) for 1976 only, were offered. Mated to a three-speed automatic, 3-speed manual or 4-speed – V8s only – Which remained the norm through the end of the decade (and the end of the rear-wheel drive X platform). By then, Cadillac had developed its own version of the X-body, called the K-body which was named the Seville, whose styling was distinct from those of its corporate cousins, and Buick replaced the Apollo with the Skylark name that had been inactive since the previous incarnation ended production at the end of the 1972 model year. 1975 Chevrolet Nova LN The LN (Luxury Nova) package (which was the top luxury trim similar to the Caprice and Malibu Classic) sent Nova into the luxury portion of the compact market; some actually thought of it as competing against a few high-end European imports. The Nova LN was called \"the most luxurious compact in Chevrolet's history,\" with wide-back reclining front seats that \"look and feel like big, soft lounge chairs.\" LN equipment included ad­ditional sound insulation, map pockets, an electric clock, a smoked instrument lens, floor shifter and center console, and a day/night mirror. Taillight lenses have additional white accents unavailable with the base model and a chrome plated grille. Above the front marker lenses, the LN had 4.3 LITER (or 5.7 LITER) decals - making it the first Chevrolet product with metric displacement badges sold in the Americas. Swing-out quarter windows could be ordered for the coupe. \"Thanks to LN,\" the sales brochure announced, \"Nova's image will never be the same again.\" The LN was more Eurocentric as opposed to the Custom which became the mid-level trim option. For 1976 the Nova LN was rebranded Concours to rival the Ford Granada and the Mercury Monarch, as well as upscale versions of the Dodge Dart and Plymouth Valiant. Like regular versions of the 1976 Nova, the Concours came in three body styles: coupe, hatchback coupe, and four-door sedan. Concours was the most luxurious Chevrolet compact to date. Rosewood vinyl decorated the upper door panels, instrument panel, and steering wheel. Concours models had an upright hood ornament, bumper guards, bright trim moldings, black bumper impact strips, and full wheel covers; more-basic Novas came with hubcaps. The Concours coupe also was the first Chevrolet coupe with a fold-down front center armrest. A V8 Concours coupe sold for $547 more than the comparable base Nova. Engines for the 1976 Chevrolet Nova were a 105-horsepower inline-six, a 165-horsepower 350-cubic-inch V-8, or a 140-horse 305-cubic-inch V-8. 1976 GM vehicles first saw use of the THM200 — from the GM T platform to GM X-Bodies (Chevrolet Nova et al.). A lighter duty, 10-bolt rear differential with a 7.5\" ring gear (also used with the Vega/Monza and produced until 2005) was phased into production - being standard equipment with the base inline-six. A \"Cabriolet\" padded vinyl top was available for Nova coupes. Modest revisions were made to the brakes, and also to fuel and exhaust system mountings. Dashboards contained new knobs. After testing the 1976 Chevrolet Nova, the Los Angeles Sheriff's Department placed the largest order for compact police cars ever seen in the U.S. Minor changes for the 1977 model year included a more modern round gauge cluster to replace the long sweeping speedometer, and a revised dash panel which changed to a flatter design. Some new colors were offered (as with the rest of the divisions) and some small trim added. A separate brochure was printed for the Concours while the \"1977 Nova\" brochure detailed only base and Custom versions. The Nova SS previously offered for 1975 and 1976 was discontinued, the option code for the SS — RPO Z26 — continued as the Nova Rally from 1977 through 1979. A badged-engineered Nova Malibu Rallye (1977 and 1978 model years – not related to the USA market Chevelle-based model and based on the Nova hatchback coupe) was sold in Mexico using the RPO Z26 package but fitted with 'Malibu Rallye' graphics and a front grille emblem. Three engines and four transmissions were available for every 1977 Chevrolet Nova, including Concours. Buyers could choose from a 110-horsepower 250-cubic-inch inline six, a 145-horsepower 305 cubic-inch two-barrel V-8, or 170-horsepower 350 cubic-inch four-barrel V-8. Shifting was accomplished by three-speed (column or floor shift) and four-speed manuals or Turbo Hydra-Matic. Novas might also be equipped with a heavy-duty suspension or the F41 sport suspension. A surprising number of police departments ordered Novas with either a 305- or 350-cubic-inch V-8 engine, following the lead of the Los Angeles Sheriff's Department, which had given the compacts an exhaustive evaluation. 1976 Chevrolet Nova 2-door coupeRear view of a 1976 Nova sedan Promoted as \"Concours by Chevrolet\", the 1977 Concours featured a new vertical bar grille and a revised stand-up hood ornament. The rear of the Concours also got new triple unit taillamps reminiscent of the Caprice. It also boasted newly designed wheel covers and wider bright wheel-opening moldings. \"International in style, it is American in function,\" the sales brochure insisted of the Concours. The brochure went on to note that Concours offered a \"very special blending of classic style and good sense.\" That last comment referenced Nova's sensible size. Novas themselves, the marketing materials said, were \"not too small, not too big, not too expensive.\" For 1978 the Concours was discontinued to clear the way for the newly downsized Malibu, and the Nova Custom inherited much of the Concours' exterior finery but lacked the stand-up hood ornament displayed by the Concours. Upholstery choices included all-vinyl or Edinburgh woven sport cloth/vinyl. More basic versions of the 1978 Chevrolet Nova had the same grille as used in 1976-1977 and added a gold-tinted Chevy bowtie emblem at the leading edge of the hood. For '78 Nova was also available with Rally equipment, which included yet another front-end layout: a diamond-pattern grille with horizontal parking lights and black headlight bezels (basically the 1976-1977 SS grille), plus triple band striping and color-keyed Rally wheels. All Nova drivers faced a new dual-spoke, soft vinyl-covered steering wheel; the same one found in the Caprice and Malibu. Any 1978 Chevrolet Nova could be ordered with a 250-cubic-inch six-cylinder engine, a 145-horsepower 305-cubic-inch V-8, or a 170-horsepower 350-cubic-inch V-8. Law enforcement agencies in 48 states were driving Novas by now, as the sales brochure boasted. Production dropped almost 100,000 for the model, to 288,000, making Nova the only Chevrolet series to show a sales decline for 1978. Sales of the Nova hatchback body style lagged well behind regular coupes and sedans, and base models handily outsold Customs. Upon introduction of the downsized GM A-body (later G-body) mid-size cars in 1978, the X-body and downsized A-platform had similar exterior dimensions. The roomier and more modern downsized A-bodies outsold their X-body counterparts. The 1979 Chevrolet Nova marked the end of the line for the rear-wheel-drive Nova. The front end was revised with rectangular headlights and a new grille for the short run (matching that of its Pontiac Phoenix cousin, which replaced the Ventura for 1977); a modified horizontal-bar grille contained vertical parking lights. New chromed hood and fender moldings were installed, and new front-bumper filler panels gave the front end a more finished look. The Custom went back to the base dual section taillights since the triple section taillights were discontinued. The lineup was the same as in 1978; the base-level hatchback, coupe, and sedan, plus the Custom coupe and sedan. As usual, base coupe and sedan proved to be the best sellers. Nova Customs had a special acoustical package including improved headlining and full hood insulation, along with other luxury extras, while the Rally Package returned, this time using the same grille as other 1979 Novas. These final Novas were promoted for their \"solid value\" and \"reputation for dependability,\" capitalizing upon a 17-year heritage that had begun with the Chevy II. Fewer than 98,000 examples were produced. Regular production ended on December 22, 1978, but some cars badged \"Nova Custom\" were built on special order with luxury amenities in early 1979. The final Chevrolet Nova (Custom) built on special order would roll off the line on March 15, 1979, and this would be the end of the rear-drive Nova for good. Chevrolet's compact models were headed into the front-wheel-drive age and for 1980, Nova's place in the lineup would be taken over by the new and very different Chevrolet Citation (the Phoenix, Omega and Skylark carried over to this platform as well, and the Seville was reassigned to another front-drive platform). The Chevrolet Nova nameplate returned in spring 1984 as a front-wheel drive subcompact vehicle for the 1985 to 1988 model years. It was assembled in Fremont, California, by NUMMI, a joint venture between General Motors and Toyota of Japan, resulting in various Corolla-based cars sold under General Motors brands, also referred to as the S-car within GM. It resurrected a name last used on the compact-sized rear-wheel drive 1979 Chevrolet Nova. The new Nova was a rebadged and mildly restyled Japanese market Toyota Sprinter, a model sold in Japan as a badge engineered version of the Toyota Corolla. Nova shared the Corolla's AE82 platform, 1.6 L (98 cu in) 4-cylinder engines and was available with 5-speed manual, 3-speed or 4-speed automatic transmissions. For the first time ever, quad headlights were used on the Nova (mimicking most other models at the time, such as the slightly-larger Chevrolet Cavalier). It was designed for manufacturability and reached an unusually high level of quality and production speed at NUMMI, compared to other US factories.[32] 1985 model The 1985 Chevrolet Nova was initially offered only in a four-door three-box, notchbacksedanbody style and in the Midwestern states. A five-door hatchback was added shortly after its introduction, and the line was distributed throughout the US and Canada beginning around traditional new-model introduction time in the fall (as were the other Chevy imports, the Suzuki-based Sprint which had been first launched on the West Coast and the Isuzu-based Spectrum which had initially been available on the Eastern Seaboard and throughout New England and New York State). The only engine was a carbureted 1.6-liter four-cylinder with 74 horsepower (55 kW). It teamed with either a five-speed manual or three-speed automatic transmission. This was the same powertrain as offered in the Corolla. The four-door sedan listed for $7,435, a rather stiff tariff by Chevrolet standards. The five-door, which added a split-folding rear seat, started at $7,669. All Nova options were grouped into seven packages, which did away with the long list of optional equipment that accompanied such cars as the Chevrolet Chevette. (Simple though it was, the subcompact Chevette offered nearly 30 options). However, adding one of the costlier packages could easily push the Nova's sticker to over $10,000. 1986 Chevrolet Nova CL sedan 1987 model The 1987 Chevrolet Nova saw only minor changes after its introduction two years earlier as a near-twin to the front-wheel-drive Toyota Corolla. A rear-window defogger was added to the list of standard equipment, while visual changes were limited to lighter silver highlights on the vertical grille bars and a change of turn signal lens colors from amber to clear/white front and red rear. CL models also got red reflective panels carrying the taillights onto the trunk/hatch, body-colored bumpers, and new aluminum wheels. The 1987 Chevrolet Nova continued in two body styles, a four-door sedan and five-door hatchback. The four-door proved by far the more popular – by about three to one. Nova's only engine was again a 74-horsepower 1.6-liter four designed by Toyota, mated to either a five-speed manual transmission or four-speed automatic. Though Corollas were priced slightly below competing Novas, Chevy's version of the car could often be bought for less because slow sales encouraged dealers to discount prices. \"Slow sales,\" however, meant slow by Chevy standards, for the Nova sold about as well as the Corolla, and buyers would find that their discounted Nova in turn had a lower resale value than the equivalent Toyota, a pattern that would persist for GM-branded NUMMI cars. Aside from some minor interior and exterior trim differences, the cars were much the same, though Novas had a slightly softer suspension that favored ride over handling. 1988 Chevrolet Nova Hatchback 1988 model The 1988 Chevrolet Nova added a sporty model to its lineup of subcompact front-wheel-drive cars. This new 1988 Chevrolet Nova Twin-Cam got its name from a double-overhead-cam version of the Toyota-built 1.6-liter four-cylinder. Novas continued to share their basic design with the Corolla, and this engine had previously been used in the Toyota FX-16, a performance version of the Corolla. The twin-cam produced 110 hp (82 kW), 36 more than its single-cam sibling. A five-speed manual transmission was standard, as in the regular Novas, but the Twin-Cam offered a four-speed automatic as an option versus the three-speed offered on other models. The more potent engine elevated the 1988 Chevrolet Nova Twin-Cam into junior sport-sedan terri­tory, but the advancement didn't come cheaply. The base Nova listed at about $8,800, the Twin-Cam went for $11,395. That price included fuel injection, sport suspension, power steering, leather-covered steering wheel, tachometer, four-wheel disc brakes, and wider tires on aluminum wheels, but it was a stiff tariff, and few were ordered (approximately 3,300 Twin-Cam models were built). There were no color choices; all 1988 Chevrolet Nova Twin-Cams wore black metallic paint with a grey interior; and there was no hatchback version offered. Every 1988 Chevrolet Nova got rear shoulder belts, rear window defogger, and AM/FM stereo radio as standard equipment. This was the last model year for the Nova name at Chevrolet. Starting with 1989, Chevrolet pushed this car into its new Geo division and renamed it the Prizm. Geo was Chevy's effort to come up with an import-sounding label to attract buyers who were not inclined to shop American. The reaction to the 1962 Chevrolet Chevy II was mainly positive. Veteran Mechanics Illustrated tester Tom McCahill was favorably impressed with a Chevy II 400 Series Nova convertible he drove at a press preview for Chevy's 1962 models, held at GM's Milford, Michigan, test track. \"Flat out, which with Powerglide was 91 mph, this little car never wavered and even over some rough strips it was one of the safest feeling 91's I have ever driven.\" The styling reminded \"Uncle\" Tom of a \"small Mercedes-Benz\", and he concluded that \"with a little hopping up, a stick shift and its low price, it should sell like cold beer on a hot Fourth of July.\" Car Life was even more enthusiastic, honoring the Chevy II with its \"Award for Engineering Excellence\". \"We think the Chevy II, in either 4- or 6-cylinder form, represents an important development in the American automotive field,\" reported the magazine. \"We think it represents a return to sensibility in terms of basic transportation; it is a car of reasonable size, adequate performance and simple elegance.\" The award was mentioned in a 1962 Chevrolet Nova advertisement. (see right) Consumer Reports described the six-cylinder Chevy II as an \"ultra-sensible, conventional car with outstanding interior space,\" but also reported \"higher than average\" interior noise levels. There were also complaints about the four-cylinder version's lack of refinement. \"CR hesitates to recommend the Four for normal use. The Four is an excellent hackabout for specialized local use – if you can stand the vibration.\" McCahill put it this w­ay: \"The four wasn't the smoothest four I have ever driven, but it had nice response and will probably still be running long after Castro shaves his beard off.\" Consumer Reports in 1963: \"New last year, the Chevy II has not yet developed into a smooth-riding, quiet, or in any sense luxurious car. It is an easy driving, agile one. By far its most important asset is a body with substantially the room of intermediate cars, but with a very compact silhouette and especially good entrance height.\" Motor Trend called the new Chevy II \"a most straightforward car – simple, honest and conventional.\" Editor Jerry Titus was fascinated with the new rear single-leaf suspension: \"How it actually works seems almost contradictory. There is a great deal of body roll, but the car does not feel unstable. The ride is soft and pleasing, but not seasick-soft with the constant pitching and rolling that some cars have.\" Interior room, steering, and brakes were commended. Performance was rated as \"moderate\" for a six-cylinder Nova convertible with Powerglide: 0-60 came up \"a shade under 16 seconds,\" and the top speed was reported to be 98 mph, but Titus felt that \"the car seems at its best below 75, where it did not feel as though it was working hard.\" The four, meanwhile, took 20 seconds to make it from 0 to 60 mph. In comparison, a 1960 90 bhp Falcon with stick shift took 21 seconds 0 to 60, also according to Motor Trend, while the 101 bhp six introduced for 1961 required 14.3 seconds with stick and 15.2 with the two-speed Fordomatic.[34] Motor Trend tested a 1964 195-bhp, two-barrel SS with Powerglide, recording 0 to 60 in 11.3 seconds, 18.0 seconds and 75 mph in the quarter-mile, and 100 mph all out. Fuel economy ranged from 12.3 mpg in heavy traffic to 19.6 on the highway. Motor Trend concluded that \"By adding a V-8 and bigger brakes, plus detail changes, Chevrolet has made a nice compact even more desirable and a much better performer.\" The mid-1980s Nova made no attempt to recapture the former \"Muscle\" glory that it once had, with the Twin Cam performance variant appearing only in the final year of the nameplate after Toyota had already moved on to the next generation of the platform. While the Chevy II and Nova were also sold in Canada, from the beginning a mildly re-trimmed version was also sold by Pontiac-Buick dealers as the Acadian. The Acadian was produced between the years 1962 and 1971. It was a stand-alone make based upon the Chevy II, which was produced in both the U.S. and Canada[35] and sold exclusively through Canadian Pontiac – Buick – GMC dealerships. Due to the Canadian tariffs on imports put into place many years before, there was no compact car available to the Canadian Pontiac dealer. The U.S. built Pontiac Tempest, which started production in 1961 was not available initially to the Canadian buyer – import duties would have made it too expensive to compete in the thrifty Canadian compact market. The Acadian was introduced to give the unhappy Canadian Pontiac – Buick dealer a car he could sell in the growing compact market. During its entire run, the Acadian offered the same body styles as were offered in the Chevy II/Nova, and the cars were virtually the same, save minor trim and badging details. Originally offered in top-line Beaumont and base Invader trim, the top trim line was renamed Canso in anticipation of the Chevelle-based Acadian Beaumont which would arrive for 1964. A sporty model, the Sport Deluxe (or \"SD\"), was equivalent to the U.S.-market Nova SS, and it also featured bucket seats, deluxe exterior trim, and special badging. Base price for the 1966 Acadian was $2,507. The 327-350 hp (L79) was available; 85 were produced. The Acadian line was now down to six models; 7,366 Acadians were sold in 1966.[36] It survived until mid-1971, after which it was replaced by the Pontiac Ventura II. In 1962 Argentina offered the 1962-64-style Chevy II as the Chevrolet 400 through 1974, and the 1968–72 Nova as the Chevrolet Chevy from late 1969 through 1978, both models overlapping for several years. An upscale model (Chevy Super) was produced from about 1973 with different trim, front turn indicators and taillights, a much better appointed interior with plastic \"wood\" trim, named Malibu with no relation to the American Chevelle. All engines were inline-sixes. The first and second generations were available, depending on year and model, with the 194 cu in (3.18 L), 230 cu in (3.8 L) and 250 cu in (4.1 L) engines. The third generation (\"Chevys\") were produced with the 230 cu in (3.8 L) and 250 cu in (4.1 L) engines with specially tuned carburetors for sporting models. The \"Chevy\" metal emblem for the third generation had the same font as the \"Nova\" emblem of 1968–1974 American Novas, and was, for the first few years, in the rearmost section of both rear fenders. Later, it was moved to the rearmost section of both front fenders, as it was in the American cars from 1969. Sidemarker lights were not mandatory and changed much during the production run, from being deleted, to leaving a small chrome plate, to the same light as in the American cars. Rear deck emblems just said \"CHEVROLET\" in chrome letters, obviating the typical \"Model by Chevrolet\" used in the American cars at the time. The hood emblem was similar to the 1969 American Novas: the bow tie, either in blue or just chrome. Initially, the Argentinian Chevy used very similar trim to the American counterpart, while more luxurious – a \"big\" car by local standards. They there standard models without accessories and were often used for taxi service. The interior layout remained the American 1968 version for the entire run. The ignition switch remained dash mounted as the U.S.-mandated steering lock was not required in Argentina. Power steering became available at the end of the production run. V8s versions weren't produced: Power windows were not available, tinted windows were darker than American versions, and the darker band on the upper edge of the windshield was not present. Very popular accessories were vinyl roofs, rally wheels, sport steering wheels, bucket seats with high backs, and tufted leatherette upholstery (many sedans were produced this way). Interiors were usually black. Steering wheels and instrument panels were only black for many years, as were seatbelts. American style interior color coordination was absent. The last year of the Nova in Argentina is called locally \"Opus 78\" (because the slogan of the publicity) and it was the most equipped, adding simil-leather bucket seats, air-conditioning, power steering, electric antenna, and a new dashboard with integrated central console. During its run on certain models, the \"Chevy\" was also available with the 3-speed Turbo Hydra-Matic automatic transmission as an option. It was marketed with the name \"Chevromatic.\" Their Super Sports, \"SS\" counterparts were both coupés and 4-door sedans, the latter of which was unheard of in the US prior to the introduction of the 1994 Impala SS. In fact, a majority were fitted with inline-sixes coupled to a ZF manual transmission with floor lever 4 speeds, a single two-barrel Holey 2300 RX 7214-A carburetor giving out 168 hp (125 kW) and a sporting exhaust note. Corsa, a local auto publication magazine tested a Chevy Coupé SS Serie 2 and obtained a 0–100 km/h (0–62 mph) time of 11.1 seconds. A popular but false urban legend claims that the vehicle sold poorly in Spanish-speaking countries because no va translates to \"doesn't go\". However, in Spanish 'nova' is a distinct word primarily used to refer to the astronomical event, and doesn't have the same meaning as 'no va'. In fact, the car actually sold quite well in Mexico, as well as many Central and South American countries. Nova was also the name of a successful brand of gasoline sold in Mexico at the time, further proving that the name confusion was not a problem.[37] A similar story has been told of the BritishVauxhall Nova (a small car that was completely unrelated to the Chevrolet Nova aside from both being built by GM). According to the story, it had to be sold as an Opel Corsa in Spain due to the same alleged language confusion. This version of the story is also a myth, as the Spanish-market version of the car was known as a Corsa from the outset. In fact, the car was called the Corsa in all markets except the United Kingdom. There was also a Nova kit car designed and built by A.D.D. from 1971. It lost a court case with GM Vauxhall over the use of the name, after it was shown that GM's Chevrolet had a prior claim."}
{"url": "https://en.m.wikipedia.org/wiki/Hallucinogen", "text": "The word hallucinogen is derived from the word hallucination.[3] The term hallucinate dates back to around 1595–1605, and is derived from the Latinhallūcinātus, the past participle of (h)allūcināri, meaning \"to wander in the mind.\"[4] Leo Hollister gave five criteria for classifying a drug as hallucinogenic.[5][6] This definition is broad enough to include a wide range of drugs and has since been shown to encompass a number of categories of drugs with different pharmacological mechanisms and behavioral effects.[6] Richard Glennon has thus given an additional two criteria that narrow the category down to classical hallucinogens.[6] Hollister's criteria for hallucinogens were as follows:[5][6] in proportion to other effects, changes in thought, perception, and mood should predominate; intellectual or memory impairment should be minimal; stupor, narcosis, or excessive stimulation should not be an integral effect; autonomic nervous system side effects should be minimal; and addictive craving should be absent. Glennon's additional criteria for classical hallucinogens are that the drugs in question must also:[6] bind at 5-HT2 serotonin receptors; and be recognized by animals trained to discriminate the drug DOM from vehicle. Most hallucinogens can be categorized based on their pharmacological mechanisms as psychedelics (which are serotonergic), dissociatives (which are generally antiglutamatergic), or deliriants (which are generally anticholinergic).[2] However, the pharmacological mechanisms of some hallucinogens, such as salvinorin A and ibogaine, do not fit into any of those categories.[2]Entactogens and cannabinoids are also sometimes considered hallucinogens.[7] Nonetheless, while the term hallucinogen is often used to refer to the broad class of drugs covered in this article, sometimes it is used to mean only classical hallucinogens (that is, psychedelics).[8] Because of this, it is important to consult the definition given in a particular source.[8] Because of the multi-faceted phenomenology brought on by hallucinogens, efforts to create standardized terminology for classifying them based on their subjective effects have not succeeded to date.[9] Classical hallucinogens or psychedelics have been described by many names. David E. Nichols wrote in 2004:[8] Many different names have been proposed over the years for this drug class. The famous German toxicologist Louis Lewin used the name phantastica earlier in this century, and as we shall see later, such a descriptor is not so farfetched. The most popular names—hallucinogen, psychotomimetic, and psychedelic (\"mind manifesting\")—have often been used interchangeably. Hallucinogen is now, however, the most common designation in the scientific literature, although it is an inaccurate descriptor of the actual effects of these drugs. In the lay press, the term psychedelic is still the most popular and has held sway for nearly four decades. Most recently, there has been a movement in nonscientific circles to recognize the ability of these substances to provoke mystical experiences and evoke feelings of spiritual significance. Thus, the term entheogen, derived from the Greek word entheos, which means \"god within\", was introduced by Ruck et al. and has seen increasing use. This term suggests that these substances reveal or allow a connection to the \"divine within\". Although it seems unlikely that this name will ever be accepted in formal scientific circles, its use has dramatically increased in the popular media and on internet sites. Indeed, in much of the counterculture that uses these substances, entheogen has replaced psychedelic as the name of choice and we may expect to see this trend continue. Robin Carhart-Harris and Guy Goodwin write that the term psychedelic is preferable to hallucinogen for describing classical psychedelics because of the term hallucinogen's \"arguably misleading emphasis on these compounds' hallucinogenic properties.\"[10] Certain hallucinogens are designer drugs, such as those in the 2C and 25-NB (NBOMe) families.[11] A designer drug is a structural or functional analog of a controlled substance (hallucinogenic or otherwise) that has been designed to mimic the pharmacological effects of the original drug while at the same time avoid being classified as illegal (by specification as a research chemical) and/or avoid detection in standard drug tests.[12] Despite several attempts that have been made, starting in the 19th and 20th centuries, to define common phenomenological structures (i.e., patterns of experience) brought on by classical psychedelics, a universally accepted taxonomy does not yet exist.[13][14] A prominent element of psychedelic experiences is visual alteration.[13] Psychedelic visual alteration often includes spontaneous formation of complex flowing geometric visual patterning in the visual field.[14] When the eyes are open, the visual alteration is overlaid onto the objects and spaces in the physical environment; when the eyes are closed the visual alteration is seen in the \"inner world\" behind the eyelids.[14] These visual effects increase in complexity with higher dosages, and also when the eyes are closed.[14] The visual alteration does not normally constitute hallucinations, because the person undergoing the experience can still distinguish between real and internally generated visual phenomena, though in some cases, true hallucinations are present.[13] More rarely, psychedelic experiences can include complex hallucinations of objects, animals, people, or even whole landscapes.[13] A number of studies by Roland R. Griffiths and other researchers have concluded that high doses of psilocybin and other classic psychedelics trigger mystical experiences in most research participants.[15][16][17][18] Mystical experiences have been measured by a number of psychometric scales, including the Hood Mysticism Scale, the Spiritual Transcendence Scale, and the Mystical Experience Questionnaire.[18] The revised version of the Mystical Experience Questionnaire, for example, asks participants about four dimensions of their experience, namely the \"mystical\" quality, positive mood such as the experience of amazement, the loss of the usual sense of time and space, and the sense that the experience cannot be adequately conveyed through words.[18] The questions on the \"mystical\" quality in turn probe multiple aspects: the sense of \"pure\" being, the sense of unity with one's surroundings, the sense that what one experienced was real, and the sense of sacredness.[18] Some researchers have questioned the interpretation of the results from these studies and whether the framework and terminology of mysticism are appropriate in a scientific context, while other researchers have responded to those criticisms and argued descriptions of mystical experiences are compatible with a scientific worldview.[19][20][21] Link R. Swanson divides overarching scientific frameworks for understanding psychedelic experiences into two waves. In the first wave, encompassing nineteenth- and twentieth-century frameworks, he includes model psychosis theory (the psychotomimetic paradigm), filtration theory, and psychoanalytic theory.[14] In the second wave of theories, encompassing twenty-first-century frameworks, Swanson includes entropic brain theory, integrated information theory, and predictive processing.[14] It is from the paradigm of filtration theory that the term psychedelic derives.[14]Aldous Huxley and Humphrey Osmond applied the pre-existing ideas of filtration theory, which held that the brain filters what enters into consciousness, to explain psychedelic experiences; Huxley believed that the brain was filtering reality itself and that psychedelics granted conscious access to \"Mind at Large\", whereas Osmond believed that the brain was filtering aspects of the mind out of consciousness.[14] Swanson writes that Osmond's view seems \"less radical, more compatible with materialist science, and less epistemically and ontologically committed\" than Huxley's.[14] Dissociatives produce analgesia, amnesia and catalepsy at anesthetic doses.[22] They also produce a sense of detachment from the surrounding environment, hence \"the state has been designated as dissociative anesthesia since the patient truly seems disassociated from his environment.\"[23] Dissociative symptoms include the disruption or compartmentalization of \"...the usually integrated functions of consciousness, memory, identity or perception.\"[24]p. 523 Dissociation of sensory input can cause derealization, the perception of the outside world as being dream-like, vague or unreal. Other dissociative experiences include depersonalization, which includes feeling dissociated from one's personality; feeling unreal; feeling able to observe one's actions but not actively take control; being unable to associate with one's self in the mirror while maintaining rational awareness that the image in the mirror is the same person.[25] In a 2004 paper, Daphne Simeon offered \"...common descriptions of depersonalisation experiences: watching oneself from a distance (similar to watching a movie); candid out-of-body experiences; a sense of just going through the motions; one part of the self acting/participating while the other part is observing;....\"[26] Some dissociatives can have CNSdepressant effects, thereby carrying similar risks as opioids, which can slow breathing or heart rate to levels resulting in death (when using very high doses). DXM in higher doses can increase heart rate and blood pressure and still depress respiration. Inversely, PCP can have more unpredictable effects and has often been classified as a stimulant and a depressant in some texts along with being as a dissociative. While many have reported that they \"feel no pain\" while under the effects of PCP, DXM and Ketamine, this does not fall under the usual classification of anesthetics in recreational doses (anesthetic doses of DXM may be dangerous). Rather, true to their name, they process pain as a kind of \"far away\" sensation; pain, although present, becomes a disembodied experience and there is much less emotion associated with it. As for probably the most common dissociative, nitrous oxide, the principal risk seems to be due to oxygen deprivation. Injury from falling is also a danger, as nitrous oxide may cause sudden loss of consciousness, an effect of oxygen deprivation. Because of the high level of physical activity and relative imperviousness to pain induced by PCP, some deaths have been reported due to the release of myoglobin from ruptured muscle cells. High amounts of myoglobin can induce renal shutdown.[31] Many users of dissociatives have been concerned about the possibility of NMDA antagonist neurotoxicity (NAN). This concern is partly due to William E. White, the author of the DXM FAQ, who claimed that dissociatives definitely cause brain damage.[32] The argument was criticized on the basis of lack of evidence[33] and White retracted his claim.[34] White's claims and the ensuing criticism surrounded original research by John Olney. In 1989, John Olney discovered that neuronal vacuolation and other cytotoxic changes (\"lesions\") occurred in brains of rats administered NMDA antagonists, including PCP and ketamine.[35] Repeated doses of NMDA antagonists led to cellular tolerance and hence continuous exposure to NMDA antagonists did not lead to cumulative neurotoxic effects. Antihistamines such as diphenhydramine, barbiturates and even diazepam have been found to prevent NAN.[36]LSD and DOB have also been found to prevent NAN.[37] Deliriants, as their name implies, induce a state of delirium in the user, characterized by extreme confusion and an inability to control one's actions. They are called deliriants because their subjective effects are similar to the experiences of people with delirious fevers. The term was introduced by David F. Duncan and Robert S. Gold to distinguish these drugs from psychedelics and dissociatives, such as LSD and ketamine respectively, due to their primary effect of causing delirium, as opposed to the more lucid states produced by the other hallucinogens.[38][page needed] Despite the fully legal status of several common deliriant plants, deliriants are largely unpopular as recreational drugs due to the severe, generally unpleasant and often dangerous nature of the hallucinogenic effects produced.[39][page needed] Shamans consume hallucinogenic substances in order to induce a trance. Once in this trance, shamans believe that they are able to communicate with the spirit world, and can see what is causing their patients' illness. The Aguaruna of Peru believe that many illnesses are caused by the darts of sorcerers. Under the influence of yaji, a hallucinogenic drink, Aguaruna shamans try to discover and remove the darts from their patients.[46] In the 1970s, Frida G. Surawicz and Richard Banta published a review of two case studies where hallucinogenic drug use appeared to play a role in \"delusions of being changed into a wolf\" (sometimes referred to as \"lycanthropy,\" or being a \"werewolf\"). They described a patient whose delusion was thought to be caused by an altered state of consciousness \"brought on by LSD and strychnine and continued casual marijuana use.\" The review was published in the Canadian Psychiatric Association Journal. While both central cases described white male patients from contemporary Appalachia, Surawicz and Banta generalized their conclusions about a link between hallucinogens and \"lycanthropy,\" based on historical accounts that reference myriad types of pharmacologically-similar drug-use alongside descriptions of \"lycanthropes.\"[47] In an 1860 book, the mycologist Mordecai Cubitt Cooke differentiated a class of drugs roughly corresponding to hallucinogens from opiates, and in 1924 the toxicologist Louis Lewin described hallucinogens in depth under the name phantastica. From the 1920s on, work in psychopharmacology and ethnobotany resulted in more detailed knowledge of various hallucinogens. In 1943, Albert Hofmann discovered the hallucinogenic properties of lysergic acid diethylamide (LSD), which raised the prospect of hallucinogens becoming more broadly available.[48] After World War II there was an explosion of interest in hallucinogenic drugs in psychiatry, owing mainly to the invention of LSD. Interest in the drugs tended to focus on either the potential for psychotherapeutic applications of the drugs (see psychedelic psychotherapy), or on the use of hallucinogens to produce a \"controlled psychosis\", in order to understand psychotic disorders such as schizophrenia. By 1951, more than 100 articles on LSD had appeared in medical journals, and by 1961, the number had increased to more than 1000 articles.[49] At the beginning of the 1950s, the existence of hallucinogenic drugs was virtually unknown to the general public in the West. However this soon changed as several influential figures were introduced to the hallucinogenic experience. Aldous Huxley's 1953 essay The Doors of Perception, describing his experiences with mescaline, and R. Gordon Wasson's 1957 Life magazine article (\"Seeking the Magic Mushroom\") brought the topic into the public limelight. In the early 1960s, counterculture icons such as Jerry Garcia, Timothy Leary, Allen Ginsberg and Ken Kesey advocated the drugs for their psychedelic effects, and a large subculture of psychedelic drug users was spawned. Psychedelic drugs played a major role in catalyzing the major social changes initiated in the 1960s.[50][51] As a result of the growing popularity of LSD and disdain for the hippies with whom it was heavily associated, LSD was banned in the United States in 1967.[52] This greatly reduced the clinical research about LSD, although limited experiments continued to take place, such as those conducted by Reese Jones in San Francisco.[53] As early as the 1960s, research into the medicinal properties of LSD was being conducted. \"Savage et al. (1962) provided the earliest report of efficacy for a hallucinogen in OCD, where after two doses of LSD, a patient who suffered from depression and violent obsessive sexual thoughts experienced dramatic and permanent improvement (Nichols 2004: 164).\"[8] Starting in the mid-20th century, psychedelic drugs have received extensive attention in the Western world. They have been and are being explored as potential therapeutic agents in treating alcoholism,[54] and other forms of drug addiction.[55][56][57] In the United States, classical hallucinogens (psychedelics) are in the most strictly prohibited class of drugs, known as Schedule 1 drugs.[8] This classification was created for drugs that meet the three following characteristics: 1) they have no currently accepted medical use, 2) there is a lack of safety for their use under medical supervision, and 3) they have a high potential for abuse.[8] However, pharmacologist David E. Nichols argues that hallucinogens were placed in this class for political rather than scientific reasons.[8] In 2006, Albert Hofmann, the chemist who discovered LSD, said he believed LSD could be valuable when used in a medical rather than recreational context, and said it should be regulated in the same way as morphine rather than more strictly.[58] No clear connection has been made between psychedelic drugs and organic brain damage. However, hallucinogen persisting perception disorder (HPPD) is a diagnosed condition wherein certain visual effects of drugs persist for a long time, sometimes permanently,[61] although the underlying cause and pathology remains unclear.[62] A large epidemiological study in the U.S. found that other than personality disorders and other substance use disorders, lifetime hallucinogen use was not associated with other mental disorders, and that risk of developing a hallucinogen use disorder was very low.[63] A 2019 systematic review and meta-analysis by Murrie et al. found that the transition rate from a diagnosis of hallucinogen-induced psychosis to that of schizophrenia was 26% (CI 14%-43%), which was lower than cannabis-induced psychosis (34%) but higher than amphetamine (22%), opioid (12%), alcohol (10%) and sedative (9%) induced psychoses. Transition rates were not affected by sex, country of the study, hospital or community location, urban or rural setting, diagnostic methods, or duration of follow-up. In comparison, the transition rate for brief, atypical and not otherwise specified psychosis was found to be 36%.[64] LSD, mescaline, psilocybin, and PCP are drugs that cause hallucinations, which can alter a person's perception of reality. LSD, mescaline, and psilocybin cause their effects by initially disrupting the interaction of nerve cells and the neurotransmitter serotonin.[67] It is distributed throughout the brain and spinal cord, where the serotonin system is involved with controlling of the behavioral, perceptual, and regulatory systems. This also includes mood, hunger, body temperature, sexual behavior, muscle control, and sensory perception. Certain hallucinogens, such as PCP, act through a glutamate receptor in the brain which is important for perception of pain, responses to the environment, and learning and memory. Thus far, there have been no properly controlled research studies on the specific effects of these drugs on the human brain, but smaller studies have shown some of the documented effects associated with the use of hallucinogens.[67] While early researchers believed certain hallucinogens mimicked the effects of schizophrenia, it has since been discovered that some hallucinogens resemble endogenous psychoses better than others. PCP and ketamine are known to better resemble endogenous psychoses because they reproduce both positive and negative symptoms of psychoses, while psilocybin and related hallucinogens typically produce effects resembling only the positive symptoms of schizophrenia.[68] While the serotonergic psychedelics (LSD, psilocybin, mescaline, etc.) do produce subjective effects distinct from NMDA antagonist dissociatives (PCP, ketamine, dextrorphan), there is obvious overlap in the mental processes that these drugs affect and research has discovered that there is overlap in the mechanisms by which both types of psychedelics mimic psychotic symptoms.[69][70][71] One double-blind study examining the differences between DMT and ketamine hypothesized that classically psychedelic drugs most resemble paranoid schizophrenia while dissociative drugs best mimicked catatonic subtypes or otherwise undifferentiated schizophrenia.[72] The researchers stated that their findings supported the view that \"a heterogeneous disorder like schizophrenia is unlikely to be modeled accurately by a single pharmacological agent.\"[72] Classical hallucinogens (psychedelics) can be divided into three main chemical classes: tryptamines (such as psilocin and DMT), ergolines (such as LSD), and phenethylamines (such as mescaline).[65] Tryptamines closely resemble serotonin chemically.[65] ^Pender JW (October 1972). \"Dissociative anesthesia\". California Medicine Some Dissociatives Have General Depressant Effects as Well, Which is Why Doctors Prescribe Them to Sedate Patients Who Are in Pain or to Help Maintain General Anesthesia During an Operation. Common Dissociative Drugs Include: PCP (Phencyclidine). 117 (4): 46–7. PMC1518731. PMID18730832. ^Lindsey, Brink (2007). The Age of Abundance: How Prosperity Transformed America's Politics and Culture. New York: Collins. p. 156. ...pot and psychedelics revealed to their users wildly different visions of reality from the \"straight\" one everybody took for granted. ... Guided into those transcendent realms, many young and impressionable minds were set aflame with visions of radical change. ... Antiwar protesters, feminists, student rebels, environmentalists, and gays all took their turns marching to the solemn strains of \"We Shall Overcome\"... ^Goffman, Ken (2004). Counterculture through the Ages: From Abraham to Acid House. New York: Villard. pp. 266–267. ISBN978-0812974751. By normative social standards, something unseemly was going on, but since LSD, the catalyst that was unleashing the celebratory chaos, was still legal [in 1966], there was little [the authorities] could do... [That year, a]cross the nation, states started passing laws prohibiting LSD. .... By their panic, as expressed through their prohibitionary legislation, the conservative forces teased out what was perhaps the central countercultural progression for this epoch. Erowid is a web site dedicated entirely to providing information about psychoactive drugs, with an impressive collection of trip reports, materials collected from the web and usenet, and a bibliography of scientific literature. PsychonautWiki[permanent dead link] is a wiki dedicated to documenting psychoactive drugs and their subjective effects, with an emphasis on hallucinogens and novel psychoactives. The site hosts experience reports, tutorials, and a curated collection of articles."}
{"url": "https://en.m.wikipedia.org/wiki/ISSN_(identifier)", "text": "ISSN An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication (periodical), such as a magazine.[1] The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.[2] ISSN encoded in an EAN-13 barcode with sequence variant 0 and issue number 05Example of an ISSN, 2049-3630, encoded in an EAN-13 bar code, with explanationISSN expanded with sequence variant 0 to a GTIN-13 and encoded in an EAN-13 barcode with an EAN-2 add-on designating issue number 13 When a serial with the same content is published in more than one media type, a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media. The ISSN system refers to these types as print ISSN (p-ISSN) and electronic ISSN (e-ISSN).[4] Consequently, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN (ISSN-L), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium.[5] An ISSN is an eight-digit code, divided by a hyphen into two four-digit numbers.[1] The last digit, which may be zero through nine or an X, is a check digit, so the ISSN is uniquely represented by its first seven digits. Formally, the general form of the ISSN (also named \"ISSN structure\" or \"ISSN syntax\") can be expressed as follows:[6] If there is no remainder, the check digit is 0; otherwise the remainder is subtracted from 11. If the result is less than 10, it yields the check digit: 11−6=5.{\\displaystyle 11-6=5\\;.} Thus, in this example, the check digit C is 5. If the result is 10 (that is, if the remainder is 1), the check digit is an uppercase X (like a Roman ten). To confirm the check digit, calculate the sum of all eight digits of the ISSN multiplied by their position in the number, counting from the right. (If the check digit is X, add 10 to the sum.) The remainder of the sum modulo 11 must be 0. There is an online ISSN checker that can validate an ISSN, based on the above algorithm.[7] ISSNs can be encoded in EAN-13 bar codes with a 977 \"country code\" (compare the 978 country code (\"bookland\") for ISBNs), followed by the 7 main digits of the ISSN (the check digit is not included), followed by 2 publisher-defined digits, followed by the EAN check digit (which need not match the ISSN check digit).[8] ISSN codes are assigned by a network of ISSN National Centres, usually located at national libraries and coordinated by the ISSN International Centre based in Paris. The International Centre is an intergovernmental organization created in 1974 through an agreement between UNESCO and the French government. ISSN-L is a unique identifier for all versions of the serial containing the same content across different media. As defined by ISO 3297:2007, the \"linking ISSN (ISSN-L)\" provides a mechanism for collocation or linking among the different media versions of the same continuing resource. The ISSN-L is one of a serial's existing ISSNs, so does not change the use or assignment of \"ordinary\" ISSNs;[9] it is based on the ISSN of the first published medium version of the publication. If the print and online versions of the publication are published at the same time, the ISSN of the print version is chosen as the basis of the ISSN-L. The International Centre maintains a database of all ISSNs assigned worldwide, the ISDS Register (International Serials Data System), otherwise known as the ISSN Register. At the end of 2016,[update] the ISSN Register contained records for 1,943,572 items.[10] The Register is not freely available for interrogation on the web, but is available by subscription. The print version of a serial typically will include the ISSN code as part of the publication information. Most serial websites contain ISSN code information. Derivative lists of publications will often contain ISSN codes; these can be found through on-line searches with the ISSN code itself or serial title. WorldCat permits searching its catalog by ISSN, by entering \"issn:\" before the code in the query field. One can also go directly to an ISSN's record by appending it to \"https://www.worldcat.org/ISSN/\", e.g. n2:1021-9749 – Search Results. This does not query the ISSN Register itself, but rather shows whether any WorldCat library holds an item with the given ISSN. ISSN and ISBN codes are similar in concept, where ISBNs are assigned to individual books. An ISBN might be assigned for particular issues of a serial, in addition to the ISSN code for the serial as a whole. An ISSN, unlike the ISBN code, is an anonymous identifier associated with a serial title, containing no information as to the publisher or its location. For this reason a new ISSN is assigned to a serial each time it undergoes a major title change. Separate ISSNs are needed for serials in different media (except reproduction microforms). Thus, the print and electronic media versions of a serial need separate ISSNs,[11] and CD-ROM versions and web versions require different ISSNs. However, the same ISSN can be used for different file formats (e.g. PDF and HTML) of the same online serial. This \"media-oriented identification\" of serials made sense in the 1970s. In the 1990s and onward, with personal computers, better screens, and the Web, it makes sense to consider only content, independent of media. This \"content-oriented identification\" of serials was a repressed demand during a decade, but no ISSN update or initiative occurred. A natural extension for ISSN, the unique-identification of the articles in the serials, was the main demand application. An alternative serials' contents model arrived with the indecs Content Model and its application, the digital object identifier (DOI), an ISSN-independent initiative, consolidated in the 2000s. Only later, in 2007, ISSN-L was defined in the new ISSN standard (ISO 3297:2007) as an \"ISSN designated by the ISSN Network to enable collocation or versions of a continuing resource linking among the different media\".[12] An ISSN can be encoded as a uniform resource name (URN) by prefixing it with \"urn:ISSN:\".[13] For example, Rail could be referred to as \"urn:ISSN:0953-4563\". URN namespaces are case-sensitive, and the ISSN namespace is all caps.[14] If the checksum digit is \"X\" then it is always encoded in uppercase in a URN. ISSN is not unique when the concept is \"a journal is a set of contents, generally copyrighted content\": the same journal (same contents and same copyrights) may have two or more ISSN codes. A URN needs to point to \"unique content\" (a \"unique journal\" as a \"set of contents\" reference). Example: the DOI name \"10.1038/nature13777\" can be represented as an HTTP string by https://doi.org/10.1038/nature13777, and is redirected (resolved) to the current article's page; but there is no ISSN online service, like http://dx.issn.org/, to resolve the ISSN of the journal (in this sample 1476-4687)."}
{"url": "https://en.m.wikipedia.org/wiki/Gros_Michel_banana", "text": "Gros Michel banana Gros Michel (French pronunciation:[ɡʁomiʃɛl]), often translated and known as \"Big Mike\", is an export cultivar of banana and was, until the 1950s, the main variety grown.[3] The physical properties of the Gros Michel make it an excellent export produce; its thick peel makes it resilient to bruising during transport and the dense bunches that it grows in make it easy to ship.[4] French naturalist Nicolas Baudin carried a few corms of this banana from Southeast Asia, depositing them at a botanical garden on the Caribbean island of Martinique. In 1835, French botanist Jean François Pouyat carried Baudin's fruit from Martinique to Jamaica.[7] Gros Michel bananas were grown on massive plantations in Honduras, Costa Rica, and elsewhere in Central America. This variety was once the dominant export banana to Europe and North America, grown in Central America but, in the 1950s, Panama disease, a wilt caused by the fungus Fusarium oxysporum f.sp. cubense, wiped out vast tracts of Gros Michel plantations in Central America, though it is still grown on non-infected land throughout the region.[8] By the 1960s, the exporters of Gros Michel bananas were unable to keep trading such a susceptible cultivar, and they started growing resistant cultivars belonging to the Cavendish subgroup (another Musa acuminata AAA).[8] There are efforts to use genetic modification to create a version of the Gros Michel which is resistant to Panama disease.[9] There have also been successful hybrids of Cavendish and Gros Michel that display a resistance to Panama disease.[10] A 2013 paper described experiments to create a version of Gros Michel which is resistant to black sigatoka, another fungal infection.[11] The Gros Michel is included as a Joker card in the Poker themed roguelike video game Balatro, where it has a one-in-four chance of being destroyed after each round. After it gets destroyed, the Cavandish card has a chance to appear in the shop; a reference to the cultivar's susceptibility to disease and its replacement.[13] The Gros Michel has a higher concentration of isoamyl acetate, the ester commonly used for \"banana\" food flavoring, than the Cavendish.[14]"}
{"url": "https://en.m.wikipedia.org/wiki/Italian-American_Mafia", "text": "In North America, the organization is often colloquially referred to as the Italian Mafia or Italian Mob, though these terms may also apply to the separate yet related Sicilian Mafia or other organized crime groups in Italy, or ethnic Italian crime groups in other countries. The organization is often referred to by its members as Cosa Nostra (Italian pronunciation:[ˈkɔːzaˈnɔstra,ˈkɔːsa-], \"our thing\" or \"this thing of ours\") and by the American government as La Cosa Nostra (LCN). The organization's name is derived from the original Mafia or Cosa Nostra, the Sicilian Mafia, with \"American Mafia\" originally referring simply to Mafia groups from Sicily operating in the United States. The Mafia in the United States emerged in impoverished Italian immigrant neighborhoods in New York's East Harlem (or \"Italian Harlem\"), the Lower East Side, and Brooklyn; also emerging in other areas of the Northeastern United States and several other major metropolitan areas (such as New Orleans[24] and Chicago) during the late 19th century and early 20th century, following waves of Italian immigration especially from Sicily and other regions of Southern Italy. Campanian, Calabrian and other Italian criminal groups in the United States, as well as independent Italian American criminals, eventually merged with Sicilian Mafiosi to create the modern pan-Italian Mafia in North America. Today, the Italian-American Mafia cooperates in various criminal activities with Italian organized crime groups, such as the Sicilian Mafia, the Camorra of Campania and the 'Ndrangheta of Calabria. The most important unit of the American Mafia is that of a \"family\", as the various criminal organizations that make up the Mafia are known. Despite the name of \"family\" to describe the various units, they are not familial groupings.[25] The word mafia (Italian:[ˈmaːfja]) derives from the Sicilian adjective mafiusu, which, roughly translated, means \"swagger\", but can also be translated as \"boldness\" or \"bravado\". In reference to a man, mafiusu (mafioso in Italian) in 19th-century Sicily signified \"fearless\", \"enterprising\", and \"proud\", according to scholar Diego Gambetta.[29] In reference to a woman, however, the feminine-form adjective mafiusa means 'beautiful' or 'attractive'. In North America, the Italian-American Mafia may be colloquially referred to as simply \"The Mafia\" or \"The Mob\". However, without context, these two terms may cause confusion; \"The Mafia\" may also refer to the Sicilian Mafia specifically or Italian organized crime in general, while \"The Mob\" can refer to other similar organized crime groups (such as the Irish Mob) or organized crime in general. The first published account of what became the Mafia in the United States dates to the spring of 1869. The New Orleans Times reported that the city's Second District had become overrun by \"well-known and notorious Sicilian murderers, counterfeiters and burglars, who, in the last month, have formed a sort of general co-partnership or stock company for the plunder and disturbance of the city.\"[citation needed] Emigration from southern Italy to the Americas was primarily to Brazil and Argentina, and New Orleans had a heavy volume of port traffic to and from both locales. Mafia groups in the United States first became influential in the New York metropolitan area, gradually progressing from small neighborhood operations in poor Italian ghettos to citywide and eventually national organizations. \"The Black Hand\" was a name given to an extortion method used in Italian neighborhoods at the turn of the 20th century. It has been sometimes mistaken for the Mafia itself, which it is not. The Black Hand was a criminal society, but there were many small Black Hand gangs. Black Hand extortion was often (wrongly) viewed as the activity of a single organization, because Black Hand criminals in Italian communities throughout the United States used the same methods of extortion.[30] Giuseppe Esposito was the first known Mafia member to emigrate to the United States.[26] He and six other Sicilians fled to New York after murdering eleven wealthy landowners, the chancellor and a vice-chancellor of a Sicilian province.[26] He was arrested in New Orleans in 1881 and extradited to Italy.[26] From the 1890s to 1920 in New York City the Five Points Gang, founded by Paul Kelly, were very powerful in the Little Italy of the Lower East Side. Kelly recruited some street hoodlums who later became some of the most famous crime bosses of the century - such as Johnny Torrio, Al Capone, Lucky Luciano and Frankie Yale. They were often in conflict with the Jewish Eastmans of the same area. There was also an influential Mafia family in East Harlem. The Neapolitan Camorra was very active in Brooklyn. In Chicago, the 19th Ward was an Italian neighborhood that became known as the \"Bloody Nineteenth\" due to the frequent violence in the ward, mostly as a result of Mafia activity, feuds, and vendettas. New Orleans was possibly the site of the first Mafia incident in the United States that received both national and international attention.[26] On October 15, 1890, New Orleans Police Superintendent David Hennessy was murdered execution-style. It is still unclear whether Italian immigrants actually killed him, or whether it was a frame-up by nativists against the reviled underclass immigrants.[26] Hundreds of Sicilians were arrested on mostly baseless charges, and nineteen were eventually indicted for the murder. An acquittal followed, with rumors of bribed and intimidated witnesses.[26] On March 14, 1891, after the acquittal, the outraged citizens of New Orleans organized a lynch mob and proceeded to kill eleven of the nineteen defendants. Two were hanged, nine were shot, and the remaining eight escaped.[31][32][33] On January 16, 1919, prohibition began in the United States with the 18th Amendment to the United States Constitution making it illegal to manufacture, transport, or sell alcohol. Despite these bans, there was still a very high demand for it from the public. This created an atmosphere that tolerated crime as a means to provide liquor to the public, even among the police and city politicians. While not explicitly related to Mafia involvement, the murder rate during the Prohibition era rose over 40%—from 6.8 per 100,000 individuals to 9.7—and within the first three months proceeding the Eighteenth Amendment, a half-million dollars in bonded whiskey was stolen from government warehouses.[34] The profits that could be made from selling and distributing alcohol were worth the risk of punishment from the government, which had a difficult time enforcing prohibition. There were over 900,000 cases of liquor shipped to the borders of U.S. cities.[35] Criminal gangs and politicians saw the opportunity to make fortunes and began shipping larger quantities of alcohol to U.S. cities. The majority of the alcohol was imported from Canada,[36][37] the Caribbean, and the American Midwest where stills manufactured illegal alcohol. In the early 1920s, fascist Benito Mussolini took control of Italy and waves of Italian immigrants fled to the United States. Sicilian Mafia members also fled to the United States, as Mussolini cracked down on Mafia activities in Italy.[38] Most Italian immigrants resided in tenement buildings. As a way to escape the poor lifestyle, some Italian immigrants chose to join the American Mafia. The Mafia took advantage of prohibition and began selling illegal alcohol. The profits from bootlegging far exceeded the traditional crimes of protection, extortion, gambling, and prostitution. Prohibition allowed Mafia families to make fortunes.[39][40] As prohibition continued, victorious factions went on to dominate organized crime in their respective cities, setting up the family structure of each city. The bootlegging industry organized members of these gangs before they were distinguished as today's known families. The new industry required members at all different employment levels, such as bosses, lawyers, truckers, and even members to eliminate competitors through threat/force. Gangs hijacked each other's alcohol shipments, forcing rivals to pay them for \"protection\" to leave their operations alone, and armed guards almost invariably accompanied the caravans that delivered the liquor.[41][42] In the 1920s, Italian Mafia families began waging wars for absolute control over lucrative bootlegging rackets. As the violence erupted, Italians fought Irish and Jewish ethnic gangs for control of bootlegging in their respective territories. In New York City, Frankie Yale waged war with the Irish American White Hand Gang. In Chicago, Al Capone and his family massacred the North Side Gang, another Irish American outfit.[43] In New York City, by the end of the 1920s, two factions of organized crime had emerged to fight for control of the criminal underworld—one led by Joe Masseria and the other by Salvatore Maranzano.[26] This caused the Castellammarese War, which led to Masseria's murder in 1931. Maranzano then divided New York City into five families.[26] Maranzano, the first leader of the American Mafia, established the code of conduct for the organization, set up the \"family\" divisions and structure, and established procedures for resolving disputes.[26] In an unprecedented move, Maranzano set himself up as boss of all bosses and required all families to pay tribute to him. This new role was received negatively, and Maranzano was murdered within six months on the orders of Charles \"Lucky\" Luciano. Luciano was a former Masseria underling who had switched sides to Maranzano and orchestrated the killing of Masseria.[44] As an alternative to the previous despotic Mafia practice of naming a single Mafia boss as capo di tutti capi, or \"boss of all bosses\", Luciano created The Commission in 1931,[26] where the bosses of the most powerful families would have equal say and vote on important matters and solve disputes between families. This group ruled over the National Crime Syndicate and brought in an era of peace and prosperity for the American Mafia.[45] By mid-century, there were 26 official Commission-sanctioned Mafia crime families, each based in a different city (except for the Five Families which were all based in New York).[46] Each family operated independently from the others and generally had exclusive territory it controlled.[26] As opposed to the older generation of \"Mustache Petes\" such as Maranzano and Masseria, who usually worked only with fellow Italians, the \"Young Turks\" led by Luciano were more open to working with other groups, most notably the Jewish-American criminal syndicates to achieve greater profits. The Mafia thrived by following a strict set of rules that originated in Sicily that called for an organized hierarchical structure and a code of silence that forbade its members from cooperating with the police (Omertà). Failure to follow any of these rules was punishable by death. The rise of power that the Mafia acquired during prohibition would continue long after alcohol was made legal again. Criminal empires which had expanded on bootleg money would find other avenues to continue making large sums of money. When alcohol ceased to be prohibited in 1933, the Mafia diversified its money-making criminal activities to include (both old and new): illegal gambling operations, loan sharking, extortion, protection rackets, drug trafficking, fencing, and labor racketeering through control of labor unions.[47] In the mid-20th century, the Mafia was reputed to have infiltrated many labor unions in the United States, most notably the Teamsters and International Longshoremen's Association.[26] This allowed crime families to make inroads into very profitable legitimate businesses such as construction, demolition, waste management, trucking, and in the waterfront and garment industry.[48] In addition they could raid the unions' health and pension funds, extort businesses with threats of a workers' strike and participate in bid rigging. In New York City, most construction projects could not be performed without the Five Families' approval. In the port and loading dock industries, the Mafia bribed union members to tip them off to valuable items being brought in. Mobsters would then steal these products and fence the stolen merchandise. Meyer Lansky made inroads into the casino industry in Cuba during the 1930s while the Mafia was already involved in exporting Cuban sugar and rum.[49] When his friend Fulgencio Batista became president of Cuba in 1952, several Mafia bosses were able to make legitimate investments in legalized casinos. One estimate of the number of casinos mobsters owned was no less than 19.[49] However, when Batista was overthrown following the Cuban Revolution, his successor Fidel Castro banned U.S. investment in the country, putting an end to the Mafia's presence in Cuba.[49] Las Vegas was seen as an \"open city\" where any family can work. Once Nevada legalized gambling, mobsters were quick to take advantage and the casino industry became very popular in Las Vegas. Since the 1940s, Mafia families from New York, Cleveland, Kansas City, Milwaukee and Chicago had interests in Las Vegas casinos. They got loans from the Teamsters' pension fund, a union they effectively controlled, and used legitimate front men to build casinos.[50] When money came into the counting room, hired men skimmed cash before it was recorded, then delivered it to their respective bosses.[50] This money went unrecorded, but the amount is estimated to be in the hundreds of millions of dollars. Operating in the shadows, the Mafia faced little opposition from law enforcement. Local law enforcement agencies did not have the resources or knowledge to effectively combat organized crime committed by a secret society they were unaware existed.[48] Many people within police forces and courts were simply bribed, while witness intimidation was also common.[48] In 1951, a U.S. Senate committee called the Kefauver Hearings determined that a \"sinister criminal organization\" known as the Mafia operated in the nation.[26] Many suspected mobsters were subpoenaed for questioning, but few testified and none gave any meaningful information. In 1957, New York State Police uncovered a meeting and arrested major figures from around the country in Apalachin, New York. The event (dubbed the \"Apalachin Meeting\") forced the FBI to recognize organized crime as a serious problem in the United States and changed the way law enforcement investigated it.[26] In 1963, Joe Valachi became the first Mafia member to turn state's evidence, and provided detailed information of its inner workings and secrets. More importantly, he revealed the Mafia's existence to the law, which enabled the Federal Bureau of Investigation to begin an aggressive assault on the Mafia's National Crime Syndicate.[51] Following Valachi's testimony, the Mafia could no longer operate completely in the shadows. The FBI put a lot more effort and resources into organized crime activities nationwide and created the Organized Crime Strike Force in various cities. While all this created more pressure on the Mafia, it did little, however, to curb its criminal activities. Progress was made by the beginning of the 1980s, when the FBI was able to rid Las Vegas casinos of Mafia control and made a determined effort to loosen the Mafia's stronghold on labor unions. By the late 1970s, the Mafia were involved in many industries,[26] including betting on college sports. Several Mafia members associated with the Lucchese crime family participated in a point shaving scandal involving the Boston College basketball team. Rick Kuhn, Henry Hill, and others associated with the Lucchese crime family, manipulated the results of the games during the 1978–1979 basketball season. Through bribing and intimidating several members of the team, they ensured their bets on the point spread of each game would go in their favor.[52] One of the most lucrative gains for the Mafia was through gas-tax fraud. They created schemes to keep the money that they owed in taxes after the sale of millions of dollars' worth of wholesale petroleum. This allowed them to sell more gasoline at even lower prices. Michael Franzese, also known as the Yuppie Don, ran and organized a gas scandal and stole over $290 million in gasoline taxes by evading the Internal Revenue Service (IRS) and shutting down the gas station before government officials could make him pay what he owed. Franzese was caught in 1985.[53][54] Labor racketeering helped the Mafia control many industries from a macroeconomic scale. This tactic helped them grow in power and influence in many cities with big labor unions such as New York, Philadelphia, Chicago, Detroit and many others. Many members of the Mafia were enlisted in unions and even became union executives. The Mafia has controlled unions all over the U.S. to extort money and resources out of big business, with recent indictments of corruption involving the New Jersey Waterfront Union, the Concrete Workers Union, and the Teamster Union.[55] Restaurants were yet another powerful means by which the Mafia could gain economic power. A large concentration of Mafia-owned restaurants was in New York City. Not only were they the setting of many killings and important meetings, but they were also an effective means of smuggling drugs and other illegal goods. From 1985 to 1987, Sicilian Mafiosi in the U.S. imported an estimated $1.65 billion worth of heroin through pizzerias, hiding the cargo in various food products.[56][57] Another one of the areas of the economy that the Mafia was most influential was Las Vegas, Nevada, beginning just after World War II with the opening of the first gambling resort, The Flamingo.[58] Many credit the Mafia with being a big part of the city's development in the mid-20th century,[59] as millions of dollars in capital flowing into new casino resorts laid the foundation for further economic growth. This capital did not come from one Mafia family alone, but many throughout the country seeking to gain even more power and wealth. Large profits from casinos, run as legitimate businesses, would help to finance many of the illegal activities of the Mafia from the 1950s into the 1980s.[58] In the 1950s more Mafia-financed casinos were constructed, such as the Stardust, Sahara, Tropicana, Desert Inn, and Riviera. Tourism in the city greatly increased through the 1960s and strengthened the local economy. The 1960s were also when the Mafia's influence in the Las Vegas economy began to dwindle, however.[58] The Nevada State government and Federal government had been working to weaken Mafia activity on the Strip. In 1969, the Nevada State Legislature passed a law that made it easier for corporations to own casinos. This brought new investors to the local economy to buy casinos from the Mafia. The U.S. Congress passed the RICO Act a year later. This law gave more authority to law enforcement to pursue the Mafia for its illegal activities. There was a sharp decline in mob involvement in Las Vegas in the 1980s. Through the RICO law, many in the Mafia were convicted and imprisoned.[60] When the Racketeer Influenced and Corrupt Organizations Act (RICO Act) became federal law in 1970, it became a highly effective tool in prosecuting mobsters. It provides for extended criminal penalties for acts performed as part of an ongoing criminal organization. Violation of the act is punishable by up to 20 years in prison per count, up to $25,000 in fines, and the violator must forfeit all properties attained while violating the RICO Act.[61] The RICO Act has proven to be a very powerful weapon because it attacks the entire corrupt entity instead of individuals who can easily be replaced with other organized crime members.[26] Between 1981 and 1992, 23 bosses from around the country were convicted under the law while between 1981 and 1988, 13 underbosses and 43 captains were convicted.[48] Over 1,000 crime family figures were convicted by 1990.[61] While this significantly crippled many Mafia families around the country, the most powerful families continued to dominate crime in their territories, even if the new laws put more mobsters in jail and made it harder to operate. John Gotti just after his arrest in 1990 A high-profile RICO case sentenced John Gotti and Frank Locascio to life in prison in 1992,[62] with the help of informant Sammy Gravano in exchange for immunity from prosecution for his crimes.[38][63] Aside from avoiding long prison stretches, the FBI could put mobsters in the United States Federal Witness Protection Program, changing their identities and supporting them financially for life. This led to dozens of mobsters testifying and providing information during the 1990s, which led to the imprisonment of hundreds of other members. As a result, the Mafia has seen a major decline in its power and influence in organized crime since the 1990s. On January 9, 2003, Bonanno crime family boss Joseph Massino was arrested and indicted, alongside Salvatore Vitale, Frank Lino and capo Daniel Mongelli, in a comprehensive racketeering indictment. The charges against Massino himself included ordering the 1981 murder of Dominick \"Sonny Black\" Napolitano.[64][65] Massino's trial began on May 24, 2004, with judge Nicholas Garaufis presiding and Greg D. Andres and Robert Henoch heading the prosecution.[66] He now faced 11 RICO counts for seven murders (due to the prospect of prosecutors seeking the death penalty for the Sciascia murder, that case was severed to be tried separately), arson, extortion, loansharking, illegal gambling, and money laundering.[67] After deliberating for five days, the jury found Massino guilty of all 11 counts on July 30, 2004. His sentencing was initially scheduled for October 12, and he was expected to receive a sentence of life imprisonment with no possibility of parole.[68] The jury also approved the prosecutors' recommended $10 million forfeiture of the proceeds of his reign as Bonanno boss on the day of the verdict.[69] Immediately after his July 30 conviction, as court was adjourned, Massino requested a meeting with Judge Garaufis, where he made his first offer to cooperate.[70] He did so in hopes of sparing his life; he was facing the death penalty if found guilty of Sciascia's murder, as one of John Ashcroft's final acts as Attorney General was to order federal prosecutors to seek the death penalty for Massino.[71] Massino thus stood to be the first Mafia boss to be executed for his crimes, and the first mob boss to face the death penalty since Lepke Buchalter was executed in 1944.[72] Massino was the first sitting boss of a New York crime family to turn state's evidence, and the second in the history of the American Mafia to do so.[73] (Philadelphia crime family boss Ralph Natale had flipped in 1999 when facing drug charges, though Natale was mostly a \"front\" boss while the real boss of the Philadelphia Mafia used Natale as a diversion for authorities.)[74] In the 21st century, the Mafia has continued to be involved in a broad spectrum of illegal activities. These include murder, extortion, corruption of public officials, gambling, infiltration of legitimate businesses, labor racketeering, loan sharking, tax fraud schemes and stock manipulation schemes.[75] Although the Mafia used to be nationwide, today most of its activities are confined to the Northeast and Chicago.[76] While other criminal organizations such as the Russian Mafia, Chinese Triads, Mexican drug cartels and others have all grabbed a share of criminal activities, the Mafia continues to be the dominant criminal organization in these regions, partly due to its strict hierarchical structure.[76] Law enforcement is concerned with the possible resurgence of the Mafia as it regroups from the turmoil of the 1990s, although FBI and local law enforcement agencies now focus more on homeland security and less on organized crime since the September 11 attacks.[77][78] To avoid FBI attention and prosecution, the modern Mafia also outsources much of its work to other criminal groups, such as motorcycle gangs.[76] The American Mafia operates on a strict hierarchical structure. While similar to its Sicilian origins, the American Mafia's modern organizational structure was created by Salvatore Maranzano in 1931. He created the Five Families, each of which would have a boss, underboss, capos, soldiers—all only of full-blooded Italian origin—while associates could come from any background.[79][80][38] All inducted members of the Mafia are called \"made\" men. This signifies that they are untouchable in the criminal underworld and any harm brought to them will be met with retaliation. With the exception of associates, all mobsters within the Mafia are \"made\" official members of a crime family. The three highest positions make up the administration. Below the administration, there are factions each headed by a caporegime (captain), who leads a crew of soldiers and associates. They report to the administration and can be seen as equivalent to managers in a business. When a boss makes a decision, he rarely issues orders directly to workers who would carry it out but instead passes instructions down through the chain of command. This way, the higher levels of the organization are insulated from law enforcement attention if the lower level members who actually commit the crime should be captured or investigated, providing plausible deniability. There are occasionally other positions in the family leadership. Frequently, ruling panels have been set up when a boss goes to jail to divide the responsibility of the family (these usually consist of three or five members). This also helps divert police attention from any one member. The family messenger and street boss were positions created by former Genovese family leader Vincent Gigante. Boss – The boss is the head of the family, usually reigning as a dictator, sometimes called the Don or \"Godfather\". The boss receives a cut of every operation. Operations are taken on by every member of the family and of the region's occupying family.[81] Depending on the family, the boss may be chosen by a vote from the caporegimes of the family. In the event of a tie, the underboss must vote. In the past, all the members of a family voted on the boss, but by the late 1950s, any gathering such as that usually attracted too much attention.[82] In practice, many of these elections are seen as having an inevitable result, such as that of John Gotti in 1986. According to Sammy Gravano, a meeting was held in a basement during which all capos were searched and Gotti's men stood ominously behind them. Gotti was then proclaimed boss. Underboss – The underboss, usually appointed by the boss, is the second in command of the family. The underboss often runs the day-to-day responsibilities of the family or oversees its most lucrative rackets. He usually gets a percentage of the family's income from the boss's cut. The underboss is usually first in line to become acting boss if the boss is imprisoned, and is also frequently seen as a logical successor. Consigliere – The consigliere is an advisor to the family and sometimes seen as the boss's \"right-hand man\". He is used as a mediator of disputes and often acts as a representative or aide for the family in meetings with other families, rival criminal organizations, and important business associates. In practice, the consigliere is normally the third-ranking member of the administration of a family and was traditionally a senior member carrying the utmost respect of the family and deeply familiar with the inner-workings of the organization. A boss will often appoint a trusted close friend or personal advisor as his official consigliere. Caporegime (or capo) – A caporegime (also captain or skipper) is in charge of a crew, a group of soldiers who report directly to him. Each crew usually contains 10–20 soldiers and many more associates. A capo is appointed by the boss and reports to him or the underboss. A captain gives a percentage of his (and his underlings') earnings to the boss and is also responsible for any tasks assigned, including murder. In labor racketeering, it is usually a capo who controls the infiltration of union locals. If a capo becomes powerful enough, he can sometimes wield more power than some of his superiors. In cases like Anthony Corallo they might even bypass the normal Mafia structure and lead the family when the boss dies. Soldier (Soldato in Italian) – A soldato or \"soldier\" is an inducted (or \"made\") member of the Mafia in general and an inducted member of a particular Mafia crime family, and traditionally they can only be of full Italian background (although today many families require men to be of only half Italian descent, on their father's side). Once a member is made he is untouchable, meaning permission from a soldier's boss must be given before he is murdered. When the books are open, meaning that a family is accepting new members, a made man may recommend an up-and-coming associate to be a new soldier. Soldiers are the main workers of the family, usually committing crimes like assault, murder, extortion, intimidation, etc. In return, they are given profitable rackets to run by their superiors and have full access to their family's connections and power. Jewish associate Meyer Lansky's (right) work with Lucky Luciano made him an important figure in developing the American Mafia. Associate – An associate is not a member of the Mafia, but works for a crime family nonetheless. Associates can include a wide range of people who work for the family. An associate can have a wide range of duties, from virtually carrying out the same duties as a soldier to being a simple errand boy. This is where prospective mobsters (\"connected guys\") start out to prove their worth. Once a crime family is accepting new members, the best associates of Italian descent are evaluated and picked to become soldiers. An associate can also be a criminal who serves as a go-between in criminal transactions or sometimes deals in drugs to keep police attention off the actual members, or they can simply be people the family does business with (restaurant owners, etc.) In other cases, an associate might be a corrupt labor union delegate or businessman.[82] Non-Italians will never go any further than this, although many non-Italian associates of the Mafia, such as Meyer Lansky, Bugsy Siegel, Murray Humphreys, Mickey Cohen, Frank Rosenthal, Gus Alex, Bumpy Johnson, Frank Sheeran, Jimmy Hoffa, Jake Guzik, Sidney Korshak, Gerard Ouimette, Moe Dalitz and James Burke, wielded extreme power within their respective crime families and carried the respect of actual Mafia members. The Mafia initiation ritual to become a made man in the Mafia emerged from various sources, such as Roman Catholic confraternities and Masonic Lodges in mid-19th century Sicily.[83] At the initiation ceremony, the inductee would have his finger pricked with a needle by the officiating member; a few drops of blood are spilled on a card bearing the likeness of a saint; the card is set on fire; finally, while the card is passed rapidly from hand to hand to avoid burns, the novice takes an oath of loyalty to the Mafia family. The oath of loyalty to the Mafia Family is called the Omerta. This was confirmed in 1986 by the pentitoTommaso Buscetta.[84] A hit, or murder, of a made man must be approved by the leadership of his family, or retaliatory hits would be made, possibly inciting a war. In a state of war, families would \"go to the mattresses,\" which means to prepare for a war or be prepared in a war-like stance. It was mainly derived from the film The Godfather, as the origin of the phrase is unknown.[85]Omertà is a key oath or code of silence in the Mafia that places importance on silence in the face of questioning by authorities or outsiders; non-cooperation with authorities, the government, or outsiders.[86][87] Traditionally, to become a made man, or full member of the Mafia, the inductee was required to be a male of full Sicilian descent,[88] later extended to males of full Italian descent,[89] and later further extended to males of half-Italian descent through their father's lineage.[88] According to Salvatore Vitale, it was decided during a Commission meeting in 2000 to restore the rule requiring both parents to be of Italian descent.[90] It is also common for a Mafia member to have a mistress.[91] Traditionally, made members were also not allowed to have mustaches—part of the Mustache Pete custom.[92][93]Homosexuality is reportedly incompatible with the American Mafia code of conduct. In 1992, John D'Amato, acting boss of the DeCavalcante family, was killed when he was suspected of engaging in homosexual activity.[94] U.S. Naval Intelligence entered into an agreement with Lucky Luciano to gain his assistance in keeping the New York waterfront free from saboteurs after the destruction of the SS Normandie.[101] This spectacular disaster convinced both sides to talk seriously about protecting the United States' East Coast on the afternoon of February 9, 1942. While it was in the process of being converted into a troopship, the luxury ocean liner, Normandie, mysteriously burst into flames with 1,500 sailors and civilians on board. All but one escaped, but 128 were injured and by the next day the ship was a smoking hull. In his report, twelve years later, William B. Herlands, Commissioner of Investigation, made the case for the U.S. government talking to top criminals, stating \"The Intelligence authorities were greatly concerned with the problems of sabotage and espionage…Suspicions were rife with respect to the leaking of information about convoy movements. The Normandie, which was being converted to war use as the Navy auxiliary Lafayette, had burned at the pier in the North River, New York City. Sabotage was suspected.\"[102] In 2007, Linda Schiro testified in an unrelated court case that her late boyfriend, Gregory Scarpa, a capo in the Colombo family, had been recruited by the FBI to help find the bodies of three civil rights workers who had been murdered in Mississippi in 1964 by the Ku Klux Klan. She said that she had been with Scarpa in Mississippi at the time and had witnessed him being given a gun, and later a cash payment, by FBI agents. She testified that Scarpa had threatened a Klansman by placing a gun in the Klansman's mouth, forcing the Klansman to reveal the location of the bodies. Similar stories of Mafia involvement in recovering the bodies had been circulating for years, and had been previously published in the New York Daily News, but had never before been introduced in court.[105][106] In several Mafia families, killing a state authority is forbidden due to the possibility of extreme police retaliation. In some rare strict cases, conspiring to commit such a murder is punishable by death. Jewish mobster and Mafia associate Dutch Schultz was reportedly killed by his Italian peers out of fear that he would carry out a plan to kill New York City prosecutor Thomas Dewey and thus bring unprecedented police attention to the Mafia. However, the Mafia has carried out hits on law enforcement, especially in its earlier history. New York police officer Joe Petrosino was shot by Sicilian mobsters while on duty in Sicily. A statue of him was later erected across the street from a Lucchese hangout.[107] In 1951, a U.S. Senate special committee, chaired by Democratic Tennessee Senator Estes Kefauver, determined that a \"sinister criminal organization\" known as the Mafia operated around the United States. The United States Senate Special Committee to Investigate Crime in Interstate Commerce (known as the \"Kefauver Hearings\"), televised nationwide, captured the attention of the American people and forced the FBI to recognize the existence of organized crime. In 1953, the FBI initiated the \"Top Hoodlum Program\". The purpose of the program was to have agents collect information on the mobsters in their territories and report it regularly to Washington to maintain a centralized collection of intelligence on racketeers.[108] Local and state law enforcement became suspicious when numerous expensive cars bearing license plates from around the country arrived in what was described as \"the sleepy hamlet of Apalachin\".[120] After setting up roadblocks, the police raided the meeting, causing many of the participants to flee into the woods and area surrounding the Barbara estate.[121] More than 60 underworld bosses were detained and indicted following the raid. Twenty of those who attended the meeting were charged with \"Conspiring to obstruct justice by lying about the nature of the underworld meeting\" and found guilty in January 1959. All were fined, up to $10,000 each, and given prison sentences ranging from three to five years. All the convictions were overturned on appeal the following year.[why?] One of the most direct and significant outcomes of the Apalachin Meeting was that it helped to confirm the existence of a nationwide criminal conspiracy, a fact that some, including Federal Bureau of Investigation director J. Edgar Hoover, had long refused to acknowledge.[118][122][123] Genovese crime family soldier Joe Valachi was convicted of narcotics violations in 1959 and sentenced to 15 years in prison.[124] Valachi's motivations for becoming an informer had been the subject of some debate: Valachi claimed to be testifying as a public service and to expose a powerful criminal organization that he had blamed for ruining his life, but it is also possible he was hoping for government protection as part of a plea bargain in which he was sentenced to life imprisonment instead of the death penalty for a murder, which he had committed in 1962 while in prison for his narcotics violation.[124] Valachi murdered a man in prison who he feared mob boss, and fellow prisoner, Vito Genovese had ordered to kill him. Valachi and Genovese were both serving sentences for heroin trafficking.[125] On June 22, 1962, using a pipe left near some construction work, Valachi bludgeoned an inmate to death who he had mistaken for Joseph DiPalermo, a Mafia member who he believed had been contracted to kill him.[124] After time with FBI handlers, Valachi came forward with a story of Genovese giving him a kiss on the cheek, which he took as a \"kiss of death\".[126][127][128] A $100,000 bounty for Valachi's death had been placed by Genovese.[129] Although Valachi's disclosures never led directly to the prosecution of any Mafia leaders, he provided many details of the history of the Mafia, operations and rituals, aided in the solution of several unsolved murders, and named many members and the major crime families. The trial exposed American organized crime to the world through Valachi's televised testimony.[134] As part of the Mafia Commission Trial, on February 25, 1985, nine New York Mafia leaders were indicted for narcotics trafficking, loansharking, gambling, labor racketeering and extortion against construction companies under the Racketeer Influenced and Corrupt Organizations Act.[135] On July 1, 1985, the original nine men, with the addition of two more New York Mafia leaders, pleaded not guilty to a second set of racketeering charges as part of the trial. Prosecutors aimed to strike at all the crime families at once using their involvement in the Commission.[136] On December 2, 1985, Gambino family underboss Neil Dellacroce died of cancer.[137] Gambino boss and de factocommission head Paul Castellano was later murdered on December 16, 1985.[138] In the early 1980s, the Bonanno family were kicked off the Commission due to the Donnie Brasco infiltration, and although Rastelli was one of the men initially indicted, this removal from the Commission actually allowed Rastelli to be removed from the Commission Trial as he was later indicted on separate labor racketeering charges. Having previously lost their seat on the Commission, the Bonannos suffered less exposure than the other families in this case.[139][140] On January 20, 2011, the United States Justice Department issued 16 indictments against Northeast American Mafia families resulting in 127 charged defendants[146] and more than 110 arrests.[147] The charges included murder, murder conspiracy, loansharking, arson, robbery, narcotics trafficking, extortion, illegal gambling and labor racketeering. It has been described as the largest operation against the Mafia in U.S. history.[148] Families that have been affected included the Five Families of New York as well as the DeCavalcante crime family of New Jersey and Patriarca crime family of New England.[149] A 13-part miniseries by NBC called The Gangster Chronicles based on the rise of many major crime bosses of the 1920s and 1930s, aired in 1981.[152]The Sopranos was an award-winning HBO television show that depicted modern day American-Italian mob culture in New Jersey. Although the show is fictional, the general storyline is based on its creator David Chase's experiences growing up and interacting with New Jersey crime families. The Mafia has been the subject of multiple crime-related video games. The Mafia series by 2K Czech and Hangar 13 consists of three games that follow the story of individuals who inadvertently become caught up with one or multiple fictional Mafia families while attempting to rise in their ranks or bring them down as revenge for something they did to them. The Grand Theft Auto series by Rockstar Games also features the Mafia prominently, mainly in the games set within the fictional Liberty City (based on New York); the games set in the \"3D universe\" canon feature the Forelli, Leone and Sindacco families, while those in the \"HD universe\" have the Ancelotti, Gambetti, Lupisella, Messina and Pavano families (a reference to the Five Families), as well as the less-influential Pegorino family. In all games, the different Mafia families serve as either employers or enemies to the player. In 2006, The Godfather was released, based on the 1972 film of the same name; it spawned a sequel, itself based on the film's sequel. ^\"Mafia's arcane rituals, and much of the organization's structure, were based largely on those of the Catholic confraternities and even Freemasonry, colored by Sicilian familial traditions and even certain customs associated with military-religious orders of chivalry like the Order of Malta.\" The MafiaArchived February 3, 2010, at the Wayback Machine from bestofsicily.comArchived May 16, 2008, at the Wayback Machine United States. Congress. Senate. Select Committee on Improper Activities in the Labor or Management Field Publication. 1959. Select Committee On Improper Activities In The Labor Or Management Field, United States. Congress. Senate (1957). Investigation of Improper Activities in the Labor Or Management Field. U.S. Government Printing Office. Retrieved January 26, 2011. Frank Zito Illinois."}
{"url": "https://en.m.wikipedia.org/wiki/Spherical_Earth", "text": "Cause Earth is massive enough that the pull of gravity maintains its roughly spherical shape. Most of its deviation from spherical stems from the centrifugal force caused by rotation around its north-south axis. This force deforms the sphere into an oblate ellipsoid.[10] Formation The Solar System formed from a dust cloud that was at least partially the remnant of one or more supernovas that produced heavy elements by nucleosynthesis. Grains of matter accreted through electrostatic interaction. As they grew in mass, gravity took over in gathering yet more mass, releasing the potential energy of their collisions and in-falling as heat. The protoplanetary disk also had a greater proportion of radioactive elements than Earth today because, over time, those elements decayed. Their decay heated the early Earth even further, and continue to contribute to Earth's internal heat budget. The early Earth was thus mostly liquid. A sphere is the only stable shape for a non-rotating, gravitationally self-attracting liquid. The outward acceleration caused by Earth's rotation is greater at the equator than at the poles (where is it zero), so the sphere gets deformed into an ellipsoid, which represents the shape having the lowest potential energy for a rotating, fluid body. This ellipsoid is slightly fatter around the equator than a perfect sphere would be. Earth's shape is also slightly lumpy because it is composed of different materials of different densities that exert slightly different amounts of gravitational force per volume. The liquidity of a hot, newly formed planet allows heavier elements to sink down to the middle and forces lighter elements closer to the surface, a process known as planetary differentiation. This event is known as the iron catastrophe; the most abundant heavier elements were iron and nickel, which now form the Earth's core. Later shape changes and effects Though the surface rocks of Earth have cooled enough to solidify, the outer core of the planet is still hot enough to remain liquid. Energy is still being released; volcanic and tectonic activity has pushed rocks into hills and mountains and blown them out of calderas. Meteors also cause impact craters and surrounding ridges. However, if the energy release from these processes halts, then they tend to erode away over time and return toward the lowest potential-energy curve of the ellipsoid. Weather powered by solar energy can also move water, rock, and soil to make Earth slightly out of round. Earth undulates as the shape of its lowest potential energy changes daily due to the gravity of the Sun and Moon as they move around with respect to Earth. This is what causes tides in the oceans' water, which can flow freely along the changing potential. History of concept and measurement The spherical shape of the Earth was known and measured by astronomers, mathematicians, and navigators from a variety of literate ancient cultures, including the Hellenic World, and Ancient India. Greek ethnographer Megasthenes, c. 300 BC, has been interpreted as stating that the contemporary Brahmans of India believed in a spherical Earth as the center of the universe.[11] The knowledge of the Greeks was inherited by Ancient Rome, and Christian and Islamic realms in the Middle Ages. Circumnavigation of the world in the Age of Discovery provided direct evidence. Improvements in transportation and other technologies refined estimations of the size of the Earth, and helped spread knowledge of it. Measurement and representation Geodesy, also called geodetics, is the scientific discipline that deals with the measurement and representation of Earth, its gravitational field and geodynamic phenomena (polar motion, Earth tides, and crustal motion) in three-dimensional time-varying space. Geodesy is primarily concerned with positioning and the gravity field and geometrical aspects of their temporal variations, although it can also include the study of Earth's magnetic field. Especially in the German speaking world, geodesy is divided into geomensuration (\"Erdmessung\" or \"höhere Geodäsie\"), which is concerned with measuring Earth on a global scale, and surveying (\"Ingenieurgeodäsie\"), which is concerned with measuring parts of the surface. as the shape of Earth's land surface as it rises above and falls below the sea. As the science of geodesy measured Earth more accurately, the shape of the geoid was first found not to be a perfect sphere but to approximate an oblate spheroid, a specific type of ellipsoid. More recent[when?] measurements have measured the geoid to unprecedented accuracy, revealing mass concentrations beneath Earth's surface. Evidence The roughly spherical shape of Earth can be empirically evidenced by many different types of observation, ranging from ground level, flight, or orbit. The spherical shape causes a number of effects and phenomena that combined disprove flat Earth beliefs. These include the visibility of distant objects on Earth's surface; lunar eclipses; appearance of the Moon; observation of the sky from altitude; observation of certain fixed stars from different locations; observing the Sun; surface navigation; grid distortion on a spherical surface; weather systems; gravity; and modern technology."}
{"url": "https://en.m.wikipedia.org/wiki/Vitruvian_Man", "text": "Vitruvian Man The Vitruvian Man (Italian: L'uomo vitruviano; [ˈlwɔːmovitruˈvjaːno]) is a drawing by the Italian Renaissance artist and scientist Leonardo da Vinci, dated to c. 1490. Inspired by the writings of the ancient Roman architect Vitruvius, the drawing depicts a nude man in two superimposed positions with his arms and legs apart and inscribed in both a circle and square. It was described by the art historian Carmen C. Bambach as \"justly ranked among the all-time iconic images of Western civilization\".[1] Although not the only known drawing of a man inspired by the writings of Vitruvius, the work is a unique synthesis of artistic and scientific ideals and often considered an archetypal representation of the High Renaissance. The drawing represents Leonardo's conception of ideal body proportions, originally derived from Vitruvius but influenced by his own measurements, the drawings of his contemporaries, and the De pictura treatise by Leon Battista Alberti. Leonardo produced the Vitruvian Man in Milan and the work was probably passed to his student Francesco Melzi. It later came into the possession of Venanzio de Pagave, who convinced the engraver Carlo Giuseppe Gerli to include it in a book of Leonardo's drawings, which widely disseminated the previously little-known image. It was later owned by Giuseppe Bossi, who wrote early scholarship on it, and eventually sold to the Gallerie dell'Accademia of Venice in 1822, where it has remained since. Due to its sensitivity to light, the drawing rarely goes on public display, but it was borrowed by the Louvre in 2019 for their exhibition marking the 500th anniversary of Leonardo's death. Contents The drawing is described by Leonardo's notes as Le proporzioni del corpo umano secondo Vitruvio,[2] variously translated as The Proportions of the Human Figure after Vitruvius,[3] or Proportional Study of a Man in the Manner of Vitruvius.[4] It is much better known as the Vitruvian Man.[2] The art historian Carlo Pedretti lists it as Homo Vitruvius, study of proportions with the human figure inscribed in a circle and a square, and later as simply Homo Vitruvius.[5] The drawing was executed primarily with pen and light-brown ink, while there are traces of brown wash (watercolor).[6][n 1] The paper measures 34.4 cm × 25.5 cm (13.5 in × 10.0 in), larger than most of Leonardo's folio manuscript sheets,[n 2] while the paper itself was originally made somewhat unevenly, given its irregular edges.[1] Close examination of the drawing reveals that it was meticulously prepared, and is devoid of \"sketchy and tentative\" lines.[8] Leonardo used metalpoint with a calipers and compass to make precise lines, and small tick marks were used for measurements.[6][8] These compass marks demonstrate an inner structure of \"measured intervals\" which is displayed in tandem with the general structure created by the geometric figures.[9] The Vitruvian Man depicts a nude man facing forward and surrounded by a square, while superimposed on a circle.[2] The man is portrayed in different stances simultaneously: His arms are stretched above his shoulders and then perpendicular to them, while his legs are together and also spread out along the circle's base.[2] The scholar Carlo Vecce notes that this approach displays multiple phases of movement at once, akin to a photograph.[10] The man's fingers and toes are arranged carefully as to not breach the surrounding shapes.[9] Commentators often note that Leonardo went out of his way to create an artistic depiction of the man, rather than a simple portrayal.[11][12] According to the biographer Walter Isaacson, the use of delicate lines, an intimate stare, and intricate hair curls, \"weaves together the human and the divine\".[11] Pedretti notes close similarities between the man and the angel of Leonardo's earlier Annunciation painting.[12] Vitruvius, the architect, says in his architectural work that the measurements of man are in nature distributed in this manner, that is 4 fingers make a palm, 4 palms make a foot, 6 palms make a cubit, 4 cubits make a man, 4 cubits make a footstep, 24 palms make a man and these measures are in his buildings. If you open your legs enough that your head is lowered by 1/14 of your height and raise your arms enough that your extended fingers touch the line of the top of your head, let you know that the center of the ends of the open limbs will be the navel, and the space between the legs will be an equilateral triangle[13] The length of the outspread arms is equal to the height of the man. From the hairline to the bottom of the chin is one-tenth of the height of the man. From below the chin to the top of the head is one-eighth of the height of the man. From above the chest to the top of the head is one-sixth of the height of the man. From above the chest to the hairline is one-seventh of the height of a man. From the chest to the head is a quarter of the height of the man. The maximum width of the shoulders contains a quarter of the man. From the elbow to the tip of the hand is a quarter of the height of a man; the distance from the elbow to the armpit is one-eighth of the height of the man; the length of the hand is one-tenth of the man. The virile member is at the half height of the man. The foot is one-seventh of the man. From below the foot to below the knee is a quarter of the man. From below the knee to the root of the member is a quarter of the man. The distances from the chin to the nose and the hairline and the eyebrows are equal to the ears and one-third of the face[14] The moderately successful architect and engineer Vitruvius lived from c. 80 – c. 20 BCE, primarily in the Roman Republic.[15] He is best known for authoring De architectura (On Architecture), later called the Ten Books on Architecture, which is the only substantial architecture treatise that survives from antiquity.[16] The work's third volume includes a discussion concerning body proportions,[1] where the figures of a man in a circle and a square are respectively referred to as homo ad circulum, homo ad quadratum.[15] Vitruvius explained that: In a temple there ought to be harmony in the symmetrical relations of the different parts to the whole. In the human body, the central point is the navel. If a man is placed flat on his back, with his hands and feet extended, and a compass centered at his navel, his fingers and toes will touch the circumference of a circle thereby described. And just as the human body yields a circular outline, so too a square may be found from it. For if we measure the distance from the soles of the feet to the top of the head, and then apply that measure to the outstretched arms, the breadth will be found to be the same as the height, as in the case of a perfect square. 19th-century historians often postulated that Leonardo had no substantial inspiration from the ancient world, propagating his stance as a 'modern genius' who rejected all of classicism.[18] This has been heavily disproven by many documented accounts from Leonardo's colleagues or records of him either owning, reading, and being influenced by writings from antiquity.[18] The treatise of Vitruvius was long lost, but rediscovered in the 15th century by Poggio Bracciolini among works such as De Rerum natura.[16] Many artists attempted to design figures which would satisfy Vitruvius' claims, with the earliest being three such images by Francesco di Giorgio Martini around the 1470s.[19][2] Leonardo may have been influenced by the architect Giacomo Andrea, with whom he records as having dined within 1490.[20] Andrea created his own Vitruvian Man drawing that year, which was unknown to scholars until the 1980s.[20] Leonardo's version of the Vitruvian Man corrected inaccuracies in Vitruvius's account, particularly related to the head, due to use of book two of the De pictura by Leon Battista Alberti.[1] Earlier drawings of the same subject \"assumed that the circle and square should be centered around the navel\", akin to Vitruvius's account, while Leonardo made the scheme work by using the man's genitals as the center of the square, and the navel as the center of the circle.[9] It is likely that Leonardo's drawings dated to 1487–1490, and entitled The proportions of the arm, were related to the Vitruvian Man, possibly serving as preparatory sketches.[21] Some commentators have speculated that Leonardo incorporated the golden ratio in the drawing, possibly due to his illustrations of Luca Pacioli's Divina proportione, largely plagiarized from Piero della Francesca,[22][n 3] concerning the ratio.[23][24] However, the Vitruvian Man is likely to have been drawn before Leonardo met Pacioli, and there has been doubt over the accuracy of such an observation.[25] As architectural scholar Vitor Murtinho explains, a circle tangent to the base of a square, with the radius and square sides related by the golden ratio, would pass exactly through the top two corners of the square, unlike Leonardo's drawing. He suggests instead constructions based on a regular octagon or on the vesica piscis.[25] Leonardo's drawing is almost always dated to around 1490 during his First Milanese period.[15][26] The exact dating is not completely agreed upon and earlier generations of art historians, including Arthur E. Popham, frequently dated the work anywhere from 1485 to 1490.[1] Two leading art historians differ in this respect; Martin Kemp gives c. 1487,[4][n 4] while Carmen C. Bambach contends that the earliest possible date—which \"one may not entirely discount\"—is 1488.[1] Bambach, in addition to Pedretti, Giovanna Nepi Scirè and Annalisa Perissa Torrini give a slightly broader range of c. 1490–1491.[7] Bambach explains that this range fits \"best with the manner of exact, engraving-like parallel hatching contained within robust pen-and-ink outlines, over traces of lead paint, stylus-ruling, and compass composition\".[1] After Leonardo's death, the drawing most likely passed to his student Francesco Melzi (1491–1570),[1] who was bequeathed most of Leonardo's possessions.[27] From then on, the drawing's provenance history is almost certain: it found its way to Cesare Monti (1594–1650), was passed to his heir Anna Luisa Monti, then to the De Page family, first Venanzio de Pagave [Wikidata] (in 1777) and then his son Gaudenzio de Page.[1][28] While owned by the elder De Page, he convinced the engraver Carlo Giuseppe Gerli to publish a book of Leonardo's drawings, which would be the first widespread dissemination of the Vitruvian Man and many other Leonardo drawings.[29] The younger de Page sold the drawing to Giuseppe Bossi, who described, discussed, and illustrated it in the fourth chapter of his 1810 monograph on Leonardo's The Last Supper, Del Cenacolo di Leonardo da Vinci (On The Last Supper of Leonardo da Vinci).[30] This chapter was published as a stand-alone study the next year Delle opinion di Leonardo da Vinci intorno alla simmetria de' corpi umani (On the opinions of Leonardo da Vinci regarding the symmetry of human bodies).[30] After Bossi's death in 1815, the drawing was sold to the abbot Luigi Celotti in 1818, and entered into the Venetian Gallerie dell'Accademia's collection in 1822, where it has since remained.[1] Because of its high artistic quality and its well-recorded history of provenance, Leonardo's authorship of the Vitruvian Man has never been doubted.[1] The Vitruvian Man is rarely displayed as extended exposure to light would cause fading; it is kept on the fourth floor of the Gallerie dell'Accademia, in a locked room.[31] In 2019, the Louvre requested to borrow the drawing for their monumental Léonard de Vinci exhibition, which celebrated the 500th anniversary of the artist’s death.[32] They faced substantial resistance from the heritage group Italia Nostra, who contended that the drawing was too fragile to be transported, and filed a lawsuit.[33] At a hearing on 16 October 2019, a judge ruled that the group had not proven their claim, but set a maximum amount of light for the drawing to be exposed to as well as a subsequent rest period to offset its overall exposure to light.[34] The Louvre promised to lend paintings by Raphael to Italy for his own 500th death anniversary; Italy's Minister for Cultural Affairs Dario Franceschini stated that \"Now a great cultural operation can start between Italy and France on the two exhibitions about Leonardo in France and Raphael in Rome.\"[35] In 2022, the Gallerie dell’Accademia, which owns the drawing, sued German jigsaw puzzle manufacturer Ravensburger for reproducing the artwork in one of the company's jigsaw puzzles. Ravensburger started selling the 1,000-piece jigsaw puzzle in Italy in 2009 and in 2019 the museum sent the company a cease-and-desist letter and demanded 10% of the revenue. Ravensburger refused to comply and subsequently was sued by the museum under Italy's 2004 Cultural Heritage and Landscape Code [it] which governs reproductions of works deemed to be under Italy's cultural heritage. In its objections, the German company claimed that it had the right to reproduce the artwork because it was already in the public domain for centuries and that the reproduction occurred outside Italy and thus not subject to Italy's Cultural Heritage Code. An Italian court rejected Ravensburger's arguments and decided in favor of the museum.[36] In a ruling dated 17 November 2022, the court ordered the puzzle company to cease producing the product for commercial purposes and levied a fine of 1,500 euros for every day that the company failed to comply.[37][38][39] Licensing fees for famous artworks are an important source of income for Italian museums, and Italian law says that museums owning famous public domain works hold the copyright on those works forever and can control who is allowed to make copies and derivative works of them.[36] Patch of the Skylab 3 mission (erroneously written as 2), showing the Vitruvian Man in the center The Vitruvian Man is often considered an archetypal representative of the High Renaissance, just as Leonardo himself came to represent the archetypal 'Renaissance man'.[40] It holds a unique distinction in aligning art, mathematics, science, classicism, and naturalism.[41] The art historian Ludwig Heinrich Heydenreich, writing for Encyclopædia Britannica, states, \"Leonardo envisaged the great picture chart of the human body he had produced through his anatomical drawings and Vitruvian Man as a cosmografia del minor mondo ('cosmography of the microcosm'). He believed the workings of the human body to be an analogy, in microcosm, for the workings of the universe.\"[42] Kemp calls the drawing \"the world's most famous drawing\",[9] while Bambach describes it as \"justly rank[ing] among the all-time iconic images of Western civilization\".[1] Reflecting on its fame, Bambach further stated in 2019 that \"the endless recent fetishizing of the image by modern commerce through ubiquitous reproductions (in popular books, advertising, and the Euro coin) has kidnapped it from the realm of Renaissance drawing, making it difficult for the viewer to appreciate it as a work of nuanced, creative expression.\"[1]"}
{"url": "https://pubmed.ncbi.nlm.nih.gov/24063680/", "text": "Abstract Background: Lepidosauria (lizards, snakes, tuatara) is a globally distributed and ecologically important group of over 9,000 reptile species. The earliest fossil records are currently restricted to the Late Triassic and often dated to 227 million years ago (Mya). As these early records include taxa that are relatively derived in their morphology (e.g. Brachyrhinodon), an earlier unknown history of Lepidosauria is implied. However, molecular age estimates for Lepidosauria have been problematic; dates for the most recent common ancestor of all lepidosaurs range between approximately 226 and 289 Mya whereas estimates for crown-group Squamata (lizards and snakes) vary more dramatically: 179 to 294 Mya. This uncertainty restricts inferences regarding the patterns of diversification and evolution of Lepidosauria as a whole. Results: Here we report on a rhynchocephalian fossil from the Middle Triassic of Germany (Vellberg) that represents the oldest known record of a lepidosaur from anywhere in the world. Reliably dated to 238-240 Mya, this material is about 12 million years older than previously known lepidosaur records and is older than some but not all molecular clock estimates for the origin of lepidosaurs. Using RAG1 sequence data from 76 extant taxa and the new fossil specimens two of several calibrations, we estimate that the most recent common ancestor of Lepidosauria lived at least 242 Mya (238-249.5), and crown-group Squamata originated around 193 Mya (176-213). Conclusion: A Early/Middle Triassic date for the origin of Lepidosauria disagrees with previous estimates deep within the Permian and suggests the group evolved as part of the faunal recovery after the end-Permain mass extinction as the climate became more humid. Our origin time for crown-group Squamata coincides with shifts towards warmer climates and dramatic changes in fauna and flora. Most major subclades within Squamata originated in the Cretaceous postdating major continental fragmentation. The Vellberg fossil locality is expected to become an important resource for providing a more balanced picture of the Triassic and for bridging gaps in the fossil record of several other major vertebrate groups. Maximum clade credibility tree (BEAST) with constrained nodes labelled according to Table 2 . Tectonic maps were redrawn from Blakey [58]. CPE indicates the Carnian Pluvial Event [61]. Calibrated nodes are numbered X and 1–12 as in Table 1 but Y, crown Archosauria, is not shown. For results from the MrBayes analysis, including posterior probabilities of separate nodes, see Additional file 5. Figure 5 The phylogenetic relationships and fossil… Figure 5 The phylogenetic relationships and fossil record of early lepidosaurs compared to molecular divergence… Figure 5 The phylogenetic relationships and fossil record of early lepidosaurs compared to molecular divergence estimates. Estimates for the origin of Lepidosauria based on previous molecular studies are listed on the right in blue with short arrows. Estimates for the origin of crown group Squamata are listed on the right in red with long arrows. Timescale based on Gradstein et al. [47]. Fossil records include those described, or referred to, in Butler et al. [105], Carroll [27], Clark and Hernandez [31], Colbert [30], Evans [8,9,26,33,91], Evans and Białynicka [34], Evans and Jones [5], Evans et al., [18], Fraser [22,23,136], Fraser and Benton [11], Heckert et al. [24], Nesbitt [180], Renesto [137], Reynoso [19,150], Robinson [29], Sues and Hopson [13], Sues and Olsen [12], Whiteside [15], and others listed in Evans et al. [181] and Jones et al. [10]."}
{"url": "https://en.m.wikipedia.org/wiki/Plesiosaurs", "text": "Plesiosaurs first appeared in the latest TriassicPeriod, possibly in the Rhaetian stage, about 203 million years ago.[5] They became especially common during the Jurassic Period, thriving until their disappearance due to the Cretaceous–Paleogene extinction event at the end of the Cretaceous Period, about 66 million years ago. They had a worldwide oceanic distribution, and some species at least partly inhabited freshwater environments.[6] Plesiosaurs were among the first fossil reptiles discovered. In the beginning of the nineteenth century, scientists realised how distinctive their build was and they were named as a separate order in 1835. The first plesiosaurian genus, the eponymous Plesiosaurus, was named in 1821. Since then, more than a hundred valid species have been described. In the early twenty-first century, the number of discoveries has increased, leading to an improved understanding of their anatomy, relationships and way of life. Plesiosaurs had a broad flat body and a short tail. Their limbs had evolved into four long flippers, which were powered by strong muscles attached to wide bony plates formed by the shoulder girdle and the pelvis. The flippers made a flying movement through the water. Plesiosaurs breathed air, and bore live young; there are indications that they were warm-blooded. Plesiosaurs showed two main morphological types. Some species, with the \"plesiosauromorph\" build, had (sometimes extremely) long necks and small heads; these were relatively slow and caught small sea animals. Other species, some of them reaching a length of up to seventeen metres, had the \"pliosauromorph\" build with a short neck and a large head; these were apex predators, fast hunters of large prey. The two types are related to the traditional strict division of the Plesiosauria into two suborders, the long-necked Plesiosauroidea and the short-neck Pliosauroidea. Modern research, however, indicates that several \"long-necked\" groups might have had some short-necked members or vice versa. Therefore, the purely descriptive terms \"plesiosauromorph\" and \"pliosauromorph\" have been introduced, which do not imply a direct relationship. \"Plesiosauroidea\" and \"Pliosauroidea\" today have a more limited meaning. The term \"plesiosaur\" is properly used to refer to the Plesiosauria as a whole, but informally it is sometimes meant to indicate only the long-necked forms, the old Plesiosauroidea. Skeletal elements of plesiosaurs are among the first fossils of extinct reptiles recognised as such.[7] In 1605, Richard Verstegen of Antwerp illustrated in his A Restitution of Decayed Intelligence plesiosaur vertebrae that he referred to fishes and saw as proof that Great Britain was once connected to the European continent.[8] The Welshman Edward Lhuyd in his Lithophylacii Brittannici Ichnographia from 1699 also included depictions of plesiosaur vertebrae that again were considered fish vertebrae or Ichthyospondyli.[9] Other naturalists during the seventeenth century added plesiosaur remains to their collections, such as John Woodward; these were only much later understood to be of a plesiosaurian nature and are today partly preserved in the Sedgwick Museum.[7] In 1719, William Stukeley described a partial skeleton of a plesiosaur, which had been brought to his attention by the great-grandfather of Charles Darwin, Robert Darwin of Elston. The stone plate came from a quarry at Fulbeck in Lincolnshire and had been used, with the fossil at its underside, to reinforce the slope of a watering-hole in Elston in Nottinghamshire. After the strange bones it contained had been discovered, it was displayed in the local vicarage as the remains of a sinner drowned in the Great Flood. Stukely affirmed its \"diluvial\" nature but understood it represented some sea creature, perhaps a crocodile or dolphin.[10] The specimen is today preserved in the Natural History Museum, its inventory number being BMNH R.1330. It is the earliest discovered more or less complete fossil reptile skeleton in a museum collection. It can perhaps be referred to Plesiosaurus dolichodeirus.[7] As this illustration shows, Conybeare by 1824 had gained a basically correct understanding of plesiosaur anatomy. During the eighteenth century, the number of English plesiosaur discoveries rapidly increased, although these were all of a more or less fragmentary nature. Important collectors were the reverends William Mounsey and Baptist Noel Turner, active in the Vale of Belvoir, whose collections were in 1795 described by John Nicholls in the first part of his The History and Antiquities of the County of Leicestershire.[11] One of Turner's partial plesiosaur skeletons is still preserved as specimen BMNH R.45 in the British Museum of Natural History; this is today referred to Thalassiodracon.[7] In the early nineteenth century, plesiosaurs were still poorly known and their special build was not understood. No systematic distinction was made with ichthyosaurs, so the fossils of one group were sometimes combined with those of the other to obtain a more complete specimen. In 1821, a partial skeleton discovered in the collection of Colonel Thomas James Birch,[12] was described by William Conybeare and Henry Thomas De la Beche, and recognised as representing a distinctive group. A new genus was named, Plesiosaurus. The generic name was derived from the Greek πλήσιος, plèsios, \"closer to\" and the Latinised saurus, in the meaning of \"saurian\", to express that Plesiosaurus was in the Chain of Being more closely positioned to the Sauria, particularly the crocodile, than Ichthyosaurus, which had the form of a more lowly fish.[13] The name should thus be rather read as \"approaching the Sauria\" or \"near reptile\" than as \"near lizard\".[14] Parts of the specimen are still present in the Oxford University Museum of Natural History.[7] Soon afterwards, the morphology became much better known. In 1823, Thomas Clark reported an almost complete skull, probably belonging to Thalassiodracon, which is now preserved by the British Geological Survey as specimen BGS GSM 26035.[7] The same year, commercial fossil collector Mary Anning and her family uncovered an almost complete skeleton at Lyme Regis in Dorset, England, on what is today called the Jurassic Coast. It was acquired by the Duke of Buckingham, who made it available to the geologist William Buckland. He in turn let it be described by Conybeare on 24 February 1824 in a lecture to the Geological Society of London,[15] during the same meeting in which for the first time a dinosaur was named, Megalosaurus. The two finds revealed the unique and bizarre build of the animals, in 1832 by Professor Buckland likened to \"a sea serpent run through a turtle\". In 1824, Conybeare also provided a specific name to Plesiosaurus: dolichodeirus, meaning \"longneck\". In 1848, the skeleton was bought by the British Museum of Natural History and catalogued as specimen BMNH 22656.[7] When the lecture was published, Conybeare also named a second species: Plesiosaurus giganteus. This was a short-necked form later assigned to the Pliosauroidea.[16] Plesiosaurs became better known to the general public through two lavishly illustrated publications by the collector Thomas Hawkins: Memoirs of Ichthyosauri and Plesiosauri of 1834[17] and The Book of the Great Sea-Dragons of 1840. Hawkins entertained a very idiosyncratic view of the animals,[18] seeing them as monstrous creations of the devil, during a pre-Adamitic phase of history.[19] Hawkins eventually sold his valuable and attractively restored specimens to the British Museum of Natural History.[20] During the first half of the nineteenth century, the number of plesiosaur finds steadily increased, especially through discoveries in the sea cliffs of Lyme Regis. Sir Richard Owen alone named nearly a hundred new species. The majority of their descriptions were, however, based on isolated bones, without sufficient diagnosis to be able to distinguish them from the other species that had previously been described. Many of the new species described at this time have subsequently been invalidated. The genus Plesiosaurus is particularly problematic, as the majority of the new species were placed in it so that it became a wastebasket taxon. Gradually, other genera were named. Hawkins had already created new genera, though these are no longer seen as valid. In 1841, Owen named Pliosaurus brachydeirus. Its etymology referred to the earlier Plesiosaurus dolichodeirus as it is derived from πλεῖος, pleios, \"more fully\", reflecting that according to Owen it was closer to the Sauria than Plesiosaurus. Its specific name means \"with a short neck\".[21] Later, the Pliosauridae were recognised as having a morphology fundamentally different from the plesiosaurids. The family Plesiosauridae had already been coined by John Edward Gray in 1825.[22] In 1835, Henri Marie Ducrotay de Blainville named the order Plesiosauria itself.[23] In 1867, physician Theophilus Turner near Fort Wallace in Kansas uncovered a plesiosaur skeleton, which he donated to Cope.[24] Cope attempted to reconstruct the animal on the assumption that the longer extremity of the vertebral column was the tail, the shorter one the neck. He soon noticed that the skeleton taking shape under his hands had some very special qualities: the neck vertebrae had chevrons and with the tail vertebrae the joint surfaces were orientated back to front.[25] Excited, Cope concluded to have discovered an entirely new group of reptiles: the Streptosauria or \"Turned Saurians\", which would be distinguished by reversed vertebrae and a lack of hindlimbs, the tail providing the main propulsion.[26] After having published a description of this animal,[27] followed by an illustration in a textbook about reptiles and amphibians,[28] Cope invited Marsh and Joseph Leidy to admire his new Elasmosaurus platyurus. Having listened to Cope's interpretation for a while, Marsh suggested that a simpler explanation of the strange build would be that Cope had reversed the vertebral column relative to the body as a whole. When Cope reacted indignantly to this suggestion, Leidy silently took the skull and placed it against the presumed last tail vertebra, to which it fitted perfectly: it was in fact the first neck vertebra, with still a piece of the rear skull attached to it.[29] Mortified, Cope tried to destroy the entire edition of the textbook and, when this failed, immediately published an improved edition with a correct illustration but an identical date of publication.[30] He excused his mistake by claiming that he had been misled by Leidy himself, who, describing a specimen of Cimoliasaurus, had also reversed the vertebral column.[31] Marsh later claimed that the affair was the cause of his rivalry with Cope: \"he has since been my bitter enemy\". Both Cope and Marsh in their rivalry named many plesiosaur genera and species, most of which are today considered invalid.[32] Around the turn of the century, most plesiosaur research was done by a former student of Marsh, Professor Samuel Wendell Williston. In 1914, Williston published his Water reptiles of the past and present.[33] Despite treating sea reptiles in general, it would for many years remain the most extensive general text on plesiosaurs.[34] In 2013, a first modern textbook was being prepared by Olivier Rieppel. During the middle of the twentieth century, the USA remained an important centre of research, mainly through the discoveries of Samuel Paul Welles. Whereas during the nineteenth and most of the twentieth century, new plesiosaurs were described at a rate of three or four novel genera each decade, the pace suddenly picked up in the 1990s, with seventeen plesiosaurs being discovered in this period. The tempo of discovery accelerated in the early twenty-first century, with about three or four plesiosaurs being named each year.[35] This implies that about half of the known plesiosaurs are relatively new to science, a result of a far more intense field research. Some of this is taking place away from the traditional areas, e.g. in new sites developed in New Zealand, Argentina, Chile,[36]Norway, Japan, China and Morocco, but the locations of the more original discoveries have proven to be still productive, with important new finds in England and Germany. Some of the new genera are a renaming of already known species, which were deemed sufficiently different to warrant a separate genus name. In 2002, the \"Monster of Aramberri\" was announced to the press. Discovered in 1982 at the village of Aramberri, in the northern Mexican state of Nuevo León, it was originally classified as a dinosaur. The specimen is actually a very large plesiosaur, possibly reaching 15 m (49 ft) in length. The media published exaggerated reports claiming it was 25 metres (82 ft) long, and weighed up to 150,000 kilograms (330,000 lb), which would have made it among the largest predators of all time.[37][38] In 2004, what appeared to be a completely intact juvenile plesiosaur was discovered, by a local fisherman, at Bridgwater Bay National Nature Reserve in Somerset, UK. The fossil, dating from 180 million years ago as indicated by the ammonites associated with it, measured 1.5 metres (4 ft 11 in) in length, and may be related to Rhomaleosaurus. It is probably the best preserved specimen of a plesiosaur yet discovered.[39][40][41] In 2008, fossil remains of an undescribed plesiosaur that was named Predator X, now known as Pliosaurus funkei, were unearthed in Svalbard.[45] It had a length of 12 m (39 ft), and its bite force of 149 kilonewtons (33,000 lbf) is one of the most powerful known.[46] In December 2017, a large skeleton of a plesiosaur was found in the continent of Antarctica, the oldest creature on the continent, and the first of its species in Antarctica.[47] Not only has the number of field discoveries increased, but also, since the 1950s, plesiosaurs have been the subject of more extensive theoretical work. The methodology of cladistics has, for the first time, allowed the exact calculation of their evolutionary relationships. Several hypotheses have been published about the way they hunted and swam, incorporating general modern insights about biomechanics and ecology. The many recent discoveries have tested these hypotheses and given rise to new ones.[original research?] The Plesiosauria have their origins within the Sauropterygia, a group of perhaps archelosaurian reptiles that returned to the sea. An advanced sauropterygian subgroup, the carnivorous Eusauropterygia with small heads and long necks, split into two branches during the Upper Triassic. One of these, the Nothosauroidea, kept functional elbow and knee joints; but the other, the Pistosauria, became more fully adapted to a sea-dwelling lifestyle. Their vertebral column became stiffer and the main propulsion while swimming no longer came from the tail but from the limbs, which changed into flippers.[48] The Pistosauria became warm-blooded and viviparous, giving birth to live young.[49] Early, basal, members of the group, traditionally called \"pistosaurids\", were still largely coastal animals. Their shoulder girdles remained weak, their pelves could not support the power of a strong swimming stroke, and their flippers were blunt. Later, a more advanced pistosaurian group split off: the Plesiosauria. These had reinforced shoulder girdles, flatter pelves, and more pointed flippers. Other adaptations allowing them to colonise the open seas included stiff limb joints; an increase in the number of phalanges of the hand and foot; a tighter lateral connection of the finger and toe phalanx series, and a shortened tail.[50][51] From the earliest Jurassic, the Hettangian stage, a rich radiation of plesiosaurs is known, implying that the group must already have diversified in the Late Triassic; of this diversification, however, only a few very basal forms have been discovered. The subsequent evolution of the plesiosaurs is very contentious. The various cladistic analyses have not resulted in a consensus about the relationships between the main plesiosaurian subgroups. Traditionally, plesiosaurs have been divided into the long-necked Plesiosauroidea and the short-necked Pliosauroidea. However, modern research suggests that some generally long-necked groups might have had short-necked members. To avoid confusion between the phylogeny, the evolutionary relationships, and the morphology, the way the animal is built, long-necked forms are therefore called \"plesiosauromorph\" and short-necked forms are called \"pliosauromorph\", without the \"plesiosauromorph\" species necessarily being more closely related to each other than to the \"pliosauromorph\" forms.[52] The latest common ancestor of the Plesiosauria was probably a rather small short-necked form. During the earliest Jurassic, the subgroup with the most species was the Rhomaleosauridae, a possibly very basal split-off of species which were also short-necked. Plesiosaurs in this period were at most five metres (sixteen feet) long. By the Toarcian, about 180 million years ago, other groups, among them the Plesiosauridae, became more numerous and some species developed longer necks, resulting in total body lengths of up to ten metres (33 feet).[53] In the middle of the Jurassic, very large Pliosauridae evolved. These were characterized by a large head and a short neck, such as Liopleurodon and Simolestes. These forms had skulls up to three metres (ten feet) long and reached a length of up to seventeen metres (56 feet) and a weight of ten tonnes. The pliosaurids had large, conical teeth and were the dominant marine carnivores of their time. During the same time, approximately 160 million years ago, the Cryptoclididae were present, shorter species with a long neck and a small head.[54] The Leptocleididae radiated during the Early Cretaceous. These were rather small forms that, despite their short necks, might have been more closely related to the Plesiosauridae than to the Pliosauridae. Later in the Early Cretaceous, the Elasmosauridae appeared; these were among the longest plesiosaurs, reaching up to fifteen metres (fifty feet) in length due to very long necks containing as many as 76 vertebrae, more than any other known vertebrate. Pliosauridae were still present as is shown by large predators, such as Kronosaurus.[54] At the beginning of the Late Cretaceous, the Ichthyosauria became extinct; perhaps a plesiosaur group evolved to fill their niches: the Polycotylidae, which had short necks and peculiarly elongated heads with narrow snouts. During the Late Cretaceous, the elasmosaurids still had many species.[54] All plesiosaurs became extinct as a result of the K-T event at the end of the Cretaceous period, approximately 66 million years ago.[55] Another way to define a clade is to let it consist of all species more closely related to a certain species that one in any case wishes to include in the clade than to another species that one to the contrary desires to exclude. Such a clade is called a \"stem clade\". Such a definition has the advantage that it is easier to include all species with a certain morphology. Plesiosauria was in 2010 by Hillary Ketchum and Roger Benson defined as such a stem-based taxon: \"all taxa more closely related to Plesiosaurus dolichodeirus and Pliosaurus brachydeirus than to Augustasaurus hagdorni\". Ketchum and Benson (2010) also coined a new clade Neoplesiosauria, a node-based taxon that was defined by as \"Plesiosaurus dolichodeirus, Pliosaurus brachydeirus, their most recent common ancestor and all of its descendants\".[54] The clade Neoplesiosauria very likely is materially identical to Plesiosauria sensu Druckenmiller & Russell, thus would designate exactly the same species, and the term was meant to be a replacement of this concept. Benson et al. (2012) found the traditional Pliosauroidea to be paraphyletic in relation to Plesiosauroidea. Rhomaleosauridae was found to be outside Neoplesiosauria, but still within Plesiosauria. The early Carnian pistosaur Bobosaurus was found to be one step more advanced than Augustasaurus in relation to the Plesiosauria and therefore it represented by definition the basalmost known plesiosaur. This analysis focused on basal plesiosaurs and therefore only one derived pliosaurid and one cryptoclidian were included, while elasmosaurids were not included at all. A more detailed analysis published by both Benson and Druckenmiller in 2014 was not able to resolve the relationships among the lineages at the base of Plesiosauria.[57] In general, plesiosaurians varied in adult length from between 1.5 metres (4.9 ft) to about 15 metres (49 ft). The group thus contained some of the largest marine apex predators in the fossil record, roughly equalling the longest ichthyosaurs, mosasaurids, sharks and toothed whales in size. Some plesiosaurian remains, such as a 2.875 metres (9.43 ft) long set of highly reconstructed and fragmentary lower jaws preserved in the Oxford University Museum and referable to Pliosaurus rossicus (previously referred to Stretosaurus[58] and Liopleurodon), indicated a length of 17 metres (56 ft). However, it was recently argued that its size cannot be currently determined due to their being poorly reconstructed and a length of 12.7 metres (42 ft) metres or less is more likely.[59] MCZ 1285, a specimen currently referable to Kronosaurus queenslandicus, from the Early Cretaceous of Australia, was estimated to have a skull length of 2.21–2.85 m (7.3–9.4 ft).[59][60] A series of neck vertebrae from the Kimmeridge Clay Formation indicate a pliosaur, probably Pliosaurus, that may have been up to 14.4 metres (47 ft) long.[61] The typical plesiosaur had a broad, flat, body and a short tail. Plesiosaurs retained their ancestral two pairs of limbs, which had evolved into large flippers.[62] Plesiosaurs were related to the earlier Nothosauridae,[63] that had a more crocodile-like body. The flipper arrangement is unusual for aquatic animals in that probably all four limbs were used to propel the animal through the water by up-and-down movements. The tail was most likely only used for helping in directional control. This contrasts to the ichthyosaurs and the later mosasaurs, in which the tail provided the main propulsion.[64] To power the flippers, the shoulder girdle and the pelvis had been greatly modified, developing into broad bone plates at the underside of the body, which served as an attachment surface for large muscle groups, able to pull the limbs downwards. In the shoulder, the coracoid had become the largest element covering the major part of the breast. The scapula was much smaller, forming the outer front edge of the trunk. To the middle, it continued into a clavicle and finally a small interclavicular bone. As with most tetrapods, the shoulder joint was formed by the scapula and coracoid. In the pelvis, the bone plate was formed by the ischium at the rear and the larger pubic bone in front of it. The ilium, which in land vertebrates bears the weight of the hindlimb, had become a small element at the rear, no longer attached to either the pubic bone or the thighbone. The hip joint was formed by the ischium and the pubic bone. The pectoral and pelvic plates were connected by a plastron, a bone cage formed by the paired belly ribs that each had a middle and an outer section. This arrangement immobilised the entire trunk.[64] To become flippers, the limbs had changed considerably. The limbs were very large, each about as long as the trunk. The forelimbs and hindlimbs strongly resembled each other. The humerus in the upper arm, and the femur in the upper leg, had become large flat bones, expanded at their outer ends. The elbow joints and the knee joints were no longer functional: the lower arm and the lower leg could not flex in relation to the upper limb elements, but formed a flat continuation of them. All outer bones had become flat supporting elements of the flippers, tightly connected to each other and hardly able to rotate, flex, extend or spread. This was true of the ulna, radius, metacarpals and fingers, as well of the tibia, fibula, metatarsals and toes. Furthermore, in order to elongate the flippers, the number of phalanges had increased, up to eighteen in a row, a phenomenon called hyperphalangy. The flippers were not perfectly flat, but had a lightly convexly curved top profile, like an airfoil, to be able to \"fly\" through the water.[64] While plesiosaurs varied little in the build of the trunk, and can be called \"conservative\" in this respect, there were major differences between the subgroups as regards the form of the neck and the skull. Plesiosaurs can be divided into two major morphological types that differ in head and neck size. \"Plesiosauromorphs\", such as Cryptoclididae, Elasmosauridae, and Plesiosauridae, had long necks and small heads. \"Pliosauromorphs\", such as the Pliosauridae and the Rhomaleosauridae, had shorter necks with a large, elongated head. The neck length variations were not caused by an elongation of the individual cervical vertebrae, but an increase in their number. Elasmosaurus has seventy-two neck vertebrae; the known record is held by the elasmosaurid Albertonectes, with seventy-six cervicals.[65] The large number of joints suggested to early researchers that the neck must have been very flexible; indeed, a swan-like curvature of the neck was assumed to be possible - in Icelandic, plesiosaurs are even called Svaneðlur, \"swan lizards\". However, modern research has confirmed an earlier conjecture of Williston that the long plate-like spines on top of the vertebrae, the processus spinosi, strongly limited vertical neck movement. Although horizontal curving was less restricted, in general, the neck must have been rather stiff and certainly was incapable of being bent into serpentine coils. This is even more true of the short-necked \"pliosauromophs\", which had as few as eleven cervical vertebrae. With early forms, the amphicoelous or amphiplat neck vertebrae bore double-headed neck ribs; later forms had single-headed ribs. In the remainder of the vertebral column, the number of dorsal vertebrae varied between about nineteen and thirty-two; of the sacral vertebrae, between two and six, and of the tail vertebrae, between about twenty-one and thirty-two. These vertebrae still possessed the original processes inherited from the land-dwelling ancestors of the Sauropterygia and had not been reduced to fish-like simple discs, as happened with the vertebrae of ichthyosaurs. The tail vertebrae possessed chevron bones. The dorsal vertebrae of plesiosaurs are easily recognisable by two large foramina subcentralia, paired vascular openings at the underside.[64] The skull of plesiosaurs showed the \"euryapsid\" condition, lacking the lower temporal fenestrae, the openings at the lower rear sides. The upper temporal fenestrae formed large openings at the sides of the rear skull roof, the attachment for muscles closing the lower jaws. Generally, the parietal bones were very large, with a midline crest, while the squamosal bones typically formed an arch, excluding the parietals from the occiput. The eye sockets were large, in general pointing obliquely upwards; the pliosaurids had more sideways directed eyes. The eyes were supported by scleral rings, the form of which shows that they were relatively flat, an adaptation to diving. The anteriorly placed internal nostrils, the choanae, have palatal grooves to channel water, the flow of which would be maintained by hydrodynamic pressure over the posteriorly placed, in front of the eye sockets, external nares during swimming. According to one hypothesis, during its passage through the nasal ducts, the water would have been 'smelled' by olfactory epithelia.[66][67] However, more to the rear, a second pair of openings is present in the palate; a later hypothesis holds that these are the real choanae and the front pair in reality represented paired salt glands.[68] The distance between the eye sockets and the nostrils was so limited because the nasal bones were strongly reduced, even absent in many species. The premaxillae directly touched the frontal bones; in the elasmosaurids, they even reached back to the parietal bones. Often, the lacrimal bones were also lacking.[51] The tooth form and number was very variable. Some forms had hundreds of needle-like teeth. Most species had larger conical teeth with a round or oval cross-section. Such teeth numbered four to six in the premaxilla and about fourteen to twenty-five in the maxilla; the number in the lower jaws roughly equalled that of the skull. The teeth were placed in tooth-sockets, had vertically wrinkled enamel and lacked a true cutting edge or carina. With some species, the front teeth were notably longer, to grab prey.[69] Soft tissue remains of plesiosaurs are rare, but sometimes, especially in shale deposits, they have been partly preserved, e.g. showing the outlines of the body. An early discovery in this respect was the holotype of Plesiosaurus conybeari (presently Attenborosaurus). From such finds it is known that the skin was smooth, without apparent scales but with small wrinkles, that the trailing edge of the flippers extended considerably behind the limb bones;[70] and that the tail bore a vertical fin, as reported by Wilhelm Dames in his description of Plesiosaurus guilelmiimperatoris (presently Seeleyosaurus).[71] The possibility of a tail fluke has been confirmed by recent studies on the caudal neural spine form of Pantosaurus, Cryptoclidus and Rhomaleosaurus zetlandicus.[72][73][74] A 2020 study claims that the caudal fin was horizontal in configuration.[75] The probable food source of plesiosaurs varied depending on whether they belonged to the long-necked \"plesiosauromorph\" forms or the short-necked \"pliosauromorph\" species. The extremely long necks of \"plesiosauromorphs\" have caused speculation as to their function from the very moment their special build became apparent. Conybeare had offered three possible explanations. The neck could have served to intercept fast-moving fish in a pursuit. Alternatively, plesiosaurs could have rested on the sea bottom, while the head was sent out to search for prey, which seemed to be confirmed by the fact the eyes were directed relatively upwards. Finally, Conybeare suggested the possibility that plesiosaurs swam on the surface, letting their necks plunge downwards to seek food at lower levels. All these interpretations assumed that the neck was very flexible. The modern insight that the neck was, in fact, rather rigid, with limited vertical movement, has necessitated new explanations. One hypothesis is that the length of the neck made it possible to surprise schools of fish, the head arriving before the sight or pressure wave of the trunk could alert them. \"Plesiosauromorphs\" hunted visually, as shown by their large eyes, and perhaps employed a directional sense of olfaction. Hard and soft-bodied cephalopods probably formed part of their diet. Their jaws were probably strong enough to bite through the hard shells of this prey type. Fossil specimens have been found with cephalopod shells still in their stomach.[76] The bony fish (Osteichthyes), which further diversified during the Jurassic, were likely prey as well. A very different hypothesis claims that \"plesiosauromorphs\" were bottom feeders. The stiff necks would have been used to plough the sea bottom, eating the benthos. This would have been proven by long furrows present in ancients seabeds.[77][78] Such a lifestyle has in 2017 been suggested for Morturneria.[79] \"Plesiosauromorphs\" were not well adapted to catching large fast-moving prey, as their long necks, though seemingly streamlined, caused enormous skin friction. Sankar Chatterjee suggested in 1989 that some Cryptocleididae were suspension feeders, filtering plankton. Aristonectes e.g. had hundreds of teeth, allowing it to sieve small Crustacea from the water.[80] The short-necked \"pliosauromorphs\" were top carnivores, or apex predators, in their respective foodwebs.[81] They were pursuit predators[82] or ambush predators of various sized prey and opportunistic feeders; their teeth could be used to pierce soft-bodied prey, especially fish.[83] Their heads and teeth were very large, suited to grab and rip apart large animals. Their morphology allowed for a high swimming speed. They too hunted visually. Plesiosaurs were themselves prey for other carnivores, as shown by bite marks left by a shark that have been discovered on a fossilized plesiosaur fin[84] and the fossilized remains of a mosasaur's stomach contents that are thought to be the remains of a plesiosaur.[85] Skeletons have also been discovered with gastroliths, stones, in their stomachs, though whether to help break down food, especially cephalopods, in a muscular gizzard, or to vary buoyancy, or both, has not been established.[86][87] However, the total weight of the gastroliths found in various specimens appears to be insufficient to modify the buoyancy of these large reptiles.[88] The first plesiosaur gastroliths, found with Mauisaurus gardneri (a nomen nudum[89]), were reported by Harry Govier Seeley in 1877.[90] The number of these stones per individual is often very large. In 1949, a fossil of Alzadasaurus (specimen SDSM 451, later renamed to Styxosaurus) showed 253 of them.[91] The size of individual stones is often considerable. In 1991 an elasmosaurid specimen, KUVP 129744, was investigated, containing a gastrolith with a diameter of seventeen centimetres and a weight of 1300 grams; and a somewhat shorter stone of 1490 grams. In total, forty-seven gastroliths were present, with a combined weight of 13 kilograms. The size of the stones has been seen as an indication that they were not swallowed by accident, but deliberately, the animal perhaps covering large distances in search of a suitable rock type.[92] The type specimen of Scalamagnus (MNA V10046) is associated with 289 gastroliths, which is unusual in comparison to most polycotylid skeletons that generally lack gastroliths. Ranging from less than 0.1 grams to 18.5 grams, the total mass of the gastroliths was about 518 grams. About three-quarters of the stones weighed less than 2 grams, with the mean mass and median mass of the stones respectively estimated at 1.9 grams and 0.8 grams. The gastroliths had high mean value and variability in sphericity, suggesting that this individual was obtaining its stones from rivers located along the western side of the Western Interior Seaway.[93] The distinctive four-flippered body-shape has caused considerable speculation about what kind of stroke plesiosaurs used. The only modern group with four flippers are the sea turtles, which only use the front pair for propulsion. Conybeare and Buckland had already compared the flippers with bird wings. However, such a comparison was not very informative, as the mechanics of bird flight in this period were poorly understood. By the middle of the nineteenth century, it was typically assumed that plesiosaurs employed a rowing movement. The flippers would have been moved forward in a horizontal position, to minimise friction, and then axially rotated to a vertical position in order to be pulled to the rear, causing the largest possible reactive force. In fact, such a method would be very inefficient: the recovery stroke in this case generates no thrust and the rear stroke generates an enormous turbulence. In the early twentieth century, the newly discovered principles of bird flight suggested to several researchers that plesiosaurs, like turtles and penguins, made a flying movement while swimming. This was e.g. proposed by Eberhard Fraas in 1905,[94] and in 1908 by Othenio Abel.[95] When flying, the flipper movement is more vertical, its point describing an oval or \"8\". Ideally, the flipper is first moved obliquely to the front and downwards and then, after a slight retraction and rotation, crosses this path from below to be pulled to the front and upwards. During both strokes, down and up, according to Bernoulli's principle, forward and upward thrust is generated by the convexly curved upper profile of the flipper, the front edge slightly inclined relative to the water flow, while turbulence is minimal. However, despite the evident advantages of such a swimming method, in 1924 the first systematic study on the musculature of plesiosaurs by David Meredith Seares Watson concluded they nevertheless performed a rowing movement.[96] During the middle of the twentieth century, Watson's \"rowing model\" remained the dominant hypothesis regarding the plesiosaur swimming stroke. In 1957, Lambert Beverly Halstead, at the time using the family name Tarlo, proposed a variant: the hindlimbs would have rowed in the horizontal plane but the forelimbs would have paddled, moved to below and to the rear.[97][98] In 1975, the traditional model was challenged by Jane Ann Robinson, who revived the \"flying\" hypothesis. She argued that the main muscle groups were optimally placed for a vertical flipper movement, not for pulling the limbs horizontally, and that the form of the shoulder and hip joints would have precluded the vertical rotation needed for rowing.[99] In a subsequent article, Robinson proposed that the kinetic energy generated by the forces exerted on the trunk by the strokes, would have been stored and released as elastic energy in the ribcage, allowing for an especially efficient and dynamic propulsion system.[100] In Robinson's model, both the downstroke and the upstroke would have been powerful. In 1982, she was criticised by Samuel Tarsitano, Eberhard Frey and Jürgen Riess, who claimed that, while the muscles at the underside of the shoulder and pelvic plates were clearly powerful enough to pull the limbs downwards, comparable muscle groups on the top of these plates to elevate the limbs were simply lacking, and, had they been present, could not have been forcefully employed, their bulging carrying the danger of hurting the internal organs. They proposed a more limited flying model in which a powerful downstroke was combined with a largely unpowered recovery, the flipper returning to its original position by the momentum of the forward moving and temporarily sinking body.[101][102] This modified flying model became a popular interpretation. Less attention was given to an alternative hypothesis by Stephen Godfrey in 1984, which proposed that both the forelimbs and hindlimbs performed a deep paddling motion to the rear combined with a powered recovery stroke to the front, resembling the movement made by the forelimbs of sea-lions.[103] In 2010, Frank Sanders and Kenneth Carpenter published a study concluding that Robinson's model had been correct. Frey & Riess would have been mistaken in their assertion that the shoulder and pelvic plates had no muscles attached to their upper sides. While these muscle groups were probably not very powerful, this could easily have been compensated by the large muscles on the back, especially the latissimus dorsi, which would have been well developed in view of the high spines on the backbone. Furthermore, the flat build of the shoulder and hip joints strongly indicated that the main movement was vertical, not horizontal.[104] Like all tetrapods with limbs, plesiosaurs must have had a certain gait, a coordinated movement pattern of the, in this case, flippers. Of all the possibilities, in practice attention has been largely directed to the question of whether the front pair and hind pair moved simultaneously, so that all four flippers were engaged at the same moment, or in an alternate pattern, each pair being employed in turn. Frey & Riess in 1991 proposed an alternate model, which would have had the advantage of a more continuous propulsion.[105] In 2000, Theagarten Lingham-Soliar evaded the question by concluding that, like sea turtles, plesiosaurs only used the front pair for a powered stroke. The hind pair would have been merely used for steering. Lingham-Soliar deduced this from the form of the hip joint, which would have allowed for only a limited vertical movement. Furthermore, a separation of the propulsion and steering function would have facilitated the general coordination of the body and prevented a too extreme pitch. He rejected Robinson's hypothesis that elastic energy was stored in the ribcage, considering the ribs too stiff for this.[106] The interpretation by Frey & Riess became the dominant one, but was challenged in 2004 by Sanders, who showed experimentally that, whereas an alternate movement might have caused excessive pitching, a simultaneous movement would have caused only a slight pitch, which could have been easily controlled by the hind flippers. Of the other axial movements, rolling could have been controlled by alternately engaging the flippers of the right or left side, and yaw by the long neck or a vertical tail fin. Sanders did not believe that the hind pair was not used for propulsion, concluding that the limitations imposed by the hip joint were very relative.[107] In 2010, Sanders & Carpenter concluded that, with an alternating gait, the turbulence caused by the front pair would have hindered an effective action of the hind pair. Besides, a long gliding phase after a simultaneous engagement would have been very energy efficient.[104] It is also possible that the gait was optional and was adapted to the circumstances. During a fast steady pursuit, an alternate movement would have been useful; in an ambush, a simultaneous stroke would have made a peak speed possible. When searching for prey over a longer distance, a combination of a simultaneous movement with gliding would have cost the least energy.[108] In 2017, a study by Luke Muscutt, using a robot model, concluded that the rear flippers were actively employed, allowing for a 60% increase of the propulsive force and a 40% increase of efficiency. There would not have been a single optimal phase for all conditions, the gait likely having been changed as the situation demanded.[109] A short-necked pliosaurid like Kronosaurus would have been capable of overtaking a long-necked plesiosaur that, however, would be more manoeuvrable. In general, it is hard to determine the maximum speed of extinct sea creatures. For plesiosaurs, this is made more difficult by the lack of consensus about their flipper stroke and gait. There are no exact calculations of their Reynolds Number. Fossil impressions show that the skin was relatively smooth, not scaled, and this may have reduced form drag.[104] Small wrinkles are present in the skin that may have prevented separation of the laminar flow in the boundary layer and thereby reduced skin friction. Sustained speed may be estimated by calculating the drag of a simplified model of the body, that can be approached by a prolate spheroid, and the sustainable level of energy output by the muscles. A first study of this problem was published by Judy Massare in 1988.[110] Even when assuming a low hydrodynamic efficiency of 0.65, Massare's model seemed to indicate that plesiosaurs, if warm-blooded, would have cruised at a speed of four metres per second, or about fourteen kilometres per hour, considerably exceeding the known speeds of extant dolphins and whales.[111] However, in 2002 Ryosuke Motani showed that the formulae that Massare had used, had been flawed. A recalculation, using corrected formulae, resulted in a speed of half a metre per second (1.8 km/h) for a cold-blooded plesiosaur and one and a half metres per second (5.4 km/h) for an endothermic plesiosaur. Even the highest estimate is about a third lower than the speed of extant Cetacea.[112] Massare also tried to compare the speeds of plesiosaurs with those of the two other main sea reptile groups, the Ichthyosauria and the Mosasauridae. She concluded that plesiosaurs were about twenty percent slower than advanced ichthyosaurs, which employed a very effective tunniform movement, oscillating just the tail, but five percent faster than mosasaurids, which were assumed to swim with an inefficient anguilliform, eel-like, movement of the body.[111] The many plesiosaur species may have differed considerably in their swimming speeds, reflecting the various body shapes present in the group. While the short-necked \"pliosauromorphs\" (e.g. Liopleurodon) may have been fast swimmers, the long-necked \"plesiosauromorphs\" were built more for manoeuvrability than for speed, slowed by a strong skin friction, yet capable of a fast rolling movement. Some long-necked forms, such as the Elasmosauridae, also have relatively short stubby flippers with a low aspect ratio, further reducing speed but improving roll.[113] Few data are available that show exactly how deep plesiosaurs dived. That they dived to some considerable depth is proven by traces of decompression sickness. The heads of the humeri and femora with many fossils show necrosis of the bone tissue, caused by a too rapid ascent after deep diving. However, this does not allow to deduce some exact depth as the damage could have been caused by a few very deep dives, or alternatively by a great number of relatively shallow descents. The vertebrae show no such damage: they were probably protected by a superior blood supply, made possible by the arteries entering the bone through the two foramina subcentralia, large openings in their undersides.[114] Descending would have been helped by a negative Archimedes Force, i.e. being denser than water. Of course, this would have had the disadvantage of hampering coming up again. Young plesiosaurs show pachyostosis, an extreme density of the bone tissue, which might have increased relative weight. Adult individuals have more spongy bone. Gastroliths have been suggested as a method to increase weight[115] or even as means to attain neutral buoyancy, swallowing or spitting them out again as needed.[116] They might also have been used to increase stability.[117] The relatively large eyes of the Cryptocleididae have been seen as an adaptation to deep diving.[118] A 2020 study has posited that sauropterygians relied on vertical tail strokes much like cetaceans. In plesiosaurs the trunk was rigid so this action was more limited and in conjunction with the flippers.[75] Traditionally, it was assumed that extinct reptile groups were cold-blooded like modern reptiles. New research during the past decades has led to the conclusion that some groups, such as theropoddinosaurs and pterosaurs, were very likely warm-blooded. Whether perhaps plesiosaurs were warm-blooded as well is difficult to determine. One of the indications of a high metabolism is the presence of fast-growing fibrolamellar bone. The pachyostosis with juvenile individuals makes it hard to establish whether plesiosaurs possessed such bone, though. However, it has been possible to check its occurrence with more basal members of the more inclusive group that plesiosaurs belonged to, the Sauropterygia. A study in 2010 concluded that fibrolamellar bone was originally present with sauropterygians.[119] A subsequent publication in 2013 found that the Nothosauridae lacked this bone matrix type but that basal Pistosauria possessed it, a sign of a more elevated metabolism.[120] It is thus more parsimonious to assume that the more derived pistosaurians, the plesiosaurs, also had a faster metabolism. A paper published in 2018 claimed that plesiosaurs had resting metabolic rates (RMR) in the range of birds based on quantitative osteohistological modelling.[121] However, these results are problematic in view of general principles of vertebrate physiology (see Kleiber's law); evidence from isotope studies of plesiosaur tooth enamel indeed suggests endothermy at lower RMRs, with inferred body temperatures of ca. 26 °C (79 °F).[122] As reptiles in general are oviparous, until the end of the twentieth century it had been seen as possible that smaller plesiosaurs may have crawled up on a beach to lay eggs, like modern turtles. Their strong limbs and a flat underside seemed to have made this feasible. This method was, for example, defended by Halstead. However, as those limbs no longer had functional elbow or knee joints and the underside by its very flatness would have generated a lot of friction, already in the nineteenth century it was hypothesised that plesiosaurs had been viviparous. Besides, it was hard to conceive how the largest species, as big as whales, could have survived a beaching. Fossil finds of ichthyosaur embryos showed that at least one group of marine reptiles had borne live young. The first to claim that similar embryos had been found in plesiosaurs was Harry Govier Seeley, who reported in 1887 having acquired a nodule with four to eight tiny skeletons.[123] In 1896, he described this discovery in more detail.[124] If authentic, the embryos of plesiosaurs would have been very small, like those of ichthyosaurs. However, in 1982 Richard Anthony Thulborn showed that Seeley had been deceived by a \"doctored\" fossil of a nest of crayfish.[125] An actual plesiosaur specimen found in 1987 eventually proved that plesiosaurs gave birth to live young:[126] This fossil of a pregnant Polycotylus latippinus shows that these animals gave birth to a single large juvenile and probably invested parental care in their offspring, similar to modern whales. The young was 1.5 metres (five feet) long and thus large compared to its mother of five metres (sixteen feet) length, indicating a K-strategy in reproduction.[127] Little is known about growth rates or a possible sexual dimorphism. From the parental care indicated by the large size of the young, it can be deduced that social behaviour in general was relatively complex.[126] It is not known whether plesiosaurs hunted in packs. Their relative brain size seems to be typical for reptiles. Of the senses, sight and smell were important, hearing less so; elasmosaurids have lost the stapes completely. It has been suggested that with some groups the skull housed electro-sensitive organs.[128][129] While the Loch Ness monster is often reported as looking like a plesiosaur, it is also often described as looking completely different. A number of reasons have been presented for it to be unlikely to be a plesiosaur. They include the assumption that the water in the loch is too cold for a presumed cold-blooded reptile to be able to survive easily, the fact that the osteology of the plesiosaur's neck makes it absolutely safe to say that the plesiosaur could not lift its head like a swan out of water as the Loch Ness monster does, the assumption that air-breathing animals would be easy to see whenever they appear at the surface to breathe,[142] the fact that the loch is too small and contains insufficient food to be able to support a breeding colony of large animals, and finally the fact that the lake was formed only 10,000 years ago at the end of the last ice age, and the latest fossil appearance of plesiosaurs dates to over 66 million years ago.[143] Frequent explanations for the sightings include waves, floating inanimate objects, tricks of the light, swimming known animals and practical jokes.[144] Nevertheless, in the popular imagination, plesiosaurs have come to be identified with the Monster of Loch Ness. This has made plesiosaurs better known to the general public.[145] ^De la Beche, H.T.; Conybeare, W.D. (1821). \"Notice of the discovery of a new animal, forming a link between the Ichthyosaurus and crocodile, together with general remarks on the osteology of Ichthyosaurus\". Transactions of the Geological Society of London. 5: 559–594. ^Everhart, M. J. (2004). \"Plesiosaurs as the food of mosasaurs; new data on the stomach contents of a Tylosaurus proriger (Squamata; Mosasauridae) from the Niobrara Formation of western Kansas\". The Mosasaur. 7: 41–46. ^Seeley, H.G. (1888). \"On the Mode of Development of the Young in Plesiosaurus\". Report of the British Association for the Advancement of Science; Held at Manchester September. 1887: 697–698. ^Seeley, H. G.; 1896; \"On a pyritous concretion from the Lias of Whitby which appears to show the external form of the body of embryos of a species of Plesiosaurus\", Annual Report of Yorkshire philosophical Society pp.20-29"}
{"url": "https://en.m.wikipedia.org/wiki/Archosaur", "text": "Older definitions of the group Archosauria rely on shared morphological characteristics, such as an antorbital fenestra in the skull, serrated teeth, and an upright stance. Some extinct reptiles, such as proterosuchids and euparkeriids, also possessed these features yet originated prior to the split between the crocodilian and bird lineages. The older morphological definition of Archosauria nowadays roughly corresponds to Archosauriformes, a group named to encompass crown-group archosaurs and their close relatives.[4] The oldest true archosaur fossils are known from the Early Triassic period, though the first archosauriforms and archosauromorphs (reptiles closer to archosaurs than to lizards or other lepidosaurs) appeared in the Permian. Archosaurs quickly diversified in the aftermath of the Permian-Triassic mass extinction (~252 Ma), becoming the largest and most ecologically dominant terrestrial vertebrates from the Middle Triassic period up until the K-Pg mass extinction (~66 Ma).[5] Birds and several crocodyliform lineages were the only archosaurs to survive the K-Pg extinction, rediversifying in the subsequent Cenozoic era. Birds in particular have become among the most species-rich groups of terrestrial vertebrates in the present day. The most obvious features include teeth set in deep sockets, antorbital and mandibular fenestrae (openings in front of the eyes and in the jaw, respectively),[6] and a pronounced fourth trochanter (a prominent ridge on the femur).[7] Being set in sockets, the teeth were less likely to be torn loose during feeding. This feature is responsible for the name \"thecodont\" (meaning \"socket teeth\"),[8] which early paleontologists applied to many Triassic archosaurs.[7] Additionally, non-muscular cheek and lip tissue appear in various forms throughout the clade, with all living archosaurs lacking non-muscular lips, unlike most non-aviansaurischian dinosaurs.[9] Some archosaurs, such as birds, are secondarily toothless. Antorbital fenestrae reduced the weight of the skull, which was relatively large in early archosaurs, rather like that of modern crocodilians. Mandibular fenestrae may also have reduced the weight of the jaw in some forms. The fourth trochanter provides a large site for the attachment of muscles on the femur. Stronger muscles allowed for erect gaits in early archosaurs, and may also be connected with the ability of the archosaurs or their immediate ancestors to survive the catastrophic Permian-Triassic extinction event.[citation needed] Synapsids are a clade that includes mammals and their extinct ancestors. The latter group are often referred to as mammal-like reptiles, but should be termed protomammals, stem mammals, or basal synapsids, because they are not true reptiles by modern cladistic classification. They were the dominant land vertebrates throughout the Permian, but most perished in the Permian–Triassic extinction event. Very few large synapsids survived the event, but one form, Lystrosaurus (a herbivorousdicynodont), attained a widespread distribution soon after the extinction.[12] Following this, archosaurs and other archosauriforms quickly became the dominant land vertebrates in the early Triassic. Fossils from before the mass extinction have only been found around the Equator, but after the event fossils can be found all over the world.[13] Suggested explanations for this include: Archosaurs made more rapid progress towards erect limbs than synapsids, and this gave them greater stamina by avoiding Carrier's constraint. An objection to this explanation is that archosaurs became dominant while they still had sprawling or semi-erect limbs, similar to those of Lystrosaurus and other synapsids.[citation needed] Archosaurs have more efficient[clarification needed] respiratory systems featuring unidirectional air flow.[14] The ability to breathe more efficiently in hypoxic conditions may have been advantageous to early archosaurs during the suspected drop in oxygen levels at the end of the Permian.[14] The Early Triassic was predominantly arid, because most of the Earth's land was concentrated in the supercontinentPangaea. Archosaurs were probably better at conserving water than early synapsids because: Modern diapsids (lizards, snakes, crocodilians, birds) excrete uric acid, which can be excreted as a paste, resulting in low water loss as opposed to a more dilute urine. It is reasonable to suppose that archosaurs (the ancestors of crocodilians, dinosaurs and pterosaurs) also excreted uric acid, and therefore were good at conserving water. The aglandular (glandless) skins of diapsids would also have helped to conserve water.[citation needed] Modern mammals excrete urea, which requires a relatively high urinary rate to keep it from leaving the urine by diffusion in the kidney tubules. Their skins also contain many glands, which also lose water. Assuming that early synapsids had similar features, e.g., as argued by the authors of Palaeos, they were at a disadvantage in a mainly arid world. The same well-respected site points out that \"for much of Australia's Plio-Pleistocene history, where conditions were probably similar, the largest terrestrial predators were not mammals but gigantic varanid lizards (Megalania) and land crocs.\"[8] However, this theory has been questioned, since it implies synapsids were necessarily less advantaged in water retention, that synapsid decline coincides with climate changes or archosaur diversity (neither of which tested) and the fact that desert dwelling mammals are as well adapted in this department as archosaurs,[15] and some cynodonts like Trucidocynodon were large sized predators.[16] A study favors competition amidst mammaliaforms as the main explanation for Mesozoic mammals being small.[17] Since the 1970s, scientists have classified archosaurs mainly on the basis of their ankles.[18] The earliest archosaurs had \"primitive mesotarsal\" ankles: the astragalus and calcaneum were fixed to the tibia and fibula by sutures and the joint bent about the contact between these bones and the foot. The Pseudosuchia appeared early in the Triassic. In their ankles, the astragalus was joined to the tibia by a suture and the joint rotated round a peg on the astragalus which fitted into a socket in the calcaneum. Early \"crurotarsans\" still walked with sprawling limbs, but some later crurotarsans developed fully erect limbs. Modern crocodilians are crurotarsans that can employ a diverse range of gaits depending on speed.[19] Euparkeria and the Ornithosuchidae had \"reversed crurotarsal\" ankles, with a peg on the calcaneum and socket on the astragalus. The earliest fossils of Avemetatarsalia (\"bird ankles\") appear in the Anisian age of the Middle Triassic. Most Ornithodirans had \"advanced mesotarsal\" ankles. This form of ankle incorporated a very large astragalus and very small calcaneum, and could only move in one plane, like a simple hinge. This arrangement, which was only suitable for animals with erect limbs, provided more stability when the animals were running. The earliest avemetatarsalians, such as Teleocrater and Asilisaurus, retained \"primitive mesotarsal\" ankles. The ornithodirans differed from other archosaurs in other ways: they were lightly built and usually small, their necks were long and had an S-shaped curve, their skulls were much more lightly built, and many ornithodirans were completely bipedal. The archosaurian fourth trochanter on the femur may have made it easier for ornithodirans to become bipeds, because it provided more leverage for the thigh muscles. In the late Triassic, the ornithodirans diversified to produce dinosaurs and pterosaurs. Archosauria is normally defined as a crown group, which means that it only includes descendants of the last common ancestors of its living representatives. In the case of archosaurs, these are birds and crocodilians. Archosauria is within the larger clade Archosauriformes, which includes some close relatives of archosaurs, such as proterochampsids and euparkeriids. These relatives are often referred to as archosaurs despite being placed outside of the crown group Archosauria in a more basal position within Archosauriformes.[20] Historically, many archosauriforms were described as archosaurs, including proterosuchids and erythrosuchids, based on the presence of an antorbital fenestra. While many researchers prefer to treat Archosauria as an unranked clade, some continue to assign it a traditional biological rank. Traditionally, Archosauria has been treated as a Superorder, though a few 21st century researchers have assigned it to different ranks including Division[21] and Class.[22] Cope's term was a Greek-Latin hybrid intended to refer to the cranial arches, but has later also been understood as \"leading reptiles\" or \"ruling reptiles\" by association with Greek ἀρχός \"leader, ruler\".[25] The term \"thecodont\", now considered an obsolete term, was first used by the English paleontologist Richard Owen in 1859 to describe Triassic archosaurs, and it became widely used in the 20th century. Thecodonts were considered the \"basal stock\" from which the more advanced archosaurs descended. They did not possess features seen in later avian and crocodilian lines, and therefore were considered more primitive and ancestral to the two groups. With the cladistic revolution of the 1980s and 90s, in which cladistics became the most widely used method of classifying organisms, thecodonts were no longer considered a valid grouping. Because they are considered a \"basal stock\", thecodonts are paraphyletic, meaning that they form a group that does not include all descendants of its last common ancestor: in this case, the more derived crocodilians and birds are excluded from \"Thecodontia\" as it was formerly understood. The description of the basal ornithodiresLagerpeton and Lagosuchus in the 1970s provided evidence that linked thecodonts with dinosaurs, and contributed to the disuse of the term \"Thecodontia\", which many cladists consider an artificial grouping.[26] With the identification of \"crocodilian normal\" and \"crocodilian reversed\" ankles by Sankar Chatterjee in 1978, a basal split in Archosauria was identified. Chatterjee considered these two groups to be Pseudosuchia with the \"normal\" ankle and Ornithosuchidae with the \"reversed\" ankle. Ornithosuchids were thought to be ancestral to dinosaurs at this time. In 1979, A.R.I. Cruickshank identified the basal split and thought that the crurotarsan ankle developed independently in these two groups, but in opposite ways. Cruickshank also thought that the development of these ankle types progressed in each group to allow advanced members to have semi-erect (in the case of crocodilians) or erect (in the case of dinosaurs) gaits.[26] In many phylogenetic analyses, archosaurs have been shown to be a monophyletic grouping, thus forming a true clade. One of the first studies of archosaur phylogeny was authored by French paleontologist Jacques Gauthier in 1986. Gauthier split Archosauria into Pseudosuchia, the crocodilian line, and Ornithosuchia, the dinosaur and pterosaur line. Pseudosuchia was defined as all archosaurs more closely related to crocodiles, while Ornithosuchia was defined as all archosaurs more closely related to birds. Proterochampsids, erythrosuchids, and proterosuchids fell successively outside Archosauria in the resulting tree. Below is the cladogram from Gauthier (1986):[27] In 1988, paleontologists Michael Benton and J.M. Clark produced a new tree in a phylogenetic study of basal archosaurs. As in Gauthier's tree, Benton and Clark's revealed a basal split within Archosauria. They referred to the two groups as Crocodylotarsi and Ornithosuchia. Crocodylotarsi was defined as an apomorphy-based taxon based on the presence of a \"crocodile-normal\" ankle joint (considered to be the defining apomorphy of the clade). Gauthier's Pseudosuchia, by contrast, was a stem-based taxon. Unlike Gauthier's tree, Benton and Clark's places Euparkeria outside Ornithosuchia and outside the crown group Archosauria altogether.[28] The clades Crurotarsi and Ornithodira were first used together in 1990 by paleontologist Paul Sereno and A.B. Arcucci in their phylogenetic study of archosaurs. They were the first to erect the clade Crurotarsi, while Ornithodira was named by Gauthier in 1986. Crurotarsi and Ornithodira replaced Pseudosuchia and Ornithosuchia, respectively, as the monophyly of both of these clades were questioned.[26][29] Sereno and Arcucci incorporated archosaur features other than ankle types in their analyses, which resulted in a different tree than previous analyses. Below is a cladogram based on Sereno (1991), which is similar to the one produced by Sereno and Arcucci:[26] Ornithodira and Crurotarsi are both node-based clades, meaning that they are defined to include the last common ancestor of two or more taxa and all of its descendants. Ornithodira includes the last common ancestor of pterosaurs and dinosaurs (which include birds), while Crurotarsi includes the last common ancestor of living crocodilians and three groups of Triassic archosaurs: ornithosuchids, aetosaurs, and phytosaurs. These clades are not equivalent to \"bird-line\" and \"crocodile-line\" archosaurs, which would be branch-based clades defined as all taxa more closely related to one living group (either birds or crocodiles) than the other. Benton proposed the name Avemetatarsalia in 1999 to include all bird-line archosaurs (under his definition, all archosaurs more closely related to dinosaurs than to crocodilians). His analysis of the small Triassic archosaur Scleromochlus placed it within bird-line archosaurs but outside Ornithodira, meaning that Ornithodira was no longer equivalent to bird-line archosaurs. Below is a cladogram modified from Benton (2004) showing this phylogeny:[24] In Sterling Nesbitt's 2011 monograph on early archosaurs, a phylogenetic analysis found strong support for phytosaurs falling outside Archosauria. Many subsequent studies supported this phylogeny. Because Crurotarsi is defined by the inclusion of phytosaurs, the placement of phytosaurs outside Archosauria means that Crurotarsi must include all of Archosauria. Nesbitt reinstated Pseudosuchia as a clade name for crocodile-line archosaurs, using it as a stem-based taxon. Below is a cladogram modified from Nesbitt (2011):[4] Crocodylomorphs, pterosaurs and dinosaurs survived the Triassic–Jurassic extinction event about 200 million years ago, but other archosaurs had become extinct at or prior to the Triassic-Jurassic boundary. Like the early tetrapods, early archosaurs had a sprawling gait because their hip sockets faced sideways, and the knobs at the tops of their femurs were in line with the femur. In the early to middle Triassic, some archosaur groups developed hip joints that allowed (or required) a more erect gait. This gave them greater stamina, because it avoided Carrier's constraint, i.e. they could run and breathe easily at the same time. There were two main types of joint which allowed erect legs: The hip sockets faced sideways, but the knobs on the femurs were at right angles to the rest of the femur, which therefore pointed downwards. Dinosaurs evolved from archosaurs with this hip arrangement. The hip sockets faced downwards and the knobs on the femurs were in line with the femur. This \"pillar-erect\" arrangement appears to have evolved independently in various archosaur lineages, for example it was common in \"Rauisuchia\" (non-crocodylomorph paracrocodylomorphs) and also appeared in some aetosaurs. It has been pointed out that an upright stance requires more energy, so it may indicate a higher metabolism and a higher body temperature.[30] Many phytosaurs and crocodyliforms dominated the rivers and swamps and even invaded the seas (e.g., the teleosaurs, Metriorhynchidae and Dyrosauridae). The Metriorhynchidae were rather dolphin-like, with paddle-like forelimbs, a tail fluke and smooth, unarmoured skins. Two clades of ornithodirans, the pterosaurs and the birds, dominated the air after becoming adapted to a volant lifestyle. The metabolism of archosaurs is still a controversial topic. They certainly evolved from cold-blooded ancestors, and the surviving non-dinosaurian archosaurs, crocodilians, are cold-blooded. But crocodilians have some features which are normally associated with a warm-blooded metabolism because they improve the animal's oxygen supply: 4-chambered hearts. Both birds and mammals have 4-chambered hearts, which completely separate the flows of oxygenated and de-oxygenated blood. Non-crocodilian reptiles have 3-chambered hearts, which are less efficient because they let the two flows mix and thus send some de-oxygenated blood out to the body instead of to the lungs. Modern crocodilians' hearts are 4-chambered, but are smaller relative to body size and run at lower pressure than those of modern birds and mammals. They also have a pulmonary bypass, which makes them functionally 3-chambered when under water, conserving oxygen. a secondary palate, which allows the animal to eat and breathe at the same time. a hepatic piston mechanism for pumping the lungs. This is different from the lung-pumping mechanisms of mammals and birds, but similar to what some researchers claim to have found in some dinosaurs.[31][32] Historically there has been uncertainty as to why natural selection favored the development of these features, which are very important for active warm-blooded creatures, but of little apparent use to cold-blooded aquatic ambush predators that spend the vast majority of their time floating in water or lying on river banks. Paleontological evidence[clarification needed] shows that the ancestors of living crocodilians were active and endothermic (warm-blooded). Some experts[who?] believe that their archosaur ancestors were warm-blooded as well. This is likely because feather-like filaments evolved to cover the whole body and were capable of providing thermal insulation.[33] Physiological, anatomical, and developmental features of the crocodilian heart support the paleontological evidence and show that the lineage reverted to ectothermy when it invaded the aquatic, ambush predator niche. Crocodilian embryos develop fully 4-chambered hearts at an early stage. Modifications to the growing heart form a pulmonary bypass shunt that includes the left aortic arch, which originates from the right ventricle, the foramen of Panizza between the left and right aortic arches, and the cog‐tooth valve at the base of the pulmonary artery. The shunt is used during diving to make the heart function as 3-chambered heart, providing the crocodilian with the neurally controlled shunting used by ectotherms. The researchers concluded that the ancestors of living crocodilians had fully 4-chambered hearts, and were therefore warm-blooded, before they reverted to a cold-blooded or ectothermic metabolism. The authors also provide other evidence for endothermy in stem archosaurs.[34][35] It is reasonable to suggest that later crocodilians developed the pulmonary bypass shunt as they became cold-blooded, aquatic, and less active. If the crocodilian ancestors and other Triassic archosaurs were warm-blooded, this would help to resolve some evolutionary puzzles: The earliest crocodylomorphs, e.g., Terrestrisuchus, were slim, leggy terrestrial predators whose build suggests a fairly active lifestyle, which requires a fairly fast metabolism. And some other crurotarsan archosaurs appear to have had erect limbs, while those of rauisuchians are very poorly adapted for any other posture. Erect limbs are advantageous for active animals because they avoid Carrier's constraint, but disadvantageous for more sluggish animals because they increase the energy costs of standing up and lying down. A recent study of the lungs of Alligator mississippiensis (the American alligator) has shown that the airflow through them is unidirectional, moving in the same direction during inhalation and exhalation.[36] This is also seen in birds and many non-avian dinosaurs, which have air sacs to further aid in respiration. Both birds and alligators achieve unidirectional air flow through the presence of parabronchi, which are responsible for gas exchange. The study has found that in alligators, air enters through the second bronchial branch, moves through the parabronchi, and exits through the first bronchial branch. Unidirectional airflow in both birds and alligators suggests that this type of respiration was present at the base of Archosauria and retained by both dinosaurs and non-dinosaurian archosaurs, such as aetosaurs, \"rauisuchians\" (non-crocodylomorph paracrocodylomorphs), crocodylomorphs, and pterosaurs.[36] The use of unidirectional airflow in the lungs of archosaurs may have given the group an advantage over synapsids, which had lungs where air moved tidally in and out through a network of bronchi that terminated in alveoli, which were cul-de-sacs. The better efficiency in gas transfer seen in archosaur lungs may have been advantageous during the times of low atmospheric oxygen which are thought to have existed during the Mesozoic.[37] Most (if not all) archosaurs are oviparous. Birds and crocodilians lay hard-shelled eggs, as did extinct dinosaurs, and crocodylomorphs. Hard-shelled eggs are present in both dinosaurs and crocodilians, which has been used as an explanation for the absence of viviparity or ovoviviparity in archosaurs.[38] However, both pterosaurs[39] and baurusuchids[40] have soft-shelled eggs, implying that hard shells are not a plesiomorphic condition. The pelvic anatomy of Cricosaurus and other metriorhynchids[41] and fossilized embryos belonging to the non-archosaur archosauromorph Dinocephalosaurus,[42] together suggest that the lack of viviparity among archosaurs may be a consequence of lineage-specific restrictions.[clarification needed] Archosaurs are ancestrally superprecocial as evidenced in various dinosaurs, pterosaurs, and crocodylomorphs.[43] However, parental care did evolve independently multiple times in crocodilians, dinosaurs, and aetosaurs.[44] In most such species the animals bury their eggs and rely on temperature-dependent sex determination. The notable exception are Neornithes which incubate their eggs and rely on genetic sex determination – a trait that might have given them a survival advantage over other dinosaurs.[45] ^Gauthier, J.A. (1986). \"Saurischian monophyly and the origin of birds\". In Padian, K. (ed.). The Origin of Birds and the Evolution of Flight. Memoirs of the California Academy of Sciences. Vol. 8. San Francisco: California Academy of Sciences. pp. 1–55."}
{"url": "https://web.archive.org/web/20141006084722/http://www.mormon.org/faq/practice-of-polygamy", "text": "These crawls are part of an effort to archive pages as they are created and archive the pages that they refer to. That way, as the pages that are referenced are changed or taken from the web, a link to the version that was live when the page was written will be preserved. Then the Internet Archive hopes that references to these archived pages will be put in place of a link that would be otherwise be broken, or a companion link to allow people to see what was originally intended by a page's authors. This is a collection of web page captures from links added to, or changed on, Wikipedia pages. The idea is to bring a reliability to Wikipedia outlinks so that if the pages referenced by Wikipedia articles are changed, or go away, a reader can permanently find what was originally referred to. While our backgrounds and experiences are diverse, we share a deep commitment to Jesus Christ, to each other, and our neighbors. Watch these stories of faith in the everyday lives of Mormons. You can also meet Mormons here. Our faith influences nearly every aspect of our lives. Beyond simply believing in Jesus Christ, we try to bring His teachings to life at home, at work and in our communities. Here are a few of the cultural priorities embraced by members of the Church around the world. We are all spiritual children of a loving Heavenly Father who sent us to this earth to learn and grow in a mortal state. As Mormons, we are followers of Jesus Christ. We live our lives to serve Him and teach of His eternal plan for each of us. Find a Church Do Mormons practice polygamy? President Gordon B. Hinckley, prior president of The Church of Jesus Christ of Latter-day Saints made the following statement in 1998 about the Church’s position on plural marriage: “This Church has nothing whatever to do with those practicing polygamy. They are not members of this Church.... If any of our members are found to be practicing plural marriage, they are excommunicated, the most serious penalty the Church can impose. Not only are those so involved in direct violation of the civil law, they are in violation of the law of this Church.” At various times, the Lord has commanded His people to practice plural marriage. For example, He gave this command to Abraham, Isaac, Jacob, Moses, David, and Solomon (Doctrine and Covenants 132:1). At other times the Lord has given other instructions. In the Book of Mormon, the Lord told the prophet Jacob “for there shall not any man among you have save it be one wife: and concubines he shall have none... for if I will, saith the Lord of Hosts, raise up seed unto me, I will command my people; otherwise they shall hearken unto these things (Jacob 2:27-30). In this dispensation, the Lord commanded some of the early Saints to practice plural marriage. The Prophet Joseph Smith and those closest to him, including Brigham Young and Heber C. Kimball, were challenged by this command, but they obeyed it. Church leaders regulated the practice. Those entering into it had to be authorized to do so, and the marriages had to be performed through the sealing power of the priesthood. In 1890, President Wilford Woodruff received a revelation that the leaders of the Church should cease teaching the practice of plural marriage (Official Declaration 1). The Lord’s law of marriage is monogamy unless he commands otherwise to help establish the House of Israel (see Encyclopedia of Mormonism Vol. 3, pp. 1091-1095). Read other answers contributed by members of The Church of Jesus Christ of Latter-day Saints. Answers are the sole responsibility of the members No. Mormons do NOT practice polygamy. There are many people outside the Church that say otherwise, but that doesn't change the truth of the matter. There is nobody living on this earth right now who can truly claim to be a member of The Church of Jesus Christ of Latter-day Saints who practices polygamy. It is against the law, both of the land and of the Church. We do not approve of such things. There are some groups of people who do practice this and also claim to be Mormons, but I assure you that they are not in any way affiliated with this Church. Show moreShow less Today, Mormons are commonly associated with polygamy, even though the practice was discontinued in the Mormon Church in 1890 - over 120 years ago! Today there are still some who claim to be Mormon and who practice polygamy, but it should be recognized that they are not part of our Church - anyone belonging to The Church of Jesus Christ of Latter Day Saints today who practices polygamy is excommunicated and their names are removed from our rosters. In modern revelation, we learn that we are never supposed to practice this unless specifically told to by the Lord (through the prophet) as was the case with Solomon and David in the Old Testament. Know that we do not want to practice polygamy. I don't know any Mormons who do. We know that if we were asked to, the Lord would help us to understand why, as He will with any commandment. However, until that day (which we do not expect to ever come), we will neither practice it nor look forward to doing so. Show moreShow less No we do not. God has, at some points in time, commanded certain man to practice polygamy (i.e. Abraham, Jacob, Moses, David, and Solomon) but monogamy is always the norm and polygamy the exception even during these times. It is true that certain early Saints were called to practice plural marriage, however this was always closely monitored and had to be authorized through church leaders. Through President Wilford Woodruff in the year 1890, God commanded this practice to end and is no longer practiced by The Church of Jesus Christ of Latter-day Saints. Show moreShow less No, Mormons do not practice polygamy and discontinued the practice over a century ago. Yet, whenever I discuss my faith, often the first question asked is, \"how many wives do you have?\" It is important to know that the religious groups that currently practice polygamy, as often seen on the news, have no affiliation with the Church of Jesus Christ of Latter-Day Saints. Even though some of these groups may claim otherwise. Show moreShow less Absolutely not!! This is one of the many myths about Mormon's that hollywood and the media continue to propogate. It is true that the early memebrs of the church did practice polygamy, but the church oficcialy denounced the practice 150 years ago!! If any church member today practices polygamy they are excommunicated. There are people who leave the LDS church and start their own religion base loosely on LDS beliefs. Some of them practice polygamy. They are NOT memebrs of the LDS (Mormon) chruch, nor do they have any affiliation with the LDS church. Show moreShow less We certainly do not. As a Mormon I've constantly been taught about the importance of family and how to strengthen those bonds. My mother and father exemplify the ideal of loving parents and I am so grateful for that loving bond my 5 siblings and I share with our mom and dad. Our faith does not teach, condone, accept, or defend polygamy in any way. Show moreShow less No, we don't. People typically mix up Mormons with their reformed break offs. Several of those groups practice polygamy, even though historically the commandment from the Lord to have plural marriages has been repealed. Show moreShow less No! The practice of polygamy by faithful Mormons ended September 24, 1890. Mormons today do not practice or preach or in any way condone the practice of plural marriage. If you hear or read about any group of people practicing polygamy you can be absolutely certain that they are not members of the Church of Jesus Christ of Latter-day Saints. Any LDS church member practicing polygamy today would be immediately excommunicated from the church. QUESTION: Why did your church previously practice plural marriage (polygamy)? This is a good honest question that deserves an honest answer. My great-great-grandfather who joined the church in 1835 was a polygamist. If you are a Christian that believes in the Holy Bible and ancient prophets, you believe in and accept the fact that God has the right to authorize plural marriage when He sees the need, and He has the right to revoke that same commandment when He sees the need to do so. Many of the ancient prophets had plural wives such as Moses, Abraham, and Isaac. If you believe in the Holy Bible, you must accept this fact. But you also know that for most of human history God commanded men to have only one wife. The key here is to do what God commands, and don’t do what He forbids. The period of time when Joseph Smith (1820-1844) was called of God to restore the “fullness of the gospel” to the earth, many important laws and ordinances were revealed and restored to the earth that had existed anciently. The power of the priesthood, temple ordinances, and the practice of plural marriage were part of that restoration. But in 1890 the Lord directed the living prophet Wilford Woodruff to end the practice of Plural marriage. Since that time the Mormon Church has not authorized any such practice. This is a great example that demonstrates how important it is to follow living prophets who receive direction from the living God for our day. Show moreShow less"}
{"url": "https://en.m.wikipedia.org/wiki/Mignon_Fogarty", "text": "Mignon's career has focused on science and writing. She has been the editor-in-chief and producer for the LongerLiving website before becoming the editorial director of CaregiverZone. Later, she became the editorial director and executive producer of GeneticHealth.com.[7] She was the producer and co-host of the podcast Absolute Science. She joined the Reynolds School of Journalism at the University of Nevada, Reno, in 2014, and, until 2017, she held the Donald W. Reynolds Chair in Media Entrepreneurship.[8] In January 2007, CNN featured Fogarty and her podcast, calling her \"a quick and dirty success\".[9] Mignon Fogarty appeared on the March 26, 2007 episode of The Oprah Winfrey Show as a grammar expert. She was on the show to answer a viewer's question about the use of possessive apostrophes. The viewer thought a previous show should have been titled \"Oprah's and Gayle's Big Adventure\", but Fogarty confirmed that \"Oprah and Gayle's Big Adventure\" was a correct use of compound possessive.[10] She went on to discuss several other common grammar errors, including \"Affect vs. Effect\" and \"Who vs. Whom\".[11] In September 2007, Fogarty and Holt/Holtzbrinck agreed to produce books coordinated with the podcasts.[12] The first audiobook to come from the Holt agreement, Grammar Girl's Quick and Dirty Tips to Clean Up Your Writing, was named one of the top five audiobooks of 2007 by iTunes.[13] Her second book, The Grammar Devotional, was published by Holt in October 2009.[17] In July 2011, Henry Holt published Grammar Girl Presents the Ultimate Writing Guide for Students, which was named a Teachers' Choice book by the International Reading Association,[18] and St. Martin's Griffin published Grammar Girl's 101 Words Every High School Graduate Needs to Know and Grammar Girl's 101 Misused Words You'll Never Confuse Again, which was a Washington Post bestseller the week of July 31, 2011.[19] On July 7, 2011, Fogarty was interviewed by Neal Conan for the NPR program Talk of the Nation.[21] In the 17-minute segment, Fogarty discussed a number of examples from her 2011 book, Grammar Girl's 101 Misused Words You'll Never Confuse Again (New York: St. Martin's Griffin, 2011),[22] and answered listeners' questions. In 2012, Fogarty began doing regular segments about language on KPCC on Southern California Public Radio.[23]"}
{"url": "https://en.m.wikipedia.org/wiki/Flat_Earth", "text": "Belief in flat Earth Near East In early Egyptian[12] and Mesopotamian thought, the world was portrayed as a disk floating in the ocean. A similar model is found in the Homeric account from the 8th century BC in which \"Okeanos, the personified body of water surrounding the circular surface of the Earth, is the begetter of all life and possibly of all gods.\"[13] The Israelites also imagined the Earth to be a disc floating on water with an arched firmament above it that separated the Earth from the heavens.[17] The sky was a solid dome with the Sun, Moon, planets, and stars embedded in it.[18] Homer's description of the disc cosmography on the shield of Achilles with the encircling ocean is repeated far later in Quintus Smyrnaeus' Posthomerica (4th century AD), which continues the narration of the Trojan War.[27] Thales thought that the Earth floated in water like a log.[33] It has been argued, however, that Thales actually believed in a spherical Earth.[34][35]Anaximander (c. 550 BC) believed that the Earth was a short cylinder with a flat, circular top that remained stable because it was the same distance from all things.[36][37]Anaximenes of Miletus believed that \"the Earth is flat and rides on air; in the same way the Sun and the Moon and the other heavenly bodies, which are all fiery, ride the air because of their flatness\".[38]Xenophanes of Colophon (c. 500 BC) thought that the Earth was flat, with its upper side touching the air, and the lower side extending without limit.[39] Belief in a flat Earth continued into the 5th century BC. Anaxagoras (c. 450 BC) agreed that the Earth was flat,[40] and his pupil Archelaus believed that the flat Earth was depressed in the middle like a saucer, to allow for the fact that the Sun does not rise and set at the same time for everyone.[41] Historians Hecataeus of Miletus believed that the Earth was flat and surrounded by water.[42]Herodotus in his Histories ridiculed the belief that water encircled the world,[43] yet most classicists agree that he still believed Earth was flat because of his descriptions of literal \"ends\" or \"edges\" of the Earth.[44] Northern Europe The ancient Norse and Germanic peoples believed in a flat-Earth cosmography with the Earth surrounded by an ocean, with the axis mundi, a world tree (Yggdrasil), or pillar (Irminsul) in the centre.[45][46] In the world-encircling ocean sat a snake called Jormungandr.[47] The Norse creation account preserved in Gylfaginning (VIII) states that during the creation of the Earth, an impassable sea was placed around it:[48] And Jafnhárr said: \"Of the blood, which ran and welled forth freely out of his wounds, they made the sea, when they had formed and made firm the Earth together, and laid the sea in a ring round. about her; and it may well seem a hard thing to most men to cross over it.\" If you take a lighted candle and set it in a room, you may expect it to light up the entire interior, unless something should hinder, though the room be quite large. But if you take an apple and hang it close to the flame, so near that it is heated, the apple will darken nearly half the room or even more. However, if you hang the apple near the wall, it will not get hot; the candle will light up the whole house; and the shadow on the wall where the apple hangs will be scarcely half as large as the apple itself. From this you may infer that the Earth-circle is round like a ball and not equally near the sun at every point. But where the curved surface lies nearest the sun's path, there will the greatest heat be; and some of the lands that lie continuously under the unbroken rays cannot be inhabited. East Asia In ancient China, the prevailing belief was that the Earth was flat and square, while the heavens were round,[50] an assumption virtually unquestioned until the introduction of European astronomy in the 17th century.[51][52][53] The English sinologist Cullen emphasizes the point that there was no concept of a round Earth in ancient Chinese astronomy:[54] Chinese thought on the form of the Earth remained almost unchanged from early times until the first contacts with modern science through the medium of Jesuit missionaries in the seventeenth century. While the heavens were variously described as being like an umbrella covering the Earth (the Kai Tian theory), or like a sphere surrounding it (the Hun Tian theory), or as being without substance while the heavenly bodies float freely (the Hsüan yeh theory), the Earth was at all times flat, although perhaps bulging up slightly. The heavens are like a hen's egg and as round as a crossbow bullet; the Earth is like the yolk of the egg, and lies in the centre. This analogy with a curved egg led some modern historians, notably Joseph Needham, to conjecture that Chinese astronomers were, after all, aware of the Earth's sphericity. The egg reference, however, was rather meant to clarify the relative position of the flat Earth to the heavens:[52] In a passage of Zhang Heng's cosmogony not translated by Needham, Zhang himself says: \"Heaven takes its body from the Yang, so it is round and in motion. Earth takes its body from the Yin, so it is flat and quiescent\". The point of the egg analogy is simply to stress that the Earth is completely enclosed by Heaven, rather than merely covered from above as the Kai Tian describes. Chinese astronomers, many of them brilliant men by any standards, continued to think in flat-Earth terms until the seventeenth century; this surprising fact might be the starting-point for a re-examination of the apparent facility with which the idea of a spherical Earth found acceptance in fifth-century BC Greece. Further examples cited by Needham supposed to demonstrate dissenting voices from the ancient Chinese consensus actually refer without exception to the Earth being square, not to it being flat.[54] Accordingly, the 13th-century scholar Li Ye, who argued that the movements of the round heaven would be hindered by a square Earth,[50] did not advocate a spherical Earth, but rather that its edge should be rounded off so as to be circular.[54] However, Needham disagrees, affirming that Li Ye believed the Earth to be spherical, similar in shape to the heavens but much smaller.[56] This was preconceived by the 4th-century scholar Yu Xi, who argued for the infinity of outer space surrounding the Earth and that the latter could be either square or round, in accordance to the shape of the heavens.[57] When Chinese geographers of the 17th century, influenced by European cartography and astronomy, showed the Earth as a sphere that could be circumnavigated by sailing around the globe, they did so with formulaic terminology previously used by Zhang Heng to describe the spherical shape of the Sun and Moon (i.e. that they were as round as a crossbow bullet).[58] As noted in the book Huainanzi,[59] in the 2nd century BC, Chinese astronomers effectively inverted Eratosthenes' calculation of the curvature of the Earth to calculate the height of the Sun above the Earth. By assuming the Earth was flat, they arrived at a distance of 100000li (approximately 200000 km). The Zhoubi Suanjing also discusses how to determine the distance of the Sun by measuring the length of noontime shadows at different latitudes, a method similar to Eratosthenes' measurement of the circumference of the Earth, but the Zhoubi Suanjing assumes that the Earth is flat.[60] Lucretius (1st century BC) opposed the concept of a spherical Earth, because he considered that an infinite universe had no center towards which heavy bodies would tend. Thus, he thought the idea of animals walking around topsy-turvy under the Earth was absurd.[64][65] By the 1st century AD, Pliny the Elder was in a position to say that everyone agreed on the spherical shape of Earth,[66] though disputes continued regarding the nature of the antipodes, and how it is possible to keep the ocean in a curved shape. South Asia An image of Thorntonbank Wind Farm (near the Belgian coast) with the lower parts of the more distant towers increasingly hidden by the horizon, demonstrating the curvature of the Earth The Vedic texts depict the cosmos in many ways.[67][68] One of the earliest Indian cosmological texts pictures the Earth as one of a stack of flat disks.[69] In the Vedic texts, Dyaus (heaven) and Prithvi (Earth) are compared to wheels on an axle, yielding a flat model. They are also described as bowls or leather bags, yielding a concave model.[70] According to Macdonell: \"the conception of the Earth being a disc surrounded by an ocean does not appear in the Samhitas. But it was naturally regarded as circular, being compared with a wheel (10.89) and expressly called circular (parimandala) in the Shatapatha Brahmana.\"[71] By about the 5th century AD, the siddhanta astronomy texts of South Asia, particularly of Aryabhata, assume a spherical Earth as they develop mathematical methods for quantitative astronomy for calendar and time keeping.[72] The medieval Indian texts called the Puranas describe the Earth as a flat-bottomed, circular disk with concentric oceans and continents.[70][73] This general scheme is present not only in the Hindu cosmologies, but also in Buddhist and Jain cosmologies of South Asia.[70] However, some Puranas include other models. The fifth canto of the Bhagavata Purana, for example, includes sections that describe the Earth both as flat and spherical.[74][75] Early Christian Church During the early period of the Christian Church, the spherical view continued to be widely held, with some notable exceptions.[76]Athenagoras, an eastern Christian writing around the year 175 AD, said that the Earth was spherical.[77]Methodius (c. 290 AD), an eastern Christian writing against \"the theory of the Chaldeans and the Egyptians\" said: \"Let us first lay bare ... the theory of the Chaldeans and the Egyptians. They say that the circumference of the universe is likened to the turnings of a well-rounded globe, the Earth being a central point. They say that since its outline is spherical, ... the Earth should be the center of the universe, around which the heaven is whirling.\"[77]Lactantius, a western Christian writer and advisor to the first Christian Roman Emperor, Constantine, writing sometime between 304 and 313 AD, ridiculed the notion of antipodes and the philosophers who fancied that \"the universe is round like a ball. They also thought that heaven revolves in accordance with the motion of the heavenly bodies. ... For that reason, they constructed brass globes, as though after the figure of the universe.\"[78][77]Arnobius, another eastern Christian writing sometime around 305 AD, described the round Earth: \"In the first place, indeed, the world itself is neither right nor left. It has neither upper nor lower regions, nor front nor back. For whatever is round and bounded on every side by the circumference of a solid sphere, has no beginning or end ...\"[77] But as to the fable that there are Antipodes, that is to say, men on the opposite side of the Earth, where the sun rises when it sets to us, men who walk with their feet opposite ours that is on no ground credible. And, indeed, it is not affirmed that this has been learned by historical knowledge, but by scientific conjecture, on the ground that the Earth is suspended within the concavity of the sky, and that it has as much room on the one side of it as on the other: hence they say that the part that is beneath must also be inhabited. But they do not remark that, although it be supposed or scientifically demonstrated that the world is of a round and spherical form, yet it does not follow that the other side of the Earth is bare of water; nor even, though it be bare, does it immediately follow that it is peopled. For Scripture, which proves the truth of its historical statements by the accomplishment of its prophecies, gives no false information; and it is too absurd to say, that some men might have taken ship and traversed the whole wide ocean, and crossed from this side of the world to the other, and that thus even the inhabitants of that distant region are descended from that one first man. Some historians do not view Augustine's scriptural commentaries as endorsing any particular cosmological model, endorsing instead the view that Augustine shared the common view of his contemporaries that the Earth is spherical, in line with his endorsement of science in De Genesi ad litteram.[80][81] C. P. E. Nothaft, responding to writers like Leo Ferrari who described Augustine as endorsing a flat Earth, says that \"...other recent writers on the subject treat Augustine’s acceptance of the earth’s spherical shape as a well-established fact\".[82][83] Christian Topography (547) by the Alexandrian monk Cosmas Indicopleustes, who had traveled as far as Sri Lanka and the source of the Blue Nile, is now widely considered the most valuable geographical document of the early medieval age, although it received relatively little attention from contemporaries. In it, the author repeatedly expounds the doctrine that the universe consists of only two places, the Earth below the firmament and heaven above it. Carefully drawing on arguments from scripture, he describes the Earth as a rectangle, 400 days' journey long by 200 wide, surrounded by four oceans and enclosed by four massive walls which support the firmament. The spherical Earth theory is contemptuously dismissed as \"pagan\".[86][87][88] Severian, Bishop of Gabala (d. 408), wrote that the Earth is flat and the Sun does not pass under it in the night, but \"travels through the northern parts as if hidden by a wall\".[89]Basil of Caesarea (329–379) argued that the matter was theologically irrelevant.[90] Europe: Early Middle Ages Early medieval Christian writers felt little urge to assume flatness of the Earth, though they had fuzzy impressions of the writings of Ptolemy and Aristotle, relying more on Pliny.[6] 9th-century Macrobian cosmic diagram showing the sphere of the Earth at the center (globus terrae) With the end of the Western Roman Empire, Western Europe entered the Middle Ages with great difficulties that affected the continent's intellectual production. Most scientific treatises of classical antiquity (in Greek) were unavailable, leaving only simplified summaries and compilations. In contrast, the Eastern Roman Empire did not fall, and it preserved the learning.[91] Still, many textbooks of the Early Middle Ages supported the sphericity of the Earth in the western part of Europe.[92] Bishop Isidore of Seville (560–636) taught in his widely read encyclopedia, the Etymologies, diverse views such as that the Earth \"resembles a wheel\"[93] resembling Anaximander in language and the map that he provided. This was widely interpreted as referring to a disc-shaped Earth.[94][95] An illustration from Isidore's De Natura Rerum shows the five zones of the Earth as adjacent circles. Some have concluded that he thought the Arctic and Antarctic zones were adjacent to each other.[96] He did not admit the possibility of antipodes, which he took to mean people dwelling on the opposite side of the Earth, considering them legendary[97] and noting that there was no evidence for their existence.[98] Isidore's T and O map, which was seen as representing a small part of a spherical Earth, continued to be used by authors through the Middle Ages, e.g. the 9th-century bishop Rabanus Maurus, who compared the habitable part of the northern hemisphere (Aristotle's northern temperate clime) with a wheel. At the same time, Isidore's works also gave the views of sphericity, for example, in chapter 28 of De Natura Rerum, Isidore claims that the Sun orbits the Earth and illuminates the other side when it is night on this side. See French translation of De Natura Rerum.[99] In his other work Etymologies, there are also affirmations that the sphere of the sky has Earth in its center and the sky being equally distant on all sides.[100][101] Other researchers have argued these points as well.[6][102][103] \"The work remained unsurpassed until the thirteenth century and was regarded as the summit of all knowledge. It became an essential part of European medieval culture. Soon after the invention of typography it appeared many times in print.\"[104] However, \"The Scholastics – later medieval philosophers, theologians, and scientists – were helped by the Arabic translators and commentaries, but they hardly needed to struggle against a flat-Earth legacy from the early middle ages (500–1050). Early medieval writers often had fuzzy and imprecise impressions of both Ptolemy and Aristotle and relied more on Pliny, but they felt (with one exception), little urge to assume flatness.\"[6] Isidore's portrayal of the five zones of the Earth St Vergilius of Salzburg (c. 700–784), in the middle of the 8th century, discussed or taught some geographical or cosmographical ideas that St Boniface found sufficiently objectionable that he complained about them to Pope Zachary. The only surviving record of the incident is contained in Zachary's reply, dated 748, where he wrote:[105] As for the perverse and sinful doctrine which he (Virgil) against God and his own soul has uttered – if it shall be clearly established that he professes belief in another world and other men existing beneath the Earth, or in (another) sun and moon there, thou art to hold a council, deprive him of his sacerdotal rank, and expel him from the Church. Some authorities have suggested that the sphericity of the Earth was among the aspects of Vergilius's teachings that Boniface and Zachary considered objectionable.[106][107] Others have considered this unlikely, and take the wording of Zachary's response to indicate at most an objection to belief in the existence of humans living in the antipodes.[108][109][110][111][112] In any case, there is no record of any further action having been taken against Vergilius. He was later appointed bishop of Salzburg and was canonised in the 13th century.[113] 12th-century depiction of a spherical Earth with the four seasons (book Liber Divinorum Operum by Hildegard of Bingen) A possible non-literary but graphic indication that people in the Middle Ages believed that the Earth (or perhaps the world) was a sphere is the use of the orb (globus cruciger) in the regalia of many kingdoms and of the Holy Roman Empire. It is attested from the time of the Christian late-Roman emperor Theodosius II (423) throughout the Middle Ages; the Reichsapfel was used in 1191 at the coronation of emperor Henry VI. However the word orbis means \"circle\", and there is no record of a globe as a representation of the Earth since ancient times in the west until that of Martin Behaim in 1492. Additionally it could well be a representation of the entire \"world\" or cosmos.[114] A recent study of medieval concepts of the sphericity of the Earth noted that \"since the eighth century, no cosmographer worthy of note has called into question the sphericity of the Earth\".[115] However, the work of these intellectuals may not have had significant influence on public opinion, and it is difficult to tell what the wider population may have thought of the shape of the Earth if they considered the question at all. Europe: Late Middle Ages Hermannus Contractus (1013–1054) was among the earliest Christian scholars to estimate the circumference of Earth with Eratosthenes' method. Thomas_Aquinas (1225–1274), the most widely taught theologian of the Middle Ages, believed in a spherical Earth and took for granted that his readers also knew the Earth is round.[116] Lectures in the medieval universities commonly advanced evidence in favor of the idea that the Earth was a sphere.[117] Jill Tattersall shows that in many vernacular works in 12th- and 13th-century French texts the Earth was considered \"round like a table\" rather than \"round like an apple\". She writes, \"[I]n virtually all the examples quoted ... from epics and from non-'historical' romances (that is, works of a less learned character) the actual form of words used suggests strongly a circle rather than a sphere\", though she notes that even in these works the language is ambiguous.[118] Portuguese navigation down and around the coast of Africa in the latter half of the 1400s gave wide-scale observational evidence for Earth's sphericity. In these explorations, the Sun's position moved more northward the further south the explorers travelled. Its position directly overhead at noon gave evidence for crossing the equator. These apparent solar motions in detail were more consistent with north–south curvature and a distant Sun, than with any flat-Earth explanation. The ultimate demonstration came when Ferdinand Magellan's expedition completed the first global circumnavigation in 1521. Antonio Pigafetta, one of the few survivors of the voyage, recorded the loss of a day in the course of the voyage, giving evidence for east–west curvature. Middle East: Islamic scholars Prior to the introduction of Greek cosmology into the Islamic world, Muslims tended to view the Earth as flat, and Muslim traditionalists who rejected Greek philosophy continued to hold to this view later on while various theologians held opposing opinions.[119][120] Beginning in the 10th century onwards, some Muslim traditionalists began to adopt the notion of a spherical Earth with the influence of Greek and Ptolemaic cosmology.[121] The Quran mentions that the Earth (al-arḍ) was \"spread out.\"[123] Whether or not this implies a flat earth was debated by Muslims.[120] Some modern historians believe the Quran saw the world as flat.[124][125] On the other hand, the 12th-century commentary, the Tafsir al-Kabir (al-Razi) by Fakhr al-Din al-Razi argues that though this verse does describe a flat surface, it is limited in its application to local regions of the Earth which are roughly flat as opposed to the Earth as a whole. Others who would support a ball-shaped Earth included Ibn Hazm.[120] Ming Dynasty in China A spherical terrestrial globe was introduced to Yuan-eraKhanbaliq (i.e. Beijing) in 1267 by the Persian astronomer Jamal ad-Din, but it is not known to have made an impact on the traditional Chinese conception of the shape of the Earth.[126] As late as 1595, an early Jesuit missionary to China, Matteo Ricci, recorded that the Ming-dynasty Chinese say: \"The Earth is flat and square, and the sky is a round canopy; they did not succeed in conceiving the possibility of the antipodes.\"[54] In the 17th century, the idea of a spherical Earth spread in China due to the influence of the Jesuits, who held high positions as astronomers at the imperial court.[127] Matteo Ricci, in collaboration with Chinese cartographers and translator Li Zhizao, published the Kunyu Wanguo Quantu in 1602, the first Chinese world map based on European discoveries.[128] The astronomical and geographical treatise Gezhicao (格致草) written in 1648 by Xiong Mingyu (熊明遇) explained that the Earth was spherical, not flat or square, and could be circumnavigated.[127] Myth of flat-Earth prevalence In the 19th century, a historical myth arose which held that the predominant cosmological doctrine during the Middle Ages was that the Earth was flat. An early proponent of this myth was the American writer Washington Irving, who maintained that Christopher Columbus had to overcome the opposition of churchmen to gain sponsorship for his voyage of exploration. Later significant advocates of this view were John William Draper and Andrew Dickson White, who used it as a major element in their advocacy of the thesis[129] that there was a long-lasting and essential conflict between science and religion.[130] Some studies of the historical connections between science and religion have demonstrated that theories of their mutual antagonism ignore examples of their mutual support.[131][132] Subsequent studies of medieval science have shown that most scholars in the Middle Ages, including those read by Christopher Columbus, maintained that the Earth was spherical.[133] Modern believers in a flat Earth face overwhelming publicly accessible evidence of Earth's sphericity. They also need to explain why governments, media outlets, schools, scientists, surveyors, airlines and other organizations accept that the world is spherical. To satisfy these tensions and maintain their beliefs, they generally embrace some form of conspiracy theory. In addition, believers tend to not trust observations they have not made themselves, and often distrust, disagree with or accuse each other of being in league with conspiracies.[137] Education Before learning from their social environment, a child's perception of their physical environment often leads to a false concept about the shape of Earth and what happens beyond the horizon. Many children think that Earth ends there and that one can fall off the edge. Education helps them gradually change their belief into a realist one of a spherical Earth.[138] ^Dr. James Hannam (May 18, 2010). \"Science Versus Christianity?\". Patheos. Archived from the original on December 2, 2023. The myth that people in the Middle Ages thought the earth is flat appears to date from the 17th century as part of the campaign by Protestants against Catholic teaching. ^Pyramid Texts, Utterance 366, 629a–29c: \"Behold, thou art great and round like the Great Round; Behold, thou are bent around, and art round like the Circle which encircles the nbwt; Behold, thou art round and great like the Great Circle which sets.\"(Faulkner 1969, 120) ^Professor of Classics (Emeritus) Mark W. Edwards in his The Iliad. A commentary (1991, p. 231) has noted of Homer's usage of the flat Earth disc in the Iliad: \"Okeanos...surrounds the pictures on the shield and he surrounds the disc of the Earth on which men and women work out their lives\". Quoted in The shield of Achilles and the poetics of ekphrasis, Andrew Sprague Becker, Rowman & Littlefield, 1995, p. 148 ^Stasinus of Cyprus wrote in his Cypria (lost, only preserved in fragment) that Oceanus surrounded the entire Earth: deep eddying Oceanus and that the Earth was flat with furthest bounds, these quotes are found preserved in Athenaeus, Deipnosophistae, VIII. 334B. ^Mimnermus of Colophon (630BC) details a flat Earth model, with the sun (Helios) bathing at the edges of Oceanus that surround the Earth (Mimnermus, frg. 11) ^Apollonius Rhodius, in his Argonautica (3rd century BC) included numerous flat Earth references (IV. 590 ff): \"Now that river, rising from the ends of the Earth, where are the portals and mansions of Nyx (Night), on one side bursts forth upon the beach of Okeanos.\" ^Posthomerica (V. 14). \"Here [on the shield of Achilles] Tethys' all-embracing arms were wrought, and Okeanos fathomless flow. The outrushing flood of Rivers crying to the echoing hills all round, to right, to left, rolled o'er the land.\" Translation by Way, A.S. 1913. ^According to John Mansley Robinson, An Introduction to Early Greek Philosophy, Houghton and Mifflin, 1968. ^Herodotus knew of the conventional view, according to which the river Ocean runs around a circular flat Earth (4.8), and of the division of the world into three – Jacoby, RE Suppl. 2.352 ff, yet rejected this personal belief (Histories, 2. 21; 4. 8; 4. 36). ^The history of Herodotus, George Rawlinson, Appleton and company, 1889, p. 409. ^D. Pingree: \"History of Mathematical Astronomy in India\", Dictionary of Scientific Biography, Vol. 15 (1978), pp. 533–633 (554ff.), Quote: \"In the Purānas, the Earth is a flat-bottomed, circular disk, in the center of which is a lofty mountain, Meru. Surrounding Meru is the circular continent Jambūdvīpa, which is in turn surrounded by a ring of water known as the Salt Ocean. There follow alternating rings of land and sea until there are seven continents and seven oceans. In the southern quarter of Jambūdvīpa lies India–Bhāratavarsa.\" ^J. L. E. Dreyer, A History of Planetary Systems from Thales to Kepler. (1906); unabridged republication as A History of Astronomy from Thales to Kepler (New York: Dover Publications, 1953). ^St. John Chrysostom, Homilies Concerning the Statues, Homily IX, paras. 7–8, in A Select Library of the Nicene and Post-Nicene Fathers of the Christian Church, Series I, Vol IX, ed. Philip Schaff, American reprint of the Edinburgh edition (1978), Wm. B. Eerdmans Publishing Co., Grand Rapids, MI, p. 403: \"When therefore thou beholdest not a small pebble, but the whole earth borne upon the waters, and not submerged, admire the power of Him who wrought these marvellous things in a supernatural manner! And whence does this appear, that the earth is borne upon the waters? The prophet declares this when he says, 'He hath founded it upon the seas, and prepared it upon the floods.' And again: 'To him who hath founded the earth upon the waters.' What sayest thou? The water is not able to support a small pebble on its surface, and yet bears up the earth, great as it is; and mountains, and hills, and cities, and plants, and men, and brutes; and it is not submerged!\" ^Lindberg, David C. (2000), \"Science and the Early Christian Church\", in Shank, Michael H. (ed.), The Scientific Enterprise in Antiquity and the Middle Ages: Readings from Isis, Chicago and London: University of Chicago Press, pp. 125–146, ISBN978-0-226-74951-8 Further reading Fraser, Raymond (2007). When The Earth Was Flat: Remembering Leonard Cohen, Alden Nowlan, the Flat Earth Society, the King James monarchy hoax, the Montreal Story Tellers and other curious matters. Black Moss Press, ISBN978-0-88753-439-3"}
{"url": "https://en.m.wikipedia.org/wiki/Local_realism", "text": "Principle of locality In physics, the principle of locality states that an object is influenced directly only by its immediate surroundings. A theory that includes the principle of locality is said to be a \"local theory\". This is an alternative to the concept of instantaneous, or \"non-local\" action at a distance. Locality evolved out of the field theories of classical physics. The idea is that for a cause at one point to have an effect at another point, something in the space between those points must mediate the action. To exert an influence, something, such as a wave or particle, must travel through the space between the two points, carrying the influence. The special theory of relativity limits the maximum speed at which causal influence can travel to the speed of light, c{\\displaystyle c}. Therefore, the principle of locality implies that an event at one point cannot cause a truly simultaneous result at another point. An event at point A{\\displaystyle A} cannot cause a result at point B{\\displaystyle B} in a time less than T=D/c{\\displaystyle T=D/c}, where D{\\displaystyle D} is the distance between the points and c{\\displaystyle c} is the speed of light in vacuum. The principle of locality plays a critical role in one of the central results of quantum mechanics. In 1935 Albert Einstein, Boris Podolsky, and Nathan Rosen, with their EPR paradox thought experiment, raised the possibility that quantum mechanics might not be a complete theory. They described two systems physically separated after interacting; this pair would be called entangled in modern terminology. They reasoned that without additions, now called hidden variables, quantum mechanics would predict illogical relationships between the physically separated measurements. In 1964 John Stewart Bell formulated Bell's theorem, an inequality which, if violated in actual experiments, implies that quantum mechanics violates local causality, a result now considered equivalent to no local hidden variables. Later work refers to local causality as local realism. During the 17th century, Newton's principle of universal gravitation was formulated in terms of \"action at a distance\", thereby violating the principle of locality. Newton himself considered this violation to be absurd: It is inconceivable that inanimate Matter should, without the Mediation of something else, which is not material, operate upon, and affect other matter without mutual Contact…That Gravity should be innate, inherent and essential to Matter, so that one body may act upon another at a distance thro' a Vacuum, without the Mediation of any thing else, by and through which their Action and Force may be conveyed from one to another, is to me so great an Absurdity that I believe no Man who has in philosophical Matters a competent Faculty of thinking can ever fall into it. Gravity must be caused by an Agent acting constantly according to certain laws; but whether this Agent be material or immaterial, I have left to the Consideration of my readers.[1] Coulomb's law of electric forces was initially also formulated as instantaneous action at a distance, but in 1880, James Clerk Maxwell showed that field equations – which obey locality – predict all of the phenomena of electromagnetism.[citation needed] These equations show that electromagnetic forces propagate at the speed of light. In 1905, Albert Einstein's special theory of relativity postulated that no matter or energy can travel faster than the speed of light, and Einstein thereby sought to reformulate physics in a way that obeyed the principle of locality. He later succeeded in producing an alternative theory of gravitation, general relativity, which obeys the principle of locality. However, a different challenge to the principle of locality developed subsequently from the theory of quantum mechanics, which Einstein himself had helped to create. Simple spacetime diagrams can help clarify the issues related to locality.[2] A way to describe the issues of locality suitable for discussion of quantum mechanics is illustrated in the diagram. A particle is created in one location, then split and measured in two other, spatially separated, locations. The two measurements are named for Alice and Bob. Alice performs measurements (A) and gets a result a{\\displaystyle \\mathbf {a} }); Bob performs (B{\\displaystyle B}) and gets result b{\\displaystyle \\mathbf {b} }. The experiment is repeated many times and the results are compared. A spacetime diagram has a time coordinate going vertical and a space coordinate going horizontal. Alice, in a local region on the left, can affect events only in a cone extending in the future as shown; the finite speed of light prevent her from affecting other areas including Bob's location in this case. Similarly we can use the diagram to reason that Bob's local circumstances cannot be altered by Alice at the same time: all events that cause an effect on Bob are in the cone below his location on the diagram. Dashed lines around Alice show her valid future locations; dashed lines around Bob show events that could have caused his present circumstance. When Alice measures quantum states in her location she gets the results labeled a{\\displaystyle \\mathbf {a} }; similarly Bob gets b{\\displaystyle \\mathbf {b} }. Models of locality attempt to explain the statistical relationship between these measured values. The simplest locality model is no locality: instantaneous action at a distance with no limits for relativity. The locality model for action at a distance is called continuous action.[2] The gray area (a circle here) is a mathematical concept called a \"screen\". Any path from a location through the screen becomes part of the physical model at that location. The gray ring indicates events from all parts of space and time can affect the probability measured by Alice or Bob. So in the case of continuous action, events at all times and places affect Alice's and Bob's model. This simple model is highly successful for solar planetary dynamics with Newtonian gravity and in electrostatics, cases where relativistic effects are insignificant. Many locality models explicitly or implicitly ignore the possible effect of future events. The spacetime diagram at the right shows the effect of such a restriction when combined with continuous action. Inputs from the future (above the dashed line) are no longer considered part of Alice's or Bob's model. Comparing this diagram with the one for continuous action makes it clear that these are not the same locality model.[2] Common sense arguments about the future not affecting the present are reasonable criteria but such assumptions alter the mathematical character of the models. John Stewart Bell when discussing his Bell's theorem uses the screening model shown at the right. Events in the common past of Alice and Bob are part of the model used in calculating probabilities for Alice and for Bob as indicated the way the screen absorbs those events. However events at Bob's location during Alice measurement and events in the future are excluded. Bell called this assumption local causality, but with the diagram we can reason about the meaning of the assumption without getting tripped up by other meanings of local combined with other meanings of causal.[2] Dash lines show relativistically valid regions in the past of Alice or Bob. The gray arc is the assumed Bell \"screen\". The relative positions of our few, easily distinguishable planets (for example) can be seen directly: understanding and measuring their relative location poses only technical issues. The submicroscopic world on the other hand is known only by measurements that average over many seemingly random ('statistical' or 'probabilistic') events and measurements can show either particle-like or wave-like results depending on their design. This world is governed by quantum mechanics.[3] The concepts of locality are more complex and they are described in the language of probability and correlation. In the 1935 Einstein–Podolsky–Rosen paradox paper, (EPR paper),[4]Albert Einstein, Boris Podolsky and Nathan Rosen imagined such an experiment. They observed that quantum mechanics predicts what is now known as quantum entanglement, and examined its consequences.[5] In their view, the classical principle of locality implied that \"no real change can take place\" at Bob's site as a result of whatever measurements Alice was doing. Since quantum mechanics does predict a wavefunction collapse that depends on Bob's choice of measurement, they concluded that this was a form of action-at-distance and that the wavefunction could not be a complete description of reality. Other physicists did not agree: they accepted the quantum wavefunction as complete and questioned the nature of locality and reality assumed in the EPR paper.[6] In 1964 John Stewart Bell investigated whether it might be possible to fulfill Einstein's goal—to \"complete\" quantum theory—with local hidden variables to explain the correlations between spatially separated particles as predicted by quantum theory. Bell established a criterion to distinguish between local hidden variables theory and quantum theory by measuring specific values of correlations between entangled particles. Subsequent experimental tests have shown that some quantum effects do violate Bell's inequalities and cannot be reproduced by a local hidden variables theory.[5] Bell's theorem depends on careful defined models of locality. Bell described local causality in terms of probability needed for analysis of quantum mechanics. Using the notation that P(r|g){\\displaystyle P(\\mathbf {r} |g)} for the probability of a result r{\\displaystyle \\mathbf {r} } with given state g{\\displaystyle g}, Bell investigated the probability distribution P(ab|AB,λ){\\displaystyle P(\\mathbf {ab} |AB,\\lambda )} where λ{\\displaystyle \\lambda } represents hidden state variables set (locally) when the two particles are initially co-located. If local causality holds, then the probabilities observed by Alice and by Bob should be only coupled by the hidden variables, and we can show that Bell proved that a consequence of this factorization are limits on the correlations observed by Alice and Bob known as Bell inequalities. Since quantum mechanics predicts correlations stronger than this limit, locally set hidden variables cannot be added to \"complete\" quantum theory as desired by the EPR paper.[7] Numerous experiments specifically designed to probe the issues of locality confirm the predictions of quantum mechanics; these include experiments where the two measurement locations are more than a kilometer apart.[7][8] The 2022 Nobel Prize in Physics was awarded to Alain Aspect, John Clauser and Anton Zeilinger, in part \"for experiments with entangled photons, establishing the violation of Bell inequalities\".[9] The specific aspect of quantum theory that leads to these correlations is termed quantum entanglement and versions of Bell's scenario are now used to verify entanglement experimentally.[7] Bell's mathematical results, when compared to experimental data, eliminate local hidden variable mathematical quantum theories. But the interpretation of the math with respect to the physical world remains under debate. Bell described the assumptions behind his work as \"local causality\", shortened to \"locality\"; later authors referred to the assumptions as local realism.[10] These different names do not alter the mathematical assumptions. A review of papers[11] using this phrase suggests that a common (classical) physics definition of realism is the assumption that measurement outcomes are well defined prior to and independent of the measurements.[12] This definition includes classical concepts like \"well-defined\" which conflicts with quantum superposition and \"prior to...measurements\" which implies (metaphysical) preexistence of properties. Specifically, the term local realism in the context of Bell's theorem cannot be viewed as a kind of \"realism\" involving locality other than the kind implied by the Bell screening assumption. This conflict between common ideas of realism and quantum mechanics requires careful analysis whenever local realism is discussed.[11]: 98 Adding a \"locality\" modifier, that the results of two spatially well-separated measurements cannot causally affect each other,[5] does not make the combination relate to Bell's proof; the only interpretation that Bell assumed was the one he called local causality.[11]: 98 Consequently, Bell's theorem does not restrict the possibility of nonlocal variables as well as theories based on retrocausality or superdeterminism.[2] Because of the probabilistic nature of wave function collapse, this apparent violation of locality in quantum mechanics cannot be used to transmit information faster than light, in accordance to the no communication theorem.[13]Asher Peres distinguishes between weak and strong nonlocality, the latter referring to the theories that allow to faster-than-light communication. Under these terms, quantum mechanics would allow weakly nonlocal correlations but not strong nonlocality.[14] One of the main principles of quantum field theory is the principle of locality.[15] The field operators and the Lagrangian density describing the dynamics of the fields are local, in the sense that interactions are not described by action-at-a-distance. This can be achieved by avoiding terms in the Lagrangian that are products of two fields that depend on distant coordinates.[15][16] Specifically, in relativistic quantum field theory, to enforce the principles of locality and causality the following condition is required: if there are two observables, each localized within two distinct spacetime regions which happen to be at a spacelike separation from each other, the observables must commute. This condition is sometimes imposed as one of the axioms of relativistic quantum field theory.[15][17]"}
{"url": "https://en.m.wikipedia.org/wiki/Obesity", "text": "Excessive consumption of energy-dense foods, sedentary work and lifestyles and lack of physical activity, changes in modes of transportation, urbanization, lack of supportive policies, lack of access to a healthy diet, genetics[1][4] While a majority of obese individuals at any given time attempt to lose weight and are often successful, maintaining weight loss long-term is rare.[16] There is no effective, well-defined, evidence-based intervention for preventing obesity. Obesity prevention requires a complex approach, including interventions at societal, community, family, and individual levels.[1][13] Changes to diet as well as exercising are the main treatments recommended by health professionals.[2] Diet quality can be improved by reducing the consumption of energy-dense foods, such as those high in fat or sugars, and by increasing the intake of dietary fiber, if these dietary choices are available, affordable, and accessible.[1]Medications can be used, along with a suitable diet, to reduce appetite or decrease fat absorption.[5] If diet, exercise, and medication are not effective, a gastric balloon or surgery may be performed to reduce stomach volume or length of the intestines, leading to feeling full earlier, or a reduced ability to absorb nutrients from food.[6][17] Obesity is a leading preventable cause of death worldwide, with increasing rates in adults and children.[18] In 2022, over 1 billion people were obese worldwide (879 million adults and 159 million children), representing more than a double of adult cases (and four times higher than cases among children) registered in 1990.[19][20] Obesity is more common in women than in men.[1] Today, obesity is stigmatized in most of the world. Conversely, some cultures, past and present, have a favorable view of obesity, seeing it as a symbol of wealth and fertility.[2][21] The World Health Organization, the US, Canada, Japan, Portugal, Germany, the European Parliament and medical societies, e.g. the American Medical Association, classify obesity as a disease. Others, such as the UK, do not.[22][23][24][25] For children, obesity measures take age into consideration along with height and weight. For children aged 5–19, the WHO defines obesity as a BMI two standard deviations above the median for their age (a BMI around 18 for a five-year old; around 30 for a 19-year old).[27][29] For children under five, the WHO defines obesity as a weight three standard deviations above the median for their height.[27] Some modifications to the WHO definitions have been made by particular organizations.[30] The surgical literature breaks down class II and III or only class III obesity into further categories whose exact values are still disputed.[31] Any BMI ≥ 35 or 40 kg/m2 is severe obesity. A BMI of ≥ 35 kg/m2 and experiencing obesity-related health conditions or ≥ 40 or 45 kg/m2 is morbid obesity. A BMI of ≥ 45 or 50 kg/m2 is super obesity. As Asian populations develop negative health consequences at a lower BMI than Caucasians, some nations have redefined obesity; Japan has defined obesity as any BMI greater than 25 kg/m2[11] while China uses a BMI of greater than 28 kg/m2.[30] The preferred obesity metric in scholarly circles is the body fat percentage (BF%) – the ratio of the total weight of person's fat to his or her body weight, and BMI is viewed merely as a way to approximate BF%.[32] Levels in excess of 32% for women and 25% for men are generally considered to indicate obesity. BMI ignores variations between individuals in amounts of lean body mass, particularly muscle mass. Individuals involved in heavy physical labor or sports may have high BMI values despite having little fat. For example, more than half of all NFL players are classified as \"obese\" (BMI ≥ 30), and 1 in 4 are classified as \"extremely obese\" (BMI ≥ 35), according to the BMI metric.[33] However, their mean body fat percentage, 14%, is well within what is considered a healthy range.[34] Similarly, Sumo wrestlers may be categorized by BMI as \"severely obese\" or \"very severely obese\" but many Sumo wrestlers are not categorized as obese when body fat percentage is used instead (having <25% body fat).[35] Some Sumo wrestlers were found to have no more body fat than a non-Sumo comparison group, with high BMI values resulting from their high amounts of lean body mass.[35] Effects on health Obesity increases a person's risk of developing various metabolic diseases, cardiovascular disease, osteoarthritis, Alzheimer disease, depression, and certain types of cancer.[36] Depending on the degree of obesity and the presence of comorbid disorders, obesity is associated with an estimated 2–20 year shorter life expectancy.[37][36] High BMI is a marker of risk for, but not a direct cause of, diseases caused by diet and physical activity.[13] Mortality Obesity is one of the leading preventable causes of death worldwide.[38][39][40] The mortality risk is lowest at a BMI of 20–25 kg/m2[41][37][42] in non-smokers and at 24–27 kg/m2 in current smokers, with risk increasing along with changes in either direction.[43][44] This appears to apply in at least four continents.[42] Other research suggests that the association of BMI and waist circumference with mortality is U- or J-shaped, while the association between waist-to-hip ratio and waist-to-height ratio with mortality is more positive.[45] In Asians the risk of negative health effects begins to increase between 22 and 25 kg/m2.[46] In 2021, the World Health Organization estimated that obesity caused at least 2.8 million deaths annually.[47] On average, obesity reduces life expectancy by six to seven years,[2][48] a BMI of 30–35 kg/m2 reduces life expectancy by two to four years,[37] while severe obesity (BMI ≥ 40 kg/m2) reduces life expectancy by ten years.[37] Complications are either directly caused by obesity or indirectly related through mechanisms sharing a common cause such as a poor diet or a sedentary lifestyle. The strength of the link between obesity and specific conditions varies. One of the strongest is the link with type 2 diabetes. Excess body fat underlies 64% of cases of diabetes in men and 77% of cases in women.[52]: 9 Metrics of health Newer research has focused on methods of identifying healthier obese people by clinicians, and not treating obese people as a monolithic group.[81] Obese people who do not experience medical complications from their obesity are sometimes called (metabolically) healthy obese, but the extent to which this group exists (especially among older people) is in dispute.[82] The number of people considered metabolically healthy depends on the definition used, and there is no universally accepted definition.[83] There are numerous obese people who have relatively few metabolic abnormalities, and a minority of obese people have no medical complications.[83] The guidelines of the American Association of Clinical Endocrinologists call for physicians to use risk stratification with obese patients when considering how to assess their risk of developing type 2 diabetes.[84]: 59–60 To come up with these criteria, BioSHaRE controlled for age and tobacco use, researching how both may effect the metabolic syndrome associated with obesity, but not found to exist in the metabolically healthy obese.[86] Other definitions of metabolically healthy obesity exist, including ones based on waist circumference rather than BMI, which is unreliable in certain individuals.[83] Another identification metric for health in obese people is calfstrength, which is positively correlated with physical fitness in obese people.[87]Body composition in general is hypothesized to help explain the existence of metabolically healthy obesity—the metabolically healthy obese are often found to have low amounts of ectopic fat (fat stored in tissues other than adipose tissue) despite having overall fat mass equivalent in weight to obese people with metabolic syndrome.[88]: 1282 Survival paradox Although the negative health consequences of obesity in the general population are well supported by the available research evidence, health outcomes in certain subgroups seem to be improved at an increased BMI, a phenomenon known as the obesity survival paradox.[89] The paradox was first described in 1999 in overweight and obese people undergoing hemodialysis[89] and has subsequently been found in those with heart failure and peripheral artery disease (PAD).[90] In people with heart failure, those with a BMI between 30.0 and 34.9 had lower mortality than those with a normal weight. This has been attributed to the fact that people often lose weight as they become progressively more ill.[91] Similar findings have been made in other types of heart disease. People with class I obesity and heart disease do not have greater rates of further heart problems than people of normal weight who also have heart disease. In people with greater degrees of obesity, however, the risk of further cardiovascular events is increased.[92][93] Even after cardiac bypass surgery, no increase in mortality is seen in the overweight and obese.[94] One study found that the improved survival could be explained by the more aggressive treatment obese people receive after a cardiac event.[95] Another study found that if one takes into account chronic obstructive pulmonary disease (COPD) in those with PAD, the benefit of obesity no longer exists.[90] Causes The \"a calorie is a calorie\" model of obesity posits a combination of excessive food energy intake and a lack of physical activity as the cause of most cases of obesity.[96] A limited number of cases are due primarily to genetics, medical reasons, or psychiatric illness.[15] In contrast, increasing rates of obesity at a societal level are felt to be due to an easily accessible and palatable diet,[97] increased reliance on cars, and mechanized manufacturing.[98][99] Some other factors have been proposed as causes towards rising rates of obesity worldwide, including insufficient sleep, endocrine disruptors, increased usage of certain medications (such as atypical antipsychotics),[100] increases in ambient temperature, decreased rates of smoking,[101] demographic changes, increasing maternal age of first-time mothers, changes to epigenetic dysregulation from the environment, increased phenotypic variance via assortative mating, social pressure to diet,[102] among others. According to one study, factors like these may play as big of a role as excessive food energy intake and a lack of physical activity;[103] however, the relative magnitudes of the effects of any proposed cause of obesity is varied and uncertain, as there is a general need for randomized controlled trials on humans before definitive statement can be made.[104] According to the Endocrine Society, there is \"growing evidence suggesting that obesity is a disorder of the energy homeostasis system, rather than simply arising from the passive accumulation of excess weight\".[105] Diet Map of dietary energy availability per person per day in 1961 (left) and 2001–2003 (right)[106] Calories per person per day (kilojoules per person per day) No data <1,600 (<6,700) 1,600–1,800 (6,700–7,500) 1,800–2,000 (7,500–8,400) 2,000–2,200 (8,400–9,200) 2,200–2,400 (9,200–10,000) 2,400–2,600 (10,000–10,900) 2,600–2,800 (10,900–11,700) 2,800–3,000 (11,700–12,600) 3,000–3,200 (12,600–13,400) 3,200–3,400 (13,400–14,200) 3,400–3,600 (14,200–15,100) >3,600 (>15,100) Average per capita energy consumption of the world from 1961 to 2002[106] Excess appetite for palatable, high-calorie food (especially fat, sugar, and certain animal proteins) is seen as the primary factor driving obesity worldwide, likely because of imbalances in neurotransmitters affecting the drive to eat.[107]Dietary energy supply per capita varies markedly between different regions and countries. It has also changed significantly over time.[106] From the early 1970s to the late 1990s the average food energy available per person per day (the amount of food bought) increased in all parts of the world except Eastern Europe. The United States had the highest availability with 3,654 calories (15,290 kJ) per person in 1996.[106] This increased further in 2003 to 3,754 calories (15,710 kJ).[106] During the late 1990s, Europeans had 3,394 calories (14,200 kJ) per person, in the developing areas of Asia there were 2,648 calories (11,080 kJ) per person, and in sub-Saharan Africa people had 2,176 calories (9,100 kJ) per person.[106][108] Total food energy consumption has been found to be related to obesity.[109] Prevalence of obesity in the adult population by region (2000 - 2016) The widespread availability of dietary guidelines[110] has done little to address the problems of overeating and poor dietary choice.[111] From 1971 to 2000, obesity rates in the United States increased from 14.5% to 30.9%.[112] During the same period, an increase occurred in the average amount of food energy consumed. For women, the average increase was 335 calories (1,400 kJ) per day (1,542 calories (6,450 kJ) in 1971 and 1,877 calories (7,850 kJ) in 2004), while for men the average increase was 168 calories (700 kJ) per day (2,450 calories (10,300 kJ) in 1971 and 2,618 calories (10,950 kJ) in 2004). Most of this extra food energy came from an increase in carbohydrate consumption rather than fat consumption.[113] The primary sources of these extra carbohydrates are sweetened beverages, which now account for almost 25 percent of daily food energy in young adults in America,[114] and potato chips.[115] Consumption of sweetened beverages such as soft drinks, fruit drinks, and iced tea is believed to be contributing to the rising rates of obesity[116][117] and to an increased risk of metabolic syndrome and type 2 diabetes.[118]Vitamin D deficiency is related to diseases associated with obesity.[119] As societies become increasingly reliant on energy-dense, big-portions, and fast-food meals, the association between fast-food consumption and obesity becomes more concerning.[120] In the United States, consumption of fast-food meals tripled and food energy intake from these meals quadrupled between 1977 and 1995.[121] Obese people consistently under-report their food consumption as compared to people of normal weight.[123] This is supported both by tests of people carried out in a calorimeter room[124] and by direct observation. Sedentary lifestyle A sedentary lifestyle may play a significant role in obesity.[52]: 10 Worldwide there has been a large shift towards less physically demanding work,[125][126][127] and currently at least 30% of the world's population gets insufficient exercise.[126] This is primarily due to increasing use of mechanized transportation and a greater prevalence of labor-saving technology in the home.[125][126][127] In children, there appear to be declines in levels of physical activity (with particularly strong declines in the amount of walking and physical education), likely due to safety concerns, changes in social interaction (such as fewer relationships with neighborhood children), and inadequate urban design (such as too few public spaces for safe physical activity).[128] World trends in active leisure time physical activity are less clear. The World Health Organization indicates people worldwide are taking up less active recreational pursuits, while research from Finland[129] found an increase and research from the United States found leisure-time physical activity has not changed significantly.[130] Physical activity in children may not be a significant contributor.[131] In both children and adults, there is an association between television viewing time and the risk of obesity.[132][133][134] Increased media exposure increases the rate of childhood obesity, with rates increasing proportionally to time spent watching television.[135] Like many other medical conditions, obesity is the result of an interplay between genetic and environmental factors.[137]Polymorphisms in various genes controlling appetite and metabolism predispose to obesity when sufficient food energy is present. As of 2006, more than 41 of these sites on the human genome have been linked to the development of obesity when a favorable environment is present.[138] People with two copies of the FTO gene (fat mass and obesity associated gene) have been found on average to weigh 3–4 kg more and have a 1.67-fold greater risk of obesity compared with those without the risk allele.[139] The differences in BMI between people that are due to genetics varies depending on the population examined from 6% to 85%.[140] Studies that have focused on inheritance patterns rather than on specific genes have found that 80% of the offspring of two obese parents were also obese, in contrast to less than 10% of the offspring of two parents who were of normal weight.[143] Different people exposed to the same environment have different risks of obesity due to their underlying genetics.[144] The thrifty gene hypothesis postulates that, due to dietary scarcity during human evolution, people are prone to obesity. Their ability to take advantage of rare periods of abundance by storing energy as fat would be advantageous during times of varying food availability, and individuals with greater adipose reserves would be more likely to survive famine. This tendency to store fat, however, would be maladaptive in societies with stable food supplies.[medical citation needed] This theory has received various criticisms, and other evolutionarily-based theories such as the drifty gene hypothesis and the thrifty phenotype hypothesis have also been proposed.[medical citation needed] Other illnesses Certain physical and mental illnesses and the pharmaceutical substances used to treat them can increase risk of obesity. Medical illnesses that increase obesity risk include several rare genetic syndromes (listed above) as well as some congenital or acquired conditions: hypothyroidism, Cushing's syndrome, growth hormone deficiency,[145] and some eating disorders such as binge eating disorder and night eating syndrome.[2] However, obesity is not regarded as a psychiatric disorder, and therefore is not listed in the DSM-IVR as a psychiatric illness.[146] The risk of overweight and obesity is higher in patients with psychiatric disorders than in persons without psychiatric disorders.[147] Obesity and depression influence each other mutually, with obesity increasing the risk of clinical depression, and also depression leading to a higher chance of developing obesity.[3] Social determinants The disease scroll (Yamai no soshi, late 12th century) depicts a woman moneylender with obesity, considered a disease of the rich.Obesity in developed countries is correlated with economic inequality. While genetic influences are important to understanding obesity, they cannot completely explain the dramatic increase seen within specific countries or globally.[148][better source needed] Though it is accepted that energy consumption in excess of energy expenditure leads to increases in body weight on an individual basis, the cause of the shifts in these two factors on the societal scale is much debated. There are a number of theories as to the cause but most believe it is a combination of various factors. The correlation between social class and BMI varies globally. Research in 1989 found that in developed countries women of a high social class were less likely to be obese. No significant differences were seen among men of different social classes. In the developing world, women, men, and children from high social classes had greater rates of obesity.[better source needed][149] In 2007 repeating the same research found the same relationships, but they were weaker. The decrease in strength of correlation was felt to be due to the effects of globalization.[150] Among developed countries, levels of adult obesity, and percentage of teenage children who are overweight, are correlated with income inequality. A similar relationship is seen among US states: more adults, even in higher social classes, are obese in more unequal states.[151] Many explanations have been put forth for associations between BMI and social class. It is thought that in developed countries, the wealthy are able to afford more nutritious food, they are under greater social pressure to remain slim, and have more opportunities along with greater expectations for physical fitness. In undeveloped countries the ability to afford food, high energy expenditure with physical labor, and cultural values favoring a larger body size are believed to contribute to the observed patterns.[150] Attitudes toward body weight held by people in one's life may also play a role in obesity. A correlation in BMI changes over time has been found among friends, siblings, and spouses.[152] Stress and perceived low social status appear to increase risk of obesity.[151][153][154] Smoking has a significant effect on an individual's weight. Those who quit smoking gain an average of 4.4 kilograms (9.7 lb) for men and 5.0 kilograms (11.0 lb) for women over ten years.[155] However, changing rates of smoking have had little effect on the overall rates of obesity.[156] In the United States, the number of children a person has is related to their risk of obesity. A woman's risk increases by 7% per child, while a man's risk increases by 4% per child.[157] This could be partly explained by the fact that having dependent children decreases physical activity in Western parents.[158] In the developing world urbanization is playing a role in increasing rate of obesity. In China overall rates of obesity are below 5%; however, in some cities rates of obesity are greater than 20%.[159] In part, this may be because of urban design issues (such as inadequate public spaces for physical activity).[128] Time spent in motor vehicles, as opposed to active transportation options such as cycling or walking, is correlated with increased risk of obesity.[160][161] Malnutrition in early life is believed to play a role in the rising rates of obesity in the developing world.[162] Endocrine changes that occur during periods of malnutrition may promote the storage of fat once more food energy becomes available.[162] Gut bacteria The study of the effect of infectious agents on metabolism is still in its early stages. Gut flora has been shown to differ between lean and obese people. There is an indication that gut flora can affect the metabolic potential. This apparent alteration is believed to confer a greater capacity to harvest energy contributing to obesity. Whether these differences are the direct cause or the result of obesity has yet to be determined unequivocally.[163] The use of antibiotics among children has also been associated with obesity later in life.[164][165] An association between viruses and obesity has been found in humans and several different animal species. The amount that these associations may have contributed to the rising rate of obesity is yet to be determined.[166] Other factors Not getting enough sleep is also associated with obesity.[167][168] Whether one causes the other is unclear.[167] Even if short sleep does increase weight gain, it is unclear if this is to a meaningful degree or if increasing sleep would be of benefit.[169] Some have proposed that chemical compounds called \"obesogens\" may play a role in obesity. Certain aspects of personality are associated with being obese.[170]Loneliness,[171]neuroticism, impulsivity, and sensitivity to reward are more common in people who are obese while conscientiousness and self-control are less common in people who are obese.[170][172] Because most of the studies on this topic are questionnaire-based, it is possible that these findings overestimate the relationships between personality and obesity: people who are obese might be aware of the social stigma of obesity and their questionnaire responses might be biased accordingly.[170] Similarly, the personalities of people who are obese as children might be influenced by obesity stigma, rather than these personality factors acting as risk factors for obesity.[170] In relation to globalization, it is known that trade liberalization is linked to obesity; research, based on data from 175 countries during 1975-2016, showed that obesity prevalence was positively correlated with trade openness, and the correlation was stronger in developing countries.[173] Pathophysiology A comparison of a mouse unable to produce leptin thus resulting in obesity (left) and a normal mouse (right) Two distinct but related processes are considered to be involved in the development of obesity: sustained positive energy balance (energy intake exceeding energy expenditure) and the resetting of the body weight \"set point\" at an increased value.[174] The second process explains why finding effective obesity treatments has been difficult. While the underlying biology of this process still remains uncertain, research is beginning to clarify the mechanisms.[174] At a biological level, there are many possible pathophysiological mechanisms involved in the development and maintenance of obesity.[175] This field of research had been almost unapproached until the leptin gene was discovered in 1994 by J. M. Friedman's laboratory.[176] While leptin and ghrelin are produced peripherally, they control appetite through their actions on the central nervous system. In particular, they and other appetite-related hormones act on the hypothalamus, a region of the brain central to the regulation of food intake and energy expenditure. There are several circuits within the hypothalamus that contribute to its role in integrating appetite, the melanocortin pathway being the most well understood.[175] The circuit begins with an area of the hypothalamus, the arcuate nucleus, that has outputs to the lateral hypothalamus (LH) and ventromedial hypothalamus (VMH), the brain's feeding and satiety centers, respectively.[177] The arcuate nucleus contains two distinct groups of neurons.[175] The first group coexpresses neuropeptide Y (NPY) and agouti-related peptide (AgRP) and has stimulatory inputs to the LH and inhibitory inputs to the VMH. The second group coexpresses pro-opiomelanocortin (POMC) and cocaine- and amphetamine-regulated transcript (CART) and has stimulatory inputs to the VMH and inhibitory inputs to the LH. Consequently, NPY/AgRP neurons stimulate feeding and inhibit satiety, while POMC/CART neurons stimulate satiety and inhibit feeding. Both groups of arcuate nucleus neurons are regulated in part by leptin. Leptin inhibits the NPY/AgRP group while stimulating the POMC/CART group. Thus a deficiency in leptin signaling, either via leptin deficiency or leptin resistance, leads to overfeeding and may account for some genetic and acquired forms of obesity.[175] Management The main treatment for obesity consists of weight loss via lifestyle interventions, including prescribed diets and physical exercise.[23][96][178][179] Although it is unclear what diets might support long-term weight loss, and although the effectiveness of low-calorie diets is debated,[180] lifestyle changes that reduce calorie consumption or increase physical exercise over the long term also tend to produce some sustained weight loss, despite slow weight regain over time.[23][180][181][182] Although 87% of participants in the National Weight Control Registry were able to maintain 10% body weight loss for 10 years,[183][clarification needed] the most appropriate dietary approach for long term weight loss maintenance is still unknown.[184] In the US, intensive behavioral interventions combining both dietary changes and exercise are recommended.[23][178][185]Intermittent fasting has no additional benefit of weight loss compared to continuous energy restriction.[184] Adherence is a more important factor in weight loss success than whatever kind of diet an individual undertakes.[184][186] Several hypo-caloric diets are effective.[23] In the short-term low carbohydrate diets appear better than low fat diets for weight loss.[187] In the long term, however, all types of low-carbohydrate and low-fat diets appear equally beneficial.[187][188] Heart disease and diabetes risks associated with different diets appear to be similar.[189] Promotion of the Mediterranean diets among the obese may lower the risk of heart disease.[187] Decreased intake of sweet drinks is also related to weight-loss.[187] Success rates of long-term weight loss maintenance with lifestyle changes are low, ranging from 2–20%.[190] Dietary and lifestyle changes are effective in limiting excessive weight gain in pregnancy and improve outcomes for both the mother and the child.[191] Intensive behavioral counseling is recommended in those who are both obese and have other risk factors for heart disease.[192] Health policy Prevalence of obesity in the adult population, top countries (2016)Prevalence of obesity in the adult population in 2016 Obesity is a complex public health and policy problem because of its prevalence, costs, and health effects.[193] As such, managing it requires changes in the wider societal context and effort by communities, local authorities, and governments.[185] Public health efforts seek to understand and correct the environmental factors responsible for the increasing prevalence of obesity in the population. Solutions look at changing the factors that cause excess food energy consumption and inhibit physical activity. Efforts include federally reimbursed meal programs in schools, limiting direct junkfood marketing to children,[194] and decreasing access to sugar-sweetened beverages in schools.[195] The World Health Organization recommends the taxing of sugary drinks.[196] When constructing urban environments, efforts have been made to increase access to parks and to develop pedestrian routes.[197] Mass media campaigns seem to have limited effectiveness in changing behaviors that influence obesity, but may increase knowledge and awareness regarding physical activity and diet, which might lead to changes in the long term. Campaigns might also be able to reduce the amount of time spent sitting or lying down and positively affect the intention to be active physically.[198][199]Nutritional labelling with energy information on menus might be able to help reducing energy intake while dining in restaurants.[200] Some call for policy against ultra-processed foods.[201][202] Medical interventions Medication Since the introduction of medicines for the management of obesity in the 1930s, many compounds have been tried. Most of them reduce body weight by small amounts, and several of them are no longer marketed for obesity because of their side effects. Out of 25 anti-obesity medications withdrawn from the market between 1964 and 2009, 23 acted by altering the functions of chemical neurotransmitters in the brain. The most common side effects of these drugs that led to withdrawals were mental disturbances, cardiac side effects, and drug abuse or drug dependence. Deaths were reportedly associated with seven products.[203] Five medications beneficial for long-term use are: orlistat, lorcaserin, liraglutide, phentermine–topiramate, and naltrexone–bupropion.[204] They result in weight loss after one year ranged from 3.0 to 6.7 kg (6.6-14.8 lbs) over placebo.[204] Orlistat, liraglutide, and naltrexone–bupropion are available in both the United States and Europe, phentermine–topiramate is available only in the United States.[205] European regulatory authorities rejected lorcaserin and phentermine-topiramate, in part because of associations of heart valve problems with lorcaserin and more general heart and blood vessel problems with phentermine–topiramate.[205] Lorcaserin was available in the United States and then removed from the market in 2020 due to its association with cancer.[206] Orlistat use is associated with high rates of gastrointestinal side effects[207] and concerns have been raised about negative effects on the kidneys.[208] There is no information on how these drugs affect longer-term complications of obesity such as cardiovascular disease or death;[5] however, liraglutide, when used for type 2 diabetes, does reduce cardiovascular events.[209] In 2019 a systematic review compared the effects on weight of various doses of fluoxetine (60 mg/d, 40 mg/d, 20 mg/d, 10 mg/d) in obese adults.[210] When compared to placebo, all dosages of fluoxetine appeared to contribute to weight loss but lead to increased risk of experiencing side effects such as dizziness, drowsiness, fatigue, insomnia and nausea during period of treatment. However, these conclusions were from low certainty evidence.[210] When comparing, in the same review, the effects of fluoxetine on weight of obese adults, to other anti-obesity agents, omega-3 gel and not receiving a treatment, the authors could not reach conclusive results due to poor quality of evidence.[210] Among antipsychotic drugs for treating schizophrenia clozapine is the most effective, but it also has the highest risk of causing the metabolic syndrome, of which obesity is the main feature. For people who gain weight because of clozapine, taking metformin may reportedly improve three of the five components of the metabolic syndrome: waist circumference, fasting glucose, and fasting triglycerides.[211] In earlier historical periods obesity was rare and achievable only by a small elite, although already recognised as a problem for health. But as prosperity increased in the Early Modern period, it affected increasingly larger groups of the population.[215] Prior to the 1970s, obesity was a relatively rare condition even in the wealthiest of nations, and when it did exist it tended to occur among the wealthy. Then, a confluence of events started to change the human condition. The average BMI of populations in first-world countries started to increase, and consequently there was a rapid increase in the proportion of people overweight and obese.[216] In 1997, the WHO formally recognized obesity as a global epidemic.[114] As of 2008, the WHO estimates that at least 500 million adults (greater than 10%) are obese, with higher rates among women than men.[217] The global prevalence of obesity more than doubled between 1980 and 2014. In 2014, more than 600 million adults were obese, equal to about 13 percent of the world's adult population.[218] The percentage of adults affected in the United States as of 2015–2016 is about 39.6% overall (37.9% of males and 41.1% of females).[219] In 2000, the World Health Organization (WHO) stated that overweight and obesity were replacing more traditional public health concerns such as undernutrition and infectious diseases as one of the most significant cause of poor health.[220] The rate of obesity also increases with age at least up to 50 or 60 years old[52]: 5 and severe obesity in the United States, Australia, and Canada is increasing faster than the overall rate of obesity.[31][221][222] The OECD has projected an increase in obesity rates until at least 2030, especially in the United States, Mexico and England with rates reaching 47%, 39% and 35%, respectively.[223] Once considered a problem only of high-income countries, obesity rates are rising worldwide and affecting both the developed and developing world.[224] These increases have been felt most dramatically in urban settings.[217] Sex- and gender-based differences also influence the prevalence of obesity. Globally there are more obese women than men, but the numbers differ depending on how obesity is measured.[225][226] Historical attitudes Ancient Greek medicine recognizes obesity as a medical disorder and records that the Ancient Egyptians saw it in the same way.[215]Hippocrates wrote that \"Corpulence is not only a disease itself, but the harbinger of others\".[2] The Indian surgeon Sushruta (6th century BCE) related obesity to diabetes and heart disorders.[230] He recommended physical work to help cure it and its side effects.[230] For most of human history, mankind struggled with food scarcity.[231] Obesity has thus historically been viewed as a sign of wealth and prosperity. It was common among high officials in Ancient East Asian civilizations.[232] In the 17th century, English medical author Tobias Venner is credited with being one of the first to refer to the term as a societal disease in a published English language book.[215][233] With the onset of the Industrial Revolution, it was realized that the military and economic might of nations were dependent on both the body size and strength of their soldiers and workers.[114] Increasing the average body mass index from what is now considered underweight to what is now the normal range played a significant role in the development of industrialized societies.[114] Height and weight thus both increased through the 19th century in the developed world. During the 20th century, as populations reached their genetic potential for height, weight began increasing much more than height, resulting in obesity.[114] In the 1950s, increasing wealth in the developed world decreased child mortality, but as body weight increased, heart and kidney disease became more common.[114][234] During this time period, insurance companies realized the connection between weight and life expectancy and increased premiums for the obese.[2] Many cultures throughout history have viewed obesity as the result of a character flaw. The obesus or fat character in Ancient Greek comedy was a glutton and figure of mockery. During Christian times, food was viewed as a gateway to the sins of sloth and lust.[21] In modern Western culture, excess weight is often regarded as unattractive, and obesity is commonly associated with various negative stereotypes. People of all ages can face social stigmatization and may be targeted by bullies or shunned by their peers.[235] Public perceptions in Western society regarding healthy body weight differ from those regarding the weight that is considered ideal – and both have changed since the beginning of the 20th century. The weight that is viewed as an ideal has become lower since the 1920s. This is illustrated by the fact that the average height of Miss America pageant winners increased by 2% from 1922 to 1999, while their average weight decreased by 12%.[236] On the other hand, people's views concerning healthy weight have changed in the opposite direction. In Britain, the weight at which people considered themselves to be overweight was significantly higher in 2007 than in 1999.[237] These changes are believed to be due to increasing rates of adiposity leading to increased acceptance of extra body fat as being normal.[237] Obesity is still seen as a sign of wealth and well-being in many parts of Africa. This has become particularly common since the HIV epidemic began.[2] The arts The first sculptural representations of the human body 20,000–35,000 years ago depict obese females. Some attribute the Venus figurines to the tendency to emphasize fertility while others feel they represent \"fatness\" in the people of the time.[21] Corpulence is, however, absent in both Greek and Roman art, probably in keeping with their ideals regarding moderation. This continued through much of Christian European history, with only those of low socioeconomic status being depicted as obese.[21] During the Renaissance some of the upper class began flaunting their large size, as can be seen in portraits of Henry VIII of England and Alessandro dal Borro.[21]Rubens (1577–1640) regularly depicted heavyset women in his pictures, from which derives the term Rubenesque. These women, however, still maintained the \"hourglass\" shape with its relationship to fertility.[238] During the 19th century, views on obesity changed in the Western world. After centuries of obesity being synonymous with wealth and social status, slimness began to be seen as the desirable standard.[21] In his 1819 print, The Belle Alliance, or the Female Reformers of Blackburn!!!, artist George Cruikshank criticised the work of female reformers in Blackburn and used fatness as a means to portray them as unfeminine.[239] Society and culture Economic impact In addition to its health impacts, obesity leads to many problems, including disadvantages in employment[240]: 29 [241] and increased business costs. These effects are felt by all levels of society, from individuals, to corporations, to governments. In 2005, the medical costs attributable to obesity in the US were an estimated $190.2 billion or 20.6% of all medical expenditures,[242][243][244] while the cost of obesity in Canada was estimated at CA$2 billion in 1997 (2.4% of total health costs).[96] The total annual direct cost of overweight and obesity in Australia in 2005 was A$21 billion. Overweight and obese Australians also received A$35.6 billion in government subsidies.[245] The estimated range for annual expenditures on diet products is $40 billion to $100 billion in the US alone.[246] The Lancet Commission on Obesity in 2019 called for a global treaty—modelled on the WHO Framework Convention on Tobacco Control—committing countries to address obesity and undernutrition, explicitly excluding the food industry from policy development. They estimate the global cost of obesity $2 trillion a year, about or 2.8% of world GDP.[247] Obesity prevention programs have been found to reduce the cost of treating obesity-related disease. However, the longer people live, the more medical costs they incur. Researchers, therefore, conclude that reducing obesity may improve the public's health, but it is unlikely to reduce overall health spending.[248] Sin taxes such as a sugary drink tax have been implemented in certain countries globally to curb dietary and consumer habits, and as an effort to offset the economic tolls. Services accommodate obese people with specialized equipment such as much wider chairs.[249] Obesity can lead to social stigmatization and disadvantages in employment.[240]: 29 When compared to their normal weight counterparts, obese workers on average have higher rates of absenteeism from work and take more disability leave, thus increasing costs for employers and decreasing productivity.[250] A study examining Duke University employees found that people with a BMI over 40 kg/m2 filed twice as many workers' compensation claims as those whose BMI was 18.5–24.9 kg/m2. They also had more than 12 times as many lost work days. The most common injuries in this group were due to falls and lifting, thus affecting the lower extremities, wrists or hands, and backs.[251] The Alabama State Employees' Insurance Board approved a controversial plan to charge obese workers $25 a month for health insurance that would otherwise be free unless they take steps to lose weight and improve their health. These measures started in January 2010 and apply to those state workers whose BMI exceeds 35 kg/m2 and who fail to make improvements in their health after one year.[252] Some research shows that obese people are less likely to be hired for a job and are less likely to be promoted.[235] Obese people are also paid less than their non-obese counterparts for an equivalent job; obese women on average make 6% less and obese men make 3% less.[240]: 30 Specific industries, such as the airline, healthcare and food industries, have special concerns. Due to rising rates of obesity, airlines face higher fuel costs and pressures to increase seating width.[253] In 2000, the extra weight of obese passengers cost airlines US$275 million.[254] The healthcare industry has had to invest in special facilities for handling severely obese patients, including special lifting equipment and bariatric ambulances.[255] Costs for restaurants are increased by litigation accusing them of causing obesity.[256] In 2005, the US Congress discussed legislation to prevent civil lawsuits against the food industry in relation to obesity; however, it did not become law.[256] With the American Medical Association's 2013 classification of obesity as a chronic disease,[24] it is thought that health insurance companies will more likely pay for obesity treatment, counseling and surgery, and the cost of research and development of fat treatment pills or gene therapy treatments should be more affordable if insurers help to subsidize their cost.[257] The AMA classification is not legally binding, however, so health insurers still have the right to reject coverage for a treatment or procedure.[257] In 2014, The European Court of Justice ruled that morbid obesity is a disability. The Court said that if an employee's obesity prevents them from \"full and effective participation of that person in professional life on an equal basis with other workers\", then it shall be considered a disability and that firing someone on such grounds is discriminatory.[258] In low-income countries, obesity can be a signal of wealth. A 2023 experimental study found that obese individuals in Uganda were more likely to access credit.[259] The principal goal of the fat acceptance movement is to decrease discrimination against people who are overweight and obese.[261][262] However, some in the movement are also attempting to challenge the established relationship between obesity and negative health outcomes.[263] A number of organizations exist that promote the acceptance of obesity. They have increased in prominence in the latter half of the 20th century.[264] The US-based National Association to Advance Fat Acceptance (NAAFA) was formed in 1969 and describes itself as a civil rights organization dedicated to ending size discrimination.[265] The International Size Acceptance Association (ISAA) is a non-governmental organization (NGO) which was founded in 1997. It has more of a global orientation and describes its mission as promoting size acceptance and helping to end weight-based discrimination.[266] These groups often argue for the recognition of obesity as a disability under the US Americans With Disabilities Act (ADA). The American legal system, however, has decided that the potential public health costs exceed the benefits of extending this anti-discrimination law to cover obesity.[263] Industry influence on research In 2015, the New York Times published an article on the Global Energy Balance Network, a nonprofit founded in 2014 that advocated for people to focus on increasing exercise rather than reducing calorie intake to avoid obesity and to be healthy. The organization was founded with at least $1.5M in funding from the Coca-Cola Company, and the company has provided $4M in research funding to the two founding scientists Gregory A. Hand and Steven N. Blair since 2008.[267][268] Reports Many organizations have published reports pertaining to obesity. In 1998, the first US Federal guidelines were published, titled \"Clinical Guidelines on the Identification, Evaluation, and Treatment of Overweight and Obesity in Adults: The Evidence Report\".[269] In 2006, the Canadian Obesity Network, now known as Obesity Canada published the \"Canadian Clinical Practice Guidelines (CPG) on the Management and Prevention of Obesity in Adults and Children\". This is a comprehensive evidence-based guideline to address the management and prevention of overweight and obesity in adults and children.[96] Childhood obesity The healthy BMI range varies with the age and sex of the child. Obesity in children and adolescents is defined as a BMI greater than the 95th percentile.[275] The reference data that these percentiles are based on is from 1963 to 1994 and thus has not been affected by the recent increases in rates of obesity.[276] Childhood obesity has reached epidemic proportions in the 21st century, with rising rates in both the developed and the developing world. Rates of obesity in Canadian boys have increased from 11% in the 1980s to over 30% in the 1990s, while during this same time period rates increased from 4 to 14% in Brazilian children.[277] In the UK, there were 60% more obese children in 2005 compared to 1989.[278] In the US, the percentage of overweight and obese children increased to 16% in 2008, a 300% increase over the prior 30 years.[279] As with obesity in adults, many factors contribute to the rising rates of childhood obesity. Changing diet and decreasing physical activity are believed to be the two most important causes for the recent increase in the incidence of child obesity.[280]Advertising of unhealthy foods to children also contributes, as it increases their consumption of the product.[281] Antibiotics in the first 6 months of life have been associated with excess weight at age seven to twelve years of age.[165] Because childhood obesity often persists into adulthood and is associated with numerous chronic illnesses, children who are obese are often tested for hypertension, diabetes, hyperlipidemia, and fatty liver disease.[96] Treatments used in children are primarily lifestyle interventions and behavioral techniques, although efforts to increase activity in children have had little success.[282] In the United States, medications are not FDA approved for use in this age group.[277] Brief weight management interventions in primary care (e.g. delivered by a physician or nurse practitioner) have only a marginal positive effect in reducing childhood overweight or obesity.[283] Multi-component behaviour change interventions that include changes to dietary and physical activity may reduce BMI in the short term in children aged 6 to 11 years, although the benefits are small and quality of evidence is low.[284] Other animals Obesity in pets is common in many countries. In the United States, 23–41% of dogs are overweight, and about 5.1% are obese.[285] The rate of obesity in cats was slightly higher at 6.4%.[285] In Australia, the rate of obesity among dogs in a veterinary setting has been found to be 7.6%.[286] The risk of obesity in dogs is related to whether or not their owners are obese; however, there is no similar correlation between cats and their owners.[287]"}
{"url": "https://en.m.wikipedia.org/wiki/Uniform_Resource_Name", "text": "Uniform Resource Name A Uniform Resource Name (URN) is a Uniform Resource Identifier (URI) that uses the urnscheme. URNs are globally unique persistent identifiers assigned within defined namespaces so they will be available for a long period of time, even after the resource which they identify ceases to exist or becomes unavailable.[1] URNs cannot be used to directly locate an item and need not be resolvable, as they are simply templates that another parser may use to find an item. Contents URNs were originally conceived to be part of a three-part information architecture for the Internet, along with Uniform Resource Locators (URLs) and Uniform Resource Characteristics (URCs), a metadata framework. As described in RFC1737,[2] and later in RFC 2141,[3] URNs were distinguished from URLs, which identify resources by specifying their locations in the context of a particular access protocol, such as HTTP or FTP. In contrast, URNs were conceived as persistent, location-independent identifiers assigned within defined namespaces, typically by an authority responsible for the namespace, so that they are globally unique and persistent over long periods of time, even after the resource which they identify ceases to exist or becomes unavailable.[1] URCs never progressed past the conceptual stage,[4] and other technologies such as the Resource Description Framework later took their place. Since RFC 3986[5] in 2005, use of the terms \"Uniform Resource Name\" and \"Uniform Resource Locator\" has been deprecated in technical standards in favor of the term Uniform Resource Identifier (URI), which encompasses both, a view proposed in 2001 by a joint working group between the World Wide Web Consortium (W3C) and Internet Engineering Task Force (IETF).[4] A URI is a string of characters used to identify or name a resource on the internet. URIs are used in many Internet protocols to refer to and access information resources. URI schemes include the http and ftp protocols, as well as hundreds of others. In the \"contemporary view\", as it is called, all URIs identify or name resources, perhaps uniquely and persistently, with some of them also being \"locators\" which are resolvable in conjunction with a specified protocol to a representation of the resources. Other URIs are not locators and are not necessarily resolvable within the bounds of the systems where they are found. These URIs may serve as names or identifiers of resources. Since resources can move, opaque identifiers which are not locators and are not bound to particular locations are arguably more likely than identifiers which are locators to remain unique and persistent over time. But whether a URI is resolvable depends on many operational and practical details, irrespective of whether it is called a \"name\" or a \"locator\". In the contemporary view, there is no bright line between \"names\" and \"locators\". In accord with this way of thinking, the distinction between Uniform Resource Names and Uniform Resource Locators is now no longer used in formal Internet Engineering Task Force technical standards, though the latter term, URL, is still in wide informal use. The term \"URN\" continues now as one of more than a hundred URI \"schemes\", urn:, paralleling http:, ftp:, and so forth. URIs of the urn: scheme are not locators, are not required to be associated with a particular protocol or access method, and need not be resolvable. They should be assigned by a procedure which provides some assurance that they will remain unique and identify the same resource persistently over a prolonged period. Some namespaces under the urn: scheme, such as urn:uuid: assign identifiers in a manner which does not require a registration authority, but most of them do. A typical URN namespace is urn:isbn, for International Standard Book Numbers. This view is continued in RFC 8141 (2017).[1] There are other URI schemes, such as tag:, info: (now largely deprecated), and ni:[6] which are similar to the urn: scheme in not being locators and not being associated with particular resolution or access protocols. <NID> is the namespace identifier, and may include letters, digits, and -. The NID is followed by the namespace-specific string <NSS>, the interpretation of which depends on the specified namespace. The NSS may contain ASCII letters and digits, and many punctuation and special characters. Disallowed ASCII and Unicode characters may be included if percent-encoded. In order to ensure the global uniqueness of URN namespaces, their identifiers (NIDs) are required to be registered with the IANA. Registered namespaces may be \"formal\" or \"informal\". An exception to the registration requirement was formerly made for \"experimental namespaces\",[8] since rescinded by RFC 8141.[1] Approximately sixty formal URN namespace identifiers have been registered. These are namespaces where Internet users are expected to benefit from their publication,[1] and are subject to several restrictions. They must: An exception to the registration requirement was formerly made for \"experimental namespaces\".[8] However, following the deprecation of the \"X-\" notation for new identifier names,[9]RFC 8141[1] did away with experimental URN namespaces, indicating a preference for use of the urn:example namespace where appropriate.[10]"}
{"url": "https://en.m.wikipedia.org/wiki/Library_of_Congress", "text": "Congress moved to Washington, D.C., in 1800 after holding sessions for eleven years in the temporary national capitals in New York City and Philadelphia. In both cities, members of the U.S. Congress had access to the sizable collections of the New York Society Library and the Library Company of Philadelphia.[7] In Washington, the library was housed in the United States Capitol for almost all of the 19th century. Much of the library's original collection was burnt by British forces during the War of 1812. Congress then purchased Thomas Jefferson's entire personal collection of 6,487 books to restore its own collection. Over the next few years, its collection slowly grew; in 1851, another fire broke out in the Capitol chambers. This destroyed a large amount of the collection, including many of Jefferson's books. After the American Civil War, the importance of the Library of Congress for legislative research increased and there was a campaign to purchase replacement copies for volumes for its lost books. The library received the right of transference of all copyrighted works, and deposit of two copies of books, maps, illustrations, and diagrams printed in the United States. The Library also built its collections through acquisitions and donations. Between 1888 and 1894, Congress constructed and moved the collection to a large adjacent library building, now known as the Thomas Jefferson Building, across the street from the Capitol. Two more adjacent library buildings, the John Adams Building, built in the 1930s, and the James Madison Memorial Building, built in the 1970s, hold expanded parts of the collection and provide space for additional library services. The library's primary mission is to research inquiries made by members of Congress, which is carried out through the Congressional Research Service. It also houses and oversees the United States Copyright Office. The library is open to the public for research, although only high-ranking government officials and library employees may borrow (i.e., temporarily take custody of) books and materials.[8] James Madison of Virginia is credited with the idea of creating a congressional library, first making such a proposition in 1783. Madison's initial proposal was rejected at the time, but represented the first real introduction of the idea of a congressional library. In the years after the Revolutionary War, the Philadelphia Library Company and New York Society Library served as surrogate congressional libraries whenever Congress held session in those respective cities.[9] The Library of Congress was established on April 24, 1800, when President John Adams signed an act of Congress, which also provided for the transfer of the seat of government from Philadelphia to the new capital city of Washington. Part of the legislation appropriated $5,000 \"for the purchase of such books as may be necessary for the use of Congress ... and for fitting up a suitable apartment for containing them.\"[10] Books were ordered from London, and the collection consisted of 740 books and three maps, which were housed in the new United States Capitol.[11] President Thomas Jefferson played an important role in establishing the structure of the Library of Congress. On January 26, 1802, he signed a bill that allowed the president to appoint the librarian of Congress and established a Joint Committee on the Library to regulate and oversee it. The new law also extended borrowing privileges to the president and vice president.[12][13] In August 1814, after defeating an American force at Bladensburg, British forces under the command of George Cockburn proceeded to occupy Washington. In retaliation for acts of destruction by American troops in the Canadas, Cockburn ordered his men to burn numerous government buildings throughout the city. Among the buildings targeted was the Library of Congress, which contained 3,000 congressional volumes at the time, most of which were destroyed in the burning.[11] These volumes had been held in the Senate wing of the Capitol; one of the few congressional volumes to survive was a government account book of receipts and expenditures for 1810.[13][14][15] The volume was taken as a souvenir by Cockburn, whose family returned it to the United States in 1940.[16] Within a month, Thomas Jefferson offered to sell his large personal library[17][18][19] as a replacement. Congress accepted his offer in January 1815, appropriating $23,950 to purchase his 6,487 books.[11] Some members of the House of Representatives opposed the outright purchase, including New Hampshire representative Daniel Webster. He wanted to return \"all books of an atheistical, irreligious, and immoral tendency\".[20] Jefferson had spent 50 years accumulating a wide variety of books in several languages, and on subjects such as philosophy, history, law, religion, architecture, travel, natural sciences, mathematics, studies of classical Greece and Rome, modern inventions, hot air balloons, music, submarines, fossils, agriculture, and meteorology.[9] He had also collected books on topics not normally viewed as part of a legislative library, such as cookbooks. But, he believed that all subjects had a place in the Library of Congress. He remarked: I do not know that it contains any branch of science which Congress would wish to exclude from their collection; there is, in fact, no subject to which a Member of Congress may not have occasion to refer.[20] Jefferson's collection was unique in that it was the working collection of a scholar, not a gentleman's collection for display. With the addition of his collection, which doubled the size of the original library, the Library of Congress was transformed from a specialist's library to a more general one.[21] His original collection was organized into a scheme based on Francis Bacon's organization of knowledge. Specifically, Jefferson had grouped his books into Memory, Reason, and Imagination, and broke them into 44 more subdivisions.[22] The library followed Jefferson's organization scheme until the late 19th century, when librarian Herbert Putnam began work on a more flexible Library of Congress Classification structure. This now applies to more than 138 million items. A February 24, 1824, report from the Committee of Ways and Means, stemming from a request by the House of Representatives \"to inquire into the expediency of appropriating five thousand dollars for the use of the Library of Congress,\"[23] recommended the appropriation and noted that ...the committee have discovered the Library of Congress, in its present state, to be defective in all the principal branches of literature; and they deem it of the first necessity, that this deficiency should be speedily supplied, at least, in the important branches of Law, Politics, Commerce, History, and Geography, as most useful to the Members of Congress.[23] On December 24, 1851, the largest fire in the library's history destroyed 35,000 books, two-thirds of the library's collection, and two-thirds of Jefferson's original transfer. Congress appropriated $168,700 to replace the lost books in 1852 but not to acquire new materials.[24] (By 2008, the librarians of Congress had found replacements for all but 300 of the works that had been documented as being in Jefferson's original collection.[25]) This marked the start of a conservative period in the library's administration by librarian John Silva Meehan and joint committee chairman James A. Pearce, who restricted the library's activities.[24] Meehan and Pearce's views about a restricted scope for the Library of Congress reflected those shared by members of Congress. While Meehan was a librarian, he supported and perpetuated the notion that \"the congressional library should play a limited role on the national scene and that its collections, by and large, should emphasize American materials of obvious use to the U.S. Congress.\"[26] In 1859, Congress transferred the library's public document distribution activities to the Department of the Interior and its international book exchange program to the Department of State.[27] During the 1850s, Smithsonian Institution librarian Charles Coffin Jewett aggressively tried to develop the Smithsonian as the United States national library. His efforts were rejected by Smithsonian secretary Joseph Henry, who advocated a focus on scientific research and publication.[28] To reinforce his intentions for the Smithsonian, Henry established laboratories, developed a robust physical sciences library, and started the Smithsonian Contributions to Knowledge, the first of many publications intended to disseminate research results.[29] For Henry, the Library of Congress was the obvious choice as the national library. Unable to resolve the conflict, Henry dismissed Jewett in July 1854. In 1865, the Smithsonian building, also called the Castle due to its Norman architectural style, was severely damaged by fire. This incident presented Henry with an opportunity related to the Smithsonian's non-scientific library. Around this time, the Library of Congress was planning to build and relocate to the new Thomas Jefferson Building, designed to be fireproof.[30] Authorized by an act of Congress, Henry transferred the Smithsonian's non-scientific library of 40,000 volumes to the Library of Congress in 1866.[31] President Abraham Lincoln appointed John G. Stephenson as librarian of Congress in 1861; the appointment is regarded as the most political to date.[32] Stephenson was a physician and spent equal time serving as librarian and as a physician in the Union Army. He could manage this division of interest because he hired Ainsworth Rand Spofford as his assistant.[32] Despite his new job, Stephenson focused on the war. Three weeks into his term as Librarian of Congress, he left Washington, D.C., to serve as a volunteer aide-de-camp at the battles of Chancellorsville and Gettysburg during the American Civil War.[32] Stephenson's hiring of Spofford, who directed the library in his absence, may have been his most significant achievement.[32] Librarian Ainsworth Rand Spofford, who directed the Library of Congress from 1865 to 1897, built broad bipartisan support to develop it as a national library and a legislative resource.[33][34] He was aided by expansion of the federal government after the war and a favorable political climate. He began comprehensively collecting Americana and American literature, led the construction of a new building to house the library, and transformed the librarian of Congress position into one of strength and independence. Between 1865 and 1870, Congress appropriated funds for the construction of the Thomas Jefferson Building, placed all copyright registration and deposit activities under the library's control, and restored the international book exchange. The library also acquired the vast libraries of the Smithsonian and of historian Peter Force, strengthening its scientific and Americana collections significantly. By 1876, the Library of Congress had 300,000 volumes; it was tied with the Boston Public Library as the nation's largest library. It moved from the Capitol building to its new headquarters in 1897 with more than 840,000 volumes, 40 percent of which had been acquired through copyright deposit.[11] A year before the library's relocation, the Joint Library Committee held hearings to assess the condition of the library and plan for its future growth and possible reorganization. Spofford and six experts sent by the American Library Association[35] testified that the library should continue its expansion to become a true national library. Based on the hearings, Congress authorized a budget that allowed the library to more than double its staff, from 42 to 108 persons. Senators Justin Morrill of Vermont and Daniel W. Voorhees of Indiana were particularly helpful in gaining this support. The library also established new administrative units for all aspects of the collection. In its bill, Congress strengthened the role of Librarian of Congress: it became responsible for governing the library and making staff appointments. As with presidential Cabinet appointments, the Senate was required to approve presidential appointees to the position.[11] In 1893, Elizabeth Dwyer became the first woman to be appointed to the staff of the library.[36] Library of Congress in its new building in 1902, since renamed for Thomas JeffersonThomas Jefferson Building, the library's main building With this support and the 1897 reorganization, the Library of Congress began to grow and develop more rapidly. Spofford's successor John Russell Young overhauled the library's bureaucracy, used his connections as a former diplomat to acquire more materials from around the world, and established the library's first assistance programs for the blind and physically disabled. Young's successor Herbert Putnam held the office for forty years from 1899 to 1939. Two years after he took office, the library became the first in the United States to hold one million volumes.[11] Putnam focused his efforts to make the library more accessible and useful for the public and for other libraries. He instituted the interlibrary loan service, transforming the Library of Congress into what he referred to as a \"library of last resort\".[37] Putnam also expanded library access to \"scientific investigators and duly qualified individuals\", and began publishing primary sources for the benefit of scholars.[11] During Putnam's tenure, the library broadened the diversity of its acquisitions. In 1903, Putnam persuaded President Theodore Roosevelt to use an executive order to transfer the papers of the Founding Fathers from the State Department to the Library of Congress. Putnam expanded foreign acquisitions as well, including the 1904 purchase of a 4,000-volume library of Indica, the 1906 purchase of G. V. Yudin's 80,000-volume Russian library, the 1908 Schatz collection of early opera librettos, and the early 1930s purchase of the Russian Imperial Collection, consisting of 2,600 volumes from the library of the Romanov family on a variety of topics. Collections of Hebraica, Chinese, and Japanese works were also acquired. On one occasion, Congress initiated an acquisition: in 1929 Congressman Ross Collins (D-Mississippi) gained approval for the library to purchase Otto Vollbehr's collection of incunabula for $1.5 million. This collection included one of three remaining perfect vellum copies of the Gutenberg Bible.[11][38] Putnam established the Legislative Reference Service (LRS) in 1914 as a separative administrative unit of the library. Based on the Progressive era's philosophy of science to be used to solve problems, and modeled after successful research branches of state legislatures, the LRS would provide informed answers to Congressional research inquiries on almost any topic. Congress passed in 1925 an act allowing the Library of Congress to establish a trust fund board to accept donations and endowments, giving the library a role as a patron of the arts. The library received donations and endowments by such prominent wealthy individuals as John D. Rockefeller, James B. Wilbur, and Archer M. Huntington. Gertrude Clarke Whittall donated five Stradivarius violins to the library. Elizabeth Sprague Coolidge's donations paid for a concert hall to be constructed within the Library of Congress building and an honorarium established for the Music Division to pay live performers for concerts. A number of chairs and consultantships were established from the donations, the most well-known of which is the Poet Laureate Consultant.[11] The library's expansion eventually filled the library's Main Building, although it used shelving expansions in 1910 and 1927. The library needed to expand into a new structure. Congress acquired nearby land in 1928 and approved construction of the Annex Building (later known as the John Adams Building) in 1930. Although delayed during the Depression years, it was completed in 1938 and opened to the public in 1939.[11] After Putnam retired in 1939, President Franklin D. Roosevelt appointed poet and writer Archibald MacLeish as his successor. Occupying the post from 1939 to 1944 during the height of World War II, MacLeish became the most widely known librarian of Congress in the library's history. MacLeish encouraged librarians to oppose totalitarianism on behalf of democracy; dedicated the South Reading Room of the Adams Building to Thomas Jefferson, and commissioned artist Ezra Winter to paint four themed murals for the room. He established a \"democracy alcove\" in the Main Reading Room of the Jefferson Building for essential documents such as the Declaration of Independence, the Constitution, and The Federalist Papers. The Library of Congress assisted during the war effort, ranging from storage of the Declaration of Independence and the United States Constitution in Fort Knox for safekeeping to researching weather data on the Himalayas for Air Force pilots. MacLeish resigned in 1944 when appointed as Assistant Secretary of State. President Harry Truman appointed Luther H. Evans as librarian of Congress. Evans, who served until 1953, expanded the library's acquisitions, cataloging, and bibliographic services. But he is best known for creating Library of Congress Missions worldwide. Missions played a variety of roles in the postwar world: the mission in San Francisco assisted participants in the meeting that established the United Nations, the mission in Europe acquired European publications for the Library of Congress and other American libraries, and the mission in Japan aided in the creation of the National Diet Library.[11] Evans' successor Lawrence Quincy Mumford took over in 1953. During his tenure, lasting until 1974, Mumford directed the initiation of construction of the James Madison Memorial Building, the third Library of Congress building on Capitol Hill. Mumford led the library during the government's increased educational spending. The library was able to establish new acquisition centers abroad, including in Cairo and New Delhi. In 1967, the library began experimenting with book preservation techniques through a Preservation Office. This has developed as the most extensive library research and conservation effort in the United States. During Mumford's administration, the last significant public debate occurred about the Library of Congress's role as both a legislative and national library. Asked by Joint Library Committee chairman Senator Claiborne Pell (D-RI) to assess operations and make recommendations, Douglas Bryant of Harvard University Library proposed several institutional reforms. These included expanding national activities and services and various organizational changes, all of which would emphasize the library's federal role rather than its legislative role. Bryant suggested changing the name of the Library of Congress, a recommendation rebuked by Mumford as \"unspeakable violence to tradition.\" The debate continued within the library community for some time. The Legislative Reorganization Act of 1970 renewed emphasis for the library on its legislative roles, requiring a greater focus on research for Congress and congressional committees, and renaming the Legislative Reference Service as the Congressional Research Service.[11] After Mumford retired in 1974, President Gerald Ford appointed historian Daniel J. Boorstin as a librarian. Boorstin's first challenge was to manage the relocation of some sections to the new Madison Building, which took place between 1980 and 1982. With this accomplished, Boorstin focused on other areas of library administration, such as acquisitions and collections. Taking advantage of steady budgetary growth, from $116 million in 1975 to over $250 million by 1987, Boorstin enhanced institutional and staff ties with scholars, authors, publishers, cultural leaders, and the business community. His activities changed the post of librarian of Congress so that by the time he retired in 1987, The New York Times called this office \"perhaps the leading intellectual public position in the nation.\" Under Billington's leadership, the library doubled the size of its analog collections from 85.5 million items in 1987 to more than 160 million items in 2014. At the same time, it established new programs and employed new technologies to \"get the champagne out of the bottle\". These included: Thomas.gov website launched in 1994 to provide free public access to U.S. federal legislative information with ongoing updates; and Congress.gov website to provide a state-of-the-art framework for both Congress and the public in 2012;[42] Kluge Center, started with a grant of $60 million from John W. Kluge in 2000, brings international scholars and researchers to use library resources and to interact with policymakers and the public. It hosts public lectures and scholarly events, provides endowed Kluge fellowships, and awards the Kluge Prize for the Study of Humanity (now worth $1.5 million), the first Nobel-level international prize for lifetime achievement in the humanities and social sciences (subjects not included in the Nobel awards);[45] Open World Leadership Center, established in 2000; by 2015 this program administered 23,000 professional exchanges for emerging post-Soviet leaders in Russia, Ukraine, and other successor states of the former USSR. Open World began as a Library of Congress project, and later was established as an independent agency in the legislative branch.[46] The library has made some of these available on the Internet for free streaming and additionally has provided brief essays on the films that have been added to the registry.[49][50] By 2015, the librarian had named 650 films to the registry.[51] The films in the collection date from the earliest period to ones produced more than ten years ago; they are selected from nominations submitted to the board. Further programs included: World Digital Library, established in association with UNESCO and 181 partners in 81 countries in 2009, makes copies of professionally curated primary materials of the world's varied cultures freely available online in multiple languages.[54] National Jukebox, launched in 2011, provides streaming free online access to more than 10,000 out-of-print music and spoken-word recordings.[55] Under Billington, public spaces of the Jefferson Building were enlarged and technologically enhanced to serve as a national exhibition venue. It has hosted more than 100 exhibitions.[58] These included exhibits on the Vatican Library and the Bibliothèque Nationale de France, several on the Civil War and Lincoln, on African-American culture, on Religion and the founding of the American Republic, the Early Americas (the Kislak Collection became a permanent display), on the global celebration commemorating the 800th anniversary of Magna Carta, and on early American printing, featuring the Rubenstein Bay Psalm Book. Onsite access to the Library of Congress has been increased. Billington gained an underground connection between the new U.S. Capitol Visitors Center and the library in 2008 in order to increase both congressional usage and public tours of the library's Thomas Jefferson Building.[40] In 2001, the library began a mass deacidification program, in order to extend the lifespan of almost 4 million volumes and 12 million manuscript sheets. In 2002, a new storage facility was completed at Fort Meade, Maryland,[5] where a collection of storage modules have preserved and made accessible more than 4 million items from the library's analog collections.[citation needed] Billington established the Library Collections Security Oversight Committee in 1992 to improve protection of the collections, and also the Library of Congress Congressional Caucus in 2008 to draw attention to the library's curators and collections. He created the library's first Young Readers Center in the Jefferson Building in 2009, and the first large-scale summer intern (Junior Fellows) program for university students in 1991.[59] Under Billington, the library sponsored the Gateway to Knowledge in 2010 to 2011, a mobile exhibition to ninety sites, covering all states east of the Mississippi, in a specially designed eighteen-wheel truck. This increased public access to library collections off-site, particularly for rural populations, and helped raise awareness of what was also available online.[60] Billington raised more than half a billion dollars of private support to supplement Congressional appropriations for library collections, programs, and digital outreach. These private funds helped the library to continue its growth and outreach in the face of a 30% decrease in staffing, caused mainly by legislative appropriations cutbacks. He created the library's first development office for private fundraising in 1987. In 1990, he established the James Madison Council, the library's first national private sector donor-support group. In 1987, Billington also asked the Government Accountability Office (GAO) to conduct the first library-wide audit. He created the first Office of the Inspector General at the library to provide regular, independent reviews of library operations. This precedent has resulted in regular annual financial audits at the library; it has received unmodified (\"clean\") opinions from 1995 onward.[40] In April 2010, the library announced plans to archive all public communication on Twitter, including all communication since Twitter's launch in March 2006.[61] As of 2015[update], the Twitter archive remains unfinished.[62] Before retiring in 2015, after 28 years of service, Billington had come \"under pressure\" as librarian of Congress.[63] This followed a GAO report that described a \"work environment lacking central oversight\" and faulted Billington for \"ignoring repeated calls to hire a chief information officer, as required by law.\"[64] When Billington announced his plans to retire in 2015, commentator George Weigel described the Library of Congress as \"one of the last refuges in Washington of serious bipartisanship and calm, considered conversation\", and \"one of the world's greatest cultural centers\".[65]Carla Hayden was sworn in as the 14th librarian of Congress on September 14, 2016, the first woman and the first African American to hold the position.[66][67] In 2017, the library announced the Librarian-in-Residence program, which aims to support the future generation of librarians by giving them the opportunity to gain work experience in five different areas of librarianship, including: Acquisitions/Collection Development, Cataloging/Metadata, and Collection Preservation.[68] On February 14, 2023, the Library announced that the Lilly Endowment gifted $2.5 million, five-year grant to \"launch programs that foster greater understanding of religious cultures in Africa, Central Asia and the Middle East\".[73] The Library plans to leverage the donation in these areas: Provide public access to \"programs that enhance knowledge about faiths practiced in the regions, including Indigenous African religious traditions, Judaism, Christianity and Islam, and their influence on daily life.\"[73] The library serves as a legal repository for copyright protection and copyright registration, and as the base for the United States Copyright Office. Regardless of whether they register their copyright, all publishers are required to submit two complete copies of their published works to the library—this requirement is known as mandatory deposit.[80] Nearly 15,000 new items published in the U.S. arrive every business day at the library. Contrary to popular belief, however, the library does not retain all of these works in its permanent collection, although it does add an average of 12,000 items per day.[4] Rejected items are used in trades with other libraries around the world, distributed to federal agencies, or donated to schools, communities, and other organizations within the United States.[4] As is true of many similar libraries, the Library of Congress retains copies of every publication in the English language that is deemed significant. The Library of Congress states that its collection fills about 838 mi (1,349 km) of bookshelves and holds more than 167 million items with over 39 million books and other print materials.[6] A 2000 study by information scientists Peter Lyman and Hal Varian suggested that the amount of uncompressed textual data represented by the 26 million books then in the collection was 10 terabytes.[81] The library's first digitization project was called \"American Memory\". Launched in 1990, it initially planned to choose 160 million objects from its collection to make digitally available on LaserDiscs and CDs that would be distributed to schools and libraries. After realizing that this plan would be too expensive and inefficient, and with the rise of the Internet, the library decided to instead make digitized material available over the Internet. This project was made official in the National Digital Library Program (NDLP), created in October 1994. By 1999, the NDLP had succeeded in digitizing over 5 million objects and had a budget of $12 million. The library has kept the \"American Memory\" name for its public domain website, which today contains 15 million digital objects, comprising over 7 petabytes of data.[82] American Memory is a source for public domain image resources, as well as audio, video, and archived Web content. Nearly all of the lists of holdings, the catalogs of the library, can be consulted directly on its website. Librarians all over the world consult these catalogs, through the Web or through other media better suited to their needs, when they need to catalog for their collection a book published in the United States. They use the Library of Congress Control Number to make sure of the exact identity of the book. Digital images are also available at Snapshots of the Past, which provides archival prints.[83] The library has a budget of $6–8 million each year for digitization, meaning that not all works can be digitized. It makes determinations about what objects to prioritize based on what is especially important to Congress or potentially interesting for the public. The 15 million digitized items represent less than 10% of the library's total 160-million-item collection. In 1995, the Library of Congress established an online archive of the proceedings of the U.S. Congress, THOMAS. The THOMAS website included the full text of proposed legislation, as well as bill summaries and statuses, Congressional Record text, and the Congressional Record Index. The THOMAS system received major updates in 2005 and 2010. A migration to a more modernized Web system, Congress.gov, began in 2012, and the THOMAS system was retired in 2016.[84] Congress.gov is a joint project of the Library of Congress, the House, the Senate and the Government Publishing Office.[85] The Library of Congress is physically housed in three buildings on Capitol Hill and a conservation center in rural Virginia. The library's Capitol Hill buildings are all connected by underground passageways, so that a library user need pass through security only once in a single visit. The library also has off-site storage facilities in Maryland for less commonly requested materials. The Thomas Jefferson Building is located between Independence Avenue and East Capitol Street on First Street SE. It first opened in 1897 as the main building of the library and is the oldest of the three buildings. Known originally as the Library of Congress Building or Main Building, it took its present name on June 13, 1980.[86] The John Adams Building is located between Independence Avenue and East Capitol Street on 2nd Street SE, the block adjacent to the Jefferson Building. The building was originally known as The Annex to the Main Building, which had run out of space. It opened its doors to the public on January 3, 1939.[87] Initially, it also housed the U.S. Copyright Office which moved to the Madison building in the 1970s. The James Madison Memorial Building is located between First and Second Streets on Independence Avenue SE. The building was constructed from 1971 to 1976, and serves as the official memorial to President James Madison.[88] The Madison Building is also home to the U.S. Copyright Office and to the Mary Pickford Theater, the \"motion picture and television reading room\" of the Library of Congress. The theater hosts regular free screenings of classic and contemporary movies and television shows.[89] The Library of Congress, through both the librarian of Congress and the register of copyrights, is responsible for authorizing exceptions to Section 1201 of Title 17 of the United States Code as part of the Digital Millennium Copyright Act. This process is done every three years, with the register receiving proposals from the public and acting as an advisor to the librarian, who issues a ruling on what is exempt. After three years have passed, the ruling is no longer valid and a new ruling on exemptions must be made.[92][93] The library is open for academic research to anyone with a Reader Identification Card. One may not remove library items from the reading rooms or the library buildings. Most of the library's general collection of books and journals are in the closed stacks of the Jefferson and Adams Buildings; specialized collections of books and other materials are in closed stacks in all three main library buildings, or are stored off-site. Access to the closed stacks is not permitted under any circumstances, except to authorized library staff, and occasionally, to dignitaries. Only the reading room reference collections are on open shelves.[94] Since 1902, American libraries have been able to request books and other items through interlibrary loan from the Library of Congress if these items are not readily available elsewhere. Through this system, the Library of Congress has served as a \"library of last resort\", according to former Librarian of Congress Herbert Putnam.[37] The Library of Congress lends books to other libraries with the stipulation that they be used only inside the borrowing library.[95] In 2017, the Library of Congress began development on a reader's card for children under the age of sixteen.[96] Cecil Hobbs: American scholar of Southeast Asian history, head of the Southern Asia Section of the Orientalia (now Asian) Division of the Library of Congress, and a major contributor to scholarship on Asia and the development of South East Asian coverage in American library collections.[99] ^Snapp, Elizabeth (April 1975). \"The Acquisition of the Vollbehr Collection of Incunabula for the Library of Congress\". The Journal of Library History. 10 (2). University of Texas Press: 152–161. JSTOR25540624. (restricted access) ^Lyman, Peter; Varian, Hal R. (October 18, 2000). \"How Much Information?\"(PDF). Archived from the original(PDF) on August 16, 2011. Retrieved October 14, 2013. 10 Terabytes: The printed collection of the US Library of Congress Anderson, Gillian B. (1989), \"Putting the Experience of the World at the Nation's Command: Music at the Library of Congress, 1800–1917\", Journal of the American Musicological Society, 42 (1): 108–49, doi:10.2307/831419, JSTOR831419"}
{"url": "https://en.m.wikipedia.org/wiki/COVID-19_misinformation", "text": "The World Health Organization (WHO) declared an \"infodemic\" of incorrect information about the virus that poses risks to global health.[5] While belief in conspiracy theories is not a new phenomenon, in the context of the COVID-19 pandemic, this can lead to adverse health effects. Cognitive biases, such as jumping to conclusions and confirmation bias, may be linked to the occurrence of conspiracy beliefs.[6] Uncertainty among experts, when combined with a lack of understanding of the scientific process by laypeople, has likewise been a factor amplifying conspiracy theories about the COVID-19 pandemic.[7] In addition to health effects, harms resulting from the spread of misinformation and endorsement of conspiracy theories include increasing distrust of news organizations and medical authorities as well as divisiveness and political fragmentation.[8] Overview In January 2020, the BBC reported on the developing issue of conspiracy theories and bad health advice regarding COVID-19. Examples at the time included false health advice shared on social media and private chats, as well as conspiracy theories such as the outbreak being planned with the participation of the Pirbright Institute.[9][10] In January, The Guardian listed seven instances of misinformation, adding the conspiracy theories about bioweapons and the link to 5G technology, and including varied false health advice.[11] In an attempt to speed up research sharing, many researchers have turned to preprint servers such as arXiv, bioRxiv, medRxiv, and SSRN. Papers are uploaded to these servers without peer review or any other editorial process that ensures research quality. Some of these papers have contributed to the spread of conspiracy theories. The most notable case was an unreviewed preprint paper uploaded to bioRxiv which claimed that the virus contained HIV \"insertions\". Following objections, the paper was withdrawn.[12][13][14] Preprints about COVID-19 have been extensively shared online and some data suggest that they have been used by the media almost 10 times more than preprints on other topics.[15] According to a study published by the Reuters Institute for the Study of Journalism, most misinformation related to COVID-19 involves \"various forms of reconfiguration, where existing and often true information is spun, twisted, recontextualised, or reworked\"; less misinformation \"was completely fabricated\". The study also found that \"top-down misinformation from politicians, celebrities, and other prominent public figures\", while accounting for a minority of the samples, captured a majority of the social media engagement. According to their classification, the largest category of misinformation (39%) was \"misleading or false claims about the actions or policies of public authorities, including government and international bodies like the WHO or the UN\".[16] In addition to social media, television and radio have been perceived as sources of misinformation. In the early stages of the COVID-19 pandemic in the United States, Fox News adopted an editorial line that the emergency response to the pandemic was politically motivated or otherwise unwarranted,[17][18] and presenter Sean Hannity claimed on-air that the pandemic was a \"hoax\" (he later issued a denial).[19] When evaluated by media analysts, the effect of broadcast misinformation has been found to influence health outcomes in the population. In a natural experiment (an experiment that takes place spontaneously, without human design or intervention), two similar television news programs that were shown on the Fox News network in February–March 2020 were compared. One program reported the effects of COVID-19 more seriously, while a second program downplayed the threat of COVID-19. The study found that audiences who were exposed to the news downplaying the threat were statistically more susceptible to increased COVID-19 infection rates and death.[20] In August 2021, television broadcaster Sky News Australia was criticised for posting videos on YouTube containing misleading medical claims about COVID-19.[21]Conservative talk radio in the US has also been perceived as a source of inaccurate or misleading commentary on COVID-19. In August and September 2021, several radio hosts who had discouraged COVID-19 vaccination, or expressed skepticism toward the COVID-19 vaccine, subsequently died from COVID-19 complications, among them Dick Farrel, Phil Valentine and Bob Enyart.[22][23] Misinformation on the subject of COVID-19 has been used by politicians, interest groups, and state actors in many countries for political purposes: to avoid responsibility, scapegoat other countries, and avoid criticism of their earlier decisions. Sometimes there is a financial motive as well.[24][25][26] Multiple countries have been accused of spreading disinformation with state-backed operations in the social media in other countries to generate panic, sow distrust, and undermine democratic debate in other countries, or to promote their models of government.[27][28][29][30] A Cornell University study of 38 million articles in English-language media around the world found that US President Donald Trump was the single largest driver of the misinformation.[31][32] Analysis published by National Public Radio in December 2021 found that as American counties showed higher vote shares for Trump in 2020, COVID-19 vaccination rates significantly decreased and death rates significantly increased. NPR attributed the findings to misinformation.[33] An alternative hypothesis under investigation, deemed unlikely by the majority of virologists given a lack of evidence, is that the virus may have accidentally escaped from the Wuhan Institute of Virology in the course of standard research.[36][39] A poll in July 2021 found that 52% of US adults believe COVID-19 escaped from a lab.[40] Unsubstantiated speculation and conspiracy theories related to this topic have gained popularity during the pandemic. Common conspiracy theories state that the virus was intentionally engineered, either as a bio-weapon or to profit from the sale of vaccines. According to the World Health Organization, genetic manipulation has been ruled out by genomic analysis.[41][36][42] Many other origin stories have also been told, ranging from claims of secret plots by political opponents to a conspiracy theory about mobile phones. In March 2020, the Pew Research Center found that a third of Americans believed COVID-19 had been created in a lab, and a quarter thought it had been engineered intentionally.[43] The spread of these conspiracy theories is magnified through mutual distrust and animosity, as well as nationalism and the use of propaganda campaigns for political purposes.[44] The promotion of misinformation has been used by American far-right groups such as QAnon, by rightwing outlets such as Fox News, by former US President Donald Trump and also other prominent Republicans to stoke anti-China sentiments,[45][46][43] and has led to increased anti-Asian activity on social media and in the real world.[47] This has also resulted in the bullying of scientists and public health officials, both online and in-person,[54] fueled by a highly political and oftentimes toxic debate on many issues.[35][55] Such spread of misinformation and conspiracy theories has the potential to negatively affect public health and diminish trust in governments and medical professionals.[56] The resurgence of the lab leak and other theories was fueled in part by the publication, in May 2021, of early emails between National Institute of Allergy and Infectious Diseases (NIAID) director Anthony Fauci and scientists discussing the issue. Per the emails in question, Kristian Andersen (author of one study debunking genomic manipulation theories) had heavily considered the possibility, and emailed Fauci proposing possible mechanisms, before ruling out deliberate manipulation with deeper technical analysis.[57][58] These emails were later misconstrued and used by critics to claim a conspiracy was occurring.[59][60] The ensuing controversy became known as the \"Proximal Origin\".[61][62] However, despite claims to the contrary in some US newspapers, no new evidence has surfaced to support any theory of a laboratory accident, and the majority of peer-reviewed research points to a natural origin. This parallels previous outbreaks of novel diseases, such as HIV, SARS and H1N1, which have also been the subject of allegations of laboratory origin.[63][64] Wuhan lab origin This section is about misinformation related to the Wuhan laboratory origin idea. For broader coverage of this topic, see COVID-19 lab leak theory. Bio-weapon One early source of the bio-weapon origin theory was former Israeli secret service officer Dany Shoham, who gave an interview to The Washington Times about the biosafety level 4 (BSL-4) laboratory at the Wuhan Institute of Virology.[65][66] A scientist from Hong Kong, Li-Meng Yan, fled China and released a preprint stating the virus was modified in a lab rather than having a natural evolution. In an ad hoc peer-review (as the paper was not submitted for traditional peer review as part of the standard scientific publishing process), her claims were labelled as misleading, unscientific, and an unethical promotion of \"essentially conspiracy theories that are not founded in fact\".[67] Yan's paper was funded by the Rule of Law Society and the Rule of Law Foundation, two non-profits linked to Steve Bannon, a former Trump strategist, and Guo Wengui, an expatriate Chinese billionaire.[68] This misinformation was further seized on by the American far-right, who have been known to promote distrust of China. In effect, this formed \"a fast-growing echo chamber for misinformation\".[45] The idea of SARS-CoV-2 as a lab-engineered weapon is an element of the Plandemic conspiracy theory, which proposes that it was deliberately released by China.[64] The Epoch Times, an anti-Chinese Communist Party (CCP) newspaper affiliated with Falun Gong, has spread misinformation related to the COVID-19 pandemic in print and via social media including Facebook and YouTube.[69][70] It has promoted anti-CCP rhetoric and conspiracy theories around the coronavirus outbreak, for example through an 8-page special edition called \"How the Chinese Communist Party Endangered the World\", which was distributed unsolicited in April 2020 to mail customers in areas of the United States, Canada, and Australia.[71][72] In the newspaper, the SARS-CoV-2 virus is known as the \"CCP virus\", and a commentary in the newspaper posed the question, \"is the novel coronavirus outbreak in Wuhan an accident occasioned by weaponizing the virus at that [Wuhan P4 virology] lab?\"[69][71] The paper's editorial board suggested that COVID-19 patients cure themselves by \"condemning the CCP\" and \"maybe a miracle will happen\".[73] In response to the propagation of theories in the US of a Wuhan lab origin, the Chinese government promulgated the conspiracy theory that the virus was developed by the United States army at Fort Detrick.[74][75] The conspiracy theory was also promoted by British MP Andrew Bridgen in March 2023.[75] Gain-of-function research One idea used to support a laboratory origin invokes previous gain-of-function research on coronaviruses. Virologist Angela Rasmussen argued that this is unlikely, due to the intense scrutiny and government oversight gain-of-function research is subject to, and that it is improbable that research on hard-to-obtain coronaviruses could occur under the radar.[76] The exact meaning of \"gain of function\" is disputed among experts.[77][78] In May 2020, Fox News host Tucker Carlson accused Anthony Fauci of having \"funded the creation of COVID\" through gain-of-function research at the Wuhan Institute of Virology (WIV).[77] Citing an essay by science writer Nicholas Wade, Carlson alleged that Fauci had directed research to make bat viruses more infectious to humans.[79] In a hearing the next day, US senator Rand Paul alleged that the US National Institutes of Health (NIH) had been funding gain-of-function research in Wuhan, accusing researchers including epidemiologist Ralph Baric of creating \"super-viruses\".[77][80] Both Fauci and NIH Director Francis Collins have denied that the US government supported such research.[77][78][79] Baric likewise rejected Paul's allegations, saying that his lab's research into the potential in bat coronaviruses for cross-species transmission was not deemed gain-of-function by NIH or the University of North Carolina, where he works.[80] A 2017 study of chimeric bat coronaviruses at the WIV listed NIH as a sponsor; however, NIH funding was only related to sample collection. Based on this and other evidence, The Washington Post rated the claim of an NIH connection to gain-of-function research on coronaviruses as \"two pinocchios\",[80][81] representing \"significant omissions and/or exaggerations\".[82] Accidental release of collected sample Another theory suggests the virus arose in humans from an accidental infection of laboratory workers by a natural sample.[39] Unfounded online speculation about this scenario has been widespread.[36] In March 2021, an investigatory report released by the WHO described this scenario as \"extremely unlikely\" and not supported by any available evidence.[83] The report acknowledged, however, that the possibility cannot be ruled out without further evidence.[39] The investigation behind this report operated as a joint collaboration between Chinese and international scientists.[84][85] At the release briefing for the report, WHO Director-General Tedros Adhanom Ghebreyesus reiterated the report's calls for a deeper probe into all evaluated possibilities, including the laboratory origin scenario.[86] The study and report were criticised by heads of state from the US, the EU, and other WHO member countries for a lack of transparency and incomplete access to data.[87][88][89] Further investigations have also been requested by some scientists, including Anthony Fauci and signatories of a letter published in Science.[90] Since May 2021, some media organizations softened previous language that described the laboratory leak theory as \"debunked\" or a \"conspiracy theory\".[91] On the other hand, scientific opinion that an accidental leak is possible, but unlikely, has remained steady.[92][35] A number of journalists and scientists have said that they dismissed or avoided discussing the lab leak theory during the first year of the pandemic as a result of perceived polarization resulting from Donald Trump's embrace of the theory.[91][46][93][94] Responding to the conspiracy theories, the CBC stated that its articles \"never claimed the two scientists were spies, or that they brought any version of [a] coronavirus to the lab in Wuhan\". While pathogen samples were transferred from the lab in Winnipeg to Beijing in March 2019, neither of the samples contained a coronavirus. The Public Health Agency of Canada has stated that the shipment conformed to all federal policies, and that the researchers in question are still under investigation, and thus it cannot be confirmed nor denied that these two were responsible for sending the shipment. The location of the researchers under investigation by the Royal Canadian Mounted Police has also not been released.[95][98][99] In a January 2020 press conference, NATO secretary-general Jens Stoltenberg, when asked about the case, stated that he could not comment specifically on it, but expressed concerns about \"increased efforts by the nations to spy on NATO allies in different ways\".[100] Accusations by China According to The Economist, conspiracy theories exist on China's internet about COVID-19 being created by the CIA in order to \"keep China down\".[101] According to an investigation by ProPublica, such conspiracy theories and disinformation have been propagated under the direction of China News Service, the country's second largest government-owned media outlet controlled by the United Front Work Department.[102]Global Times and Xinhua News Agency have similarly been implicated in propagating disinformation related to COVID-19's origins.[103]NBC News however has noted that there have also been debunking efforts of US-related conspiracy theories posted online, with a WeChat search of \"Coronavirus [disease 2019] is from the U.S.\" reported to mostly yield articles explaining why such claims are unreasonable.[104][a] A member of the U.S. military athletics delegation based at Fort Belvoir, who competed in the 50mi Road Race at the Wuhan games, became the subject of online targeting by netizens accusing her of being \"patient zero\" of the COVID-19 outbreak in Wuhan, and was later interviewed by CNN, to clear her name from the \"false accusations in starting the pandemic\".[119] In January 2021, Hua Chunying renewed the conspiracy theory from Zhao Lijian and Geng Shuang that the SARS-CoV-2 virus originating in the United States at the U.S. biological weapons labFort Detrick. This conspiracy theory quickly went trending on the Chinese social media platform Weibo, and Hua Chunying continued to cite evidence on Twitter, while asking the government of the United States to open up Fort Detrick for further investigation to determine if it is the source of the SARS-CoV-2 virus.[120][121] In August 2021, a Chinese Foreign Ministry spokesman repeatedly used an official podium to elevate the Fort Detrick's origin unproven idea.[122] According to a report from Foreign Policy, Chinese diplomats and government officials in concert with China's propaganda apparatus and covert networks of online agitators and influencers have responded, focused on repeating Zhao Lijian's allegation relating to Fort Detrick in Maryland, and the \"over 200 U.S. biolabs\" around the world.[123] Accusations by Russia In February 2020, US officials alleged that Russia is behind an ongoing disinformation campaign, using thousands of social media accounts on Twitter, Facebook and Instagram to deliberately promote unfounded conspiracy theories, claiming the virus is a biological weapon manufactured by the CIA and the US is waging economic war on China using the virus.[124][125][126][b] In March 2022, amid the 2022 Russian invasion of Ukraine, the Russian Defense Ministry stated that US President Joe Biden's son, Hunter Biden, as well as billionaire George Soros, were closely tied to Ukrainian biolabs. American right-wing media personalities, such as Tucker Carlson, highlighted the story, while Chinese Communist Party-owned tabloid Global Times further stated that the labs had been studying bat coronaviruses, which spread widely on the Chinese internet for insinuating that the United States had created SARS-CoV-19 in Ukrainian laboratories.[132][133] Accusations by other countries According to Washington, DC-based nonprofit Middle East Media Research Institute, numerous writers in the Arabic press have promoted the conspiracy theory that COVID-19, as well as SARS and the swine flu virus, were deliberately created and spread to sell vaccines against these diseases, and it is \"part of an economic and psychological war waged by the U.S. against China with the aim of weakening it and presenting it as a backward country and a source of diseases\".[134][c] Accusations in Turkey of Americans creating the virus as a weapon have been reported,[135][136] and a YouGov poll from August 2020 found that 37% of Turkish respondents believed the US government was responsible for creating and spreading the virus.[137] An Iranian cleric in Qom said Donald Trump targeted the city with coronavirus \"to damage its culture and honor\".[138]Reza Malekzadeh, Iran's deputy health minister and former Minister of Health, rejected claims that the virus was a biological weapon, pointing out that the US would be suffering heavily from it. He said Iran was hard-hit because its close ties to China and reluctance to cut air ties introduced the virus, and because early cases had been mistaken for influenza.[139][d] In Iraq, pro-Iranian social media users waged a Twitter campaign during Trump's Presidency to end U.S. presence in the country by blaming it for the virus. The campaign centered around hashtags such as #Bases_of_the_American_pandemic and #Coronavirus_is_Trump's_weapon. A March 2020 survey by USCENTCOM found that 67% of Iraqi respondents believed a foreign force was behind COVID-19, with 72% of them naming the USA as that force.[148] Theories blaming the USA have also circulated in the Philippines,[e] Venezuela[f] and Pakistan.[153] An October 2020 Globsec poll of Eastern European countries found that 38% of respondents in Montenegro and Serbia, 37% of those in North Macedonia, and 33% in Bulgaria believed the USA deliberately created COVID-19.[154][155] Jewish origin In the Muslim world Iran's Press TV asserted that \"Zionist elements developed a deadlier strain of coronavirus against Iran.\"[156] Similarly, some Arab media outlets accused Israel and the United States of creating and spreading COVID-19, avian flu, and SARS.[157] Users on social media offered other theories, including the allegation that Jews had manufactured COVID-19 to precipitate a global stock market collapse and thereby profit via insider trading,[158] while a guest on Turkish television posited a more ambitious scenario in which Jews and Zionists had created COVID-19, avian flu, and Crimean–Congo hemorrhagic fever to \"design the world, seize countries, [and] neuter the world's population\".[159] Turkish politician Fatih Erbakan reportedly said in a speech: \"Though we do not have certain evidence, this virus serves Zionism's goals of decreasing the number of people and preventing it from increasing, and important research expresses this.\"[160] In the United States An alert by the US Federal Bureau of Investigation regarding the possible threat of far-right extremists intentionally spreading COVID-19 mentioned blame being assigned to Jews and Jewish leaders for causing the pandemic and several statewide shutdowns.[164] In Germany Flyers have been found on German tram cars, falsely blaming Jews for the pandemic.[165] In Britain According to a study carried out by the University of Oxford in early 2020, nearly one-fifth of respondents in England believed to some extent that Jews were responsible for creating or spreading the virus with the motive of financial gain.[167][168] Muslims spreading virus In India, Muslims have been blamed for spreading infection following the emergence of cases linked to a Tablighi Jamaat religious gathering.[169] There are reports of vilification of Muslims on social media and attacks on individuals in India.[170] Claims have been made that Muslims are selling food contaminated with SARS-CoV-2 and that a mosque in Patna was sheltering people from Italy and Iran.[171] These claims were shown to be false.[172] In the UK, there are reports of far-right groups blaming Muslims for the pandemic and falsely claiming that mosques remained open after the national ban on large gatherings.[173] Piers Corbyn was described as \"dangerous\" by physician and broadcaster Hilary Jones during their joint interview on Good Morning Britain in early September 2020. Corbyn described COVID-19 as a \"psychological operation to close down the economy in the interests of mega-corporations\" and stated \"vaccines cause death\".[176] 5G mobile networks Openreach engineers appealed on anti-5G Facebook groups, saying they are not involved in mobile networks, and workplace abuse is making it difficult for them to maintain phonelines and broadband.5G towers have been burned by people falsely blaming them for COVID-19. The first conspiracy theories purporting a link between COVID-19 and 5G mobile networks had already appeared by the end of January 2020. Such claims spread rapidly on social media networks, leading to the spread of misinformation in what has been likened to a \"digital wildfire\".[177] Cowan's claims were repeated by Mark Steele, a conspiracy theorist who claimed to have first-hand knowledge that 5G was in fact a weapon system capable of causing symptoms identical to those produced by the virus.[186]Kate Shemirani, a former nurse who had been struck off the UK nursing registry and had become a promoter of conspiracy theories, repeatedly claimed that these symptoms were identical to those produced by exposure to electromagnetic fields.[187][188] Steve Powis, national medical director of NHS England, described theories linking 5G mobile-phone networks to COVID-19 as the \"worst kind of fake news\".[189] Viruses cannot be transmitted by radio waves, and COVID-19 has spread and continues to spread in many countries that do not have 5G networks.[190] There were 20 suspected arson attacks on phone masts in the UK over the 2020 Easter weekend.[189] These included an incident in Dagenham where three men were arrested on suspicion of arson, a fire in Huddersfield that affected a mast used by emergency services, and a fire in a mast that provides mobile connectivity to the NHS Nightingale Hospital Birmingham.[189] Some telecom engineers reported threats of violence, including threats to stab and murder them, by individuals who believe them to be working on 5G networks.[191] In April 2020, Gardaí and fire services were called to fires at 5G masts in County Donegal, Ireland.[192] The Gardaí were treating the fires as arson.[192] After the arson attacks, British Cabinet Office Minister Michael Gove said the theory that COVID-19 virus may be spread by 5G wireless communication is \"just nonsense, dangerous nonsense as well\".[193] Telecommunications provider Vodafone announced that two Vodafone masts and two it shares with O2, another provider, had been targeted.[194][195] By April 2020, at least 20 mobile-phone masts in the UK had been vandalised.[196] Because of the slow rollout of 5G in the UK, many of the damaged masts had only 3G and 4G equipment.[196] Mobile-phone and home broadband operators estimated there were at least 30 incidents where engineers maintaining equipment were confronted in the week up to 6 April.[196] As of 30 May, there had been 29 incidents of attempted arson at mobile-phone masts in the Netherlands, including one case where \"Fuck 5G\" was written.[197][198] There have also been incidents in Ireland and Cyprus.[199] Facebook has deleted messages encouraging attacks on 5G equipment.[196] Engineers working for Openreach, a division of British Telecom, posted pleas on anti-5G Facebook groups asking to be spared abuse as they are not involved with maintaining mobile networks.[200] Industry lobby group Mobile UK said the incidents were affecting the maintenance of networks that support home working and provide critical connections to vulnerable customers, emergency services, and hospitals.[200] A widely circulated video showed a woman accusing employees of broadband company Community Fibre of installing 5G as part of a plan to kill the population.[200] Of those who believed that 5G networks caused COVID-19 symptoms, 60% stated that much of their knowledge about the virus came from YouTube.[201] In April 2020, YouTube announced that it would reduce the amount of content claiming links between 5G and COVID-19.[194] Videos that are conspiratorial about 5G that do not mention COVID-19 would not be removed, though they might be considered \"borderline content\" and therefore removed from search recommendations, losing advertising revenue.[194] The discredited claims had been circulated by British conspiracy theorist David Icke in videos (subsequently removed) on YouTube and Vimeo, and an interview by London Live TV network, prompting calls for action by Ofcom.[202][203] It took YouTube on average 41 days to remove Covid-related videos containing false information in the first half of 2020.[204] Ofcom issued guidance to ITV following comments by Eamonn Holmes about 5G and COVID-19 on This Morning.[205] Ofcom said the comments were \"ambiguous\" and \"ill-judged\" and they \"risked undermining viewers' trust in advice from public authorities and scientific evidence\".[205] Ofcom also found local channel London Live in breach of standards for an interview it had with David Icke. It said that he had \"expressed views which had the potential to cause significant harm to viewers in London during the pandemic\".[205] In April 2020, The Guardian revealed that Jonathan Jones, an evangelical pastor from Luton, had provided the male voice on a recording blaming 5G for deaths caused by COVID-19.[206] He claimed to have formerly headed the largest business unit at Vodafone, but insiders at the company said that he was hired for a sales position in 2014 when 5G was not a priority for the company and that 5G would not have been part of his job.[206] He had left Vodafone after less than a year.[206] American scientist selling virus to China In April 2020, rumors circulated on Facebook, alleging that the US Government had \"just discovered and arrested\" Charles Lieber, chair of the Chemistry and Chemical Biology Department at Harvard University for \"manufacturing and selling\" the novel coronavirus (COVID-19) to China. According to a report from Reuters, posts spreading the rumor were shared in multiple languages over 79,000 times on Facebook.[210] Lieber was arrested in January 2020, and later charged with two federal counts of making an allegedly false statement about his links to a Chinese university, unrelated to the virus. The rumor of Lieber, a chemist in an area entirely unrelated to the virus research, developing COVID-19 and selling it to China has been discredited.[211] Meteor origin In 2020, a group of researchers that included Edward J. Steele and Chandra Wickramasinghe, the foremost living proponent of panspermia, speculated in ten research papers that COVID-19 originated from a meteor spotted as a bright fireball over the city of Songyuan in Northeast China in October 2019 and that a fragment of the meteor landed in the Wuhan area, which started the first COVID-19 outbreaks. However, the group of researchers did not provide any direct evidence proving this conjecture.[212] In an August 2020 article, Astronomy.com called the meteor origin conjecture \"so remarkable that it makes the others look boring by comparison\".[212] NCMI intelligence report In April 2020, ABC News reported that, in November 2019, \"U.S. intelligence officials were warning that a contagion was sweeping through China's Wuhan region, changing the patterns of life and business and posing a threat to the population\". The article stated that the National Center for Medical Intelligence (NCMI), had produced an intelligence report in November 2019 which raised concerns about the situation. The director of the NCMI, Col. R. Shane Day said \"media reporting about the existence/release of a National Center for Medical Intelligence Coronavirus-related product/assessment in November 2019 is not correct. No such NCMI product exists\".[213][214] PCR testing Social media posts have falsely claimed that Kary Mullis, the inventor of polymerase chain reaction (PCR), said that PCR testing for SARS-CoV-2 does not work. Mullis, who received the Nobel Prize in Chemistry for the invention of PCR, died in August 2019 before the emergence of the SARS-CoV-2 virus and never made these statements.[215][216][217] Several posts claim Mullis said \"PCR tests cannot detect free infectious viruses at all\",[215] that PCR testing was designed to detect any non-human DNA[216] or the DNA and RNA of the person being tested,[217] or that the process of DNA amplification used in PCR will lead to contamination of the samples.[216] A video of a 1997 interview with Mullis has also been widely circulated, in which Mullis says PCR will find \"anything\"; the video description asserts that this means PCR cannot be used to reliably detect SARS-CoV-2.[218] In reality, the reverse transcription polymerase chain reaction (RT-PCR) test for SARS-CoV-2 is highly sensitive to the virus, and testing laboratories have controls in place to prevent and detect contamination.[215][216] However, the tests only reveal the presence of the virus and not whether it remains infectious.[215] Symptoms and severity In early 2020, there were a number of viral photos and videos that were mischaracterized as showing an extreme severity to COVID-19 exposure. In January and February 2020, a number of videos from China were circulated on social media that purported to show people infected with COVID-19 either suddenly collapsing, or having already collapsed, on the street.[221] Some of these videos were republished or referenced by some tabloid newspapers, including the Daily Mail and The Sun.[221] However, the people in these videos are generally believed to have been suffering from something other than COVID-19, such as one who was drunk.[222] A video from February 2020 purported to be of dead COVID-19 victims in China was actually a video from Shenzhen of people sleeping on the street.[223] Similarly, a photo that circulated in March 2020 of dozens of people lying down in the street, purported to be of COVID-19 victims in either China or Italy, was in fact a photo of living people from a 2014 art project in Germany.[224] Incidence and mortality Correctly reporting the number of people who were sick or who had died was difficult, especially during the earliest days of the pandemic.[225] In China Chinese under-reporting during early 2020 Leaked documents show that China's public reporting of cases gave an incomplete picture during the early stages of the pandemic. For example, in February 2020, China publicly reported 2,478 new confirmed cases. However, confidential internal documents that later leaked to CNN showed 5,918 new cases in February. These were broken down as 2,345 confirmed cases[clarification needed], 1,772 clinically diagnosed cases and 1,796 suspected cases.[226][227] Nurse whistleblower In January 2020, a video circulated online appearing to be of a nurse named Jin Hui[228] in Hubei, describing a far more dire situation in Wuhan than reported by Chinese officials. However, the BBC said that, contrary to its English subtitles in one of the video's existing versions, the woman does not claim to be either a nurse or a doctor in the video and that her suit and mask do not match the ones worn by medical staff in Hubei.[9] The video claimed that more than 90,000 people had been infected with the virus in China, that the virus could spread from one person to 14 people (R0 = 14) and that the virus was starting a second mutation.[229] The video attracted millions of views on various social media platforms and was mentioned in numerous online reports. The claimed R0 of 14 in the video was noted by the BBC to be inconsistent with the expert estimation of 1.4 to 2.5 at that time.[230] The video's claim of 90,000 infected cases was noted to be 'unsubstantiated'.[9][229] Alleged leak of death toll by Tencent In February 2020, Taiwan News published an article claiming that Tencent may have accidentally leaked the real numbers of death and infection in China. Taiwan News suggested that the Tencent Epidemic Situation Tracker had briefly showed infected cases and death tolls many times higher of the official figure, citing a Facebook post by a 38-year-old Taiwanese beverage store owner and an anonymous Taiwanese netizen.[231] The article, referenced by other news outlets such as the Daily Mail and widely circulated on Twitter, Facebook and 4chan, sparked a wide range of conspiracy theories that the screenshot indicates the real death toll instead of the ones published by health officials.[232] The author of the original news article defended the authenticity and newsworthiness of the leak on a WION program.[232] Mass cremation in Wuhan In February 2020, a report emerged on Twitter claiming that data showed a massive increase in sulfur emissions over Wuhan, China. The Twitter thread then claimed the reason was due to the mass cremation those who died from COVID-19. The story was shared on multiple media outlets, including Daily Express, Daily Mail, and Taiwan News.[233][232]Snopes debunked the misinformation, pointing out that the maps used by the claims were not real-time observations of sulfur dioxide (SO2) concentrations above Wuhan. Instead, the data was a computer-generated model based on historical information and forecast on SO2 emissions.[234] A story in The Epoch Times in February 2020 shared a map from the Internet that falsely alleged massive sulfur dioxide releases from crematoriums during the COVID-19 pandemic in China, speculating that 14,000 bodies may have been burned.[235] A fact check by AFP reported that the map was a NASA forecast taken out of context.[235] Decline in cellphone subscriptions There was a decrease of nearly 21 million cellphone subscriptions among the three largest cellphone carriers in China, which led to misinformation that this is evidence for millions of deaths due to COVID-19 in China.[236] The drop is attributed to cancellations of phone services due to a downturn in the social and economic life during the outbreak.[236] In the US Accusations have been made of under-reporting, over-reporting, and other problems. Necessary data was corrupted in some places, for example, on the state level in the United States.[237] The public health handling of the pandemic has been hampered by the use of archaic technology (including fax machines and incompatible formats),[225] poor data flow and management (or even no access to data), and general lack of standardization and leadership.[238] Privacy laws hampered contact tracing and case finding efforts, which resulted in under-diagnosis and under-reporting.[239] Allegations of inflated death counts In August 2020, President Donald Trump retweeted a conspiracy theory alleging that COVID-19 deaths are systematically overcounted, and that only 6% of the reported deaths in the United States were actually from the disease.[240] This 6% number is based on only counting death certificates where COVID-19 is the sole condition listed. The lead mortality statistician at the Centers for Disease Control and Prevention's (CDC) National Center for Health Statistics said that those death certificates likely did not include all the steps that led to the death and thus were incomplete. The CDC collects data based on case surveillance, vital records, and excess deaths.[241] A FactCheck.org article on the issue reported that while 6% of the death certificates included COVID-19 exclusively as the cause of death and 94% had additional conditions that contributed to it, COVID-19 was listed as the underlying cause of death in 92% of them, as it may directly cause other severe conditions such as pneumonia or acute respiratory distress syndrome.[242] The U.S. experienced 882,000 \"excess deaths\" (i.e., deaths above the baseline expected from normal mortality in previous years) between February 2020 and January 2022, which is somewhat higher than the officially recorded mortality from COVID-19 during that period (835,000 deaths). Analysis of weekly data from each U.S. state shows that the calculated excess deaths are strongly correlated with COVID-19 infections, undercutting the notion that the deaths were primarily caused by some factor other than the disease.[243] Misleading Johns Hopkins News-Letter article In November 2020, an article by Genevieve Briand (assistant director for the Master's program in Applied Economics at JHU)[244] was published in the student-run Johns Hopkins News-Letter claiming to have found \"no evidence that COVID-19 create[d] any excess deaths\".[245] The article was later retracted after it was used to promote conspiracy theories on right-wing social media accounts and misinformation websites,[246] but the presentation was not removed from YouTube, where it had been viewed more than 58,000 times as of 3 December 2020.[247] Briand compared data from spring 2020 and January 2018, ignoring expected seasonal variations in mortality and unusual peaks in the spring and summer of 2020 compared to previous spring and summer months.[245] Briand's article failed to account for the total excess mortality from all causes reported during the pandemic,[248] with 300,000 deaths associated with the virus per CDC data in 2020.[248] Deaths per age group were also shown as a proportion percentage rather than as raw numbers, obscuring the effects of the pandemic when the number of deaths increases but the proportions are maintained.[248] The article also suggested that deaths attributed to cardiac and respiratory diseases in infected persons were incorrectly categorized as deaths due to COVID-19. This view fails to recognize that those with such conditions are more vulnerable to the virus and therefore more likely to die from it.[245] The retraction of Briand's article went viral on social media under false claims of censorship.[249] Misinformation targeting Taiwan In February 2020, the Taiwanese Central News Agency reported that large amounts of misinformation had appeared on Facebook claiming the pandemic in Taiwan was out of control, the Taiwanese government had covered up the total number of cases, and that President Tsai Ing-wen had been infected. The Taiwan fact-checking organization had suggested the misinformation on Facebook shared similarities with mainland China due to its use of simplified Chinese characters and mainland China vocabulary. The organization warned that the purpose of the misinformation is to attack the government.[250][251][252] In March 2020, Taiwan's Ministry of Justice Investigation Bureau warned that China was trying to undermine trust in factual news by portraying the Taiwanese government reports as fake news. Taiwanese authorities have been ordered to use all possible means to track whether the messages were linked to instructions given by the Chinese Communist Party. The PRC's Taiwan Affairs Office denied the claims, calling them lies, and said that Taiwan's Democratic Progressive Party was \"inciting hatred\" between the two sides. They then claimed that the \"DPP continues to politically manipulate the virus\".[253] According to The Washington Post, China has used organized disinformation campaigns against Taiwan for decades.[254] Nick Monaco, the research director of the Digital Intelligence Lab at Institute for the Future, analyzed the posts and concluded that the majority appear to have come from ordinary users in China, not the state. However, he criticized the Chinese government's decision to allow the information to spread beyond China's Great Firewall, which he described as \"malicious\".[255] According to Taiwan News, nearly one in four cases of misinformation are believed to be connected to China.[256] In March 2020, the American Institute in Taiwan announced that it was partnering with the Taiwan FactCheck Center to help combat misinformation about the COVID-19 outbreak.[257] Misrepresented World Population Project map In early February 2020, a decade-old map illustrating a hypothetical viral outbreak published by the World Population Project (part of the University of Southampton) was misappropriated by a number of Australian media news outlets (and British tabloids The Sun, Daily Mail and Metro)[258] which claimed the map represented the COVID-19 pandemic. This misinformation was then spread via the social media accounts of the same media outlets, and while some outlets later removed the map, the BBC reported, in February, that a number of news sites had yet to retract the map.[258] \"Casedemic\" COVID-19 deniers use the word casedemic as a shorthand for a conspiracy theory holding that COVID-19 is harmless and that the reported disease figures are merely a result of increased testing. The concept is particularly attractive to anti-vaccination activists, who use it to argue that public health measures, and particularly vaccines, are not needed to counter what they say is a fake epidemic.[259][260][261][262] David Gorski writes that the word casedemic was seemingly coined by Ivor Cummins—an engineer whose views are popular among COVID-19 deniers—in August 2020.[259] The term has been adopted by alternative medicine advocate Joseph Mercola, who has exaggerated the effect of false positives in polymerase chain reaction (PCR) tests to construct a false narrative that testing is invalid because it is not perfectly accurate (see also § PCR testing, above). In reality, the problems with PCR testing are well-known and accounted for by public health authorities. Such claims also disregard the possibility of asymptomatic spread, the number of potentially-undetected cases during the initial phases of the pandemic in comparison to the present due to increased testing and knowledge since, and other variables that can influence PCR tests.[259] Disease spread Early in the pandemic, little information was known about how the virus spreads, when the first people became sick, or who was most vulnerable to infection, serious complications, or death. During 2020, it became clear that the main route of spread was through exposure to the virus-laden respiratory droplets produced by an infected person.[263] There were also some early questions about whether the disease might have been present earlier than reported; however, subsequent research disproved this idea.[264][265] California herd immunity in 2019 In March 2020, Victor Davis Hanson publicized a theory that COVID-19 may have been in California in the fall of 2019 resulting in a level of herd immunity to at least partially explain differences in infection rates in cities such as New York City vs Los Angeles.[266] Jeff Smith of Santa Clara County stated that evidence indicated the virus may have been in California since December 2019.[267] Early genetic and antibody analyses refute the idea that the virus was in the United States prior to January 2020.[264][265][268][269][needs update] Patient Zero In March 2020, conspiracy theorists started the false rumor that Maatje Benassi, a US army reservist, was \"Patient Zero\" of the pandemic, the first person to be infected with COVID-19.[270] Benassi was targeted because of her participation in the 2019 Military World Games at Wuhan before the pandemic started, even though she never tested positive for the virus. Conspiracy theorists even connected her family to the DJ Benny Benassi as a Benassi virus plot, even though they are not related and Benny had also not had the virus.[271] Airborne Before mid-2021 the World Health Organization (WHO) denied that COVID readily spread through the air;[272] although, they acknowledged such spread could occur during certain medical procedures as of July 2020.[273] In February 2020 the Director-General of WHO, Tedros Adhanom Ghebreyesus, initially stated COVID was airborne during a press conference, only to retract this statement a few minutes later.[274] In March 2020 WHO tweeted \"FACT: #COVID19 is NOT airborne.\"[275][276] The air quality researcher Lidia Morawska viewed their initial position as \"spreading misinformation\".[272] Hundreds of scientists, by mid 2020, viewed airborne spread as occurring and called on the WHO to change their position.[277] Concerns were raised that \"conservative voices\" within the WHO committee tasked with these guidelines were preventing new evidence from being incorporated.[277][278] Surfaces Early in the pandemic it was claimed that COVID-19 could be spread by contact with contaminated surfaces or fomites—even though this is an uncommon transmission route for other respiratory viruses. This led to recommendations that high-contact surfaces (like playground equipment or school desks) be frequently deep-cleaned and that certain items (like groceries or mailed packages) be disinfected.[279] Ultimately, the US Centers for Disease Control and Prevention (CDC) concluded that the likelihood of transmission under these scenarios was less than 1 in 10,000.[280] They further concluded that handwashing reduced the risk of exposure to COVID-19, but surface disinfection did not.[280] Beginning in February 2020, reports quickly spread via Facebook, implied that a Cameroonian student in China had been completely cured of the virus due to his African genetics. While a student was successfully treated, other media sources have indicated that no evidence implies Africans are more resistant to the virus and labeled such claims as false information.[281] Kenyan Secretary of Health Mutahi Kagwe explicitly refuted rumors that \"those with black skin cannot get coronavirus [disease 2019]\", while announcing Kenya's first case in March.[282] This false claim was cited as a contributing factor in the disproportionately high rates of infection and death observed among African Americans.[283][284] There have been claims of \"Indian immunity\": that the people of India have more immunity to the COVID-19 virus due to living conditions in India. This idea was deemed \"absolute drivel\" by Anand Krishnan, professor at the Centre for Community Medicine of the All India Institute of Medical Sciences (AIIMS). He said there was no population immunity to the COVID-19 virus yet, as it is new, and it is not even clear whether people who have recovered from COVID-19 will have lasting immunity, as this happens with some viruses but not with others.[285] A group of Jordanian researchers published a report claiming that Arabs are less vulnerable to COVID-19 due to a genetic variation specific to those of Middle East heritage. This paper had not been debunked by November 2020.[288] Xenophobic blaming by ethnicity and religion COVID-19-related xenophobic attacks have been made against individuals with the attacker blaming the victim for COVID-19 on the basis of the victim's ethnicity. People who are considered to look Chinese have been subjected to COVID-19-related verbal and physical attacks in many other countries, often by people accusing them of transmitting the virus.[289][290][291] Within China, there has been discrimination (such as evictions and refusal of service in shops) against people from anywhere closer to Wuhan (where the pandemic started) and against anyone perceived as being non-Chinese (especially those considered African), as the Chinese government has blamed continuing cases on re-introductions of the virus from abroad (90% of reintroduced cases were by Chinese passport-holders). Neighbouring countries have also discriminated against people seen as Westerners.[292][293][294] People have also simply blamed other local groups along the lines of pre-existing social tensions and divisions, sometimes citing reporting of COVID-19 cases within that group. For instance, Muslims have been widely blamed, shunned, and discriminated against in India (including some violent attacks), amid unfounded claims that Muslims are deliberately spreading COVID-19, and a Muslim event at which the disease did spread has received far more public attention than many similar events run by other groups and the government.[295]White supremacist groups have blamed COVID-19 on non-whites and advocated deliberately infecting minorities they dislike, such as Jews.[296] Bat soup Some media outlets, including Daily Mail and RT, as well as individuals, disseminated a video showing a Chinese woman eating a bat, falsely suggesting it was filmed in Wuhan and connecting it to the outbreak.[297][298] However, the widely circulated video contains unrelated footage of a Chinese travel vlogger, Wang Mengyun, eating bat soup in the island country of Palau in 2016.[297][298][299][300] Wang posted an apology on Weibo,[299][300] in which she said she had been abused and threatened,[299] and that she had only wanted to showcase Palauan cuisine.[299][300] The spread of misinformation about bat consumption has been characterized by xenophobic and racist sentiment toward Asians.[301][302][303] In contrast, scientists suggest the virus originated in bats and migrated into an intermediary host animal before infecting people.[301][304] Large gatherings South Korean \"conservative populist\" Jun Kwang-hun told his followers there was no risk to mass public gatherings as the virus was impossible to contract outdoors. Many of his followers are elderly.[305] Lifetime of the virus Misinformation has spread that the lifetime of SARS-CoV-2 is only 12 hours and that staying home for 14 hours during the Janata curfew would break the chain of transmission.[306] Another message claimed that observing the Janata curfew would result in the reduction of COVID-19 cases by 40%.[306] Mosquitoes It has been claimed that mosquitoes transmit COVID-19. There is no evidence that this is true.[190] Contaminated objects A fake Costcoproduct recall notice circulated on social media purporting that Kirkland-brand bath tissue had been contaminated with COVID-19 (meaning SARS-CoV-2) due to the item being made in China. No evidence supports that SARS-CoV-2 can survive on surfaces for prolonged periods of time (as might happen during shipping), and Costco has not issued such a recall.[307][308][309] A warning claiming to be from the Australia Department of Health said COVID-19 spreads through petrol pumps and that everyone should wear gloves when filling up petrol in their cars.[310] There were claims that wearing shoes in one's home was the reason behind the spread of COVID-19 in Italy.[311] Cruise ships as safe havens Claims by cruise-ship operators notwithstanding, there are many cases of coronaviruses in hot climates; some countries in the Caribbean, the Mediterranean, and the Persian Gulf are severely affected. In March 2020, the Miami New Times reported that managers at Norwegian Cruise Line had prepared a set of responses intended to convince wary customers to book cruises, including \"blatantly false\" claims that COVID-19 \"can only survive in cold temperatures, so the Caribbean is a fantastic choice for your next cruise\", that \"Scientists and medical professionals have confirmed that the warm weather of the spring will be the end of the Coronavirus [sic]\", and that the virus \"cannot live in the amazingly warm and tropical temperatures that your cruise will be sailing to\".[312] Flu is seasonal (becoming less frequent in the summer) in some countries, but not in others. While it is possible that COVID-19 will also show some seasonality, this has not yet been determined.[313][314][315][medical citation needed] When COVID-19 spread along international air travel routes, it did not bypass tropical locations.[316]Outbreaks on cruise ships, where an older population lives in close quarters, frequently touching surfaces which others have touched, were common.[317][318] Breastfeeding While commercial companies that make breastmilk substitutes promote their products during the pandemic, the WHO and UNICEF advise that women should continue to breastfeed during the COVID-19 pandemic even if they have confirmed or suspected COVID-19. Evidence as of May 2020[update] indicates that it is unlikely that COVID-19 can be transmitted through breast milk.[319] Sexual transmission and infertility Chinese researchers who found the virus in the semen of men infected with COVID-19, claimed that this opened up a small chance the disease could be sexually transmitted, though this claim has been questioned by other academics since this has been shown with many other viruses such as Ebola and Zika.[321] A team of Italian scholars found that 11 of 43 men who recovered from infections, or one-quarter of the test subjects, had either azoospermia (no sperm in semen) or oligospermia (low sperm count). Mechanisms through which infectious diseases affect sperm is roughly divided into two categories. One involves viruses entering the testes, where they attack spermatogonia. The other involves high fever exposing the testes to heat and thereby killing sperm.[321] Prevention People tried many different things to prevent infection. Sometimes the misinformation was false claims of efficacy, such as claims that the virus could not spread during religious ceremonies, and at other times the misinformation was false claims of inefficacy, such as claiming that alcohol-based hand sanitizer did not work. In other cases, especially with regard to public health advice about wearing face masks, additional scientific evidence resulted in different advice over time.[322] Hand sanitizer, antibacterial soaps Washing in soap and water for at least 20 seconds is the best way to clean hands. The second-best is a hand sanitizer that is at least 60% alcohol.[323] Claims that hand sanitizer is merely \"antibacterial not antiviral\", and therefore ineffective against COVID-19, have spread widely on Twitter and other social networks. While the effectiveness of sanitiser depends on the specific ingredients, most hand sanitiser sold commercially inactivates SARS-CoV-2, which causes COVID-19.[324][325] Hand sanitizer is recommended against COVID-19,[190] though unlike soap, it is not effective against all types of germs.[326] Washing in soap and water for at least 20 seconds is recommended by the US Centers for Disease Control and Prevention (CDC) as the best way to clean hands in most situations. However, if soap and water are not available, a hand sanitizer that is at least 60% alcohol can be used instead, unless hands are visibly dirty or greasy.[323][327] The CDC and the Food and Drug Administration both recommend plain soap; there is no evidence that antibacterial soaps are any better, and limited evidence that they might be worse long-term.[328][329] Public use of face masks Authorities, especially in Asia, recommended wearing face masks in public early in the pandemic. In other parts of the world, authorities made conflicting (or contradictory) statements.[330] Several governments and institutions, such as in the United States, initially dismissed the use of face masks by the general population, often with misleading or incomplete information about their effectiveness.[331][332][333] Commentators have attributed the anti-mask messaging to attempts at managing mask shortages caused by initial inaction, remarking that the claims went beyond the science, or were simply lies.[333][334][335][336] The U.S. Surgeon GeneralJerome Adams urged people to wear face masks and acknowledged that it is difficult to correct earlier messaging that masks do not work for the general public.[337] In February 2020, U.S. Surgeon GeneralJerome Adams tweeted \"Seriously people—STOP BUYING MASKS! They are NOT effective in preventing general public from catching #Coronavirus [disease 2019]\"; he later reversed his position with increasing evidence that masks can limit the spread of COVID-19.[338][339] In June 2020, Anthony Fauci (a key member of the White House Coronavirus Task Force) confirmed that the American public were told not to wear masks from the beginning, due to a shortage of masks, and then explained that masks do actually work.[340][341][342][343] Some media outlets claimed that neck gaiters were worse than not wearing masks at all in the COVID-19 pandemic, misinterpreting a study which was intended to demonstrate a method for evaluating masks (and not actually to determine the effectiveness of different types of masks).[344][345][346] The study also only looked at one wearer wearing the one neck gaiter made from a polyester/spandex blend, which is not sufficient evidence to support the claim about gaiters made in the media.[345] The study found that the neck gaiter, which was made from a thin and stretchy material, appeared to be ineffective at limiting airborne droplets expelled from the wearer; Isaac Henrion, one of the co-authors, suggests that the result was likely due to the material rather than the style, stating that \"Any mask made from that fabric would probably have the same result, no matter the design.\"[347]Warren S. Warren, a co-author, said that they tried to be careful with their language in interviews, but added that the press coverage has \"careened out of control\" for a study testing a measuring technique.[344] Individuals have speciously claimed legal or medical exemptions to avoid complying with mask mandates.[352] Individuals have, for instance, claimed that the Americans with Disabilities Act (ADA; designed to prohibit discrimination based on disabilities) allows exemption from mask requirements. The United States Department of Justice (DOJ) responded that the Act \"does not provide a blanket exemption to people with disabilities from complying with legitimate safety requirements necessary for safe operations\".[353] The DOJ also issued a warning about cards (sometimes featuring DOJ logos or ADA notices) that claim to \"exempt\" their holders from wearing masks, stating that these cards are fraudulent and not issued by any government agency.[354][355] In Kenya, in April 2020, the Governor of Nairobi Mike Sonko came under scrutiny for including small bottles of the cognac Hennessy in care packages, falsely claiming that alcohol serves as \"throat sanitizer\".[363][364] In 2020, tobacco smoking spread on social media as a false remedy to COVID-19 after a few small observational studies were published in which tobacco smoking was shown to be preventative against SARS-CoV-2. In April 2020, researchers at a Paris hospital noted an inverse relationship between smoking and COVID-19 infections, which led to an increase in tobacco sales in France. These results were at first so astonishing that the French government initiated a clinical trial with transdermalnicotine patches. More recent clinical evidence based on larger studies clearly demonstrates that smokers have an increased chance of COVID-19 infection and experience more severe respiratory symptoms.[365][366] In early 2020, several viral tweets spread around Europe and Africa, suggesting that snorting cocaine would sterilize one's nostrils of SARS-CoV-2. In response, the French Ministry of Health released a public service announcement debunking this claim, saying \"No, cocaine does NOT protect against COVID-19. It is an addictive drug that causes serious side effects and is harmful to people's health.\" The World Health Organization also debunked the claim.[367] Warm or hot drinks There were several claims that drinking warm drinks at a temperature of around 30 °C (86 °F) protects one from COVID-19, most notably by Alberto Fernández, the president of Argentina said \"The WHO recommends that one drink many hot drinks because heat kills the virus.\" Scientists commented that the WHO had made no such recommendation, and that drinking hot water can damage the oral mucosa.[368] Religious protection A number of religious groups have claimed protection due to their faith. Some refused to stop practices, such as gatherings of large groups, that promoted the transmission of the virus. In Israel, some Ultra-Orthodox Jews initially refused to close synagogues and religious seminaries and disregarded government restrictions because \"The Torah protects and saves\",[369] which resulted in an eight-fold faster rate of infection among some groups.[370] In South Korea the River of Grace Community Church in Gyeonggi Province spread the virus after spraying salt water into their members' mouths in the belief that it would kill the virus,[371] while the Shincheonji Church of Jesus in Daegu where a church leader claimed that no Shincheonji worshipers had caught the virus in February while hundreds died in Wuhan, later caused the biggest spread of the virus in the country.[372][373] In Tanzania, President John Magufuli, instead of banning congregations, urged the faithfuls to go to pray in churches and mosques in the belief that it will protect them. He said that COVID-19 is a devil, therefore \"cannot survive in the body of Jesus Christ; it will burn\" (the \"body of Jesus Christ\" refers to the Christian church).[374][375] Despite the COVID-19 pandemic, in March 2020, the Church of Greece announced that Holy Communion, in which churchgoers eat pieces of bread soaked in wine from the same chalice, would continue as a practice.[376] The Holy Synod said Holy Communion \"cannot be the cause of the spread of illness\", with Metropolitan Seraphim saying the wine was without blemish because it represented the blood and body of Christ, and that \"whoever attends Holy Communion is approaching God, who has the power to heal\".[376] The Church refused to restrict Christians from taking Holy Communion,[377] which was supported by several clerics,[378] some politicians, and health professionals.[378][379] The Greek Association of Hospital Doctors criticized these professionals for putting their religious beliefs before science.[378] A review of the medical publications on the subject, published by a Greek physician, claims that the transmission of any infectious disease through the Holy Communion has never been documented. This controversy divided the Greek society, the politics and medical experts.[380] The Islamic missionary movement Tablighi Jamaat organised Ijtema mass gatherings in Malaysia, India, and Pakistan whose participants believed that God will protect them, causing the biggest rise in COVID-19 cases in these and other countries.[381][382][383] In Iran, the head of Fatima Masumeh Shrine encouraged pilgrims to visit the shrine despite calls to close the shrine, saying that they \"consider this holy shrine to be a place of healing\".[384] In Somalia, false claims have spread Muslims are immune to the virus.[385] Helicopter spraying In Sri Lanka, the Philippines and India, it has been claimed that one should stay at home on particular days when helicopters spray \"COVID-19 disinfectant\" over homes. No such spraying has taken place, nor is it planned, nor, as of July 2020, is there any such agent that could be sprayed.[386][387] Food In India, fake news circulated that the World Health Organization warned against eating cabbage to prevent COVID-19 infection.[388] Claims that the poisonous fruit of the Datura plant is a preventive measure for COVID-19 resulted in eleven people being hospitalized in India. They ate the fruit, following the instructions from a TikTok video that propagated misinformation regarding the prevention of COVID-19.[389][390] Claims that vegetarians are immune to COVID-19 spread online in India, causing \"#NoMeat_NoCoronaVirus\" to trend on Twitter.[391] Such claims are false.[392] Vitamin D In February 2020, claims that Vitamin D pills could help prevent COVID-19 circulated on social media in Thailand.[393] Some conspiracy theorists have claimed that vitamin D was being intentionally suppressed as a preventative option by governments.[394] One meta-analysis found weak evidence that increased vitamin D levels may reduce the likelihood of intensive care admission for people with COVID-19; but found no effect of mortality.[395] A preprint of a journal article from Indonesia purporting to show a beneficial effect of vitamin D for COVID-19 went viral across social media, and was cited several times in mainstream academic literature, including in a recommendation from NICE. Tabloid newspapers such as the Daily Mail and The Sun likewise promoted the story. Subsequent investigation, however, found none of the authors seemed to be known of at the hospitals listed as their affiliations, suggesting the paper was entirely fraudulent.[396] A study of YouTube content concerning vitamin D and COVID-19 in 2020 found that over three quarters of the 77 videos analysed as part of the study contained false and misleading information. Most alarmingly according to the study's authors, the majority of the purveyors of misinformation in these videos were medical professionals. The study concluded that much of the advice given by these YouTube videos may result in adverse health outcomes such as increases in rates of skin cancer if viewers followed it.[397] Vaccines Anti-vaccination activists and other people in many countries have spread a variety of unfounded conspiracy theories and other misinformation about COVID-19 vaccines based on misunderstood or misrepresented science, religion, and law. These have included exaggerated claims about side effects, misrepresentations about how the immune system works and when and how COVID-19 vaccines are made, a story about COVID-19 being spread by 5G, and other false or distorted information. This misinformation has proliferated and may have made many people averse to vaccination.[398] This has led to governments and private organizations around the world introducing measures to incentivize or coerce vaccination, such as lotteries,[399] mandates,[400] and free entry to events,[401] which has in turn led to further misinformation about the legality and effect of these measures themselves.[402] In the US, some prominent biomedical scientists who publicly advocate vaccination have been attacked and threatened in emails and on social media by anti-vaccination activists.[403] Hospital conditions Some conservative figures in the United States, such as Richard Epstein,[404] downplayed the scale of the pandemic, saying it has been exaggerated as part of an effort to hurt President Trump. Some people pointed to empty hospital parking lots as evidence that the virus has been exaggerated. Despite the empty parking lots, many hospitals in New York City and other places experienced thousands of COVID-19-related hospitalizations.[405] In the course of 2020, conspiracy theorists used the #FilmYourHospital hashtag to encourage people to record videos in seemingly empty, or sparsely populated hospitals, in order to prove that the pandemic was a \"hoax\".[406] Treatment Widely circulated posts on social media have made many unfounded claims of treatment methods of COVID-19. Some of these claims are scams, and some promoted methods are dangerous and unhealthy.[190][407] The president of MadagascarAndry Rajoelina launched and promoted in April 2020 a herbal drink based on an artemisia plant as a miracle cure that can treat and prevent COVID-19 despite a lack of medical evidence. The drink has been exported to other African countries.[410][411] Based on in-vitro studies, extracts of E. purpurea (Echinaforce) showed virucidal activity against coronaviruses, including SARS-CoV-2. Because the data was experimental and solely derived from cell cultures, antiviral effects in humans have not been elucidated. As a result, regulatory agencies have not recommended the use of Echinacea preparations for the prophylaxis and treatment of COVID-19.[412] Vitamin C During the early years of the COVID-19 pandemic, vitamin C was the subject of more FDA warning letters than any other quack treatment for COVID-19.[413] In April 2021, the US National Institutes of Health (NIH) COVID-19 Treatment Guidelines stated that \"there are insufficient data to recommend either for or against the use of vitaminC for the prevention or treatment of COVID-19.\"[414] In an update posted December 2022, the NIH position was unchanged: There is insufficient evidence for the COVID-19 Treatment Guidelines Panel (the Panel) to recommend either for or against the use of vitamin C for the treatment of COVID-19 in nonhospitalized patients. There is insufficient evidence for the Panel to recommend either for or against the use of vitamin C for the treatment of COVID-19 in hospitalized patients.[415] Common cold and flu treatments In March 2020, a photo circulated online showing a 30-year-old Indian textbook that lists aspirin, antihistamines, and nasal spray as treatments for coronavirus diseases. False claims spread asserting that the book was evidence that COVID-19 started much earlier than reported and that common cold treatments could be a cure for COVID-19. The textbook actually talks about coronaviruses in general, as a family of viruses.[416] A rumor circulated on social media posts on Weibo, Facebook and Twitter claiming that Chinese experts said saline solutions could kill COVID-19. There is no evidence for this.[417] A tweet from French health minister Olivier Véran, a bulletin from the French health ministry, and a small speculative study in The Lancet Respiratory Medicine raised concerns about ibuprofen worsening COVID-19, which spread extensively on social media. The European Medicines Agency[418] and the World Health Organization recommended COVID-19 patients keep taking ibuprofen as directed, citing lack of convincing evidence of any danger.[419] Traditional Chinese Medicine (TCM) prescriptions Since its third version, the COVID management guidelines from the Chinese National Health Commission recommends using Traditional Chinese medicines to treat the disease.[429] In Wuhan, China Central Television reported that local authorities have pushed for a set of TCM prescriptions to be used for every case since early February.[430] One formula was promoted at the national level by mid-February.[431] The local field hospitals were explicitly TCM-oriented. According to state media, as of 16 March 2020, 91.91% of all Hubei patients have used TCM, with the rate reaching 99% in field hospitals and 94% in bulk quarantine areas.[432] In March 2020, the online insert of the official People's Daily, distributed in The Daily Telegraph, published an article stating that Traditional Chinese medicine \"helps fight coronavirus [disease 2019]\".[433] Chloroquine and hydroxychloroquine There were claims that chloroquine was used to cure more than 12,000 COVID-19 patients in Nigeria.[434] In March 2020, Adrian Bye, a tech startup leader who is not a doctor, suggested to cryptocurrency investors Gregory Rigano and James Todaro that \"chloroquine will keep most people out of hospital\". (Bye later admitted that he had reached this conclusion through \"philosophy\" rather than medical research.) Two days later, Rigano and Todaro promoted chloroquine in a self-published article that claimed affiliation with the Stanford University School of Medicine, the National Academy of Sciences and the Birmingham School of Medicine – the three institutions mentioned that they had no links to the article, and Google removed the article for violating its terms of service.[435] Early in the COVID-19 pandemic, laboratory research suggested ivermectin might have a role in preventing or treating COVID-19.[441] Online misinformation campaigns and advocacy boosted the drug's profile among the public. While scientists and physicians largely remained skeptical, some nations adopted ivermectin as part of their pandemic-control efforts. Some people, desperate to use ivermectin without a prescription, took veterinary preparations, which led to shortages of supplies of ivermectin for animal treatment. The FDA responded to this situation by saying \"You are not a horse\" in a Tweet to draw attention to the issue, which they were later sued for.[442][443] Dangerous treatments Some QAnon proponents, including Jordan Sather and others, have promoted gargling \"Miracle Mineral Supplement\" (actually chlorine dioxide, a chemical used in some industrial applications as a bleach that may cause life-threatening reactions and even death) as a way of preventing or curing the disease. The Food and Drug Administration has warned multiple times that drinking MMS is \"dangerous\" as it may cause \"severe vomiting\" and \"acute liver failure\".[449] Twelve people were hospitalized in India when they ingested the poisonous thornapple (Datura stramonium AKA Jimsonweed) after seeing the plant recommended as a 'coronavirus [disease 2019] home remedy' in a TikTok video.[390][450][451] Datura species contain many substances poisonous to humans, mainly through anticholinergic effects.[452][453] Silver In February 2020, televangelistJim Bakker promoted a colloidal silver solution, sold on his website, as a remedy for COVID-19; naturopath Sherrill Sellman, a guest on his show, falsely stated that it \"hasn't been tested on this strain of the coronavirus, but it's been tested on other strains of the coronavirus and has been able to eliminate it within 12 hours\".[454][clarification needed] The US Food and Drug Administration and New York Attorney General's office both issued cease-and-desist orders against Bakker, and he was sued by the state of Missouri over the sales.[455][456] The New York Attorney General's office also issued a cease-and-desist order to radio host Alex Jones, who was selling silver-infused toothpaste that he falsely claimed could kill the virus and had been verified by federal officials,[457] causing a Jones spokesman to deny the products had been sold for the purpose of treating any disease.[458] The FDA later threatened Jones with legal action and seizure of several silver-based products if he continued to promote their use against COVID-19.[459] Mustard oil The yoga guru Ramdev claimed that one can treat COVID-19 by pouring mustard oil through the nose, causing the virus to flow into the stomach where it would be destroyed by gastric acid. He also claimed that if a person can hold their breath for a minute, it means they do not have any type of coronavirus, symptomatic or asymptomatic. Both these claims were found to be false.[460][461] Spiritual healing Another televangelist, Kenneth Copeland, claimed on Victory Channel during a programme called \"Standing Against Coronavirus\", that he can cure television viewers of COVID-19 directly from the television studio. The viewers had to touch the television screen to receive the spiritual healing.[466][467] Organ trafficking In India, baseless rumours spread saying that people were being taken to care centres and killed to harvest their organs, with their bodies then being swapped to avoid suspicion. These rumours spread more quickly through online platforms such as WhatsApp, and resulted in protests, attacks against healthcare workers, and reduced willingness to seek COVID-19 testing and treatment.[468] Other Name of the disease Social media posts and Internet memes claimed that COVID-19 derives from \"Chinese Originated Viral Infectious Disease 19\", or similar, as supposedly the \"19th virus to come out of China\".[469] In fact, the WHO named the disease as follows: CO stands for corona, VI for virus, Dfor disease and 19 for when the outbreak was first identified (31 December 2019).[470] Another false social media rumor claimed COVID-19 was an acronym derived from a series of ancient symbols interpreted as \"see a sheep surrender.\"[471] Simpsons prediction Claims that The Simpsons had predicted the COVID-19 pandemic in 1993, accompanied by a doctored screenshot from the episode \"The Fool Monty\" (where the text \"Corona Virus\" was layered over the original text \"Apocalypse Meow\", without blocking it from view), were later found to be false. The claim had been widely spread on social media.[472][473] Return of wildlife During the pandemic, many false and misleading images or news reports about the environmental impact of the COVID-19 pandemic were shared by clickbait journalism sources and social media.[474] A viral post that originated on Weibo and spread on Twitter claimed that a pack of elephants descended on a village under quarantine in China's Yunnan, got drunk on corn wine, and passed out in a tea garden.[475] A Chinese news report debunked the claim that the elephants got drunk on corn wine and noted that wild elephants were a common sight in the village; the image attached to the post was originally taken at the Asian Elephant Research Center in Yunnan in December 2019.[474] Following reports of reduced pollution levels in Italy as a result of lockdowns, images purporting to show swans and dolphins swimming in Venice canals went viral on social media. The image of the swans was revealed to have been taken in Burano, where swans are common, while footage of the dolphins was filmed at a port in Sardinia hundreds of miles away.[474] The Venice mayor's office clarified that the reported water clarity in the canals was due to the lack of sediment being kicked up by boat traffic, not a reduction in water pollution as initially reported.[476] Virus remains in body permanently It has been wrongly claimed that anyone infected with COVID-19 will have the virus in their bodies for life. While there is no curative treatment, most infected people recover from the disease and eliminate the virus from their bodies.[190] Antisemitism An October 2021 report by the UK-based anti-racism group Hope not Hate found that COVID-19 conspiracy theories were a primary gateway into antisemitic rhetoric, due to what they described as \"conspiratorial antisemitism\".[486][487] According to the report, \"An important bridge between COVID-19 conspiracy theories and antisemitism are ideologies that provide overarching explanations for smaller alleged deceptions. For example, the need for anti-5G campaigners to explain why telecom companies, healthcare providers and authorities are conspiring to expose the population to supposedly dangerous radiation has driven attention towards 'superconspiracies'.\"[488] Also in October 2021, the fact-checking organisation Logically found that antisemitic conspiracy theories related to the pandemic were being promoted on one of the largest COVID-19 conspiracy groups on Telegram, including posts highlighting Jewish people in leadership positions at Moderna, Pfizer, the CDC and US President Joe Biden's White House, and claims that mask and vaccine mandates were similar to the Holocaust.[487] Efforts to combat misinformation In February 2020, the World Health Organization (WHO) described a \"massive infodemic\", citing an over-abundance of reported information, which was false, about the virus that \"makes it hard for people to find trustworthy sources and reliable guidance when they need it\". The WHO stated that the high demand for timely and trustworthy information has incentivised the creation of a direct WHO 24/7 myth-busting hotline where its communication and social media teams have been monitoring and responding to misinformation through its website and social media pages.[489][490][491] The WHO specifically debunked several claims as false, including the claim that a person can tell if they have the virus or not simply by holding their breath; the claim that drinking large amounts of water will protect against the virus; and the claim that gargling salt water prevents infection.[492] Social media In early February 2020, Facebook, Twitter, and Google announced that they were working with WHO to address misinformation on their platforms.[493] In a blog post, Facebook stated that it would remove content flagged by global health organizations and local authorities that violate its content policy on misinformation leading to \"physical harm\".[494] Facebook is also giving free advertising to WHO.[495] Nonetheless, a week after Trump's speculation that sunlight could kill the virus, The New York Times found \"780 Facebook groups, 290 Facebook pages, nine Instagram accounts and thousands of tweets pushing UV light therapies\", material which those companies declined to remove from their platforms.[496] In August 2020, Facebook removed seven million posts with misinformation about COVID-19.[497] At the end of February 2020, Amazon removed more than a million products that claimed to cure or protect against COVID-19, and removed tens of thousands of listings for health products whose prices were \"significantly higher than recent prices offered on or off Amazon\", although numerous items were \"still being sold at unusually high prices\" as of 28 February.[498] Millions of instances of COVID-19 misinformation have occurred across multiple online platforms.[499] Other researchers monitoring the spread of fake news observed certain rumors started in China; many of them later spread to Korea and the United States, prompting several universities in Korea to start the multilingual \"Facts Before Rumors\" campaign to evaluate common claims seen online.[500][501][502][503] The proliferation of such misinformation on social media has led to workshops for the application of machine learning resources to detect misinformation.[504] Party and ideology partisanship has also contributed to the public's lack of trust in messages delivered via social media channels, leading to a greater proclivity to follow fake news and misinformation campaigns. According to research, COVID mass media communication should prioritize increasing trust in scientific medicine over attempting to bridge the issue's partisan divide.[505] In addition, the divisive nature of the issue, being mired in existing political tensions, has led to online bullying of scientists.[35] Wikipedia The media have praised Wikipedia's coverage of COVID-19 and its combating the inclusion of misinformation through efforts led by the English-language Wikipedia's WikiProject Medicine, among other groups.[506][507][508] From May 2020, Wikipedia's consensus for the COVID-19 pandemic page has been to \"not mention the theory that the virus was accidentally leaked from a laboratory in the article.\"[509] However, in June 2021, Wikipedia editors began debating the inclusion of the lab leak hypothesis.[510] WHO began working with Wikipedia to provide much of its infographics and reports on COVID-19 to help fight misinformation, with plans to use similar approaches for fighting misinformation about other infectious diseases in the future.[511] Newspapers and scholarly journals Initially, many newspapers with paywalls lowered them for some or all their COVID-19 coverage.[512][513] Many scientific publishers made scientific papers related to the outbreak open access (free).[514] The scientific publishing community, while intent on producing quality scholarly publications, has itself been negatively impacted by the infiltration of inferior or false research leading to the retraction of several articles on the topic of COVID-19, as well as polluting valid and reliable scientific study, bringing into question the reliability of research undertaken.[515]Retraction Watch maintains a database of retracted COVID-19 articles.[516] Podcasts In January 2022, 270 US healthcare professionals, scientists and professors wrote an open letter to Spotify complaining that podcast host Joe Rogan had a \"concerning history of broadcasting misinformation, particularly regarding the Covid-19 pandemic\" and describing him as a \"menace to public health\". This was in part due to Rogan platforming and promoting the conspiracy theories of Robert W. Malone who was one of two recent guests on The Joe Rogan Experience who compared pandemic policies to the holocaust. The letter described the interview as a \"mass-misinformation events of this scale have extraordinarily dangerous ramifications\".[517][518] Government censorship In many countries, censorship was performed by governments, with \"fake news\" laws being enacted to criminalize certain types of speech regarding COVID-19. Often, people were arrested for making posts online. In March 2020, the Turkish Interior Ministry reported 93 suspects and 19 arrests of social media users whose posts were \"targeting officials and spreading panic and fear by suggesting the virus had spread widely in Turkey and that officials had taken insufficient measures\".[519] In April 2020, Iran's military said that 3600 people had been arrested for \"spreading rumors\" about COVID-19 in the country.[520] In Cambodia, at least 17 individuals who expressed concerns about the spread of COVID-19 were arrested between January and March 2020 on \"fake news\" charges.[521][522] In April 2020, Algerian lawmakers passed a law criminalizing \"fake news\" deemed harmful to \"public order and state security\".[523] Scams The WHO has warned of criminal scams involving perpetrators who misrepresent themselves as representatives of the WHO seeking personal information from victims via email or phone.[559] Also, the Federal Communications Commission has advised consumers not to click on links in suspicious emails and not to give out personal information in emails, text messages or phone calls.[560] The Federal Trade Commission has also warned on charity scams related to the pandemic, and has advised consumers not to donate in cash, gift cards, or wire transfers.[561] Cybersecurity firm Check Point stated there has been a large increase in phishing attacks to lure victims into unwittingly installing a computer virus under the guise of emails related to COVID-19 containing attachments. Cyber-criminals use deceptive domains such as \"cdc-gov.org\" instead of the correct \"cdc.gov\", or even spoof the original domain so it resembles specific websites. More than 4,000 domains related to COVID-19 have been registered.[562] Police in New Jersey, United States, reported incidents of criminals knocking on people's doors and claiming to be from the CDC. They then attempt to sell products at inflated prices or otherwise scam victims under the guise of educating and protecting the public from COVID-19.[563] Since the passage in March 2020, of the CARES Act, criminals have taken advantage of the stimulus bill by asking people to pay in advance to receive their stimulus payment. Because of this, the IRS has advised consumers to only use the official IRS COVID-19 web address to submit information to the IRS (and not in response to a text, email, or phone call).[566] In response to these schemes, many financial companies, like Wells Fargo[567] and LoanDepot,[568] as well as health insurers, like Humana,[569] for example, have posted similar advisories on their websites. Notes ^Multiple conspiracy articles in Chinese from the SARS era resurfaced during the outbreak with altered details, claiming SARS is biological warfare. Some said BGI Group from China sold genetic information of the Chinese people to the US, which then specifically targeted the genome of Chinese individuals.[105] In January 2020, Chinese military enthusiast website Xilu published an article, claimed how the US artificially combined the virus to \"precisely target Chinese people.\"[106][107] The article was removed in early February. The article was further distorted on social media in Taiwan, which claimed \"Top Chinese military website admitted novel coronavirus was Chinese-made bioweapon.\"[108] Taiwan Fact-check center debunked the original article and its divergence, suggesting the original Xilu article distorted the conclusion from a legitimate research on Chinese scientific magazine Science China Life Sciences, which never mentioned the virus was engineered.[108] The fact-check center explained Xilu is a military enthusiastic tabloid established by a private company, thus it does not represent the voice of Chinese military.[108] Some articles on popular sites in China have also cast suspicion on US military athletes participating in the Wuhan 2019 Military World Games, which lasted until the end of October 2019, and have suggested they deployed the virus. They claim the inattentive attitude and disproportionately below-average results of American athletes in the games indicate they might have been there for other purposes and they might actually be bio-warfare operatives. Such posts stated that their place of residence during their stay in Wuhan was also close to the Huanan Seafood Wholesale Market, where the first known cluster of cases occurred.[109] In March 2020, this conspiracy theory was endorsed by Zhao Lijian, a spokesperson from the Ministry of Foreign Affairs of the People's Republic of China.[110][111][112][113] In March 2020, the US government summoned Chinese Ambassador Cui Tiankai to Washington over the coronavirus conspiracy theory.[114] Over the next month, conspiracy theorists narrowed their focus to one US Army Reservist, a woman who participated in the games in Wuhan as a cyclist, claiming she is \"patient zero.\" According to a CNN report, these theories have been spread by George Webb, who has nearly 100,000 followers on YouTube, and have been amplified by Chinese Communist Party media,[115] for example the CPC-owned newspaper Global Times.[116] ^The acting assistant secretary of state for Europe and Eurasia, Philip Reeker, said \"Russia's intent is to sow discord and undermine U.S. institutions and alliances from within\" and \"by spreading disinformation about coronavirus, Russian malign actors are once again choosing to threaten public safety by distracting from the global health response.\"[124] Russia denies the allegation, saying \"this is a deliberately false story.\"[127] According to US-based The National Interest magazine, although official Russian channels had been muted on pushing the US biowarfare conspiracy theory, other Russian media elements do not share the Kremlin's restraint.[128]Zvezda, a news outlet funded by the Russian Defense Ministry, published an article titled \"Coronavirus: American biological warfare against Russia and China\", claiming that the virus is intended to damage the Chinese economy, weakening its hand in the next round of trade negotiations.[128] Ultra-nationalist politician and leader of the Liberal Democratic Party of Russia, Vladimir Zhirinovsky, claimed on a Moscow radio station that the virus was an experiment by the Pentagon and pharmaceutical companies. Politician Igor Nikulin made rounds on Russian television and news media, arguing that Wuhan was chosen for the attack because the presence of a BSL-4 virus lab provided a cover story for the Pentagon and CIA about a Chinese bio-experiment leak.[128] An EU-document claims 80 attempts by Russian media to spread disinformation related to the epidemic.[129] According to the East StratCom Task Force, the Russian-funded Sputnik news agency had published stories speculating that the virus could have been invented in Latvia (by a Latvian affiliate), that it was used by Chinese Communist Party to curb protests in Hong Kong, that it was introduced intentionally to reduce the number of elder people in Italy, that it was targeted against the Yellow Vests movement, and making many other speculations. Sputnik branches in countries including Armenia, Belarus, Spain, and in the Middle East came up with versions of these stories.[130][131] ^According to Radio Farda, Iranian cleric Seyyed Mohammad Saeedi accused US President Donald Trump of targeting Qom with coronavirus \"to damage its culture and honor.\" Saeedi claimed that Trump is fulfilling his promise to hit Iranian cultural sites, if Iranians took revenge for the airstrike that killed of Quds Force Commander Qasem Soleimani.[138] Iranian TV personality Ali Akbar Raefipour claimed the coronavirus was part of a \"hybrid warfare\" programme waged by the United States on Iran and China.[140] Brigadier General Gholam Reza Jalali, head of Iranian Civil Defense Organization, claimed the coronavirus is likely a biological attack on China and Iran with economic goals.[141][142]Hossein Salami, the head of Islamic Revolutionary Guard Corps (IRGC), claimed that the COVID-19 pandemic in Iran may be due to a US \"biological attack.\"[143] Several Iranian politicians, including Hossein Amir-Abdollahian, Rasoul Falahati, Alireza Panahian, Abolfazl Hasanbeigi and Gholamali Jafarzadeh Imanabadi, also made similar remarks.[144] Iranian Supreme Leader, the Ayatollah Ali Khamenei, made similar suggestions.[139] Former Iranian president Mahmoud Ahmadinejad sent a letter to the United Nations in March 2020, claiming that \"it is clear to the world that the mutated coronavirus was produced in lab\" and that COVID-19 is \"a new weapon for establishing and/or maintaining political and economic upper hand in the global arena.\"[145] The late[146] Ayatollah Hashem Bathaie Golpayegani claimed that \"America is the source of coronavirus, because America went head to head with China and realised it cannot keep up with it economically or militarily.\"[147]Reza Malekzadeh, Iran's deputy health minister and former Minister of Health, rejected claims that the virus was a biological weapon, pointing out that the US would be suffering heavily from it. He said Iran was hard-hit because its close ties to China and reluctance to cut air ties introduced the virus, and because early cases had been mistaken for influenza.[139] ^A Filipino Senator, Tito Sotto, played a bioweapon conspiracy video in a February 2020 Senate hearing, suggesting the coronavirus is biowarfare waged against China.[149][150] ^Venezuela Constituent Assembly member Elvis Méndez declared that the coronavirus was a \"bacteriological sickness created in '89, in '90 and historically\" and that it was a sickness \"inoculated by the gringos.\" Méndez theorized that the virus was a weapon against Latin America and China and that its purpose was \"to demoralize the person, to weaken to install their system.\"[151] President Nicolás Maduro made similar claims, claiming that the epidemic was a biological weapon targeted at China.[152] ^Bursztyn L, Rao A, Roth C, Yanagizawa-Drott D (19 April 2020). \"Misinformation During a Pandemic\". Becker Friedman Institute for Economics at the University of Chicago. Archived from the original on 9 March 2021. Retrieved 21 April 2020. ^Ryan J. \"How the coronavirus origin story is being rewritten by a guerrilla Twitter group\". CNET. Archived from the original on 16 June 2021. Retrieved 21 June 2021. Bostickson has dubbed him a \"Chinese puppet,\" and others have erroneously suggested that Holmes, with researchers working at the Wuhan Institute of Virology including Shi Zhengli, conspired to keep the origins of the pandemic a secret. Holmes has blocked many Drastic members on Twitter because member's tweets have descended into personal attacks. He vehemently denies Bostickson's baseless claims. ^Fay Cortez M. \"The Last–And Only–Foreign Scientist in the Wuhan Lab Speaks Out\". www.bloomberg.com. Archived from the original on 3 July 2021. Retrieved 28 June 2021. One of a dozen experts appointed to an international taskforce in November to study the origins of the virus, Anderson hasn't sought public attention, especially since being targeted by U.S. extremists in early 2020 after she exposed false information about the pandemic posted online. The vitriol that ensued prompted her to file a police report. The threats of violence many coronavirus scientists have experienced over the past 18 months have made them hesitant to speak out because of the risk that their words will be misconstrued. ^Achenbach J (20 June 2021). \"Scientists battle over the ultimate origin story: Where did the coronavirus come from?\". The Washington Post. Archived from the original on 22 June 2021. Retrieved 9 July 2021. Perlman, a mild-mannered, grandfatherly virologist at the University of Iowa, didn't know the author of the dyspeptic email and had nothing to do with the emergence of the coronavirus. But he had co-signed a letter to the Lancet in February 2020 saying SARS-CoV-2 was not a bioengineered virus and condemning 'conspiracy theories suggesting that COVID-19 does not have a natural origin.' ^Greenberg J (2 June 2021). \"No, emails to Fauci don't show early agreement that virus was man-made\". PolitiFact. Archived from the original on 12 June 2021. Retrieved 18 June 2021. The only email that came close to matching that claim noted that while some evidence suggested the virus might be man-made, more work was needed and that opinion could change. The email presented a possibility — a starting point for more research — not a conclusion. The man who wrote that email concluded that the virus developed naturally in a scientific journal article in March 2020. ^\"WHO-convened global study of origins of SARS-CoV-2: China Part\". World Health Organization. Archived from the original on 19 May 2021. Retrieved 21 May 2021. WHO gratefully acknowledges the work of the joint team, including Chinese and international scientists and WHO experts who worked on the technical sections of this report, and those who worked on studies to prepare data and information for the joint mission. Barh D, Silva Andrade B, Tiwari S, Giovanetti M, Góes-Neto A, Alcantara LC, et al. (September 2020). \"Natural selection versus creation: a review on the origin of SARS-COV-2\". Le Infezioni in Medicina (in Italian). 28 (3): 302–311. PMID32920565. Archived from the original on 2 November 2020. Retrieved 15 June 2021. Conspiracy theories about a possible accidental leak from either of these laboratories known to be experimenting with bats and bat CoVs that has shown some structural similarity to human SARS-CoV-2 has been suggested, but largely dismissed by most authorities. ^中國家長指稱「武漢肺炎是美國投放病毒」 網友傻爆眼 [Chinese parents claim that \"Wuhan pneumonia is a virus delivered by the United States\" netizens are stupid] (in Chinese (China)). Archived from the original on 19 February 2020. ^武汉病毒4个关键蛋白被替换，可精准攻击华人 [Four key proteins of Wuhan virus have been replaced, which can accurately attack Chinese]. 西陆网 (in Chinese (China)). Archived from the original on 11 February 2020. Retrieved 7 February 2020. ^Luo P, Liao Y (30 January 2020). \"泛科學：關於新冠肺炎的20個傳言，哪些是真哪些是假？\" [Pan Science: 20 rumors about new coronary pneumonia, which are true and which are false?]. Theinitium.com (in Chinese). Archived from the original on 2 February 2020. Retrieved 27 February 2020. ^武漢肺炎疫情謠言多 事實查核中心指3大共同點 [There are many rumors about the Wuhan pneumonia epidemic, the fact-checking center points to 3 common points] (in Chinese (Taiwan)). Central News Agency. 26 February 2020. Archived from the original on 11 March 2020. Retrieved 28 February 2020. ^Gander K (11 February 2020). \"Could Coronavirus Really Be Killed by Hot Weather? Scientists Weigh In\". Newsweek. Archived from the original on 10 March 2020. Retrieved 12 March 2020. Ravinder Kanda, senior lecturer in evolutionary genomics at Oxford Brookes University, U.K., told Newsweek: 'Little is known about the seasonal dynamics of this particular virus—we cannot take it for granted that the warmer weather will simply drive the virus out of existence.' ^Gunia A (28 February 2020). \"Will Warmer Weather Stop the Spread of the Coronavirus? Don't Count on It, Say Experts\". Time. Archived from the original on 9 March 2020. Retrieved 12 March 2020. Nancy Messionnier of the Centers for Disease Control and Prevention, warned against assuming the number of cases will slow as the weather warms. 'I think it's premature to assume that,' she said during a call with reporters on February 12. 'We haven't been through even a single year with this pathogen.' ^Farber M (20 February 2020). \"Will the coronavirus die out as the weather warms?\". Fox News. Archived from the original on 5 March 2020. Retrieved 12 March 2020. 'We hope that the gradual spring will help this virus recede, but our crystal ball is not very clear. The new coronavirus is a respiratory virus, and we know respiratory viruses are often seasonal, but not always. For example, influenza (flu) tends to be seasonal in the US, but in other parts of the world, it exists year-round. Scientists don't fully understand why even though we have been studying [the] flu for many years,' William Schaffner, the medical director of the National Foundation for Infectious Diseases, told Fox News in an email. ^\"COVID-19 ALERT: Fraudulent Facemask Flyers – USAO-MDNC – Department of Justice\". justice.gov. Greensboro, NC: United States Department of Justice. 25 June 2020. Archived from the original on 28 August 2020. Retrieved 21 September 2020. U.S. Attorney Matthew G.T. Martin of the Middle District of North Carolina today urged the public to be aware regarding fraudulent postings, cards, or flyers on the internet regarding the Americans with Disabilities Act (ADA) and the use of face masks due to the COVID-19 pandemic, many of which include the United States Department of Justice's seal. \"Do not be fooled by the chicanery and misappropriation of the DOJ eagle,\" said U.S. Attorney Martin. \"These cards do not carry the force of law. The 'Freedom to Breathe Agency,' or 'FTBA,' is not a government agency.\" ^Upadhyay A, Som V (3 March 2020). Bhaskar S (ed.). \"Novel Coronavirus Outbreak: \"India's Response And Surveillance Has Been Quite Robust,\" Says WHO's Chief Scientist\". NDTV. Retrieved 5 March 2020. Q: In a situation like this when we need scientific solution to a medical crisis, when you get in our country for examples, political leaders saying things like cow dung or cow urine can be beneficial in fixing something like coronavirus, do we end up taking a step back after such statements, as we need to deal with the issue in a modern scientific manner. A: I completely agree, I think all the public figures including politicians need to be extra careful when it comes to making such statements, because they have such a huge following. It's really important for them to say things that are based on some scientific evidence... when it comes to the claims of cures of this infection we should be extremely careful about our statements and it should be made by the people who know what they're talking about. And has to be backed by evidence. ^Tafoya QJ (2021). \"Appendix – COVID-19-Directed Medications\". In Ramadan AR, Gamaleldin O (eds.). Neurological Care and the COVID-19 Pandemic (1st ed.). Elsevier. pp. 173–174. doi:10.1016/B978-0-323-82691-4.00016-9. ISBN978-0-323-82691-4. S2CID239763031. The WHO, the European Medicines Agency, and the IDSA all recommend against the use of ivermectin for treatment of COVID-19, with the NIH stating that there is insufficient data to recommend for or against its use outside the context of a clinical trial."}
{"url": "https://www.bbc.co.uk/bitesize/guides/z3bbb9q/revision/3", "text": "As concentrations of these gases build up, they are more effective at preventing heat being lost into space. The amount of heat being lost from the atmosphere is less than the energy entering the atmosphere. As a result the temperature of the atmosphere increases."}
{"url": "https://web.archive.org/web/20110928011038/http://www.natmus.dk/sw81068.asp", "text": "The Internet Archive discovers and captures web pages through many different web crawls. At any given time several distinct crawls are running, some for months, and some every day or longer. View the web archive through the Wayback Machine. King Christian and the Star of David Nazi-Germany was based on a racist ideology, which included defamation of the \"subhuman\" Jew. From 1933 onwards anti-Jewish restrictions were introduced, and from September 1, 1941 it was made compulsory for Jews in Germany to wear a yellow Star of David, thus making it easier for the German “Aryans\" to dissociate themselves from the Jews. Although similar legislation was introduced in other countries occupied by the Germans, the star was never introduced in Denmark, and this have given rise to explanations of an almost mythological nature. One very popular legend states, that when the Germans raised the subject of Danish Jews wearing the Star of David, the Danish King Christian X threatened that he himself would be the first to wear one, and the Germans consequently withdrew their demand. In her dissertation \"The Rescue of the Danish Jewry\" (1969) Dr. Leni Yahil examines this legend. Dr. Yahil concludes that the legend is unfounded. The Danish King never threatened to wear the Star of David, because the Germans never demanded that the star be introduced in Denmark. The German plenipotentiary in Denmark, von Renthe-Fink, and the commanding general Lüdke agreed not to make such a demand. Despite the occupation, Germany still recognised Denmark as a sovereign and neutral country, and Renthe-Fink did not want to disturb the possibilities for peaceful collaboration with the Danish government. Persecution of the Danish Jews were not initiated by the Germans until October 1943, one month after the Danish government had left office. An extended version of the legend goes that on the day the Germans demanded the Star of David introduced in Denmark, the majority of the Danish population was wearing the star, so that the Germans could not distinguish between Jews and non-Jews. This version goes back to Leon Uris' novel \"Exodus\" and is even further from the truth."}
{"url": "http://www.sellbox.com/myth-eisbn-every-ebook-edition-needs-unique-number/", "text": "The Myth of the eISBN: When eBooks Need an ISBN The International Standard Book Number, or ISBN, was first devised in 1967. The intent was, and still is, to uniquely identify a specific edition or variation of a book. Fast-forward to the age of digital reading, and some people have begun calling an ISBN for an eBook an eISBN. The truth is, there is no such thing as an eISBN. An eBook is simply a different variation of a book, just the way a hardcover edition differs from a paperback, or from an audiobook, for that matter. Bowker’s MyIdentifiers website—the official source of ISBNs for US publishers—allows publishers to choose from seven different book mediums (a form a book can take). Note that an eBook is just one of seven types of mediums in this screenshot from our Bowker account. There are no “types” of ISBNs! And once you select one of those seven choices in the Medium drop-down, the Format field to the right provides the format options available for the medium you chose. For example, if you choose Print for the medium, the Format drop-down gives you 16 different types of print books to choose from (paperback and hardcover being the most common). When does a book need an ISBN, regardless of print or digital? Bottom-line rule: this is up to the store or distributor that is helping you sell your book or eBook. Here are two examples: Amazon does not require Kindle eBooks to have an ISBN when you upload your eBook using KDP. However, if you use Draft2Digital to distribute your book to the Kindle store—or any store they distribute to—it’s Draft2Digital that requires your book to have an ISBN. (You can provide it, or they provide a free one.) Your print or eBook, sold from your website, does not require an ISBN. But if you want to sell it in a store, it’s up to the store or distributor (for example, IngramSpark) helping you sell your book as to whether or not your book needs an ISBN. It is safe to say that all printed books require an ISBN if the publisher wants to sell that book in a store. Do all types of eBook files use the same ISBN? Only if they are the same book. Typically, the eBook file (EPUB) uploaded to the major eBook retailers—Amazon, Apple, B&N, Google, Kobo—is the same content. The eBook your reader buys from any of those stores is going to have the same content and display in the same way. But sometimes this isn’t the case. Different products require unique identification Several years ago, the Boston Globe released an eBook titled 68 Blocks. The version in the Apple iBooks store was a completely different product than the eBook in the Amazon Kindle store, although their branding is exactly the same. The Apple edition was created with the proprietary iBooks Author tool and differs from the Kindle edition significantly. The Apple version makes extensive use of video, while the Kindle version is all text and images. They should have assigned different ISBNs but did not. Another example is the popular children’s book Chicka Chicka Boom Boom. Purchased from Amazon, the eBook’s text can be enlarged by double clicking, and there is no sound. Purchased from Apple's iBooks store (at four times the price, as of this writing), the book is enhanced with audio read-along. Both eBooks share the same ISBN, 978-1-4424-3891-0, but again, they should have different numbers. Contrast these examples with the superb Warner Bros. series Inside the Script (unfortunately, it is no longer available). The differences between the Kindle and iBooks editions were recognized by the assignment of different ISBNs. If the book differs from another version, regardless of its medium and format, it should have a different ISBN. Best practices for eBook publishers (and other industry stakeholders) If you are distributing directly to the major stores, you don’t need an ISBN. If you use an aggregator such as Draft2Digital or PublishDrive, they offer a free ISBN, but user beware. Whoever owns the ISBN is technically the publisher. Read the fine print. We’ve never seen a contract that allows you to use one company’s ISBN with another service. For example, if you start out by distributing your book using Draft2Digital and use their free ISBN and later want to move your eBook to PublishDrive, your eBook needs a new ISBN. And remember, changing the ISBN might result in a new listing on Amazon and the possible loss of customer reviews. Frequently Asked Questions What is an eISBN? There is no such thing as an eISBN. (Ignore the fact that Google and Kobo use this term.) What is the difference between ISBN and eISBN? They are the same. When people or entities mistakenly refer to an eISBN, they just mean an ISBN that’s been assigned to an eBook. Does an ISBN matter? The requirement for an ISBN is up to the store or distributor you use to help sell your book. If you want to sell your book through a store or use a distributor such as IngramSpark, it matters a great deal. It nearly always matters for printed books. It’s also required by aggregators such as PublishDrive and Draft2Digital. How do I get an eISBN? Buy ISBNs directly from Bowker via their website, MyIdentifiers.com. In the meantime, please stop putting an “e” in front of ISBN! Should different eBook file types have different ISBNs? If the content is different between the books, the answer is yes. For example, if one has pictures and the other is all text, it’s a different book. Or if one has videos and the other doesn’t, it’s a different book. Equally important, make this distinction clear on the cover of the book. Books About The Author David Wogahn is the president and founder of AuthorImprints. He’s written five books in the Countdown to Book Launch® series including My Publishing Imprint and Register Your Book, and is the author of the first eBook training course offered by LinkedIn Learning (formerly Lynda.com). Previously, he was co-founder of a VC-backed internet start-up known today as the CBS College Sports Network; vice president of multimedia for Times Mirror, the parent company of the Los Angeles Times; and he managed the IBM donation to the 1984 Los Angeles Summer Olympics. David is a member of the Authors Guild, and a past speaker for the Independent Book Publishers Association(IBPA), Publishing University, the Alliance of Independent Authors (ALLi), and the Santa Barbara Writers Conference. I am converting our paper book to an ebook. I noticed on a Hunger games ebook, that there was an e-isbn number included in the copyright information. Glad to find your information and know that is incorrect! The correct way is to get a new ISBN for each format I use (EPUB, MOBI). What is the best (and most economical) way to get these numbers? Should the paper format ISBN numbers be referenced when Applying? Our book currently has an ISBN number for Paperback, one for Hardcover, and also a Library of Congress number. I should also mention that the content will Not change from paper to electronic, but I will not include an index and some of the fonts and formatting etc will obviously change. Hi Cindie. The only official source of ISBNs in the US is Bowker at MyIdentifiers.com. Also, Bowker has changed their position and you do *not* need a separate ISBN for the EPUB and Mobi…just one number will do unless those two files have different content (i.e. one has video embedded and the other doesn’t). If you already have an ISBN, you must have an account with Bowker. Just buy more numbers for that account. I have recorded a 10 part training video on ISBNs and it covers all this. It is currently free for a limited time. You can learn more at http://www.EpubHelp.com. I agree with you in part. I agree that different versions/editions of a book need a different ISBN. However, as a matter of our quality control and workflow we create the mobi from the epub, therefore it is the same edition of the book, and we use the same ISBN. We only use different ISBNs as a standard procedure for the enhanced edition, or for a completely different layout (as in the case of an iBooks Author version of the same book). Yes, that is the accepted best practice now: use the same ISBN if the content is the same. Bowker’s guidance about assigning ISBNs has changed since this post was written. Nevertheless, the publisher may wish to assign unique ISBNs to different formats if they are tracking and reporting sales by format (not unlike how and why a serial number is used). Thanks for sharing your experiences."}
{"url": "https://en.m.wikipedia.org/wiki/Naltrexone/bupropion", "text": "In September 2014, a sustained release formulation of the drug was approved for marketing in the United States under the brand name Contrave.[8][9] The combination was subsequently approved in the European Union in the spring of 2015, where it is sold under the name Mysimba.[5][10] It was approved in Canada under the Contrave brand name in 2018.[11] Contents Naltrexone/bupropion is indicated, as an adjunct to a reduced-calorie diet and increased physical activity, as anti-obesity medication for the management of weight in adults with an initial body mass index (BMI) of:[4][5] Each Contrave tablet contains 8 mg naltrexone and 90 mg bupropion.[12] Once full dosing is reached (after 4 weeks of administration), the total dosage of Contrave for overweightness or obesity is two tablets twice daily or 32 mg naltrexone and 360 mg bupropion per day.[12] The FDA has issued a boxed warning regarding an increased risk for suicidal thoughts and behavior in children, adolescents, and young adults under the age of 25.[4] This is attributed to the bupropion component, as the FDA requires all antidepressants to include that boxed warning on medication package inserts.[13] The safety and effectiveness in children under the age of 18 has not been studied.[4] Naltrexone is a pure opioid antagonist, which further augments bupropion's activation of the POMC.[14] Combined, naltrexone/bupropion has an effect on the reward pathway that results in reduced food craving.[15] In 2009, Monash University physiologist Michael Cowley was awarded one of Australia's top research honors, the Commonwealth Science Minister's Prize for Life Scientist of the Year, in recognition of his elucidation of these pathways, which led to the development of the combination medication.[16] Orexigen submitted a New Drug Application (NDA) for this drug combination to the FDA on 31 March 2010.[17] Having paid a fee under the Prescription Drug User Fee Act, Orexigen was given a deadline for the FDA to approve or reject the drug of 31 January 2011. On 7 December 2010, an FDA Advisory Committee voted 13-7 for the approval of Contrave, and voted 11-8 for the conduct of a post-marketing cardiovascular outcomes study.[18] Subsequently, on 2 February 2011, the FDA rejected the drug and it was decided that an extremely large-scale study of the long-term cardiovascular effects of Contrave would be needed, before approval could be considered.[19] It was ultimately approved in the United States in the fall of 2014.[9] In May 2015, Orexigen ended a safety study of its diet drug earlier than planned, because an independent panel of experts says the drug maker “inappropriately” compromised the trial by prematurely releasing interim data. The early data release reported a reduction in heart attacks that was no longer observed when a more complete view of the data was analyzed.[21] In 2018, Orexigen sold its assets, including Contrave, to Nalpropion Pharmaceuticals.[22][23] On 22 September 2020, the FDA issued a Warning Letter to Nalpropion Pharmaceuticals LLC on concerns of a sponsored Google link making \"false or misleading claims about the risks associated with and efficacy of Contrave\"[24] on multiple issues. Nalproprion subsequently issued \"An important correction from CONTRAVE® (naltrexone HCl/bupropion HCl) Extended-Release Tablets\" [25] The sustained-release formulation, Contrave, is marketed by Takeda under license from the combination medication's developer, Orexigen Therapeutics.[9] As of 2015, Orexigen received 20% of net sales from Takeda.[26] At the time of its approval by the FDA, Wells Fargoanalyst Matthew Andrews estimated that Contrave's US sales would reach approximately US$200 million in 2016, exceeding that of the dominant alternative obesity medications lorcaserin and phentermine/topiramate.[27] Despite being initially impeded by technical issues, the growth in filled prescriptions in the first months after approval was very rapid — substantially exceeding the equivalent early uptake of either of the two alternative medications just cited.[28] The first quarter of sales for Contrave (Q1 2015) showed net sales of US$11,500,000.[26]"}
{"url": "https://en.m.wikipedia.org/wiki/Adipomastia", "text": "Adipomastia Adipomastia, also known colloquially as fatty breasts,[2] is a condition defined as an excess of skin and/or a flat layer of adipose tissue (that doesn't protude like female breasts) in the breasts without true gynecomastia.[1][3][4] It is commonly present in men with obesity, and is particularly apparent in men who have undergone massive weight loss.[5][6] A related/synonymous term is pseudogynecomastia.[7] The condition is different and should be distinguished from gynecomastia (\"women's breasts\"), which involves female-like protruding fat tissue and/or glandular tissue in a male.[1][7] The two conditions can usually be distinguished easily by palpation to check for the presence of glandular tissue.[6][8] Another difference between the conditions is that breast pain/tenderness does not occur in pseudogynecomastia.[5] Sometimes, gynecomastia and pseudogynecomastia are present together; this is related to the fact that fat tissue expresses aromatase, the enzyme responsible for the synthesis of estrogen, and estrogen is produced to a disproportionate extent in men with excessive amounts of fat, resulting in simultaneous glandular enlargement.[5][9] Contents Adipomastia can be classified as grade one, two, or three. Grade one is characterized as having minimum excess fat and skin on the chest, as well as limited change in nipple placement and inframammary fold descent. Grade 1a has no lateral excess skin roll, while grade 1b shows lateral chest skin roll. Grade two is classified as a nipple-areola complex and inframammary fold below the optimum inframammary fold, a lateral chest roll, and limited upper abdominal laxity. Grade three is described as a nipple-areola complex and inframammary fold beneath the optimum inframammary fold, lateral chest roll, and substantial upper abdominal laxity.[10] Ultrasonic and suction-assisted lipectomy, followed by secondary excisional procedures, can be used to treat Grade 1a Adipomastia. Ultrasonic and suction-assisted lipectomy, as well as direct excision of the lateral chest roll, are used to treat grade 1b Adipomastia. Secondary excisional procedures can be used to treat remaining deformities that do not retract properly.[10] Grade 2 Adipomastia may be managed with a dermoglandular pedicled reconstruction.[10] Grade 3 Adipomastia is treated with free-nipple grafting due to the degree of resection.[10]"}
{"url": "https://en.m.wikipedia.org/wiki/Reproductive_medicine", "text": "Reproductive medicine Reproductive medicine is a branch of medicine concerning the male and female reproductive systems. It encompasses a variety of reproductive conditions, their prevention and assessment, as well as their subsequent treatment and prognosis. The study of reproductive medicine is thought to date back to Aristotle, where he came up with the “Haematogenous Reproduction Theory”.[1] However, evidence-based reproductive medicine is traceable back to the 1970s.[2] Since then, there have been many milestones for reproductive medicine, including the birth of Louise Brown, the first baby to be conceived through IVF in 1978.[3] Despite this, it was not until 1989 that it became a clinical discipline thanks to the work of Iain Chalmers in developing the systematic review and the Cochrane collection.[2] Reproductive medicine deals with prevention, diagnosis and management of the following conditions. This section will give examples of a number of common conditions affecting the human reproductive system. Male assessment also starts with a history and physical examination to look for any visible abnormalities. Investigations of semen samples also take place to assess the volume, motility and number of sperm, as well as identifying infections.[14] The anamnesis or medical history taking of issues related to reproductive or sexual medicine may be inhibited by a person's reluctance to disclose intimate or uncomfortable information. Even if such an issue is on the person's mind, they often do not start talking about such an issue without the physician initiating the subject by a specific question about sexual or reproductive health.[18] Some familiarity with the doctor generally makes it easier for person to talk about intimate issues such as sexual subjects, but for some people, a very high degree of familiarity may make the person reluctant to reveal such intimate issues.[18] When visiting a health provider about sexual issues, having both partners of a couple present is often necessary, and is typically a good thing, but may also prevent the disclosure of certain subjects, and, according to one report, increases the stress level.[18] For therapies such as IVF, many countries have strict guidelines. In the UK, referrals are only given to women under 40 who have either undergone 12 cycles of artificial insemination, or have tried and failed to conceive for 2 years.[19] While NICE recommends NHS clinical commissioning groups (CCGs) to provide 3 NHS funded cycles of IVF, many only offer 1 cycle, with some only offering IVF in exceptional circumstances on the NHS. If an individual does not meet the criteria or has gone through the maximum number of NHS-funded cycles, the individual will have to pay for private treatment[20]"}
{"url": "https://en.m.wikipedia.org/wiki/Automation", "text": "Automation describes a wide range of technologies that reduce human intervention in processes, mainly by predetermining decision criteria, subprocess relationships, and related actions, as well as embodying those predeterminations in machines.[1][2] Automation has been achieved by various means including mechanical, hydraulic, pneumatic, electrical, electronic devices, and computers, usually in combination. Complicated systems, such as modern factories, airplanes, and ships typically use combinations of all of these techniques. The benefit of automation includes labor savings, reducing waste, savings in electricity costs, savings in material costs, and improvements to quality, accuracy, and precision. Minimum human intervention is required to control many large facilities, such as this electrical generating station. Automation includes the use of various equipment and control systems such as machinery, processes in factories, boilers,[3] and heat-treating ovens, switching on telephone networks, steering, stabilization of ships, aircraft and other applications and vehicles with reduced human intervention.[4] Examples range from a household thermostat controlling a boiler to a large industrial control system with tens of thousands of input measurements and output control signals. Automation has also found a home in the banking industry. It can range from simple on-off control to multi-variable high-level algorithms in terms of control complexity. In the simplest type of an automatic control loop, a controller compares a measured value of a process with a desired set value and processes the resulting error signal to change some input to the process, in such a way that the process stays at its set point despite disturbances. This closed-loop control is an application of negative feedback to a system. The mathematical basis of control theory was begun in the 18th century and advanced rapidly in the 20th. The term automation, inspired by the earlier word automatic (coming from automaton), was not widely used before 1947, when Ford established an automation department.[5] It was during this time that the industry was rapidly adopting feedback controllers, which were introduced in the 1930s.[6] It was a preoccupation of the Greeks and Arabs (in the period between about 300 BC and about 1200 AD) to keep accurate track of time. In Ptolemaic Egypt, about 270 BC, Ctesibius described a float regulator for a water clock, a device not unlike the ball and cock in a modern flush toilet. This was the earliest feedback-controlled mechanism.[13] The appearance of the mechanical clock in the 14th century made the water clock and its feedback control system obsolete. The PersianBanū Mūsā brothers, in their Book of Ingenious Devices (850 AD), described a number of automatic controls.[14] Two-step level controls for fluids, a form of discontinuous variable structure controls, were developed by the Banu Musa brothers.[15] They also described a feedback controller.[16][17] The design of feedback control systems up through the Industrial Revolution was by trial-and-error, together with a great deal of engineering intuition. It was not until the mid-19th century that the stability of feedback control systems was analyzed using mathematics, the formal language of automatic control theory.[citation needed] In 1771 Richard Arkwright invented the first fully automated spinning mill driven by water power, known at the time as the water frame.[23] An automatic flour mill was developed by Oliver Evans in 1785, making it the first completely automated industrial process.[24][25] A flyball governor is an early example of a feedback control system. An increase in speed would make the counterweights move outward, sliding a linkage that tended to close the valve supplying steam, and so slowing the engine. A centrifugal governor was used by Mr. Bunce of England in 1784 as part of a model steam crane.[26][27] The centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt's partner Boulton saw one at a flour mill Boulton & Watt were building.[21] The governor could not actually hold a set speed; the engine would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped with this governor were not suitable for operations requiring constant speed, such as cotton spinning.[21] Several improvements to the governor, plus improvements to valve cut-off timing on the steam engine, made the engine suitable for most industrial uses before the end of the 19th century. Advances in the steam engine stayed well ahead of science, both thermodynamics and control theory.[21] The governor received relatively little scientific attention until James Clerk Maxwell published a paper that established the beginning of a theoretical basis for understanding control theory. Relay logic was introduced with factory electrification, which underwent rapid adaption from 1900 through the 1920s. Central electric power stations were also undergoing rapid growth and the operation of new high-pressure boilers, steam turbines and electrical substations created a large demand for instruments and controls. Central control rooms became common in the 1920s, but as late as the early 1930s, most process controls were on-off. Operators typically monitored charts drawn by recorders that plotted data from instruments. To make corrections, operators manually opened or closed valves or turned switches on or off. Control rooms also used color-coded lights to send signals to workers in the plant to manually make certain changes.[28] The development of the electronic amplifier during the 1920s, which was important for long-distance telephony, required a higher signal-to-noise ratio, which was solved by negative feedback noise cancellation. This and other telephony applications contributed to the control theory. In the 1940s and 1950s, German mathematician Irmgard Flügge-Lotz developed the theory of discontinuous automatic controls, which found military applications during the Second World War to fire control systems and aircraft navigation systems.[6] Controllers, which were able to make calculated changes in response to deviations from a set point rather than on-off control, began being introduced in the 1930s. Controllers allowed manufacturing to continue showing productivity gains to offset the declining influence of factory electrification.[29] Factory productivity was greatly increased by electrification in the 1920s. U.S. manufacturing productivity growth fell from 5.2%/yr 1919–29 to 2.76%/yr 1929–41. Alexander Field notes that spending on non-medical instruments increased significantly from 1929 to 1933 and remained strong thereafter.[29] The automatic telephone switchboard was introduced in 1892 along with dial telephones. By 1929, 31.9% of the Bell system was automatic.[38]: 158 Automatic telephone switching originally used vacuum tube amplifiers and electro-mechanical switches, which consumed a large amount of electricity. Call volume eventually grew so fast that it was feared the telephone system would consume all electricity production, prompting Bell Labs to begin research on the transistor.[39] The logic performed by telephone switching relays was the inspiration for the digital computer. The first commercially successful glass bottle-blowing machine was an automatic model introduced in 1905.[40] The machine, operated by a two-man crew working 12-hour shifts, could produce 17,280 bottles in 24 hours, compared to 2,880 bottles made by a crew of six men and boys working in a shop for a day. The cost of making bottles by machine was 10 to 12 cents per gross compared to $1.80 per gross by the manual glassblowers and helpers. Sectional electric drives were developed using control theory. Sectional electric drives are used on different sections of a machine where a precise differential must be maintained between the sections. In steel rolling, the metal elongates as it passes through pairs of rollers, which must run at successively faster speeds. In paper making paper, the sheet shrinks as it passes around steam-heated drying arranged in groups, which must run at successively slower speeds. The first application of a sectional electric drive was on a paper machine in 1919.[41] One of the most important developments in the steel industry during the 20th century was continuous wide strip rolling, developed by Armco in 1928.[42] Automated pharmacology production Before automation, many chemicals were made in batches. In 1930, with the widespread use of instruments and the emerging use of controllers, the founder of Dow Chemical Co. was advocating continuous production.[43] Self-acting machine tools that displaced hand dexterity so they could be operated by boys and unskilled laborers were developed by James Nasmyth in the 1840s.[44]Machine tools were automated with Numerical control (NC) using punched paper tape in the 1950s. This soon evolved into computerized numerical control (CNC). Today extensive automation is practiced in practically every type of manufacturing and assembly process. Some of the larger processes include electrical power generation, oil refining, chemicals, steel mills, plastics, cement plants, fertilizer plants, pulp and paper mills, automobile and truck assembly, aircraft production, glass manufacturing, natural gas separation plants, food and beverage processing, canning and bottling and manufacture of various kinds of parts. Robots are especially useful in hazardous applications like automobile spray painting. Robots are also used to assemble electronic circuit boards. Automotive welding is done with robots and automatic welders are used in applications like pipelines. Perhaps the most cited advantage of automation in industry is that it is associated with faster production and cheaper labor costs. Another benefit could be that it replaces hard, physical, or monotonous work.[45] Additionally, tasks that take place in hazardous environments or that are otherwise beyond human capabilities can be done by machines, as machines can operate even under extreme temperatures or in atmospheres that are radioactive or toxic. They can also be maintained with simple quality checks. However, at the time being, not all tasks can be automated, and some tasks are more expensive to automate than others. Initial costs of installing the machinery in factory settings are high, and failure to maintain a system could result in the loss of the product itself. Moreover, some studies seem to indicate that industrial automation could impose ill effects beyond operational concerns, including worker displacement due to systemic loss of employment and compounded environmental damage; however, these findings are both convoluted and controversial in nature, and could potentially be circumvented.[46] Human adaptiveness is often poorly understood by automation initiators. It is often difficult to anticipate every contingency and develop fully preplanned automated responses for every situation. The discoveries inherent in automating processes can require unanticipated iterations to resolve, causing unanticipated costs and delays. People anticipating employment income may be seriously disrupted by others deploying automation where no similar income is readily available. The paradox of automation says that the more efficient the automated system, the more crucial the human contribution of the operators. Humans are less involved, but their involvement becomes more critical. Lisanne Bainbridge, a cognitive psychologist, identified these issues notably in her widely cited paper \"Ironies of Automation.\"[49] If an automated system has an error, it will multiply that error until it is fixed or shut down. This is where human operators come in.[50] A fatal example of this was Air France Flight 447, where a failure of automation put the pilots into a manual situation they were not prepared for.[51] Many operations using automation have large amounts of invested capital and produce high volumes of products, making malfunctions extremely costly and potentially hazardous. Therefore, some personnel is needed to ensure that the entire system functions properly and that safety and product quality are maintained.[52] As a process becomes increasingly automated, there is less and less labor to be saved or quality improvement to be gained. This is an example of both diminishing returns and the logistic function. As more and more processes become automated, there are fewer remaining non-automated processes. This is an example of the exhaustion of opportunities. New technological paradigms may, however, set new limits that surpass the previous limits. Many roles for humans in industrial processes presently lie beyond the scope of automation. Human-level pattern recognition, language comprehension, and language production ability are well beyond the capabilities of modern mechanical and computer systems (but see Watson computer). Tasks requiring subjective assessment or synthesis of complex sensory data, such as scents and sounds, as well as high-level tasks such as strategic planning, currently require human expertise. In many cases, the use of humans is more cost-effective than mechanical approaches even where the automation of industrial tasks is possible. Therefore, algorithmic management as the digital rationalization of human labor instead of its substitution has emerged as an alternative technological strategy.[53] Overcoming these obstacles is a theorized path to post-scarcity economics.[54] Increased automation often causes workers to feel anxious about losing their jobs as technology renders their skills or experience unnecessary. Early in the Industrial Revolution, when inventions like the steam engine were making some job categories expendable, workers forcefully resisted these changes. Luddites, for instance, were English textile workers who protested the introduction of weaving machines by destroying them.[55] More recently, some residents of Chandler, Arizona, have slashed tires and pelted rocks at self-driving car, in protest over the cars' perceived threat to human safety and job prospects.[56] The relative anxiety about automation reflected in opinion polls seems to correlate closely with the strength of organized labor in that region or nation. For example, while a study by the Pew Research Center indicated that 72% of Americans are worried about increasing automation in the workplace, 80% of Swedes see automation and artificial intelligence (AI) as a good thing, due to the country's still-powerful unions and a more robust national safety net.[57] In the U.S., 47% of all current jobs have the potential to be fully automated by 2033, according to the research of experts Carl Benedikt Frey and Michael Osborne. Furthermore, wages and educational attainment appear to be strongly negatively correlated with an occupation's risk of being automated.[58] Even highly skilled professional jobs like a lawyer, doctor, engineer, journalist are at risk of automation.[59] Prospects are particularly bleak for occupations that do not presently require a university degree, such as truck driving.[60] Even in high-tech corridors like Silicon Valley, concern is spreading about a future in which a sizable percentage of adults have little chance of sustaining gainful employment.[61] \"In The Second Machine Age, Erik Brynjolfsson and Andrew McAfee argue that \"...there's never been a better time to be a worker with special skills or the right education, because these people can use technology to create and capture value. However, there's never been a worse time to be a worker with only 'ordinary' skills and abilities to offer, because computers, robots, and other digital technologies are acquiring these skills and abilities at an extraordinary rate.\"[62] As the example of Sweden suggests, however, the transition to a more automated future need not inspire panic, if there is sufficient political will to promote the retraining of workers whose positions are being rendered obsolete. According to a 2020 study in the Journal of Political Economy, automation has robust negative effects on employment and wages: \"One more robot per thousand workers reduces the employment-to-population ratio by 0.2 percentage points and wages by 0.42%.\"[63] Research by Carl Benedikt Frey and Michael Osborne of the Oxford Martin School argued that employees engaged in \"tasks following well-defined procedures that can easily be performed by sophisticated algorithms\" are at risk of displacement, and 47% of jobs in the US were at risk. The study, released as a working paper in 2013 and published in 2017, predicted that automation would put low-paid physical occupations most at risk, by surveying a group of colleagues on their opinions.[64] However, according to a study published in McKinsey Quarterly[65] in 2015 the impact of computerization in most cases is not the replacement of employees but the automation of portions of the tasks they perform.[66] The methodology of the McKinsey study has been heavily criticized for being intransparent and relying on subjective assessments.[67] The methodology of Frey and Osborne has been subjected to criticism, as lacking evidence, historical awareness, or credible methodology.[68][69] Additionally, the Organisation for Economic Co-operation and Development (OECD) found that across the 21 OECD countries, 9% of jobs are automatable.[70] The Obama administration pointed out that every 3 months \"about 6 percent of jobs in the economy are destroyed by shrinking or closing businesses, while a slightly larger percentage of jobs are added.\"[71] A recent MIT economics study of automation in the U.S. from 1990 to 2007 found that there may be a negative impact on employment and wages when robots are introduced to an industry. When one robot is added per one thousand workers, the employment to population ratio decreases between 0.18 and 0.34 percentages and wages are reduced by 0.25–0.5 percentage points. During the time period studied, the US did not have many robots in the economy which restricts the impact of automation. However, automation is expected to triple (conservative estimate) or quadruple (a generous estimate) leading these numbers to become substantially higher.[72] Based on a formula by Gilles Saint-Paul, an economist at Toulouse 1 University, the demand for unskilled human capital declines at a slower rate than the demand for skilled human capital increases.[73] In the long run and for society as a whole it has led to cheaper products, lower average work hours, and new industries forming (i.e., robotics industries, computer industries, design industries). These new industries provide many high salary skill-based jobs to the economy. By 2030, between 3 and 14 percent of the global workforce will be forced to switch job categories due to automation eliminating jobs in an entire sector. While the number of jobs lost to automation is often offset by jobs gained from technological advances, the same type of job loss is not the same one replaced and that leading to increasing unemployment in the lower-middle class. This occurs largely in the US and developed countries where technological advances contribute to higher demand for highly skilled labor but demand for middle-wage labor continues to fall. Economists call this trend \"income polarization\" where unskilled labor wages are driven down and skilled labor is driven up and it is predicted to continue in developed economies.[74] Unemployment is becoming a problem in the U.S. due to the exponential growth rate of automation and technology. According to Kim, Kim, and Lee (2017:1), \"[a] seminal study by Frey and Osborne in 2013 predicted that 47% of the 702 examined occupations in the U.S. faced a high risk of decreased employment rate within the next 10–25 years as a result of computerization.\" As many jobs are becoming obsolete, which is causing job displacement, one possible solution would be for the government to assist with a universal basic income (UBI) program. UBI would be a guaranteed, non-taxed income of around 1000 dollars per month, paid to all U.S. citizens over the age of 21. UBI would help those who are displaced take on jobs that pay less money and still afford to get by. It would also give those that are employed with jobs that are likely to be replaced by automation and technology extra money to spend on education and training on new demanding employment skills. UBI, however, should be seen as a short-term solution as it doesn't fully address the issue of income inequality which will be exacerbated by job displacement. Lights-out manufacturing is a production system with no human workers, to eliminate labor costs. Lights out manufacturing grew in popularity in the U.S. when General Motors in 1982 implemented humans \"hands-off\" manufacturing to \"replace risk-averse bureaucracy with automation and robots\". However, the factory never reached full \"lights out\" status.[75] The costs of automation to the environment are different depending on the technology, product or engine automated. There are automated engines that consume more energy resources from the Earth in comparison with previous engines and vice versa.[citation needed] Hazardous operations, such as oil refining, the manufacturing of industrial chemicals, and all forms of metal working, were always early contenders for automation.[dubious – discuss][citation needed] The automation of vehicles could prove to have a substantial impact on the environment, although the nature of this impact could be beneficial or harmful depending on several factors. Because automated vehicles are much less likely to get into accidents compared to human-driven vehicles, some precautions built into current models (such as anti-lock brakes or laminated glass) would not be required for self-driving versions. Removing these safety features would also significantly reduce the weight of the vehicle, thus increasing fuel economy and reducing emissions per mile. Self-driving vehicles are also more precise concerning acceleration and breaking, and this could contribute to reduced emissions. Self-driving cars could also potentially utilize fuel-efficient features such as route mapping that can calculate and take the most efficient routes. Despite this potential to reduce emissions, some researchers theorize that an increase in the production of self-driving cars could lead to a boom in vehicle ownership and use. This boom could potentially negate any environmental benefits of self-driving cars if a large enough number of people begin driving personal vehicles more frequently.[77] Automation of homes and home appliances is also thought to impact the environment, but the benefits of these features are also questioned. A study of energy consumption of automated homes in Finland showed that smart homes could reduce energy consumption by monitoring levels of consumption in different areas of the home and adjusting consumption to reduce energy leaks (e.g. automatically reducing consumption during the nighttime when activity is low). This study, along with others, indicated that the smart home's ability to monitor and adjust consumption levels would reduce unnecessary energy usage. However, new research suggests that smart homes might not be as efficient as non-automated homes. A more recent study has indicated that, while monitoring and adjusting consumption levels do decrease unnecessary energy use, this process requires monitoring systems that also consume a significant amount of energy. This study suggested that the energy required to run these systems is so much so that it negates any benefits of the systems themselves, resulting in little to no ecological benefit.[78] Another major shift in automation is the increased demand for flexibility and convertibility in manufacturing processes. Manufacturers are increasingly demanding the ability to easily switch from manufacturing Product A to manufacturing Product B without having to completely rebuild the production lines. Flexibility and distributed processes have led to the introduction of Automated Guided Vehicles with Natural Features Navigation. Digital electronics helped too. Former analog-based instrumentation was replaced by digital equivalents which can be more accurate and flexible, and offer greater scope for more sophisticated configuration, parametrization, and operation. This was accompanied by the fieldbus revolution which provided a networked (i.e. a single cable) means of communicating between control systems and field-level instrumentation, eliminating hard-wiring. Discrete manufacturing plants adopted these technologies fast. The more conservative process industries with their longer plant life cycles have been slower to adopt and analog-based measurement and control still dominate. The growing use of Industrial Ethernet on the factory floor is pushing these trends still further, enabling manufacturing plants to be integrated more tightly within the enterprise, via the internet if necessary. Global competition has also increased demand for Reconfigurable Manufacturing Systems.[79] Engineers can now have numerical control over automated devices. The result has been a rapidly expanding range of applications and human activities. Computer-aided technologies (or CAx) now serve as the basis for mathematical and organizational tools used to create complex systems. Notable examples of CAx include computer-aided design (CAD software) and computer-aided manufacturing (CAM software). The improved design, analysis, and manufacture of products enabled by CAx has been beneficial for industry.[80] Human-machine interfaces (HMI) or computer human interfaces (CHI), formerly known as man-machine interfaces, are usually employed to communicate with PLCs and other computers. Service personnel who monitor and control through HMIs can be called by different names. In the industrial process and manufacturing environments, they are called operators or something similar. In boiler houses and central utility departments, they are called stationary engineers.[82] Host simulation software (HSS) is a commonly used testing tool that is used to test the equipment software. HSS is used to test equipment performance concerning factory automation standards (timeouts, response time, processing time).[83] Many agricultural operations are automated with machinery and equipment to improve their diagnosis, decision-making and/or performing. Agricultural automation can relieve the drudgery of agricultural work, improve the timeliness and precision of agricultural operations, raise productivity and resource-use efficiency, build resilience, and improve food quality and safety.[90] Increased productivity can free up labour, allowing agricultural households to spend more time elsewhere.[91] The technological evolution in agriculture has resulted in progressive shifts to digital equipment and robotics.[90] Motorized mechanization using engine power automates the performance of agricultural operations such as ploughing and milking.[92] With digital automation technologies, it also becomes possible to automate diagnosis and decision-making of agricultural operations.[90] For example, autonomous crop robots can harvest and seed crops, while drones can gather information to help automate input application.[91] Precision agriculture often employs such automation technologies[91] Motorized mechanization has generally increased in recent years.[93] Sub-Saharan Africa is the only region where the adoption of motorized mechanization has stalled over the past decades.[94][91] Automation technologies are increasingly used for managing livestock, though evidence on adoption is lacking. Global automatic milking system sales have increased over recent years,[95] but adoption is likely mostly in Northern Europe,[96] and likely almost absent in low- and middle-income countries.[97][91] Automated feeding machines for both cows and poultry also exist, but data and evidence regarding their adoption trends and drivers is likewise scarce.[91][93] Many supermarkets and even smaller stores are rapidly introducing self-checkout systems reducing the need for employing checkout workers. In the U.S., the retail industry employs 15.9 million people as of 2017 (around 1 in 9 Americans in the workforce). Globally, an estimated 192 million workers could be affected by automation according to research by Eurasia Group.[98] Online shopping could be considered a form of automated retail as the payment and checkout are through an automated online transaction processing system, with the share of online retail accounting jumping from 5.1% in 2011 to 8.3% in 2016. [citation needed] However, two-thirds of books, music, and films are now purchased online. In addition, automation and online shopping could reduce demands for shopping malls, and retail property, which in the USA is currently estimated to account for 31% of all commercial property or around 7 billion square feet (650 million square metres). Amazon has gained much of the growth in recent years for online shopping, accounting for half of the growth in online retail in 2016.[98] Other forms of automation can also be an integral part of online shopping, for example, the deployment of automated warehouse robotics such as that applied by Amazon using Kiva Systems. The food retail industry has started to apply automation to the ordering process; McDonald's has introduced touch screen ordering and payment systems in many of its restaurants, reducing the need for as many cashier employees.[99]The University of Texas at Austin has introduced fully automated cafe retail locations.[100] Some Cafes and restaurants have utilized mobile and tablet \"apps\" to make the ordering process more efficient by customers ordering and paying on their device.[101] Some restaurants have automated food delivery to tables of customers using a Conveyor belt system. The use of robots is sometimes employed to replace waiting staff.[102] Automation in construction is the combination of methods, processes, and systems that allow for greater machine autonomy in construction activities. Construction automation may have multiple goals, including but not limited to, reducing jobsite injuries, decreasing activity completion times, and assisting with quality control and quality assurance.[103] Automated mining involves the removal of human labor from the mining process.[104] The mining industry is currently in the transition towards automation. Currently, it can still require a large amount of human capital, particularly in the third world where labor costs are low so there is less incentive for increasing efficiency through automation. The Defense Advanced Research Projects Agency (DARPA) started the research and development of automated visual surveillance and monitoring (VSAM) program, between 1997 and 1999, and airborne video surveillance (AVS) programs, from 1998 to 2002. Currently, there is a major effort underway in the vision community to develop a fully-automated tracking surveillance system. Automated video surveillance monitors people and vehicles in real-time within a busy environment. Existing automated surveillance systems are based on the environment they are primarily designed to observe, i.e., indoor, outdoor or airborne, the number of sensors that the automated system can handle and the mobility of sensors, i.e., stationary camera vs. mobile camera. The purpose of a surveillance system is to record properties and trajectories of objects in a given area, generate warnings or notify the designated authorities in case of occurrence of particular events.[105] [T]he Secretary of Transportation shall develop an automated highway and vehicle prototype from which future fully automated intelligent vehicle-highway systems can be developed. Such development shall include research in human factors to ensure the success of the man-machine relationship. The goal of this program is to have the first fully automated highway roadway or an automated test track in operation by 1997. This system shall accommodate the installation of equipment in new and existing motor vehicles. Full automation commonly defined as requiring no control or very limited control by the driver; such automation would be accomplished through a combination of sensor, computer, and communications systems in vehicles and along the roadway. Fully automated driving would, in theory, allow closer vehicle spacing and higher speeds, which could enhance traffic capacity in places where additional road building is physically impossible, politically unacceptable, or prohibitively expensive. Automated controls also might enhance road safety by reducing the opportunity for driver error, which causes a large share of motor vehicle crashes. Other potential benefits include improved air quality (as a result of more-efficient traffic flows), increased fuel economy, and spin-off technologies generated during research and development related to automated highway systems.[107] Business process automation (BPA) is the technology-enabled automation of complex business processes.[109] It can help to streamline a business for simplicity, achieve digital transformation, increase service quality, improve service delivery or contain costs. BPA consists of integrating applications, restructuring labor resources and using software applications throughout the organization. Robotic process automation (RPA; or RPAAI for self-guided RPA 2.0) is an emerging field within BPA and uses AI. BPAs can be implemented in a number of business areas including marketing, sales and workflow. Home automation (also called domotics) designates an emerging practice of increased automation of household appliances and features in residential dwellings, particularly through electronic means that allow for things impracticable, overly expensive or simply not possible in recent past decades. The rise in the usage of home automation solutions has taken a turn reflecting the increased dependency of people on such automation solutions. However, the increased comfort that gets added through these automation solutions is remarkable.[110] Automation is essential for many scientific and clinical applications.[111] Therefore, automation has been extensively employed in laboratories. From as early as 1980 fully automated laboratories have already been working.[112] However, automation has not become widespread in laboratories due to its high cost. This may change with the ability of integrating low-cost devices with standard laboratory equipment.[113][114]Autosamplers are common devices used in laboratory automation. Industrial automation deals primarily with the automation of manufacturing, quality control, and material handling processes. General-purpose controllers for industrial processes include programmable logic controllers, stand-alone I/O modules, and computers. Industrial automation is to replace the human action and manual command-response activities with the use of mechanized equipment and logical programming commands. One trend is increased use of machine vision[115] to provide automatic inspection and robot guidance functions, another is a continuing increase in the use of robots. Industrial automation is simply required in industries. The rise of industrial automation is directly tied to the \"Fourth Industrial Revolution\", which is better known now as Industry 4.0. Originating from Germany, Industry 4.0 encompasses numerous devices, concepts, and machines,[116] as well as the advancement of the industrial internet of things (IIoT). An \"Internet of Things is a seamless integration of diverse physical objects in the Internet through a virtual representation.\"[117] These new revolutionary advancements have drawn attention to the world of automation in an entirely new light and shown ways for it to grow to increase productivity and efficiency in machinery and manufacturing facilities. Industry 4.0 works with the IIoT and software/hardware to connect in a way that (through communication technologies) add enhancements and improve manufacturing processes. Being able to create smarter, safer, and more advanced manufacturing is now possible with these new technologies. It opens up a manufacturing platform that is more reliable, consistent, and efficient than before. Implementation of systems such as SCADA is an example of software that takes place in Industrial Automation today. SCADA is a supervisory data collection software, just one of the many used in Industrial Automation.[118] Industry 4.0 vastly covers many areas in manufacturing and will continue to do so as time goes on.[116] Industrial robotics is a sub-branch in industrial automation that aids in various manufacturing processes. Such manufacturing processes include machining, welding, painting, assembling and material handling to name a few.[119] Industrial robots use various mechanical, electrical as well as software systems to allow for high precision, accuracy and speed that far exceed any human performance. The birth of industrial robots came shortly after World War II as the U.S. saw the need for a quicker way to produce industrial and consumer goods.[120] Servos, digital logic and solid-state electronics allowed engineers to build better and faster systems and over time these systems were improved and revised to the point where a single robot is capable of running 24 hours a day with little or no maintenance. In 1997, there were 700,000 industrial robots in use, the number has risen to 1.8M in 2017[121] In recent years, AI with robotics is also used in creating an automatic labeling solution, using robotic arms as the automatic label applicator, and AI for learning and detecting the products to be labelled.[122] Industrial automation incorporates programmable logic controllers in the manufacturing process. Programmable logic controllers (PLCs) use a processing system which allows for variation of controls of inputs and outputs using simple programming. PLCs make use of programmable memory, storing instructions and functions like logic, sequencing, timing, counting, etc. Using a logic-based language, a PLC can receive a variety of inputs and return a variety of logical outputs, the input devices being sensors and output devices being motors, valves, etc. PLCs are similar to computers, however, while computers are optimized for calculations, PLCs are optimized for control tasks and use in industrial environments. They are built so that only basic logic-based programming knowledge is needed and to handle vibrations, high temperatures, humidity, and noise. The greatest advantage PLCs offer is their flexibility. With the same basic controllers, a PLC can operate a range of different control systems. PLCs make it unnecessary to rewire a system to change the control system. This flexibility leads to a cost-effective system for complex and varied control systems.[123] PLCs can range from small \"building brick\" devices with tens of I/O in a housing integral with the processor, to large rack-mounted modular devices with a count of thousands of I/O, and which are often networked to other PLC and SCADA systems. They can be designed for multiple arrangements of digital and analog inputs and outputs (I/O), extended temperature ranges, immunity to electrical noise, and resistance to vibration and impact. Programs to control machine operation are typically stored in battery-backed-up or non-volatile memory. It was from the automotive industry in the USA that the PLC was born. Before the PLC, control, sequencing, and safety interlock logic for manufacturing automobiles was mainly composed of relays, cam timers, drum sequencers, and dedicated closed-loop controllers. Since these could number in the hundreds or even thousands, the process for updating such facilities for the yearly model change-over was very time-consuming and expensive, as electricians needed to individually rewire the relays to change their operational characteristics. When digital computers became available, being general-purpose programmable devices, they were soon applied to control sequential and combinatorial logic in industrial processes. However, these early computers required specialist programmers and stringent operating environmental control for temperature, cleanliness, and power quality. To meet these challenges, the PLC was developed with several key attributes. It would tolerate the shop-floor environment, it would support discrete (bit-form) input and output in an easily extensible manner, it would not require years of training to use, and it would permit its operation to be monitored. Since many industrial processes have timescales easily addressed by millisecond response times, modern (fast, small, reliable) electronics greatly facilitate building reliable controllers, and performance could be traded off for reliability.[124] Agent-assisted automation refers to automation used by call center agents to handle customer inquiries. The key benefit of agent-assisted automation is compliance and error-proofing. Agents are sometimes not fully trained or they forget or ignore key steps in the process. The use of automation ensures that what is supposed to happen on the call actually does, every time. There are two basic types: desktop automation and automated voice solutions. Desktop automation refers to software programming that makes it easier for the call center agent to work across multiple desktop tools. The automation would take the information entered into one tool and populate it across the others so it did not have to be entered more than once, for example. Automated voice solutions allow the agents to remain on the line while disclosures and other important information is provided to customers in the form of pre-recorded audio files. Specialized applications of these automated voice solutions enable the agents to process credit cards without ever seeing or hearing the credit card numbers or CVV codes.[125] An electromechanical timer, normally used for open-loop control based purely on a timing sequence, with no feedback from the process In open-loop control, the control action from the controller is independent of the \"process output\" (or \"controlled process variable\"). A good example of this is a central heating boiler controlled only by a timer, so that heat is applied for a constant time, regardless of the temperature of the building. The control action is the switching on/off of the boiler, but the controlled variable should be the building temperature, but is not because this is open-loop control of the boiler, which does not give closed-loop control of the temperature. In closed loop control, the control action from the controller is dependent on the process output. In the case of the boiler analogy this would include a thermostat to monitor the building temperature, and thereby feed back a signal to ensure the controller maintains the building at the temperature set on the thermostat. A closed loop controller therefore has a feedback loop which ensures the controller exerts a control action to give a process output the same as the \"reference input\" or \"set point\". For this reason, closed loop controllers are also called feedback controllers.[126] The definition of a closed loop control system according to the British Standard Institution is \"a control system possessing monitoring feedback, the deviation signal formed as a result of this feedback being used to control the action of a final control element in such a way as to tend to reduce the deviation to zero.\"[127] Likewise; \"A Feedback Control System is a system which tends to maintain a prescribed relationship of one system variable to another by comparing functions of these variables and using the difference as a means of control.\"[128] One of the simplest types of control is on-off control. An example is a thermostat used on household appliances which either open or close an electrical contact. (Thermostats were originally developed as true feedback-control mechanisms rather than the on-off common household appliance thermostat.) Sequence control, in which a programmed sequence of discrete operations is performed, often based on system logic that involves system states. An elevator control system is an example of sequence control. In a PID loop, the controller continuously calculates an error valuee(t){\\displaystyle e(t)} as the difference between a desired setpoint and a measured process variable and applies a correction based on proportional, integral, and derivative terms, respectively (sometimes denoted P, I, and D) which give their name to the controller type. The theoretical understanding and application date from the 1920s, and they are implemented in nearly all analog control systems; originally in mechanical controllers, and then using discrete electronics and latterly in industrial process computers. Sequential control may be either to a fixed sequence or to a logical one that will perform different actions depending on various system states. An example of an adjustable but otherwise fixed sequence is a timer on a lawn sprinkler. States refer to the various conditions that can occur in a use or sequence scenario of the system. An example is an elevator, which uses logic based on the system state to perform certain actions in response to its state and operator input. For example, if the operator presses the floor n button, the system will respond depending on whether the elevator is stopped or moving, going up or down, or if the door is open or closed, and other conditions.[129] Early development of sequential control was relay logic, by which electrical relays engage electrical contacts which either start or interrupt power to a device. Relays were first used in telegraph networks before being developed for controlling other devices, such as when starting and stopping industrial-sized electric motors or opening and closing solenoid valves. Using relays for control purposes allowed event-driven control, where actions could be triggered out of sequence, in response to external events. These were more flexible in their response than the rigid single-sequence cam timers. More complicated examples involved maintaining safe sequences for devices such as swing bridge controls, where a lock bolt needed to be disengaged before the bridge could be moved, and the lock bolt could not be released until the safety gates had already been closed. The total number of relays and cam timers can number into the hundreds or even thousands in some factories. Early programming techniques and languages were needed to make such systems manageable, one of the first being ladder logic, where diagrams of the interconnected relays resembled the rungs of a ladder. Special computers called programmable logic controllers were later designed to replace these collections of hardware with a single, more easily re-programmed unit. In a typical hard-wired motor start and stop circuit (called a control circuit) a motor is started by pushing a \"Start\" or \"Run\" button that activates a pair of electrical relays. The \"lock-in\" relay locks in contacts that keep the control circuit energized when the push-button is released. (The start button is a normally open contact and the stop button is a normally closed contact.) Another relay energizes a switch that powers the device that throws the motor starter switch (three sets of contacts for three-phase industrial power) in the main power circuit. Large motors use high voltage and experience high in-rush current, making speed important in making and breaking contact. This can be dangerous for personnel and property with manual switches. The \"lock-in\" contacts in the start circuit and the main power contacts for the motor are held engaged by their respective electromagnets until a \"stop\" or \"off\" button is pressed, which de-energizes the lock in relay.[130] This state diagram shows how UML can be used for designing a door system that can only be opened and closed. Commonly interlocks are added to a control circuit. Suppose that the motor in the example is powering machinery that has a critical need for lubrication. In this case, an interlock could be added to ensure that the oil pump is running before the motor starts. Timers, limit switches, and electric eyes are other common elements in control circuits. Solenoid valves are widely used on compressed air or hydraulic fluid for powering actuators on mechanical components. While motors are used to supply continuous rotary motion, actuators are typically a better choice for intermittently creating a limited range of movement for a mechanical component, such as moving various mechanical arms, opening or closing valves, raising heavy press-rolls, applying pressure to presses. Computers can perform both sequential control and feedback control, and typically a single computer will do both in an industrial application. Programmable logic controllers (PLCs) are a type of special-purpose microprocessor that replaced many hardware components such as timers and drum sequencers used in relay logic–type systems. General-purpose process control computers have increasingly replaced stand-alone controllers, with a single computer able to perform the operations of hundreds of controllers. Process control computers can process data from a network of PLCs, instruments, and controllers to implement typical (such as PID) control of many individual variables or, in some cases, to implement complex control algorithms using multiple inputs and mathematical manipulations. They can also analyze data and create real-time graphical displays for operators and run reports for operators, engineers, and management. Control of an automated teller machine (ATM) is an example of an interactive process in which a computer will perform a logic-derived response to a user selection based on information retrieved from a networked database. The ATM process has similarities with other online transaction processes. The different logical responses are called scenarios. Such processes are typically designed with the aid of use cases and flowcharts, which guide the writing of the software code. The earliest feedback control mechanism was the water clock invented by Greek engineer Ctesibius (285–222 BC). ^Landes, David. S. (1969). The Unbound Prometheus: Technological Change and Industrial Development in Western Europe from 1750 to the Present. Cambridge, New York: Press Syndicate of the University of Cambridge. p. 475. ISBN978-0-521-09418-4. ^ abcIn Brief to The State of Food and Agriculture 2022. Leveraging automation in agriculture for transforming agrifood systems. Rome: Food and Agriculture Organization of the United Nations. 2022. doi:10.4060/cc2459en. ISBN978-92-5-137005-6. ^ abcdefThe State of Food and Agriculture 2022.Leveraging agricultural automation for transforming agrifood systems. Rome: Food and Agriculture Organization of the United Nations. 2022. doi:10.4060/cb9479en. ISBN978-92-5-136043-9. ^Economics of adoption for digital automated technologies in agriculture. Background paper for The State of Food and Agriculture 2022. Rome: Food and Agriculture Organization of the United Nations. 2022. doi:10.4060/cc2624en. ISBN978-92-5-137080-3."}
{"url": "https://en.m.wikipedia.org/wiki/Cohen_syndrome", "text": "This syndrome is caused by pathogenic variants (mutations) in the VPS13Bgene at chromosomal locus 8q22.[2] It has an autosomal recessive transmission with variable expression.[3] Variants in VSP13B also cause Mirhosseini–Holmes–Walton syndrome, which is now considered to be the same entity as Cohen syndrome[4][5][6][7] Some of the symptoms of Cohen syndrome can be addressed through early intervention with medical specialists. Those who have this disease may benefit from early exposure to speech, physical, and occupational therapy to correct symptoms such as joint overflexibility, developmental delays, hypotonia, and motor clumsiness.[8] Diagnosis may potentially be delayed due to the lack of a definitive molecular test as well as the clinical variability of published case reports.[9] Glasses are beneficial to those who have severe nearsightedness, whereas individuals with retinal degeneration need training for the visually impaired, which is usually more beneficial when this is addressed at a young age. Younger patients start out having unimpaired vision, but it starts to deteriorate at a young age and does so slowly.[10] If vision is able to improve with the use of glasses, they should be worn to help facilitate concept development. Retinal degeneration cannot be ameliorated with glasses.[11] The type of therapy needed for each individual varies, as not every affected individual would benefit from speech, physical, and occupational therapies. The type of therapy for each person is highly individualized. Individuals who have Cohen syndrome may also benefit from psychosocial support.[12] Many people who have Cohen syndrome also have neutropenia which is a condition in which an individual has an abnormally low number of white blood cells called neutrophils. Having this condition may make these individuals susceptible to infections. Granulocyte-colony stimulating factor (G-CSF) is one possible treatment for neutropenia.[12] Monitoring weight gain and growth is crucial, as well as annual ophthalmologic and hematologic evaluations and checkups.[8] While there are treatments available to people with Cohen syndrome, there are no known cures for the disease.[citation needed] Over the past several years, there have been approximately 50 new cases worldwide. There are population groups with this condition in Australia, New Zealand, the UK and the US. It still seems to go undiagnosed, leaving the number of known cases less than 500.[citation needed]"}
{"url": "https://en.m.wikipedia.org/wiki/Depression_(mood)", "text": "Depression (mood) Depression is a mental state of low mood and aversion to activity.[3] It affects more than 280 million people of all ages (about 3.5% of the global population).[4] Depression affects a person's thoughts, behavior, feelings, and sense of well-being.[5] Depressed people often experience loss of motivation or interest in, or reduced pleasure or joy from, experiences that would normally bring them pleasure or joy.[6] Depressed mood is a symptom of some mood disorders such as major depressive disorder and dysthymia;[7] it is a normal temporary reaction to life events, such as the loss of a loved one; and it is also a symptom of some physical diseases and a side effect of some drugs and medical treatments. It may feature sadness, difficulty in thinking and concentration and a significant increase or decrease in appetite and time spent sleeping. People experiencing depression may have feelings of dejection or hopelessness and may experience suicidal thoughts. It can either be short term or long term. Contributing factors Life events Adversity in childhood, such as bereavement, neglect, mental abuse, physical abuse, sexual abuse, or unequal parental treatment of siblings can contribute to depression in adulthood.[8][9] Childhood physical or sexual abuse in particular significantly correlates with the likelihood of experiencing depression over the survivor's lifetime.[10] People who have experienced four or more adverse childhood experiences are 3.2 to 4.0 times more likely to suffer from depression.[11] Poor housing quality, non-functionality, lack of green spaces, and exposure to noise and air pollution are linked to depressive moods, emphasizing the need for consideration in planning to prevent such outcomes.[12] Studies have consistently shown that physicians have had the highest depression and suicide rates compared to people in many other lines of work—for suicide, 40% higher for male physicians and 130% higher for female physicians.[13][14][15] Childhood and adolescence Depression in childhood and adolescence is similar to adult major depressive disorder, although young sufferers may exhibit increased irritability or behavioral dyscontrol instead of the more common sad, empty, or hopeless feelings seen with adults.[22] Children who are under stress, experiencing loss, or have other underlying disorders are at a higher risk for depression. Childhood depression is often comorbid with mental disorders outside of other mood disorders; most commonly anxiety disorder and conduct disorder. Depression also tends to run in families.[23] Personality Depression is associated with low extraversion,[24] and people who have high levels of neuroticism are more likely to experience depressive symptoms and are more likely to receive a diagnosis of a depressive disorder.[25] Side effect of medical treatment It is possible that some early-generation beta-blockers induce depression in some patients, though the evidence for this is weak and conflicting. There is strong evidence for a link between alpha interferon therapy and depression. One study found that a third of alpha interferon-treated patients had developed depression after three months of treatment. (Beta interferon therapy appears to have no effect on rates of depression.) There is moderately strong evidence that finasteride when used in the treatment of alopecia increases depressive symptoms in some patients. Evidence linking isotretinoin, an acne treatment, to depression is strong.[26] Other medicines that seem to increase the risk of depression include anticonvulsants, antimigraine drugs, antipsychotics and hormonal agents such as gonadotropin-releasing hormone agonist.[27] Substance-induced Several drugs of abuse can cause or exacerbate depression, whether in intoxication, withdrawal, and from chronic use. These include alcohol, sedatives (including prescription benzodiazepines), opioids (including prescription pain killers and illicit drugs such as heroin), stimulants (such as cocaine and amphetamines), hallucinogens, and inhalants.[28] Studies have found that anywhere from 30 to 85 percent of patients suffering from chronic pain are also clinically depressed.[33][34][35] A 2014 study by Hooley et al. concluded that chronic pain increased the chance of death by suicide by two to three times.[36] In 2017, the British Medical Association found that 49% of UK chronic pain patients also had depression.[37] Psychiatric syndromes A number of psychiatric syndromes feature depressed mood as a main symptom. The mood disorders are a group of disorders considered to be primary disturbances of mood. These include major depressive disorder (commonly called major depression or clinical depression) where a person has at least two weeks of depressed mood or a loss of interest or pleasure in nearly all activities; and dysthymia, a state of chronic depressed mood, the symptoms of which do not meet the severity of a major depressive episode. Another mood disorder, bipolar disorder, features one or more episodes of abnormally elevated mood, cognition, and energy levels, but may also involve one or more episodes of depression.[38] When the course of depressive episodes follows a seasonal pattern, the disorder (major depressive disorder, bipolar disorder, etc.) may be described as a seasonal affective disorder. Inflammation There is evidence for a link between inflammation and depression.[41] Inflammatory processes can be triggered by negative cognitions or their consequences, such as stress, violence, or deprivation. Thus, negative cognitions can cause inflammation that can, in turn, lead to depression.[42][43][dubious – discuss] In addition, there is increasing evidence that inflammation can cause depression because of the increase of cytokines, setting the brain into a \"sickness mode\".[44] Classical symptoms of being physically sick, such as lethargy, show a large overlap in behaviors that characterize depression. Levels of cytokines tend to increase sharply during the depressive episodes of people with bipolar disorder and drop off during remission.[45] Furthermore, it has been shown in clinical trials that anti-inflammatory medicines taken in addition to antidepressants not only significantly improves symptoms but also increases the proportion of subjects positively responding to treatment.[46] Inflammations that lead to serious depression could be caused by common infections such as those caused by a virus, bacteria or even parasites.[47] Historical legacy Researchers have begun to conceptualize ways in which the historical legacies of racism and colonialism may create depressive conditions.[48][49] Measures Measures of depression include, but are not limited to: Beck Depression Inventory-11 and the 9-item depression scale in the Patient Health Questionnaire (PHQ-9).[50] Both of these measures are psychological tests that ask personal questions of the participant, and have mostly been used to measure the severity of depression. The Beck Depression Inventory is a self-report scale that helps a therapist identify the patterns of depression symptoms and monitor recovery. The responses on this scale can be discussed in therapy to devise interventions for the most distressing symptoms of depression.[6] Management Depressed mood may not require professional treatment, and may be a normal temporary reaction to life events, a symptom of some medical condition, or a side effect of some drugs or medical treatments. A prolonged depressed mood, especially in combination with other symptoms, may lead to a diagnosis of a psychiatric or medical condition which may benefit from treatment. Many have linked depression to a life falling short of expectations and have blamed influencers for setting unrealistic expectations.[51][52][53] Physical activity has a protective effect against the emergence of depression in some people.[55] There is limited evidence suggesting yoga may help some people with depressive disorders or elevated levels of depression, but more research is needed.[56][57] Reminiscence of old and fond memories is another alternative form of treatment, especially for the elderly who have lived longer and have more experiences in life. It is a method that causes a person to recollect memories of their own life, leading to a process of self-recognition and identifying familiar stimuli. By maintaining one's personal past and identity, it is a technique that stimulates people to view their lives in a more objective and balanced way, causing them to pay attention to positive information in their life stories, which would successfully reduce depressive mood levels.[58] There is limited evidence that continuing antidepressant medication for one year reduces the risk of depression recurrence with no additional harm.[59] Recommendations for psychological treatments or combination treatments in preventing recurrence are not clear.[59] Epidemiology Depression is the leading cause of disability worldwide, the United Nations (UN) health agency reported, estimating that it affects more than 300 million people worldwide – the majority of them women, young people and the elderly. An estimated 4.4 percent of the global population has depression, according to a report released by the UN World Health Organization (WHO), which shows an 18 percent increase in the number of people living with depression between 2005 and 2015.[60][61][62] Depression is a major mental-health cause of disease burden. Its consequences further lead to significant burden in public health, including a higher risk of dementia, premature mortality arising from physical disorders, and maternal depression impacts on child growth and development.[63] Approximately 76% to 85% of depressed people in low- and middle-income countries do not receive treatment;[64] barriers to treatment include: inaccurate assessment, lack of trained health-care providers, social stigma and lack of resources.[4] The stigma comes from misguided societal views that people with mental illness are different from everyone else, and they can choose to get better only if they wanted to.[65] Due to this more than half of the people with depression do not receive help with their disorders. The stigma leads to a strong preference for privacy. An analysis of 40,350 undergraduates from 70 institutions by Posselt and Lipson found that undergraduates who perceived their classroom environments as highly competitive had a 37% higher chance of developing depression and a 69% higher chance of developing anxiety.[66] Several studies have suggested that unemployment roughly doubles the risk of developing depression.[67][68][69][70][71] The World Health Organization has constructed guidelines – known as The Mental Health Gap Action Programme (mhGAP) – aiming to increase services for people with mental, neurological and substance-use disorders.[4] Depression is listed as one of conditions prioritized by the programme. Trials conducted show possibilities for the implementation of the programme in low-resource primary-care settings dependent on primary-care practitioners and lay health-workers.[72] Examples of mhGAP-endorsed therapies targeting depression include Group Interpersonal Therapy as group treatment for depression and \"Thinking Health\", which utilizes cognitive behavioral therapy to tackle perinatal depression.[4] Furthermore, effective screening in primary care is crucial for the access of treatments. The mhGAP adopted its approach of improving detection rates of depression by training general practitioners. However, there is still weak evidence supporting this training.[63] According to 2011 study, people who are high in hypercompetitive traits are also likely to measure higher for depression and anxiety.[73] History The term depression was derived from the Latin verb deprimere, \"to press down\".[74] From the 14th century, \"to depress\" meant to subjugate or to bring down in spirits. It was used in 1665 in English author Richard Baker'sChronicle to refer to someone having \"a great depression of spirit\", and by English author Samuel Johnson in a similar sense in 1753.[75] In Ancient Greece, disease was thought due to an imbalance in the four basic bodily fluids, or humors. Personality types were similarly thought to be determined by the dominant humor in a particular person. Derived from the Ancient Greekmelas, \"black\", and kholé, \"bile\",[76]melancholia was described as a distinct disease with particular mental and physical symptoms by Hippocrates in his Aphorisms, where he characterized all \"fears and despondencies, if they last a long time\" as being symptomatic of the ailment.[77] During the 18th century, the humoral theory of melancholia was increasingly being challenged by mechanical and electrical explanations; references to dark and gloomy states gave way to ideas of slowed circulation and depleted energy.[78] German physician Johann Christian Heinroth, however, argued melancholia was a disturbance of the soul due to moral conflict within the patient. In the 20th century, the German psychiatrist Emil Kraepelin distinguished manic depression. The influential system put forward by Kraepelin unified nearly all types of mood disorder into manic–depressive insanity. Kraepelin worked from an assumption of underlying brain pathology, but also promoted a distinction between endogenous (internally caused) and exogenous (externally caused) types.[79] Other psycho-dynamic theories were proposed. Existential and humanistic theories represented a forceful affirmation of individualism.[80] Austrian existential psychiatrist Viktor Frankl connected depression to feelings of futility and meaninglessness.[81] Frankl's logotherapy addressed the filling of an \"existential vacuum\" associated with such feelings, and may be particularly useful for depressed adolescents.[82][83] Researchers theorized that depression was caused by a chemical imbalance in neurotransmitters in the brain, a theory based on observations made in the 1950s of the effects of reserpine and isoniazid in altering monoamine neurotransmitter levels and affecting depressive symptoms.[84] During the 1960s and 70s, manic-depression came to refer to just one type of mood disorder (now most commonly known as bipolar disorder) which was distinguished from (unipolar) depression. The terms unipolar and bipolar had been coined by German psychiatrist Karl Kleist.[79] In July 2022, British psychiatrist Joanna Moncrieff, also psychiatrist Mark Horowtiz and others proposed in a study on academic journal Molecular Psychiatry that depression is not caused by a serotonin imbalance in the human body, unlike what most of the psychiatry community points to, and that therefore anti-depressants do not work against the illness.[85][86] However, such study was met with criticism from some psychiatrists, who argued the study's methodology used an indirect trace of serotonin, instead of taking direct measurements of the molecule.[87] Moncrieff said that, despite her study's conclusions, no one should interrupt their treatment if they are taking any anti-depressant.[87] See also Alain Ehrenberg, French sociologist, author of Weariness of the Self: Diagnosing the History of Depression in the Contemporary Age"}
{"url": "https://en.m.wikipedia.org/wiki/Genetics_of_obesity", "text": "Genetics of obesity Like many other medical conditions, obesity is the result of an interplay between environmental and genetic factors.[2][3] Studies have identified variants in several genes that may contribute to weight gain and body fat distribution; although, only in a few cases are genes the primary cause of obesity.[4][5] Polymorphisms in various genes controlling appetite and metabolism predispose to obesity under certain dietary conditions. The percentage of obesity that can be attributed to genetics varies widely, depending on the population examined, from 6% to 85%,[6] with the typical estimate at 50%. It is likely that in each person a number of genes contribute to the likelihood of developing obesity in small part, with each gene increasing or decreasing the odds marginally, and together determining how an individual responds to the environmental factors.[7] As of 2006, more than 41 sites on the human genome have been linked to the development of obesity when a favorable environment is present.[8] Some of these obesogenic (weight gain) or leptogenic (weight loss) genes may influence the obese individual's response to weight loss or weight management.[9] Contents Although genetic deficiencies are currently considered rare, variations in these genes may predispose to common obesity.[10][11][12] Many candidate genes are highly expressed in the central nervous system.[13] Several additional loci have been identified.[14] Also, several quantitative trait loci for BMI have been identified. Adults who were homozygous for a particular FTOallele weighed about 3 kilograms more and had a 1.6-fold greater rate of obesity than those who had not inherited this trait.[19] This association disappeared, though, when those with FTO polymorphisms participated in moderately intensive physical activity equivalent to three to four hours of brisk walking.[20] Some studies have focused upon inheritance patterns without focusing upon specific genes. One study found that 80% of the offspring of two obese parents were obese, in contrast to less than 10% of the offspring of two parents who were of normal weight.[30] The thrifty gene hypothesis postulates that due to dietary scarcity during human evolution people are prone to obesity. Their ability to take advantage of rare periods of abundance by storing energy as fat would be advantageous during times of varying food availability, and individuals with greater adipose reserves would more likely survive famine. This tendency to store fat, however, would be maladaptive in societies with stable food supplies.[31] This is the presumed reason that Pima Native Americans, who evolved in a desert ecosystem, developed some of the highest rates of obesity when exposed to a Western lifestyle.[32] Numerous studies of laboratory rodents provide strong evidence that genetics play an important role in obesity.[33][34] The risk of obesity is determined by not only specific genotypes but also gene-gene interactions. However, there are still challenges associated with detecting gene-gene interactions for obesity.[35] There are also genes that can be protective against obesity. For instance, in GPR75 variants were identified as such alleles in ~640,000 sequenced exomes which may be relevant to e.g. therapeutic strategies against obesity.[36][37] Other candidate anti-obesity-related genes include ALK,[38]TBC1D1,[39] and SRA1.[40] The term \"non-syndromic obesity\" is sometimes used to exclude these conditions.[41] In people with early-onset severe obesity (defined by an onset before 10 years of age and body mass index over three standard deviations above normal), 7% harbor a single locus mutation.[42]"}
{"url": "https://en.m.wikipedia.org/wiki/Renal_cell_carcinoma", "text": "Renal cell carcinoma (RCC) is a kidney cancer that originates in the lining of the proximal convoluted tubule, a part of the very small tubes in the kidney that transport primary urine. RCC is the most common type of kidney cancer in adults, responsible for approximately 90–95% of cases.[1] It is more common in men (with a male-to-female ratio of up to 2:1).[2] It is most commonly diagnosed in the elderly (especially in people over 75 years of age).[3] Renal cell carcinoma Micrograph of the most common type of renal cell carcinoma (clear cell)—on right of the image; non-tumour kidney is on the left of the image. Nephrectomy specimen. H&E stain Initial treatment is most commonly either partial or complete removal of the affected kidney(s).[4] Where the cancer has not metastasised (spread to other organs) or burrowed deeper into the tissues of the kidney, the five-year survival rate is 65–90%,[5] but this is lowered considerably when the cancer has spread. Historically, medical practitioners expected a person to present with three findings. This classic triad[10] is 1: haematuria, which is when there is blood present in the urine, 2: flank pain, which is pain on the side of the body between the hip and ribs, and 3: an abdominal mass, similar to bloating but larger. It is now known that this classic triad of symptoms only occurs in 10–15% of cases, and is usually indicative that the renal cell carcinoma (RCC) is in an advanced stage.[10] Today, RCC is often asymptomatic (meaning few to no symptoms) and is generally detected incidentally when a person is being examined for other ailments.[11] The greatest risk factors for RCC are lifestyle-related; smoking, obesity and hypertension (high blood pressure) have been estimated to account for up to 50% of cases.[16] Occupational exposure to some chemicals such as asbestos, cadmium, lead, chlorinated solvents, petrochemicals and PAH (polycyclic aromatic hydrocarbon) has been examined by multiple studies with inconclusive results.[17][18][19] Another suspected risk factor is the long term use of non-steroidal anti-inflammatory drugs (NSAIDS).[20] Finally, studies have found that women who have had a hysterectomy are at more than double the risk of developing RCC than those who have not.[21] Moderate alcohol consumption, on the other hand, has been shown to have a protective effect.[22] The most significant disease affecting risk however is not genetically linked – patients with acquired cystic disease of the kidney requiring dialysis are 30 times more likely than the general population to develop RCC.[26] The tumour arises from the cells of the proximal renal tubular epithelium.[1] It is considered an adenocarcinoma.[7] There are two subtypes: sporadic (that is, non-hereditary) and hereditary.[1] Both such subtypes are associated with mutations in the short-arm of chromosome 3, with the implicated genes being either tumour suppressor genes (VHL and TSC) or oncogenes (like c-Met).[1] The first steps taken to diagnose this condition are consideration of the signs and symptoms, and a medical history (the detailed medical review of past health state) to evaluate any risk factors. Based on the symptoms presented, a range of biochemical tests (using blood and/or urine samples) may also be considered as part of the screening process to provide sufficient quantitative analysis of any differences in electrolytes, kidney and liver function, and blood clotting times.[25] Upon physical examination, palpation of the abdomen may reveal the presence of a mass or an organ enlargement.[27] Renal cell carcinoma (RCC) is not a single entity, but rather a collection of different types of tumours, each derived from the various parts of the nephron (epithelium or renal tubules) and possessing distinct genetic characteristics, histological features, and, to some extent, clinical phenotypes.[25] Array-based karyotyping can be used to identify characteristic chromosomal aberrations in renal tumors with challenging morphology.[34][35] Array-based karyotyping performs well on paraffin embedded tumours[36] and is amenable to routine clinical use. See also Virtual Karyotype for CLIA certified laboratories offering array-based karyotyping of solid tumours. The 2004 World Health Organization (WHO) classification of genitourinary tumours recognizes over 40 subtypes of renal neoplasms. Since the publication of the latest iteration of the WHO classification in 2004, several novel renal tumour subtypes have been described:[37] Laboratory tests are generally conducted when the patient presents with signs and symptoms that may be characteristic of kidney impairment. They are not primarily used to diagnose kidney cancer, due to its asymptomatic nature and are generally found incidentally during tests for other illnesses such as gallbladder disease.[39] In other words, these cancers are not detected usually because they do not cause pain or discomfort when they are discovered. Laboratory analysis can provide an assessment on the overall health of the patient and can provide information in determining the staging and degree of metastasis to other parts of the body (if a renal lesion has been identified) before treatment is given.[citation needed] The presence of blood in urine is a common presumptive sign of renal cell carcinoma. The haemoglobin of the blood causes the urine to be rusty, brown or red in colour. Alternatively, urinalysis can test for sugar, protein and bacteria which can also serve as indicators for cancer. A complete blood cell count can also provide additional information regarding the severity and spreading of the cancer.[40] The CBC provides a quantified measure of the different cells in the whole blood sample from the patient. Such cells examined for in this test include red blood cells (erythrocytes), white blood cells (leukocytes) and platelets (thrombocytes). A common sign of renal cell carcinoma is anaemia whereby the patient exhibits deficiency in red blood cells.[41] CBC tests are vital as a screening tool for examination the health of patient prior to surgery. Inconsistencies with platelet counts are also common amongst these cancer patients and further coagulation tests, including erythrocyte sedimentation rate (ESR), prothrombin time (PT), activated partial thromboplastin time (APTT) should be considered.[citation needed] Blood chemistry tests are conducted if renal cell carcinoma is suspected as cancer has the potential to elevate levels of particular chemicals in blood. For example, liver enzymes such as aspartate aminotransferase (AST) and alanine aminotransferase (ALT) are found to be at abnormally high levels.[42] The staging of the cancer can also be determined by abnormal elevated levels of calcium, which suggests that the cancer may have metastasised to the bones.[43] In this case, a doctor should be prompted for a CT scan. Blood chemistry tests also assess the overall function of the kidneys and can allow the doctor to decide upon further radiological tests.[citation needed] The characteristic appearance of renal cell carcinoma (RCC) is a solid renal lesion which disturbs the renal contour. It will frequently have an irregular or lobulated margin and may be seen as a lump on the lower pelvic or abdomen region. Traditionally, 85 to 90% of solid renal masses will turn out to be RCC but cystic renal masses may also be due to RCC.[44] However, the advances of diagnostic modalities are able to incidentally diagnose a great proportion of patients with renal lesions that may appear to be small in size and of benign state. Ten percent of RCC will contain calcifications, and some contain macroscopic fat (likely due to invasion and encasement of the perirenal fat).[45] Deciding on the benign or malignant nature of the renal mass on the basis of its localized size is an issue as renal cell carcinoma may also be cystic. As there are several benign cystic renal lesions (simple renal cyst, haemorrhagic renal cyst, multilocular cystic nephroma, polycystic kidney disease), it may occasionally be difficult for the radiologist to differentiate a benign cystic lesion from a malignant one.[46] The Bosniak classification system for cystic renal lesions classifies them into groups that are benign and those that need surgical resection, based on specific imaging features.[47] The main imaging tests performed in order to identify renal cell carcinoma are pelvic and abdominal CT scans, ultrasound tests of the kidneys (ultrasonography), MRI scans, intravenous pyelogram (IVP) or renal angiography.[48] Among these main diagnostic tests, other radiologic tests such as excretory urography, positron-emission tomography (PET) scanning, ultrasonography, arteriography, venography, and bone scanning can also be used to aid in the evaluation of staging renal masses and to differentiate non-malignant tumours from malignant tumours.[citation needed] Contrast-enhanced computed tomography (CT) scanning is routinely used to determine the stage of the renal cell carcinoma in the abdominal and pelvic regions. CT scans have the potential to distinguish solid masses from cystic masses and may provide information on the localization, stage or spread of the cancer to other organs of the patient. Key parts of the human body which are examined for metastatic involvement of renal cell carcinoma may include the renal vein, lymph node and the involvement of the inferior vena cava.[49] According to a study conducted by Sauk et al., multidetector CT imaging characteristics have applications in diagnosing patients with clear renal cell carcinoma by depicting the differences of these cells at the cytogenic level.[50] Ultrasonographic examination can be useful in evaluating questionable asymptomatic kidney tumours and cystic renal lesions if computed tomography imaging is inconclusive. This safe and non-invasive radiologic procedure uses high frequency sound waves to generate an interior image of the body on a computer monitor. The image generated by the ultrasound can help diagnose renal cell carcinoma based on the differences of sound reflections on the surface of organs and the abnormal tissue masses. Essentially, ultrasound tests can determine whether the composition of the kidney mass is mainly solid or filled with fluid.[48] A percutaneous biopsy can be performed by a radiologist using ultrasound or computed tomography to guide sampling of the tumour for the purpose of diagnosis by pathology. However this is not routinely performed because when the typical imaging features of renal cell carcinoma are present, the possibility of an incorrectly negative result together with the risk of a medical complication to the patient may make it unfavourable from a risk-benefit perspective.[51] However, biopsy tests for molecular analysis to distinguish benign from malignant renal tumours is of investigative interest.[51] Magnetic resonance imaging (MRI) scans provide an image of the soft tissues in the body using radio waves and strong magnets. MRI can be used instead of CT if the patient exhibits an allergy to the contrast media administered for the test.[52][53] Sometimes prior to the MRI scan, an intravenous injection of a contrasting material called gadolinium is given to allow for a more detailed image. Patients on dialysis or those who have renal insufficiency should avoid this contrasting material as it may induce a rare, yet severe, side effect known as nephrogenic systemic fibrosis.[54] A bone scan or brain imaging is not routinely performed unless signs or symptoms suggest potential metastatic involvement of these areas. MRI scans should also be considered to evaluate tumour extension which has grown in major blood vessels, including the vena cava, in the abdomen. MRI can be used to observe the possible spread of cancer to the brain or spinal cord should the patient present symptoms that suggest this might be the case.[citation needed] Intravenous pyelogram (IVP) is a useful procedure in detecting the presence of abnormal renal mass in the urinary tract. This procedure involves the injection of a contrasting dye into the arm of the patient. The dye travels from the blood stream and into the kidneys which in time, passes into the kidneys and bladder. This test is not necessary if a CT or MRI scan has been conducted.[55] Renal angiography uses the same principle as IVP, as this type of X-ray also uses a contrasting dye. This radiologic test is important in diagnosing renal cell carcinoma as an aid for examining blood vessels in the kidneys. This diagnostic test relies on the contrasting agent which is injected in the renal artery to be absorbed by the cancerous cells.[56] The contrasting dye provides a clearer outline of abnormally-oriented blood vessels believed to be involved with the tumour. This is imperative for surgeons as it allows the patient's blood vessels to be mapped prior to operation.[49] The staging of renal cell carcinoma is the most important factor in predicting its prognosis.[57] Staging can follow the TNM staging system, where the size and extent of the tumour (T), involvement of lymph nodes (N) and metastases (M) are classified separately. Also, it can use overall stage grouping into stage I–IV, with the 1997 revision of AJCC described below:[57] Stage I Tumour of a diameter of 7 cm (approx. 2 3⁄4 inches) or smaller, and limited to the kidney. No lymph node involvement or metastases to distant organs. Stage II Tumour larger than 7.0 cm but still limited to the kidney. No lymph node involvement or metastases to distant organs. Stage III any of the following Tumor of any size with involvement of a nearby lymph node but no metastases to distant organs. Tumour of this stage may be with or without spread to fatty tissue around the kidney, with or without spread into the large veins leading from the kidney to the heart. Tumour with spread to fatty tissue around the kidney and/or spread into the large veins leading from the kidney to the heart, but without spread to any lymph nodes or other organs. Stage IV any of the following Tumour that has spread directly through the fatty tissue and the fascia ligament-like tissue that surrounds the kidney. Involvement of more than one lymph node near the kidney Involvement of any lymph node not near the kidney Distant metastases, such as in the lungs, bone, or brain. At diagnosis, 30% of renal cell carcinomas have spread to the ipsilateral renal vein, and 5–10% have continued into the inferior vena cava.[58] The gross and microscopic appearance of renal cell carcinomas is highly variable. The renal cell carcinoma may present reddened areas where blood vessels have bled, and cysts containing watery fluids.[59] The body of the tumour shows large blood vessels that have walls composed of cancerous cells. Gross examination often shows a yellowish, multilobulated tumor in the renal cortex, which frequently contains zones of necrosis, haemorrhage and scarring. In a microscopic context, there are four major histologic subtypes of renal cell cancer: clear cell (conventional RCC, 75%), papillary (15%), chromophobic (5%), and collecting duct (2%). Sarcomatoid changes (morphology and patterns of IHC that mimic sarcoma, spindle cells) can be observed within any RCC subtype and are associated with more aggressive clinical course and worse prognosis. Under light microscopy, these tumour cells can exhibit papillae, tubules or nests, and are quite large, atypical, and polygonal.[citation needed] Recent studies have brought attention to the close association of the type of cancerous cells to the aggressiveness of the condition. Some studies suggest that these cancerous cells accumulate glycogen and lipids, their cytoplasm appear \"clear\", the nuclei remain in the middle of the cells, and the cellular membrane is evident.[60] Some cells may be smaller, with eosinophilic cytoplasm, resembling normal tubular cells. The stroma is reduced, but well vascularised. The tumour compresses the surrounding parenchyma, producing a pseudocapsule.[61] The most common cell type exhibited by renal cell carcinoma is the clear cell, which is named by the dissolving of the cells' high lipid content in the cytoplasm. The clear cells are thought to be the least likely to spread and usually respond more favourably to treatment. However, most of the tumours contain a mixture of cells. The most aggressive stage of renal cancer is believed to be the one in which the tumour is mixed, containing both clear and granular cells.[62] The recommended histologic grading schema for RCC is the Fuhrman system (1982), which is an assessment based on the microscopic morphology of a neoplasm with haematoxylin and eosin (H&E staining). This system categorises renal cell carcinoma with grades 1, 2, 3, 4 based on nuclear characteristics. The details of the Fuhrman grading system for RCC are shown below:[63] Nuclear grade is believed to be one of the most imperative prognostic factors in patients with renal cell carcinoma.[25] However, a study by Delahunt et al. (2007) has shown that the Fuhrman grading is ideal for clear cell carcinoma but may not be appropriate for chromophobe renal cell carcinomas and that the staging of cancer (accomplished by CT scan) is a more favourable predictor of the prognosis of this disease.[64] In relation to renal cancer staging, the Heidelberg classification system of renal tumours was introduced in 1976 as a means of more completely correlating the histopathological features with the identified genetic defects.[65] Micrograph of embolic material in a kidney removed because of renal cell carcinoma (cancer not shown). H&E stain. The type of treatment depends on multiple factors and the individual, some of which include the stage of renal cell carcinoma (organs and parts of the body affected/unaffected), type of renal cell carcinoma, pre-existing or comorbid conditions and overall health and age of the person.[10][67] Every form of treatment has both risks and benefits; a health care professional will provide the best options that suit the individual circumstances. If it has spread outside of the kidneys, often into the lymph nodes, the lungs or the main vein of the kidney, then multiple therapies are used including surgery and medications. RCC is resistant to chemotherapy and radiotherapy in most cases but does respond well to immunotherapy with interleukin-2 or interferon-alpha, biologic, or targeted therapy. In early-stage cases, cryotherapy and surgery are the preferred options. Active surveillance or \"watchful waiting\" is becoming more common as small renal masses or tumours are being detected and also within the older generation when surgery is not always suitable.[68] Active surveillance involves completing various diagnostic procedures, tests and imaging to monitor the progression of the RCC before embarking on a more high risk treatment option like surgery.[68] In the elderly, patients with co-morbidities, and in poor surgical candidates, this is especially useful. Different procedures may be most appropriate, depending on circumstances. The recommended treatment for renal cell cancer may be nephrectomy or partial nephrectomy, surgical removal of all or part of the kidney.[4] This may include some of the surrounding organs or tissues or lymph nodes. If cancer is only in the kidneys, which is about 60% of cases, it can be cured roughly 90% of the time with surgery. Small renal tumors (< 4 cm) are treated increasingly by partial nephrectomy when possible.[69][70][71] Most of these small renal masses manifest indolent biological behavior with excellent prognosis.[72]Nephron-sparing partial nephrectomy is used when the tumor is small (less than 4 cm in diameter) or when the patient has other medical concerns such as diabetes or hypertension.[10] The partial nephrectomy involves the removal of the affected tissue only, sparing the rest of the kidney, Gerota's fascia and the regional lymph nodes. This allows for more renal preservation as compared to the radical nephrectomy, and this can have positive long-term health benefits.[73] Larger and more complex tumors can also be treated with partial nephrectomy by surgeons with a lot of kidney surgery experience.[74] Surgical nephrectomy may be \"radical\" if the procedure removes the entire affected kidney including Gerota's fascia, the adrenal gland which is on the same side as the affected kidney, and the regional retroperitoneal lymph nodes, all at the same time.[10] This method, although severe, is effective. But it is not always appropriate, as it is a major surgery that contains the risk of complication both during and after the surgery and can have a longer recovery time.[75] It is important to note that the other kidney must be fully functional, and this technique is most often used when there is a large tumour present in only one kidney. Left renal tumor with inferior vena cava thrombus into the right atrium In cases where the tumor has spread into the renal vein, inferior vena cava, and possibly the right atrium, this portion of the tumor can be surgically removed, as well. When the tumor involved the inferior vena cava, it is important to classify which parts of the vena cava are involved and to plan accordingly, as sometimes complete resection will involve an incision into the chest with increased morbidity. For this reason, Dr. Gaetano Ciancio adapted liver mobilization techniques from liver transplant to address retrohepatic or even suprahepatic inferior vena caval thrombus associated with renal tumors.[76] With this technique, the whole abdominal inferior vena cava is able to be mobilized. This facilitates milking of the tumor down below the major hepatic veins by the surgeon's fingers, bypassing the need for a thoracoabdominal incision or cardiopulmonary bypass.[77] In cases of known metastases, surgical resection of the kidney (\"cytoreductive nephrectomy\") may improve survival,[78] as well as resection of a solitary metastatic lesion. Kidneys are sometimes embolized prior to surgery to minimize blood loss.[79] Surgery is increasingly performed via laparoscopic techniques. Commonly referred to as key hole surgery, this surgery does not have the large incisions seen in a classically performed radical or partial nephrectomy, but still successfully removes either all or part of the kidney. Laparoscopic surgery is associated with shorter stays in the hospital and quicker recovery time but there are still risks associated with the surgical procedure. These have the advantage of being less of a burden for the patient and the disease-free survival is comparable to that of open surgery.[4] For small exophytic lesions that do not extensively involve the major vessels or urinary collecting system, a partial nephrectomy (also referred to as \"nephron sparing surgery\") can be performed. This may involve temporarily stopping blood flow to the kidney while the mass is removed as well as renal cooling with an ice slush. Mannitol can also be administered to help limit damage to the kidney. This is usually done through an open incision although smaller lesions can be done laparoscopically with or without robotic assistance. Laparoscopic cryotherapy can also be done on smaller lesions. Typically a biopsy is taken at the time of treatment. Intraoperative ultrasound may be used to help guide placement of the freezing probes. Two freeze/thaw cycles are then performed to kill the tumor cells. As the tumor is not removed followup is more complicated (see below) and overall disease-free rates are not as good as those obtained with surgical removal. Surgery for metastatic disease: If metastatic disease is present surgical treatment may still a viable option. Radical and partial nephrectomy can still occur, and in some cases, if the metastasis is small this can also be surgically removed.[10] This depends on what stage of growth and how far the disease has spread. Percutaneousablation therapies use image-guidance by radiologists to treat localized tumors if a surgical procedure is not a good option. Although the use of laparoscopic surgical techniques for complete nephrectomies has reduced some of the risks associated with surgery,[80] surgery of any sort in some cases will still not be feasible. For example, the elderly, people who already have severe renal dysfunction, or people who have several comorbidities, surgery of any sort is not warranted.[81] A probe is placed through the skin and into the tumor using real-time imaging of both the probe tip and the tumor by computed tomography, ultrasound, or even magnetic resonance imaging guidance, and then destroying the tumor with heat (radiofrequency ablation) or cold (cryotherapy). These modalities are at a disadvantage compared to traditional surgery in that pathologic confirmation of complete tumor destruction is not possible. Therefore, long-term follow-up is crucial to assess completeness of tumour ablation.[82][83] Ideally, percutaneous ablation is restricted to tumours smaller than 3.5 cm and to guide the treatment. However, there are some cases where ablation can be used on tumors that are larger.[81] Radio frequency ablation uses an electrode probe which is inserted into the affected tissue, to send radio frequencies to the tissue to generate heat through the friction of water molecules. The heat destroys the tumor tissue.[10] Cell death will generally occur within minutes of being exposed to temperatures above 50 °C. Cryoablation also involves the insertion of a probe into the affected area,[10] however, cold is used to kill the tumor instead of heat. The probe is cooled with chemical fluids which are very cold. The freezing temperatures cause the tumor cells to die by causing osmotic dehydration, which pulls the water out of the cell destroying the enzyme, organelles, cell membrane and freezing the cytoplasm.[81] Cancers often grow in an unbridled fashion because they are able to evade the immune system.[9]Immunotherapy is a method that activates the person's immune system and uses it to their own advantage.[9] It was developed after observing that in some cases there was spontaneous regression.[84] Immunotherapy capitalises on this phenomenon and aims to build up a person's immune response to cancer cells.[84] Other targeted therapy medications inhibit growth factors that have been shown to promote the growth and spread of tumours.[85][86] Most of these medications were approved within the past ten years.[87] These treatments are:[88] For patients with metastatic cancer, sunitinib probably results in more progression of the cancer than pembrolizumab, axitinib and avelumab.[94] In comparison to pembrolizumab and axitinib, it probably results in more death, but it may slightly reduce serious unwanted effects.[94] When compared with combinations of immunotherapy (nivolumab and ipilimumab), sunitinib may lead to more progression and serious effects.[94] There may be little to no difference in progression, survival and serious effects between pazopanib and sunitib.[94] Activity has also been reported for ipilimumab[95] but it is not an approved medication for renal cancer.[96] Chemotherapy and radiotherapy are not as successful in the case of RCC. RCC is resistant in most cases but there is about a 4–5% success rate, but this is often short-lived with more tumours and growths developing later.[10] Adjuvant therapy, which refers to therapy given after a primary surgery, had for a long time not been found to be beneficial in renal cell cancer.[98] However in 2021 Pembrolizumab was approved for adjuvant treatment after showing promising disease-free survival improvements.[99] Conversely, neoadjuvant therapy is administered before the intended primary or main treatment. In some cases neoadjuvant therapy has been shown to decrease the size and stage of the RCC to then allow it to be surgically removed.[86] This is a new form of treatment and the effectiveness of this approach is still being assessed in clinical trials. Metastatic renal cell carcinoma (mRCC) is the spread of the primary renal cell carcinoma from the kidney to other organs. Approximately 25–30% of people have this metastatic spread by the time they are diagnosed with renal cell carcinoma.[100] This high proportion is explained by the fact that clinical signs are generally mild until the disease progresses to a more severe state.[101] The most common sites for metastasis are the lymph nodes, lung, bones, liver and brain.[11] How this spread affects the staging of the disease and hence prognosis is discussed in the \"Diagnosis\" and \"Prognosis\" sections. MRCC has a poor prognosis compared to other cancers, although average survival times have increased in the last few years due to treatment advances. Average survival time in 2008 for the metastatic form of the disease was under a year,[102] and by 2013 this improved to an average of 22 months.[103] Despite this improvement the five-year survival rate for mRCC remains under 10%[104] and 20–25% of patients remain unresponsive to all treatments and in these cases, the disease has a rapid progression.[103] The available treatments for RCC discussed in the \"Treatment\" section are also relevant for the metastatic form of the disease. Options include interleukin-2, which is a standard therapy for advanced renal cell carcinoma.[98] From 2007 to 2013, seven new treatments have been approved specifically for mRCC (sunitinib, temsirolimus, bevacizumab, sorafenib, everolimus, pazopanib and axitinib).[8] These new treatments are based on the fact that renal cell carcinomas are very vascular tumours – they contain a large number of blood vessels. The drugs aim to inhibit the growth of new blood vessels in the tumours, hence slowing growth and in some cases, reducing the size of the tumours.[105] Side effects unfortunately are quite common with these treatments and include:[106] Radiotherapy and chemotherapy are more commonly used in the metastatic form of RCC to target the secondary tumours in the bones, liver, brain and other organs. While not curative, these treatments do provide relief for symptoms associated with the spread of tumours.[103] The prognosis is influenced by several factors, including tumour size, degree of invasion and metastasis, histologic type, and nuclear grade.[25] Staging is the most important factor in the outcome of renal cell cancer. The following numbers are based on patients first diagnosed in 2001 and 2002 by the National Cancer Data Base:[107] Stage Description 5 Year Survival Rate I Confined to the kidney 81% II Extend through the renal capsule, confined to Gerota's Fascia 74% III Include the renal vein, or the hilar lymph nodes 53% IV Includes tumors that are invasive to adjacent organs (except the adrenal glands), or distant metastases 8% A Korean study estimated a disease-specific overall five-year survival rate of 85%.[108] Taken as a whole, if the disease is limited to the kidney, only 20–30% develop metastatic disease after nephrectomy.[109] More specific subsets show a five-year survival rate of around 90–95% for tumors less than 4 cm. For larger tumors confined to the kidney without venous invasion, survival is still relatively good at 80–85%.[citation needed] For tumors that extend through the renal capsule and out of the local fascial investments, the survivability reduces to near 60%.[citation needed] Factors as general health and fitness or the severity of their symptoms impact the survival rates. For instance, younger people (among 20–40 years old) have a better outcome despite having more symptoms at presentation, possibly due to lower rates spread of cancer to the lymph nodes (stage III). Histological grade is related to the aggressiveness of the cancer, and it is classified in 4 grades, with 1 having the best prognosis (five-year survival over 89%), and 4 with the worst prognosis (46% of five-year survival). Some people have the renal cell cancer detected before they have symptoms (incidentally) because of the CT scan (Computed Tomography Imaging) or ultrasound. Incidentally diagnosed renal cell cancer (no symptoms) differs in outlook from those diagnosed after presenting symptoms of renal cell carcinoma or metastasis. The five-year survival rate was higher for incidental than for symptomatic tumours: 85.3% versus 62.5%. Incidental lesions were significantly lower stage than those that cause symptoms, since 62.1% patients with incidental renal cell carcinoma were observed with Stage I lesions, against 23% were found with symptomatic renal cell carcinoma.[110] If it has metastasized to the lymph nodes, the five-year survival is around 5% to 15%. For metastatic renal cell carcinoma, factors which may present a poor prognosis include a low Karnofsky performance-status score (a standard way of measuring functional impairment in patients with cancer), a low haemoglobin level, a high level of serum lactate dehydrogenase, and a high corrected level of serum calcium.[111][112] For non-metastatic cases, the Leibovich scoring algorithm may be used to predict post-operative disease progression.[113] Renal cell carcinoma is one of the cancers most strongly associated with paraneoplastic syndromes, most often due to ectopic hormone production by the tumour. The treatment for these complications of RCC is generally limited beyond treating the underlying cancer. The incidence of the disease varies according to geographic, demographic and, to a lesser extent, hereditary factors. There are some known risk factors, however the significance of other potential risk factors remains more controversial. The incidence of the cancer has been increasing in frequency worldwide at a rate of approximately 2–3% per decade[102] until the last few years where the number of new cases has stabilised.[17] The incidence of RCC varies between sexes, ages, races and geographic location around the world. Men have a higher incidence than women (approximately 1.6:1)[98] and the vast majority are diagnosed after 65 years of age.[98] Asians reportedly have a significantly lower incidence of RCC than whites and while African countries have the lowest reported incidences, African Americans have the highest incidence of the population in the United States.[17] Developed countries have a higher incidence than developing countries, with the highest rates found in North America, Europe and Australia / New Zealand.[114] Miril published the earliest unequivocal case of renal carcinoma in 1810.[116] He described the case of Françoise Levelly, a 35-year-old woman, who presented to Brest Civic Hospital on April 6, 1809, supposedly in the late stages of pregnancy.[115] Koenig published the first classification of renal tumours based on macroscopic morphology in 1826. Koenig divided the tumors into scirrhous, steatomatous, fungoid and medullary forms.[117] Following the classification of the tumour, researchers attempted to identify the tissue of origin for renal carcinoma. The pathogenesis of renal epithelial tumours was debated for decades. The debate was initiated by Paul Grawitz when in 1883, he published his observations on the morphology of small, yellow renal tumours. Grawitz concluded that only alveolar tumours were of adrenal origin, whereas papillary tumours were derived from renal tissue.[115] In 1893, Paul Sudeck challenged the theory postulated by Grawitz by publishing descriptions of renal tumours in which he identified atypical features within renal tubules and noted a gradation of these atypical features between the tubules and neighboring malignant tumour. In 1894, Otto Lubarsch, who supported the theory postulated by Grawitz coined the term hypernephroid tumor, which was amended to hypernephroma by Felix Victor Birch-Hirschfeld to describe these tumours.[118] Vigorous criticism of Grawitz was provided by Oskar Stoerk in 1908, who considered the adrenal origin of renal tumours to be unproved. Despite the compelling arguments against the theory postulated by Grawitz, the term hypernephroma, with its associated adrenal connotation, persisted in the literature.[115] Foot and Humphreys, and Foote et al. introduced the term Renal Celled Carcinoma to emphasize a renal tubular origin for these tumours. Their designation was slightly altered by Fetter to the now widely accepted term Renal Cell Carcinoma.[119]"}
{"url": "https://en.m.wikipedia.org/wiki/National_Health_Services", "text": "Taken together, the four services in 2015–16 employed around 1.6 million people with a combined budget of £136.7 billion.[6] In 2014, the total health sector workforce across the United Kingdom was 2,165,043 making it the fifth largest employer and largest non-military public organisation in the world.[7][8][9] When purchasing consumables such as medications, the four healthcare services have significant market power that influences the global price, typically keeping prices lower.[10] A small number of products are procured jointly through contracts shared between services.[11] Several other countries either copy the United Kingdom's model or directly rely on Britain's assessments for their own decisions on state-financed drug reimbursements.[12] Following the 1942 Beveridge Report's recommendation to create \"comprehensive health and rehabilitation services for prevention and cure of disease\", cross-party consensus emerged on introducing a National Health Service of some description.[16] Conservative MP and Health Minister, Henry Willink later advanced this notion of a National Health Service in 1944 with his consultative White Paper \"A National Health Service\" which was circulated in full and short versions to colleagues, as well as in newsreel.[17] The NHS was born out of the ideal that healthcare should be available to all, regardless of wealth. Although being freely accessible regardless of wealth maintained Henry Willink's principle of free healthcare for all, Conservative MPs were in favour of maintaining local administration of the NHS through existing arrangements with local authorities fearing that an NHS which owned hospitals on a national scale would lose the personal relationship between doctor and patient.[23] Conservative MPs voted in favour of their amendment to Bevan's Bill to maintain local control and ownership of hospitals and against Bevan's plan for national ownership of all hospitals. The Labour government defeated Conservative amendments and went ahead with the NHS as it remains today; a single large national organisation (with devolved equivalents) which forced the transfer of ownership of hospitals from local authorities and charities to the new NHS. Bevan's principle of ownership with no private sector involvement has since been diluted, with later Labour governments implementing large scale financing arrangements with private builders in private finance initiatives and joint ventures.[24] At its launch by Bevan on 5 July 1948 it had at its heart three core principles: That it meet the needs of everyone, that it be free at the point of delivery, and that it be based on clinical need, not ability to pay.[25] Three years after the founding of the NHS, Bevan resigned from the Labour government in opposition to the introduction of charges for the provision of dentures, dentists,[26] and glasses; resigning in support was fellow minister and future Prime MinisterHarold Wilson.[27] The following year, Winston Churchill's Conservative government introduced prescription fees. However, Wilson's government abolished them in 1965; they were later re-introduced but with exemptions for those on low income.[28] These charges were the first of many controversies over changes to the NHS throughout its history.[29] This section's factual accuracy may be compromised due to out-of-date information. The reason given is: Because of Brexit, EEA nationals that do not have settled status are now subject to the same restrictions that applied to non-EEA, non-UK residents. Please help update this article to reflect recent events or newly available information.(September 2022) Everyone living in the UK can use the NHS without being asked to pay the full cost of the service, though NHS dentistry and optometry do have standard charges in each of the four national health services in the UK.[31] In addition, most patients in England have to pay charges for prescriptions though some patients are exempted.[5] Aneurin Bevan, in considering the provision of NHS services to overseas visitors wrote, in 1952, that it would be \"unwise as well as mean to withhold the free service from the visitor to Britain. How do we distinguish a visitor from anybody else? Are British citizens to carry means of identification everywhere to prove that they are not visitors? For if the sheep are to be separated from the goats both must be classified. What began as an attempt to keep the Health Service for ourselves would end by being a nuisance to everybody.\"[32] The provision of free treatment to non-UK-residents, formerly interpreted liberally, has been increasingly restricted, with new overseas visitor hospital charging regulations introduced in 2015.[33] Citizens of the EU holding a valid European Health Insurance Card and persons from certain other countries with which the UK has reciprocal arrangements concerning health care can get emergency treatment without charge.[34] The NHS is free at the point of use, for general practitioner (GP) and emergency treatment not including admission to hospital, to non-residents.[35] People with the right to medical care in European Economic Area (EEA) nations are also entitled to free treatment by using the European Health Insurance Card. Those from other countries with which the UK has reciprocal arrangements also qualify for free treatment.[36][37] Since 6 April 2015, non-EEA nationals who are subject to immigration control must have the immigration status of indefinite leave to remain at the time of treatment and be properly settled, to be considered ordinarily resident. People not ordinarily resident in the UK are in general not entitled to free hospital treatment, with some exceptions such as refugees.[4][38] People not ordinarily resident may be subject to an interview to establish their eligibility, which must be resolved before non-emergency treatment can commence. Patients who do not qualify for free treatment are asked to pay in advance or to sign a written undertaking to pay, except for emergency treatment.[citation needed] People from outside the EEA coming to the UK for a temporary stay of more than six months are required to pay an immigration health surcharge at the time of visa application, and will then be entitled to NHS treatment on the same basis as a resident. This includes overseas students with a visa to study at a recognised institution for six months or more, but not visitors on a tourist visa.[39] In 2016 the surcharge was £200 per year, with exemptions and reductions in some cases.[40] This was increased to £400 in 2018. The discounted rate for students and those on the Youth Mobility Scheme will increase from £150 to £300.[41] From 15 January 2007, anyone who is working outside the UK as a missionary for an organisation with its principal place of business in the UK is fully exempt from NHS charges for services that would normally be provided free of charge to those resident in the UK. This is regardless of whether they derive a salary or wage from the organisation, or receive any type of funding or assistance from the organisation for the purposes of working overseas.[42] This is in recognition of the fact that most missionaries would be unable to afford private health care and those working in developing countries should not effectively be penalised for their contribution to development and other work.[citation needed] This section needs to be updated. The reason given is: only three of the fourteen paragraphs in this subsection relate to the situation since the 2010s, though the 2010s are relevant. Please help update this article to reflect recent events or newly available information.(February 2023) In 2016, the systems were 98.8% funded from general taxation and National Insurance contributions, plus small amounts from patient charges for some services.[44][45] As of 2016, about 10% of GDP was spent on health and was the most spent in the public sector.[46] In 2019, the UK spent roughly 10.2% of GDP on healthcare compared to 11.7% in Germany and 11.1% in France.[47] The money to pay for the NHS comes directly from taxation. The 2008/09 budget roughly equated to a contribution of £1,980 per person in the UK.[48][needs update] Some 60% of the NHS budget is used to pay staff. A further 20% pays for pharmaceuticals and other supplies, with the remaining 20% split between buildings, equipment, training costs, medical equipment, catering and cleaning. Nearly 80% of the total budget is distributed by local trusts in line with the particular health priorities in their areas.[49][needs update] When the NHS was launched in 1948 it had a budget of £437 million[50] (equivalent to £16.91 billion in 2021). In 2016–2017, the budget was £122.5 billion.[51] In 1955/56 health spending was 11.2% of the public services budget. In 2015/16 it was 29.7%.[52] This equates to an average rise in spending over the full 60-year period of about 4% a year once inflation has been taken into account. Under the Blair government spending levels increased by around 6% a year on average. Between 2010 and 2017 spending growth was constrained to just over 1% a year.[52] A 2019 report said that a study by the 'Centre for Health Economics' at the University of York found that between 2004/05 and 2016/17 the productivity of the NHS has increased nearly two and a half times as quickly as the larger economy.[53] Between 2010 and 2017, there was a cap of 1% on pay rises for staff continuing in the same role. Unions representing doctors, dentists, nurses and other health professionals have called on the government to end the cap on health service pay, claiming the cap is damaging the health service and damaging patient care.[54] In 2017, the pay rise was likely to be below the level of inflation and to mean a real-terms pay cut.[55] In 2017, the House of Commons Library research predicted that that real-terms NHS funding per head was to fall in 2018–19, and stay the same for two years afterwards.[56][needs update] In January 2018, The Guardian reported that GPs faced excessive workloads throughout Britain and that this put the GP's health and that of their patients at risk.[57] The Royal College of Physicians surveyed doctors across the UK, with two-thirds maintaining patient safety had deteriorated during the year to 2018: 80% feared they would be unable to provide safe patient care in the coming year while 84% felt increased pressure on the NHS was demoralising the workforce.[58] In June 2018, at a time when the NHS was short of doctors, foreign doctors were forced to leave the UK due to visa restrictions.[59] A study reported in November 2018 found that a fifth of doctors had faced bullying from seniors in the previous year due to pressure at work.[60] In May 2018 it was reported that the NHS was under-resourced compared to health provisions in other developed nations. A King's Fund study of OECD data from 21 nations, revealed that the NHS has among the lowest numbers of doctors, nurses and hospital beds per capita in the western world.[61] In May 2018, it was said that nurses within the NHS said that patient care was compromised by the shortage of nurses and the lack of experienced nurses with the necessary qualifications.[62] In June 2018 it was reported that the NHS performed below average in preventing deaths from cancer, strokes and heart disease.[63] In September 2018 it was reported that staff shortages at histology departments were delaying diagnosis and start of treatment for cancer patients.[64] In England and Scotland cancer wards and children's wards have to close because the hospital cannot attract sufficient qualified doctors and nurses to run the wards safely. In November 2018 it was reported that cancer patients and child patients were having to travel very long distances to get treatment and their relatives had to travel far to visit the patients. In wards which had not closed staff sometimes worked under stress due to staff shortages. It was also predicted then, that Brexit was likely to aggravate those problems.[65] In November 2019, it was reported that due to the shortage of nurses the NHS was then relying on less qualified staff like healthcare assistants and nursing associates.[66] For the period between 2010 and 2018 the Health Foundation funded research by Birmingham University said there was insufficient and falling NHS capital spending that put patient care and put staff productivity at risk. The Health Foundation said that £3.5 billion more a year would be required to get capital spending to the OECD average. Spending limits were effecting service efficiency, and patient care. Shortages of equipment and equipment failures had an impact as did relying on ageing diagnostic equipment.[67] In 2018, British Prime Minister Theresa May announced that NHS in England would receive a 3.4% increase in funding every year to 2024, which would allow it to receive an extra £20bn a year in real terms funding.[68] There is concern that a high proportion of this money will go to service NHS debts rather than for improved patient care. In June 2018, it was reported that there were calls for the government to write off the NHS debt. Saffron Cordery of NHS Providers said that hospitals needed help to do their work without being up in deficit, as two-thirds were in the year to 2018.[69] It was also said that some expressed doubt over whether May could carry out this proposed increase in funding.[70] The next day, Health Secretary Jeremy Hunt backed the extra £20bn annual increase in NHS funding and responded to criticism by stating that taxation would be used to carry out the funding and that details would be revealed at the next budget.[68][71] In June 2018 it was reported that the Institute for Fiscal Studies had stated a 5% real-terms increase was needed for real change. Paul Johnson of the IFS said the 3.4% was greater than recent increases, but less than the long-term average.[72] In July 2016, health experts[who?] said the money would \"help stem further decline in the health service, but it's simply not enough to address the fundamental challenges facing the NHS, or fund essential improvements to services that are flagging.\"[73] In November 2018, it was thought that inflation may erode the real value of this funding increase.[74] As part of the 2018 funding increase the UK Government asked the NHS in England to produce a 10-year plan as to how this funding would be used. In September 2019, it was reported that cancer survival rates in the UK had been rising fast but probably still lagged behind the best results internationally, mainly because of late diagnosis.[75] In March 2019 it was reported that death rates from breast cancer were falling faster in Britain than in any other of the six largest countries in Europe, and were estimated then to have improved beyond the European average.[76][77] In October 2018, according to Breast Cancer Care, 72% of NHS trusts across the UK did not provide dedicated specialist nurses for patients with incurable breast cancer.\"[78][75] In September 2019 it was reported that Cancer Research UK maintained that more NHS cancer personnel were needed to enable the UK to catch up The NHS in England was expanding early diagnosis services with the goal of increasing the proportion of cancers diagnosed early (at stages 1 and 2) from 53% to 75% in the decade to 2028.[79] In September 2018, it was reported that the NHS was the first health service in Europe to negotiate coverage for novel CAR-T cancer therapy, with agreement reached within 10 days of its European marketing authorisation.[80] The Guardian reported that data for 2020 suggests a change, and that the doctors' trade union and professional association, the BMA, say this was largely due to raised spending during the pandemic and the effect of Covid on the whole economy, since the GDP of the UK dropped more than that of all other G7 nations.[47] The BMA also said in December 2022 that the NHS experienced \"historical underfunding and under-resourcing\" during the ten years before COVID.[47][81][82] The King's Fund maintains The investment in services started in 2021 was badly needed, but despite it restoring key services and performance standards will require years. Shortages of workers and growing staff numbers experiencing burnout and thinking of leaving the NHS may stop progress. If the new funding is to be efficiently used the NHS will need a comprehensive and fully funded workforce strategy to ensure future supply of staff.[83] In March 2022, Rishi Sunak doubled the annual efficiency target for the NHS in England. The 2.2 per cent target would deliver annual savings of saving of £4.75 billion. At the same time the additional Covid funding is being removed in 2022–23.[84] At the same time Sir Charles Bean, recently leader of the Office for Budget Responsibility said that \"the rising trend in health and social care spending and pensions will be adding something like another £75 billion spending over the next five years, £150 billion, potentially over the next decade\" as if treatments are available to keep people alive longer, then people will want them.[85] In July 2022, The Telegraph reported that the think tank Civitas found that health spending was costing about £10,000 per household in the UK. This, they said, reflected the third highest share of GDP of any nation in Europe. This was said to show that the UK \"has one of the most costly health systems – and some of the worst outcomes\". The findings were made before the government increased health spending significantly, with a 1.25% increase in National Insurance, in April 2022. Civitas said this showed evidence \"runaway\" spending as health spending in the UK had increased by more than any country despite the significant drop in national income due to the COVID pandemic.[86] The NHS is the largest employer in Europe, with one in every 25 adults in England working for the NHS.[87] As of February 2023, NHS England employed 1.4 million staff.[88] Nursing staff accounted for the largest cohort at more than 330,000 employees, followed by clinical support staff at 290,000, scientific and technical staff at 163,000 and doctors at 133,000.[89] In June 2018, the Royal College of Physicians calculated that medical training places need to be increased from 7,500 to 15,000 by 2030 to take account of part-time working among other factors. At that time there were 47,800 consultants working in the UK of which 15,700 were physicians. About 20% of consultants work less than full-time.[91] Brexit, in 2020, was predicted to affect medical practitioners from EU countries who worked in the NHS, accounting for more than 1-in-10 doctors at the time.[92] A 2017 survey suggested 60% of these doctors were considering leaving, with a record 17,197 EU staff leaving the NHS in 2016.[93] The figures led to calls to reassure European workers over their future in the UK.[94] A study by the Centre for Progressive Policy called for NHS trusts to become \"exemplar employers\" by improving social mobility and pay especially for those \"trusts in poorer places where they can play a particularly large role in determining the economic wellbeing of the local population.\" They found the NHS to be \" a middle ranking employer in comparison to other large organisations and falls short on social mobility and the real Living Wage\", and ranked trusts using a 'good employer index'. Ambulance trusts were ranked worst.[95] On 6 June 2022, The Guardian said that a survey of more than 20,000 frontline staff by the nursing trade union and professional body, the Royal College of Nursing, found that only a quarter of shifts had the planned number of registered nurses on duty.[96] The NHS is facing a shortage of general practitioners. From 2015 to 2022, the number of GPs has fallen by 1,622. Some family doctors have 2,500 patients each, forcing patients to attend A&E instead. Certain regions have fewer than 50 GPs per 100,000 people, while other regions have more than 70, presenting a challenge to the NHS's founding principle of equal treatment. A growing number of family doctors are reporting unsustainable workloads, and many have chosen to work part-time. A Health and Social Care spokesperson said that the department is making 4,000 training positions available for GPs every year, which help create an extra 50 million appointments annually.[97] In 2023, a report revealed that NHS staff had faced over 20,000 sexual misconduct from patients from 2017 to 2022 across 212 NHS Trusts. Some of the claims included rape, sexual harassment, stalking, and sexualised remarks at staff from patients. In the report, female staff members were told by hospitals to continue caring for patients even after there were issues regarding abuse or harassment without further safeguards in place. Other parts of the report showed that staff were pressured into not reporting grievances. In June 2023, the delayed NHS Long term Workforce Plan was announced, set up to train more doctors and nurses and create new roles within the health service.[98] There was a concern that a disorderly Brexit might have compromised patients' access to vital medicines. In February 2018, many medical organisations planned for a worst-case Brexit scenario because \"time is running out\" for a transition deal to follow the UK's formal exit that occurred in March 2019.[99] Pharmaceutical organisations working with the Civil Service to keep medicine supplies available in the case of a no-deal Brexit had to sign 26 Non-Disclosure Agreements (NDAs) to prevent them from giving the public information. The figures were given on 21 December 2018 after Rushanara Ali asked a parliamentary question.[100] Social care will cost more in future according to research by Liverpool University, University College London, and others. Professor Helen Stokes-Lampard of the Royal College of GPs said: [when?] \"It's a great testament to medical research, and the NHS, that we are living longer – but we need to ensure that our patients are living longer with a good quality of life. For this to happen we need a properly funded, properly staffed health and social care sector with general practice, hospitals and social care all working together – and all communicating well with each other, in the best interests of delivering safe care to all our patients.\"[101] Some patients have to wait excessively long for mental health care. The Royal College of Psychiatrists found[when?] that some patients must wait up to thirteen months for the right care. Wendy Burn of the Royal College of Psychiatrists said, \"It is a scandal that patients are waiting so long for treatment. The failure to give people with mental illnesses the prompt help they need is ruining their lives.\" Even patients who are suicidal or who have attempted suicide are sometimes denied treatment; patients are told they are not ill enough or waiting lists are too long. During very long waits for treatment, one in three patients deteriorate, and they may become unemployed or get divorced. One in four patients throughout the UK wait over three months to see an NHS mental health professional, with 6% waiting at least a year.[102] The National Audit Office found mental health provisions for children and young people will not meet growing demand, despite promises of increased funding. Even if promises to provide £1.4bn more for the sector are kept, there will be \"significant unmet need\" due to staff shortages, inadequate data and failure to control spending by NHS clinical commissioning groups. Currently one-quarter of young people needing mental health services can get NHS help. The Department of Health and Social Care hopes to raise the ratio to 35%. Efforts to improve mental health provisions could reveal previously unmet demand.[103] Meg Hillier of the select committee on public accounts said: \"The government currently estimates that less than a third of children and young people with a diagnosable mental health condition are receiving treatment. But the government doesn't understand how many children and young people are in need of treatment or how funding is being spent locally. The government urgently needs to set out how departments, and national and local bodies, are going to work together to achieve its long-term ambition.\" Amyas Morse said, \"Current targets to improve care are modest and even if met would still mean two-thirds of those who need help are not seen. Rising estimates of demand may indicate that the government is even further away than it thought.\"[103] In response, NHS England has embarked on a major programme to expand mental health services, whose budgets are now growing faster than the NHS overall.[104] MIND the mental health charity responded saying: \"We are pleased that the plan includes a commitment of £2.3bn a year towards mental health, to help redress the balance. The plan promises that this money will see around two million more people with anxiety, depression and other mental health problems receive help, including new parents, and 24 hour access to crisis care. The plan also includes a guarantee that investment in primary, community and mental health care will grow faster than the growing overall NHS budget so that different parts of the NHS come together to provide better, joined-up care in partnership with local government. Since the funding announcement in the summer, Mind has been working with the NHS, Government and voluntary sector to help shape the long term plan. This longer-term strategy was developed in consultation with people with mental health problems to ensure their views are reflected.\"[104] Performance of the NHS is generally assessed separately at the level of England, Wales, Scotland and Northern Ireland. Since 2004 the Commonwealth Fund has produced surveys, \"Mirror, Mirror on the Wall\", comparing the performance of health systems in 11 wealthy countries in which the UK generally ranks highly. In the 2021 survey the NHS dropped from first overall to fourth as it had fallen in key areas, including 'access to care and equity.'[105] The Euro Health Consumer Index attempted to rank the NHS against other European health systems from 2014 to 2018. The right-leaning[106] think tank Civitas produced an International Health Care Outcomes Index in 2022 ranking the performance of the UK health care system against 18 similar, wealthy countries since 2000. It excluded the impact of the COVID-19 pandemic as data stopped in 2019. The UK was near the bottom of most tables except households who faced catastrophic health spending.[107] A comparative analysis of health care systems in 2010, by The Commonwealth Fund, a left-leaning US health charity, put the NHS second in a study of seven rich countries.[108][109] The report put the United Kingdom health systems above those of Germany, Canada and the United States; the NHS was deemed the most efficient among those health systems studied. A 2018 study by the King's Fund, Health Foundation, Nuffield Trust, and the Institute for Fiscal Studies to mark the NHS 70th anniversary concluded that the main weakness of the NHS was healthcare outcomes. Mortality for cancer, heart attacks and stroke, was higher than average among comparable countries. The NHS was doing well at protecting people from heavy financial costs when ill. Waiting times were about the same, and the management of longterm illness was better than in other comparable countries. Efficiency was good, with low administrative costs and high use of cheaper generic medicines.[110] Twenty-nine hospital trusts and boards out of 157 had not met any waiting-time target in the year 2017–2018.[111] The Office for National Statistics reported in January 2019 that productivity in the English NHS had been growing at 3%, considerably faster than across the rest of the UK economy.[112] In 2019, The Times, commenting on a study in the British Medical Journal, reported that \"Britain spent the least on health, £3,000 per person, compared with an average of £4,400, and had the highest number of deaths that might have been prevented with prompt treatment\". The BMJ study compared \"the healthcare systems of other developed countries in spending, staff numbers and avoidable deaths\".[113] Over 130,000 deaths since 2012 in the UK could have been prevented if progress in public health policy had not stopped due to austerity, analysis by the Institute for Public Policy Research found. Dean Hochlaf of the IPPR said: \"We have seen progress in reducing preventable disease flatline since 2012.\"\".[114] The key NHS performance indicators (18 weeks (RTT), 4 hours (A&E) and cancer (2 week wait) have not been achieved since February 2016, July 2015 and December 2015 respectively.[115] Overall satisfaction with the NHS in 2021 fell, more sharply in Scotland than in England, 17 points to 36% – the lowest level since 1997 according to the British Social Attitudes Survey. Dissatisfaction with hospital and GP waiting times were the biggest cause of the fall.[117] The NHS Confederation polled 182 health leaders and 9 in 10 warned that inadequate capital funding harmed their \"ability to meet safety requirements for patients\" in health settings including hospitals, ambulance, community and mental health services and GP practices.[118] In 2016 it was reported that there appeared to be support for higher taxation to pay for extra spending on the NHS as an opinion poll in 2016 showed that 70% of people were willing to pay an extra penny in the pound in income tax if the money were ringfenced and guaranteed for the NHS.[119] Two thirds of respondents to a King's Fund poll, reported in September 2017, favoured increased taxation to help finance the NHS.[120] A YouGov poll reported in May 2018 showed that 74% of the UK public believed there were too few nurses.[121] The trade union, Unite, said in March 2019 that the NHS had been under pressure as a result of economic austerity.[122] A 2018 public survey reported that public satisfaction with the NHS had fallen from 70% in 2010 to 53% in 2018.[123] The NHS is consistently ranked as the institution that makes people proudest to be British, beating the royal family, Armed Forces and the BBC.[124] One 2019 survey ranked nurses and doctors – not necessarily NHS staff – amongst the most trustworthy professions in the UK.[125] In November 2022 a survey by Ipsos and the Health Foundation found just a third of respondents agreed the NHS gave a good service nationally, and 82% thought the NHS needed more funding. 62% expected care standards to fall during the following 12 months. Sorting out pressure and workload on staff and increasing staff numbers were the chief priorities the poll found. Improving A&E waiting times and routine services were also concerns.[126] Just 10% of UK respondents felt their government had the correct plans for the NHS. The Health Foundation stated in spite of these concerns, the public is committed to the founding principles of the NHS and 90% of respondents believe the NHS should be free, 89% believe NHS should provide a comprehensive service for everyone, and 84% believe the NHS should be funded mainly through taxation.[127] In 2020, the NHS issued medical advice in combating COVID-19 and partnered with tech companies to create computer dashboards to help combat the nation's coronavirus pandemic.[128][129] During the pandemic, the NHS also established integrated COVID into its 1-1-1 service line as well.[130] Following his discharge from the St. Thomas' Hospital in London on 13 April 2020 after being diagnosed with COVID-19, British Prime Minister Boris Johnson described NHS medical care as \"astonishing\" and said that the \"NHS saved my life. No question.\"[131][132] In this time, the NHS underwent major re-organisation to prepare for the COVID-19 pandemic.[133] On 5 July 2021, Queen Elizabeth II awarded the NHS the George Cross.[134] The George Cross, the highest award for gallantry available to civilians and is slightly lower in stature to the Victoria Cross, is bestowed for acts of the greatest heroism or most conspicuous courage. In a handwritten note the Queen said the award was being made to all NHS staff past and present for their \"courage, compassion and dedication\" throughout the pandemic.[135] In 2015, the UK had 2.6 hospital beds per 1,000 people.[136] In September 2017, the King's Fund documented the number of NHS hospital beds in England as 142,000, describing this as less than 50% of the number 30 years previously.[137] In 2019 one tenth of the beds in the UK were occupied by a patient who was alcohol-dependent.[138] ^Rimmer MP, Al Wattar BH, on behalf of the UKARCOG Members. Provision of obstetrics and gynaecology services during the COVID-19 pandemic: a survey of junior doctors in the UK National Health Service. BJOG 2020; https://doi.org/10.1111/1471-0528.16313. Brady, Robert A. Crisis in Britain. Plans and Achievements of the Labour Government (1950) pp. 352–41 excerpt Gorsky, Martin. \"The British National Health Service 1948–2008: A Review of the Historiography,\" Social History of Medicine, Dec 2008, Vol. 21 Issue 3, pp. 437–60 Hacker, Jacob S. \"The Historical Logic of National Health Insurance: Structure and Sequence in the Development of British, Canadian, and U.S. Medical Policy,\" Studies in American Political Development, April 1998, Vol. 12 Issue 1, pp. 57–130. Hilton, Claire. (26 August 2016). Whistle-blowing in the National Health Service since the 1960s History and Policy. Retrieved 11 May 2017."}
{"url": "https://en.m.wikipedia.org/wiki/Fat_cells", "text": "White fat cells contain a single large lipid droplet surrounded by a layer of cytoplasm, and are known as unilocular. The nucleus is flattened and pushed to the periphery. A typical fat cell is 0.1 mm in diameter[2] with some being twice that size, and others half that size. However, these numerical estimates of fat cell size depend largely on the measurement method and the location of the adipose tissue.[2] The fat stored is in a semi-liquid state, and is composed primarily of triglycerides, and cholesteryl ester. White fat cells secrete many proteins acting as adipokines such as resistin, adiponectin, leptin and apelin. An average human adult has 30 billion fat cells with a weight of 30 lbs or 13.5 kg. If a child or adolescent gains sufficient excess weight, fat cells may increase in absolute number until age twenty-four.[3] If an adult (who never was obese as a child or adolescent) gains excess weight, fat cells generally increase in size, not number, though there is some inconclusive evidence suggesting that the number of fat cells might also increase if the existing fat cells become large enough (as in particularly severe levels of obesity).[3] The number of fat cells is difficult to decrease through dietary intervention, though some evidence suggests that the number of fat cells can decrease if weight loss is maintained for a sufficiently long period of time (>1 year; though it is extremely difficult for people with larger and more numerous fat cells to maintain weight loss for that long a time).[3] A large meta-analysis has shown that white adipose tissue cell size is dependent on measurement methods, adipose tissue depots, age, and body mass index; for the same degree of obesity, increases in fat cell size were also associated with the dysregulations in glucose and lipid metabolism.[2] Brown fat cells are polyhedral in shape. Brown fat is derived from dermatomyocyte cells. Unlike white fat cells, these cells have considerable cytoplasm, with several lipid droplets scattered throughout, and are known as multilocular cells. The nucleus is round and, although eccentrically located, it is not in the periphery of the cell. The brown color comes from the large quantity of mitochondria. Brown fat, also known as \"baby fat,\" is used to generate heat. Marrow adipocytes are unilocular like white fat cells. The marrow adipose tissue depot is poorly understood in terms of its physiologic function and relevance to bone health. Marrow adipose tissue expands in states of low bone density but additionally expands in the setting of obesity.[4] Marrow adipose tissue response to exercise approximates that of white adipose tissue.[4][5][6][7] Exercise reduces both adipocyte size as well as marrow adipose tissue volume, as quantified by MRI or μCT imaging of bone stained with the lipid binder osmium. Pre-adipocytes are undifferentiated fibroblasts that can be stimulated to form adipocytes. Studies have shed light into potential molecular mechanisms in the fate determination of pre-adipocytes although the exact lineage of adipocyte is still unclear.[8][9] The variation of body fat distribution resulting from normal growth is influenced by nutritional and hormonal status dependent on intrinsic differences in cells found in each adipose depot.[10] Fat cells in some mice have been shown to drop in count due to fasting and other properties were observed when exposed to cold.[12] If the adipocytes in the body reach their maximum capacity of fat, they may replicate to allow additional fat storage. Adult rats of various strains became obese when they were fed a highly palatable diet for several months. Analysis of their adipose tissue morphology revealed increases in both adipocyte size and number in most depots. Reintroduction of an ordinary chow diet[13] to such animals precipitated a period of weight loss during which only mean adipocyte size returned to normal. Adipocyte number remained at the elevated level achieved during the period of weight gain.[14] According to some reports and textbooks, the number of adipocytes can increase in childhood and adolescence, though the amount is usually constant in adults. Individuals who become obese as adults, rather than as adolescents, have no more adipocytes than they had before.[15] People who have been fat since childhood generally have an inflated number of fat cells. People who become fat as adults may have no more fat cells than their lean peers, but their fat cells are larger. In general, people with an excess of fat cells find it harder to lose weight and keep it off than the obese who simply have enlarged fat cells.[3] Body fat cells have regional responses to the overfeeding that was studied in adult subjects. In the upper body, an increase of adipocyte size correlated with upper-body fat gain; however, the number of fat cells was not significantly changed. In contrast to the upper body fat cell response, the number of lower-body adipocytes did significantly increase during the course of experiment. Notably, there was no change in the size of the lower-body adipocytes.[16] Approximately 10% of fat cells are renewed annually at all adult ages and levels of body mass index without a significant increase in the overall number of adipocytes in adulthood.[15] SREBF1 (sterol regulatory element-binding transcription factor 1) is a transcription factor synthesized as an inactive precursor protein inserted into the endoplasmic reticulum (ER) membrane by two membrane-spanning helices. Also anchored in the ER membrane is SCAP (SREBF-cleavage activating protein), which binds SREBF1. The SREBF1-SCAP complex is retained in the ER membrane by INSIG1 (insulin-induced gene 1 protein). When sterol levels are depleted, INSIG1 releases SCAP and the SREBF1-SCAP complex can be sorted into transport vesicles coated by the coatomerCOPII that are exported to the Golgi apparatus. In the Golgi apparatus, SREBF1 is cleaved and released as a transcriptionally active mature protein. It is then free to translocate to the nucleus and activate the expression of its target genes.[21] Proteolytic activation of SREBF-controlled lipid biosynthesis. Clinical studies have repeatedly shown that even though insulin resistance is usually associated with obesity, the membrane phospholipids of the adipocytes of obese patients generally still show an increased degree of fatty acid unsaturation.[22] This seems to point to an adaptive mechanism that allows the adipocyte to maintain its functionality, despite the increased storage demands associated with obesity and insulin resistance. A study conducted in 2013[22] found that, while INSIG1 and SREBF1 mRNA expression was decreased in the adipose tissue of obese mice and humans, the amount of active SREBF1 was increased in comparison with normal mice and non-obese patients. This downregulation of INSIG1 expression combined with the increase of mature SREBF1 was also correlated with the maintenance of SREBF1-target gene expression. Hence, it appears that, by downregulating INSIG1, there is a resetting of the INSIG1/SREBF1 loop, allowing for the maintenance of active SREBF1 levels. This seems to help compensate for the anti-lipogenic effects of insulin resistance and thus preserve adipocyte fat storage abilities and availability of appropriate levels of fatty acid unsaturation in face of the nutritional pressures of obesity."}
{"url": "https://web.archive.org/web/20110611160708/http://earthtrends.wri.org/searchable_db/index.php?theme=8&variable_ID=212&action=select_countries", "text": "The Internet Archive discovers and captures web pages through many different web crawls. At any given time several distinct crawls are running, some for months, and some every day or longer. View the web archive through the Wayback Machine."}
{"url": "https://en.m.wikipedia.org/wiki/Cognitive_computing", "text": "Contents At present, there is no widely agreed upon definition for cognitive computing in either academia or industry.[1][3][4] In general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain[5][6][7][8][9] (2004) and helps to improve human decision-making.[10] In this sense, cognitive computing is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus. Cognitive computing applications link data analysis and adaptive page displays (AUI) to adjust content for a particular type of audience. As such, cognitive computing hardware and applications strive to be more affective and more influential by design. The term \"cognitive system\" also applies to any artificial construct able to perform a cognitive process where a cognitive process is the transformation of data, information, knowledge, or wisdom to a new level in the DIKW Pyramid.[11] While many cognitive systems employ techniques having their origination in artificial intelligence research, cognitive systems, themselves, may not be artificially intelligent. For example, a neural network trained to recognize cancer on an MRI scan may achieve a higher success rate than a human doctor. This system is certainly a cognitive system but is not artificially intelligent. Cognitive systems may be engineered to feed on dynamic data in real-time, or near real-time,[12] and may draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs (visual, gestural, auditory, or sensor-provided).[13] Even if cognitive computing can not take the place of teachers, it can still be a heavy driving force in the education of students. Cognitive computing being used in the classroom is applied by essentially having an assistant that is personalized for each individual student. This cognitive assistant can relieve the stress that teachers face while teaching students, while also enhancing the student's learning experience over all.[15] Teachers may not be able to pay each and every student individual attention, this being the place that cognitive computers fill the gap. Some students may need a little more help with a particular subject. For many students, Human interaction between student and teacher can cause anxiety and can be uncomfortable. With the help of Cognitive Computer tutors, students will not have to face their uneasiness and can gain the confidence to learn and do well in the classroom.[16] While a student is in class with their personalized assistant, this assistant can develop various techniques, like creating lesson plans, to tailor and aid the student and their needs. Healthcare Numerous tech companies are in the process of developing technology that involves cognitive computing that can be used in the medical field. The ability to classify and identify is one of the main goals of these cognitive devices.[17] This trait can be very helpful in the study of identifying carcinogens. This cognitive system that can detect would be able to assist the examiner in interpreting countless numbers of documents in a lesser amount of time than if they did not use Cognitive Computer technology. This technology can also evaluate information about the patient, looking through every medical record in depth, searching for indications that can be the source of their problems. Commerce Together with Artificial Intelligence, it has been used in warehouse management systems to collect, store, organize and analyze all related supplier data. All these aims at improving efficiency, enabling faster decision-making, monitoring inventory and fraud detection[18] Human Cognitive Augmentation In situations where humans are using or working collaboratively with cognitive systems, called a human/cog ensemble, results achieved by the ensemble are superior to results obtainable by the human working alone. Therefore, the human is cognitively augmented.[19][20][21] In cases where the human/cog ensemble achieves results at, or superior to, the level of a human expert then the ensemble has achieved synthetic expertise.[22] In a human/cog ensemble, the \"cog\" is a cognitive system employing virtually any kind of cognitive computing technology. The powers of cognitive computing and artificial intelligence hold the potential to affect almost every task that humans are capable of performing. This can negatively affect employment for humans, as there would be no such need for human labor anymore. It would also increase the inequality of wealth; the people at the head of the cognitive computing industry would grow significantly richer, while workers without ongoing, reliable employment would become less well off.[23] The more industries start to use cognitive computing, the more difficult it will be for humans to compete.[23] Increased use of the technology will also increase the amount of work that AI-driven robots and machines can perform. Only extraordinarily talented, capable and motivated humans would be able to keep up with the machines. The influence of competitive individuals in conjunction with artificial intelligence/cognitive computing with has the potential to change the course of humankind.[24]"}
{"url": "https://en.m.wikipedia.org/wiki/Solenoid_valve", "text": "Solenoid valve Solenoid valves differ in the characteristics of the electric current they use, the strength of the magnetic field they generate, the mechanism they use to regulate the fluid, and the type and characteristics of fluid they control. The mechanism varies from linear action, plunger-type actuators to pivoted-armature actuators and rocker actuators. The valve can use a two-port design to regulate a flow or use a three or more port design to switch flows between ports. Multiple solenoid valves can be placed together on a manifold. Solenoid valves are the most frequently used control elements in fluidics. Their tasks are to shut off, release, dose, distribute or mix fluids. They are found in many application areas. Solenoids offer fast and safe switching, high-reliability, long service life, good medium compatibility of the materials used, low control power and compact design. Contents There are many valve design variations. Ordinary valves can have many ports and fluid paths. A 2-way valve, for example, has 2 ports; if the valve is open, then the two ports are connected and fluid may flow between the ports; if the valve is closed, then ports are isolated. If the valve is open when the solenoid is not energized, then the valve is termed normally open (N.O.). Similarly, if the valve is closed when the solenoid is not energized, then the valve is termed normally closed (N.C.).[1] There are also 3-way and more complicated designs.[2] A 3-way valve has 3 ports; it connects one port to either of the two other ports (typically a supply port and an exhaust port). The solenoid valve (small black box at the top of the photo) with input air line (small green tube) used to actuate a larger rack and pinionactuator (gray box) which controls the water pipe valve Solenoid valves are also characterized by how they operate. A small solenoid can generate a limited force. An approximate relationship between the required solenoid force Fs, the fluid pressure P, and the orifice area A for a direct acting solenoid valve is:[3] Fs=P∗A=Pπd2/4{\\displaystyle F_{s}=P*A=P\\pi d^{2}/4} Where d is the orifice diameter. A typical solenoid force might be 15 N (3.4 lbf). An application might be a low pressure (e.g., 10 psi (69 kPa)) gas with a small orifice diameter (e.g., 3⁄8 in (9.5 mm) for an orifice area of 0.11 in2 (7.1×10−5 m2) and approximate force of 1.1 lbf (4.9 N)). If the force required is low enough, the solenoid is able to directly actuate the main valve. These are simply called Direct-Acting solenoid valves. When electricity is supplied, electrical energy is converted to mechanical energy, physically moving a barrier to either obstruct flow (if it is N.O.) or allow flow (if it is N.C.). A spring is often used to return the valve to its resting position once power is shut off. Direct-acting valves are useful for their simplicity, although they do require a large amount of power relative to other types of solenoid valves.[4] If fluid pressures are high and orifice diameter is large, a solenoid may not generate enough force on its own to actuate the valve. To solve this, a Pilot-Operated solenoid valve design can be used.[1] Such a design uses the pressurized fluid itself to apply the forces required to actuate the valve, with the solenoid as a \"pilot\" directing the fluid (see subsection below). These valves are used in dishwashers, irrigation systems, and other applications where large pressures and/or volumes are desired. Pilot-operated solenoids tend to consume less energy than direct-action, although they will not work at all without sufficient fluid pressure and are more susceptible to getting clogged if the fluid has solid impurities.[4] Power consumption and supply requirements of the solenoid vary with application, being primarily determined by fluid pressure and orifice diameter. For example, a popular 3⁄4-inch 150 psi sprinkler valve, intended for 24 VAC (50–60 Hz) residential systems, has a momentary inrush of 7.2 VA, and a holding power requirement of 4.6 VA.[5] Comparatively, an industrial 1⁄2-inch 10,000 psi valve, intended for 12, 24, or 120 VAC systems in high-pressure fluid and cryogenic applications, has an inrush of 300 VA and a holding power of 22 VA.[6] Neither valve lists a minimum pressure required to remain closed in the unpowered state. While there are multiple design variants, the following is a detailed breakdown of a typical pilot-operated solenoid valve. They may use metal seals or rubber seals, and may also have electrical interfaces to allow for easy control. The diagram to the right shows the design of a basic valve, controlling the flow of water in this example. The top half shows the valve in its closed state. An inlet stream of pressurized water enters at A. B is an elastic diaphragm and above it is a spring pushing it down. The diaphragm has a pinhole through its center which allows a very small amount of water to flow through. This water fills cavity C so that pressure is roughly equal on both sides of the diaphragm. However, the pressurized water in cavity C acts across a much greater area of the diaphragm than the water in inlet A. From the equation F=P∗A{\\displaystyle F=P*A}, the force from cavity C pushing downward is greater than the force from inlet A pushing upward, and the diaphragm remains closed. Diaphragm B will stay closed as long as small drain passage D remains blocked by a pin, which is controlled by solenoid E. In a normally closed valve, supplying an electric current to the solenoid will raise the pin via magnetic force, and the water in cavity C drains out through passage D faster than the pinhole can refill it. Less water in cavity C means the pressure on that side of the diaphragm drops, proportionately dropping the force too. With the downward force of cavity C now less than the upward force of inlet A, the diaphragm is pushed upward, thus opening the valve. Water now flows freely from A to F. When the solenoid is deactivated and passage D is closed, water once again accumulates in cavity C, closing the diaphragm once the downward force exerted is great enough. This process is the opposite for a normally open pilot-operated valve. In that case, the pin is naturally held open by a spring, passage D is open, and cavity C is never able to fill up enough, pushing open diaphragm B and allowing unobstructed flow. Supplying an electric current to the solenoid pushes the pin into a closed position, blocking passage D, allowing water to accumulate in cavity C, and ultimately closing diaphragm B. In this way, a pilot-operated solenoid valve can be conceptualized as two valves working together: a direct-acting solenoid valve which functions as the \"brain\" to direct the \"muscle\" of a much more powerful main valve which gets actuated pneumatically or hydraulically. This is why pilot-operated valves will not work without a sufficient pressure differential between input and output, the \"muscle\" needs to be strong enough to push back against the diaphragm and open it. Should the pressure at the output rise above that of the input, the valve would open regardless of the state of the solenoid and pilot valve. Example core tubes. Non-magnetic core tubes are used to isolate the fluid from the coil. The core tube encloses the plugnut, the core spring, and the core. The coil slips over the core tube; a retaining clip engages the depression near the closed end of the core tube and holds the coil on the core tube. The core or plunger is the magnetic component that moves when the solenoid is energized. The core is coaxial with the solenoid. The core's movement will make or break the seals that control the movement of the fluid. When the coil is not energized, springs will hold the core in its normal position. The plugnut is also coaxial. The core tube contains and guides the core. It also retains the plugnut and may seal the fluid. To optimize the movement of the core, the core tube needs to be nonmagnetic. If the core tube were magnetic, then it would offer a shunt path for the field lines.[11] In some designs, the core tube is an enclosed metal shell produced by deep drawing. Such a design simplifies the sealing problems because the fluid cannot escape from the enclosure, but the design also increases the magnetic path resistance because the magnetic path must traverse the thickness of the core tube twice: once near the plugnut and once near the core. In some other designs, the core tube is not closed but rather an open tube that slips over one end of the plugnut. To retain the plugnut, the tube might be crimped to the plugnut. An O-ring seal between the tube and the plugnut will prevent the fluid from escaping. The solenoid coil consists of many turns of copper wire that surround the core tube and induce the movement of the core. The coil is often encapsulated in epoxy. The coil also has an iron frame that provides a low magnetic path resistance. The valve body must be compatible with the fluid; common materials are brass, stainless steel, aluminum, and plastic.[12] The seals must be compatible with the fluid. To simplify the sealing issues, the plugnut, core, springs, shading ring, and other components are often exposed to the fluid, so they must be compatible as well. The requirements present some special problems. The core tube needs to be non-magnetic to pass the solenoid's field through to the plugnut and the core. The plugnut and core need a material with good magnetic properties such as iron, but iron is prone to corrosion. Stainless steels can be used because they come in both magnetic and non-magnetic varieties.[13] For example, a solenoid valve might use 304 stainless steel for the body, 305 stainless steel for the core tube, 302 stainless steel for the springs, and 430 F stainless steel (a magnetic stainless steel[14]) for the core and plugnut.[1] Solenoid valves are used in fluid power pneumatic and hydraulic systems, to control cylinders, fluid power motors or larger industrial valves. Automatic irrigation sprinkler systems also use solenoid valves with an automatic controller. Domestic washing machines and dishwashers use solenoid valves to control water entry into the machine. They are also often used in paintball gun triggers to actuate the CO2 hammer valve. Solenoid valves are usually referred to simply as \"solenoids.\" Solenoid valves can be used for a wide array of industrial applications, including general on-off control, calibration and test stands, pilot plant control loops, process control systems, and various original equipment manufacturer applications.[15]"}
{"url": "https://en.m.wikipedia.org/wiki/Self-driving_car", "text": "Self-driving car A self-driving car, also known as an autonomous car (AC), driverless car, robotic car or robo-car,[1][2][3] is a car that is capable of operating with reduced or no human input.[4][5] Self-driving cars are responsible for all driving activities including perceiving the environment, monitoring important systems, and controlling the vehicle, including navigating from origin to destination.[6] Multiple vendors are pursuing autonomy, although as of early 2024, no system had achieved full autonomy. Waymo was the first to offer rides in self-driving taxis (\"robotaxis\") to the public,[7] and currently offers services in Phoenix, Arizona, and San Francisco, California. Cruise offered taxi service in San Francisco,[8] but suspended service in 2023. Honda was the first manufacturer to sell an SAE Level 3 car,[9][10][11] followed by Mercedes-Benz,[12]BMW Group and Kia. Baidu's Apollo Go is currently the world's largest robotaxi operator, operating in over 10 Chinese cities[13] and completing over 4.1 million orders by September 2023.[14][15] Other companies include DeepRoute.ai, which launched a robotaxi service in Shenzhen in 2021,[16] and Pony.ai. Trials began in the 1950s. The first semi-autonomous car was developed in 1977, by Japan's Tsukuba Mechanical Engineering Laboratory.[18] It required specially marked streets that were interpreted by two cameras on the vehicle and an analog computer. The vehicle reached speeds of 30 km/h (19 mph) with the support of an elevated rail.[19][20] The US allocated US$650 million in 1991 for research on the National Automated Highway System,[31] which demonstrated automated driving, combining highway-embedded automation with vehicle technology, and cooperative networking between the vehicles and highway infrastructure. The programme concluded with a successful demonstration in 1997.[32] Partly funded by the National Automated Highway System and DARPA, Navlab drove 4,584 km (2,848 mi) across the US in 1995, 4,501 km (2,797 mi) or 98% autonomously.[33] In 2015, Delphi piloted a Delphi technology-based Audi, over 5,472 km (3,400 mi) through 15 states, 99% autonomously.[34] In 2015, Nevada, Florida, California, Virginia, Michigan, and Washington DC allowed autonomous car testing on public roads.[35] From 2016 to 2018, the European Commission funded development for connected and automated driving through Coordination Actions CARTRE and SCOUT programs.[36] The Strategic Transport Research and Innovation Agenda (STRIA) Roadmap for Connected and Automated Transport was published in 2019.[37] In November 2017, Waymo announced testing of autonomous cars without a safety driver.[38] However, an employee was in the car to handle emergencies.[39] In December 2018, Waymo was the first to commercialize a robotaxi service, in Phoenix, Arizona.[40] In October 2020, Waymo launched a robotaxi service in a (geofenced) part of the area.[41][42] The cars were monitored in real-time, and remote engineers intervened to handle exceptional conditions.[43][42] In March 2021, Honda began leasing in Japan a limited edition of 100 Legend Hybrid EX sedans equipped with newly approved Level 3 automated driving equipment that had been safety certified, using their autonomous \"Traffic Jam Pilot\" driving technology, and legally allowed drivers to take their eyes off the road.[9][10][45][11] In December 2020, Waymo became the first service provider to offer driverless taxi rides to the general public, in a part of Phoenix, Arizona. In March 2021, Honda was the first manufacturer to sell a legally approved Level 3 car.[9][10][11]Nuro began autonomous commercial delivery operations in California in 2021.[46]DeepRoute.ai launched robotaxi service in Shenzhen in July 2021.[16] Nuro was approved for Level 4 in Palo Alto in August, 2023.[47] In December 2021, Mercedes-Benz received approval for a Level 3 car.[12] In February 2022, Cruise became the second service provider to offer driverless taxi rides to the general public, in San Francisco.[8] In December 2022, several manufacturers scaled back plans for self-driving technology, including Ford and Volkswagen.[48] In 2023, Cruise suspended its robotaxi service.[49] As of August 2023[update], vehicles operating at Level 3 and above were an insignificant market factor:[citation needed] For instance, for level 3, in addition to 100 Honda vehicles in 2021, in 2023, few companies such as Mercedes (such as S Class) and BMW have claimed get level 3 regulatory approval for specific car models in specific countries or jurisdictions (such as Germany and Nevada).[50] Organizations such as SAE have proposed terminology standards. However, most terms have no standard definition and are employed variously by vendors and others. Proposals to adopt aviation automation terminology for cars have not prevailed.[51] Names such as AutonoDrive, PilotAssist, Full-Self Driving or DrivePilot are used even though the products offer an assortment of features that may not match the names.[52] Despite offering a system ot called Full Self-Driving, Tesla stated that its system did not autonomously handle all driving tasks.[53] In the United Kingdom, a fully self-driving car is defined as a car so registered, rather than one that supports a specific feature set.[54] The Association of British Insurers claimed that the usage of the word autonomous in marketing was dangerous because car ads make motorists think \"autonomous\" and \"autopilot\" imply that the driver can rely on the car to control itself, even though they do not. An ADAS is a system that automates specific driving features, such as keeping the car within its lane, cruise control, and emergency braking. An ADAS requires a human driver to handle tasks that the ADAS does not support. Autonomy implies that an automation system is under the control of the vehicle rather than a driver. Automation is function-specific, handling issues such as speed control, but leaves broader decision-making to the driver.[55] Euro NCAP defined autonomous as \"the system acts independently of the driver to avoid or mitigate the accident\".[56] In Europe, the words automated and autonomous can be used together. For instance, Regulation (EU) 2019/2144 supplied:[57] \"automated vehicle\" means a vehicle that can move without continuous driver supervision, but that driver intervention is still expected or required in some ODDs;[57] \"fully automated vehicle\" means a vehicle that can move entirely without driver supervision;[57] Some driving automation systems may indeed be autonomous if they perform all of their functions independently and self-sufficiently, but if they depend on communication and/or cooperation with outside entities, they should be considered cooperative rather than autonomous. Operational design domain (ODD) is a term for a particular operating context for an automated system, often used in the field of autonomous vehicles. The context is defined by a set of conditions, including environmental, geographical, time of day, and other conditions. For vehicles, traffic and roadway characteristics are included. Manufacturers use ODD to indicate where/how their product operates safely. A given system may operate differently according to the immediate ODD.[59] The concept presumes that automated systems have limitations.[60] Relating system function to the ODDs it supports is important for developers and regulators to establish and communicate safe operating conditions. Systems should operate within those limitations. Some systems recognize the ODD and modify their behavior accordingly. For example, an autonomous car might recognize that traffic is heavy and disable its automated lane change feature. [60] Vendors have taken a variety of approaches to the self-driving problem. Tesla's approach is to allow their \"full self-driving\" (FSD) system to be used in all ODDs as a Level 2 (hands/on, eyes/on) ADAS.[61] Waymo picked specific ODDs (city streets in Phoenix and San Francisco) for their Level 5 robotaxi service.[62] Mercedes Benz offers Level 3 service in Las Vegas in highway traffic jams at speeds up to 40 miles per hour (64 km/h).[63] Mobileye's SuperVision system offers hands-off/eyes-on driving on all road types at speeds up to 130 kilometres per hour (81 mph).[64] GM's hands-free Super Cruise operates on specific roads in specific conditions, stopping or returning control to the driver when ODD changes. In 2024 the company announced plans to expand road coverage from 400,000 miles to 750,000 miles.[65] Ford's BlueCruise hands-off system operates on 130,000 miles of US divided highways.[66] The Union of Concerned Scientists defined self-driving as \"cars or trucks in which human drivers are never required to take control to safely operate the vehicle. Also known as autonomous or 'driverless' cars, they combine sensors and software to control, navigate, and drive the vehicle.\"[67] The British Automated and Electric Vehicles Act 2018 law defines a vehicle as \"driving itself\" if the vehicle is \"not being controlled, and does not need to be monitored, by an individual\".[68] Another British government definition stated, \"Self-driving vehicles are vehicles that can safely and lawfully drive themselves\".[69] In British English, the word automated alone has several meanings, such as in the sentence: \"Thatcham also found that the automated lane keeping systems could only meet two out of the twelve principles required to guarantee safety, going on to say they cannot, therefore, be classed as 'automated driving', preferring 'assisted driving'\".[70] The first occurrence of the \"automated\" word refers to an Unece automated system, while the second refers to the British legal definition of an automated vehicle. British law interprets the meaning of \"automated vehicle\" based on the interpretation section related to a vehicle \"driving itself\" and an insured vehicle.[71] In November 2023 the British Government introduced the Automated Vehicles Bill. It proposed definitions for related terms:[72] Self-driving: \"A vehicle “satisfies the self-driving test” if it is designed or adapted with the intention that a feature of the vehicle will allow it to travel autonomously, and it is capable of doing so, by means of that feature, safely and legally.\" Autonomy: A vehicle travels “autonomously” if it is controlled by the vehicle, and neither the vehicle nor its surroundings are monitored by a person who can intervene. Control: control of vehicle motion. Safe: a vehicle that conforms to an acceptably safe standard. Legal: a vehicle that offers an acceptably low risk of committing a traffic infraction. A six-level classification system – ranging from fully manual to fully automated – was published in 2014 by SAE International as J3016, Taxonomy and Definitions for Terms Related to On-Road Motor Vehicle Automated Driving Systems; the details are revised occasionally.[75] This classification is based on the role of the driver, rather than the vehicle's capabilities, although these are related. After SAE updated its classification in 2016, (J3016_201609),[76] the National Highway Traffic Safety Administration (NHTSA) adopted the SAE standard.[77][78] The classification is a topic of debate, with various revisions proposed.[79][80] Full-time performance by the driver of all aspects of driving, even when \"enhanced by warning or intervention systems\" Driver Driver Driver n/a 1 Driver Assistance Driving mode-specific control by an ADAS of either steering or speed Uses information about the driving environment and with the expectation that the driver performs all other driving tasks. Driver and system Some 2 Partial Automation Driving mode-specific execution by one or more driver assistance systems of both steering and speed System 3 Conditional Automation Driving mode-specific control by an ADAS of all aspects of driving Driver must appropriately respond to a request to intervene. System 4 High Automation If a driver does not respond appropriately to a request to intervene, the car can stop safely. System Many 5 Full Automation System controls the vehicle under all conditions. All SAE Automation Levels have been criticized for their technological focus. It has been argued that the structure of the levels suggests that automation increases linearly and that more automation is better, which may not be the case.[82] SAE Levels also do not account for changes that may be required to infrastructure[83] and road user behavior.[84][85] Mobileye CEO Amnon Shashua and CTO Shai Shalev-Shwartz proposed an alternative taxonomy for autonomous driving systems, claiming that a more consumer-friendly approach was needed. Its categories reflect the amount of driver engagement that is required.[86][87] Some vehicle makers have informally adopted some of the terminology involved, while not formally committing to it.[88][89][90][91] The first level, hands-on/eyes-on, implies that the driver is fully engaged in operating the vehicle, but is supervised by the system, which intervenes according to the features it supports (e.g., adaptive cruise control, automatic emergency braking). The driver is entirely responsible, with hands on the wheel, and eyes on the road.[87] Eyes-off/hands-off means that the driver can stop monitoring the system, leaving the system in full control. Eyes-off requires that no errors be reproducible (not triggered by exotic transitory conditions) or frequent, that speeds are contextually appropriate (e.g., 80 mph on limited-access roads), and that the system handle typical maneuvers (e.g., getting cut off by another vehicle). The automation level could vary according to the road (e.g., eyes-off on freeways, eyes-on on side streets).[87] The perception system processes visual and audio data from outside and inside the car to create a local model of the vehicle, the road, traffic, traffic controls and other observable objects, and their relative motion. The control system then takes actions to move the vehicle, considering the local model, road map, and driving regulations.[92][93][94][95] Several classifications have been proposed to describe ADAS technology. One proposal is to adopt these categories: navigation, path planning, perception, and car control.[96] Navigation involves the use of maps to define a path between origin and destination. Hybrid navigation is the use of multiple navigation systems. Some systems use basic maps, relying on perception to deal with anomalies. Such a map understands which roads lead to which others, whether a road is a freeway, a highway, are one-way, etc. Other systems require highly detailed maps, including lane maps, obstacles, traffic controls, etc. Maps are necessary for navigation. Map sophistication varies from simple graphs that show which roads connect to each other, with details such as one-way vs two-way, to those that are highly detailed, with information about lanes, traffic controls, roadworks, and more.[97] Researchers at the MITComputer Science and Artificial Intelligence Laboratory (CSAIL) developed a system called MapLite, which allows self-driving cars to drive with simple maps. The system combines the GPS position of the vehicle, a \"sparse topological map\" such as OpenStreetMap (which has only 2D road features), with sensors that observe road conditions.[105] One issue with highly-detailed maps is updating them as the world changes. Vehicles that can operate with less-detailed maps do not require frequent updates or geo-fencing. Sensors are necessary for the vehicle to properly respond to the driving environment. Sensor types include cameras, LiDAR, ultrasound, and radar. Control systems typically combine data from multiple sensors.[106] Multiple sensors can provide a more complete view of the surroundings and can be used to cross-check each other to correct errors.[107] For example, radar can image a scene in, e.g., a nighttime snowstorm, that defeats cameras and LiDAR, albeit at reduced precision. After experimenting with radar and ultrasound, Tesla adopted a vision-only approach, asserting that humans drive using only vision, and that cars should be able to do the same, while citing the lower cost of cameras versus other sensor types.[108] By contrast, Waymo makes use of the higher resolution of LiDAR sensors and cites the declining cost of that technology.[109] Path planning finds a sequence of segments that a vehicle can use to move from origin to destination. Techniques used for path planning include graph-based search and variational-based optimization techniques. Graph-based techniques can make harder decisions such as how to pass another vehicle/obstacle. Variational-based optimization techniques require more stringent restrictions on the vehicle's path to prevent collisions.[110] The large scale path of the vehicle can be determined by using a voronoi diagram, an occupancy grid mapping, or a driving corridor algorithm. The latter allows the vehicle to locate and drive within open space that is bounded by lanes or barriers.[111] Driver monitoring is used to assess the driver's attention and alertness. Techniques in use include eye monitoring, and requiring the driver to maintain torque on the steering wheel.[112] It attempts to understand driver status and identify dangerous driving behaviors.[113] Software controls the vehicle, and can provide entertainment and other services. Over-the-air updates can deliver bug fixes and additional features over the internet. Software updates are one way to accomplish recalls that in the past required a visit to a service center. In March 2021, the UNECE regulation on software update and software update management systems was published.[119] A safety model is software that attempts to formalize rules that ensure that ACs operate safely.[120] IEEE is attempting to forge a standard for safety models as \"IEEE P2846: A Formal Model for Safety Considerations in Automated Vehicle Decision Making\".[121] In 2022, a research group at National Institute of Informatics (NII, Japan) enhanced Mobileye's Reliable Safety System as \"Goal-Aware RSS\" to enable RSS rules to deal with complex scenarios via program logic.[122] The US has standardized the use of turquoise lights to inform other drivers that a vehicle is driving autonomously. It will be used in the 2026 Mercedes-Benz EQS and S-Class sedans with Drive Pilot, an SAE Level 3 driving system.[citation needed] As of 2023, the Turquoise light had not been standardized by the P.R.C or the UN-ECE.[123] The primary obstacle to ACs is the advanced software and mapping required to make them work safely across the wide variety of conditions that drivers experience.[124] In addition to handling day/night driving in good and bad weather[125] on roads of arbitrary quality, ACs must cope with other vehicles, road obstacles, poor/missing traffic controls, flawed maps, and handle endless edge cases, such as following the instructions of a police officer managing traffic at a crash site. Other obstacles include cost, liability,[126][127] consumer reluctance,[128] ethical dilemmas,[129][130] security,[131][132][133][134] privacy,[125] and legal/regulatory framework.[135] Further, AVs could automate the work of professional drivers, eliminating many jobs, which could slow acceptance.[136] In 2018 and 2019 former Apple engineers were charged with stealing information related to Apple's self-driving car project.[146][147][148] In 2021 the United States Department of Justice (DOJ) accused Chinese security officials of coordinating a hacking campaign to steal information from government entities, including research related to autonomous vehicles.[149][150] China has prepared \"the Provisions on Management of Automotive Data Security (Trial) to protect its own data\".[151][152] Testing of Chinese automated cars in the US has raised concern over which US data are collected by Chinese vehicles to be stored in Chinese country and concern with any link with the Chinese communist party.[155] ACs complicate the need for drivers to communicate with each other, e.g., to decide which car enters an intersection first. In an AC without a driver, traditional means such as hand signals do not work (no driver, no hands).[156] Conversely, it would be advantageous for the AC to be able to interpret such signals from human drivers. ACs must be able to predict the behavior of possibly moving vehicles, pedestrians, etc in real time in order to proceed safely.[94] The task becomes more challenging the further into the future the prediction extends, requiring rapid revisions to the estimate to cope with unpredicted behavior. One approach is to wholly recompute the position and trajectory of each object many times per second. Another is to cache the results of an earlier prediction for use in the next one to reduce computational complexity.[157][158] Risk compensation is a common human behavior. The safer a system is perceived to be, the more likelier people are to test its limits by engaging in riskier behavior. (People who wear seat belts drive faster). For example Tesla Autopilot users in some cases stop monitoring the vehicle.[citation needed] Consumers will avoid ACs unless they trust them as safe.[160][161] Robotaxis operating in San Francisco received pushback over perceived safety risks.[162] Automatic elevators were invented in 1900, but did not become common until operator strikes and trust was built with advertising and features such as an emergency stop button.[163][164] Standards for liability have yet to be adopted to address crashes and other incidents. Liability could rest with the vehicle occupant, its owner, the vehicle manufacturer, or even the ADAS technology supplier, possibly depending on the circumstances of the crash.[165] One public opinion survey reported that harm reduction was preferred, except that passengers wanted the vehicle to prefer them, while pedestrians took the opposite view. Utilitarian regulations were unpopular.[169] Some ACs require an internet connection to function, opening the possibility that a hacker might gain access to private information such as destinations, routes, camera recordings, media preferences, and/or behavioral patterns, although this is true of an internet-connected device.[170][171][172] ACs make use of road infrastructure (e.g., traffic signs, turn lanes) and may require modifications to that infrastructure to fully achieve their safety and other goals.[173] In March 2023, the Japanese government unveiled a plan to set up a dedicated highway lane for ACs.[174] In April 2023, JR East announced their challenge to raise their self-driving level of Kesennuma Linebus rapid transit (BRT) in rural area from the current Level 2 to Level 4 at 60 km/h.[175] ACs can be tested via digital simulations,[176][177] in a controlled test environment,[178] and/or on public roads. Road testing typically requires some form of permit[179] or a commitment to adhere to acceptable operating principles.[180] For example, New York requires a test driver to be in the vehicle, prepared to override the ADAS as necessary.[181] In California, self-driving car manufacturers are required to submit annual reports describing how often their vehicles autonomously disengaged from autonomous mode.[182] This is one measure of system robustness (ideally, the system should never disengage).[183] In 2017, Waymo reported 63 disengagements over 352,545 mi (567,366 km) of testing, an average distance of 5,596 mi (9,006 km) between disengagements, the highest (best) among companies reporting such figures. Waymo also logged more autonomous miles than other companies. Their 2017 rate of 0.18 disengagements per 1,000 mi (1,600 km) was an improvement over the 0.2 disengagements per 1,000 mi (1,600 km) in 2016, and 0.8 in 2015. In March 2017, Uber reported an average of 0.67 mi (1.08 km) per disengagement. In the final three months of 2017, Cruise (owned by GM) averaged 5,224 mi (8,407 km) per disengagement over 62,689 mi (100,888 km).[184] Reporting companies use varying definitions of what qualifies as a disengagement, and such definitions can change over time.[187][183] Executives of self-driving car companies have criticized disengagements as a deceptive metric, because it does not consider varying road conditions.[188] In September 2022, Biprogy released Driving Intelligence Validation Platform (DIVP) as part of Japanese national project \"SIP-adus\", which is interoperable with Open Simulation Interface (OSI) of ASAM.[198][199][200] In 2023 David R. Large, senior research fellow with the Human Factors Research Group at the University of Nottingham, disguised himself as a car seat in a study to test people's reactions to driverless cars. He said, \"We wanted to explore how pedestrians would interact with a driverless car and developed this unique methodology to explore their reactions.\" The study found that, in the absence of someone in the driving seat, pedestrians trust certain visual prompts more than others when deciding whether to cross the road.[203] On 20 January 2016, the first of five known fatal crashes of a Tesla with Autopilot occurred, in China's Hubei province.[205] Initially, Tesla stated that the vehicle was so badly damaged from the impact that their recorder was not able to determine whether the car had been on Autopilot at the time. However, the car failed to take evasive action. Another fatal Autopilot crash occurred in May in Florida in a Tesla Model S[206][207] that crashed into a tractor-trailer. In a civil suit between the father of the driver killed and Tesla, Tesla documented that the car had been on Autopilot.[208] According to Tesla, \"neither Autopilot nor the driver noticed the white side of the tractor-trailer against a brightly lit sky, so the brake was not applied.\" Tesla claimed that this was Tesla's first known Autopilot death in over 130 million miles (210 million kilometers) with Autopilot engaged. Tesla claimed that on average one fatality occurs every 94 million miles (151 million kilometers) across all vehicle types in the US.[209][210][211] However, this number also includes motorcycle/pedestrian fatalities.[212][213] The ultimate NTSB report concluded Tesla was not at fault; the investigation revealed that for Tesla cars, the crash rate dropped by 40 percent after Autopilot was installed.[214] In June 2015, Google confirmed that 12 vehicles had suffered collisions as of that date. Eight involved rear-end collisions at a stop sign or traffic light, in two of which the vehicle was side-swiped by another driver, one in which another driver rolled a stop sign, and one where a driver was controlling the car manually.[215] In July 2015, three employees suffered minor injuries when their vehicle was rear-ended by a car whose driver failed to brake. This was the first collision that resulted in injuries.[216] According to Google Waymo's accident reports as of early 2016, their test cars had been involved in 14 collisions, of which other drivers were at fault 13 times, although in 2016 the car's software caused a crash.[217] On 14 February 2016 a Google vehicle attempted to avoid sandbags blocking its path. During the maneuver it struck a bus. Google stated, \"In this case, we clearly bear some responsibility, because if our car hadn't moved, there wouldn't have been a collision.\"[218][219] Google characterized the crash as a misunderstanding and a learning experience. No injuries were reported.[217] In March 2018, Elaine Herzberg died after she was hit by an AC tested by Uber's Advanced Technologies Group (ATG) in Arizona. A safety driver was in the car. Herzberg was crossing the road about 400 feet from an intersection.[220] Some experts said a human driver could have avoided the crash.[221] Arizona governor Doug Ducey suspended the company's ability to test its ACs citing an \"unquestionable failure\" of Uber to protect public safety.[222] Uber also stopped testing in California until receiving a new permit in 2020.[223][224] NTSB's final report determined that the immediate cause of the accident was that safety driver Rafaela Vasquez failed to monitor the road, because she was distracted by her phone, but that Uber's \"inadequate safety culture\" contributed. The report noted that the victim had \"a very high level\" of methamphetamine in her body.[225] The board called on federal regulators to carry out a review before allowing automated test vehicles to operate on public roads.[226][227] On 12 August 2021, a 31-year-old Chinese man was killed after his NIO ES8 collided with a construction vehicle.[citation needed] NIO's self-driving feature was in beta and could not deal with static obstacles.[229] The vehicle's manual clearly stated that the driver must take over near construction sites. Lawyers of the deceased's family questioned NIO's private access to the vehicle, which they argued did not guarantee the integrity of the data.[230] In November 2021, the California Department of Motor Vehicles (DMV) notified Pony.ai that it was suspending its testing permit following a reported collision in Fremont on 28 October.[231] In May 2022, DMV revoked Pony.ai's permit for failing to monitor the driving records of its safety drivers.[232] In a 2011 online survey of 2,006 US and UK consumers, 49% said they would be comfortable using a \"driverless car\".[235] A 2012 survey of 17,400 vehicle owners found 37% who initially said they would be interested in purchasing a \"fully autonomous car\". However, that figure dropped to 20% if told the technology would cost US$3,000 more.[236] In a 2012 survey of about 1,000 German drivers, 22% had a positive attitude, 10% were undecided, 44% were skeptical and 24% were hostile.[237] A 2013 survey of 1,500 consumers across 10 countries found 57% \"stated they would be likely to ride in a car controlled entirely by technology that does not require a human driver\", with Brazil, India and China the most willing to trust automated technology.[238] In a 2014 US telephone survey, over three-quarters of licensed drivers said they would consider buying a self-driving car, rising to 86% if car insurance were cheaper. 31.7% said they would not continue to drive once an automated car was available.[239] In 2015, a survey of 5,000 people from 109 countries reported that average respondents found manual driving the most enjoyable. 22% did not want to pay more money for autonomy. Respondents were found to be most concerned about hacking/misuse, and were also concerned about legal issues and safety. Finally, respondents from more developed countries were less comfortable with their vehicle sharing data.[240] The survey reported consumer interest in purchasing an AC, stating that 37% of surveyed current owners were either \"definitely\" or \"probably\" interested.[240] In 2016, a survey of 1,603 people in Germany that controlled for age, gender, and education reported that men felt less anxiety and more enthusiasm, whereas women showed the opposite. The difference was pronounced between young men and women and decreased with age.[241] In a 2016 US survey of 1,584 people, \"66 percent of respondents said they think autonomous cars are probably smarter than the average human driver\". People were worried about safety and hacking risk. Nevertheless, only 13% of the interviewees saw no advantages in this new kind of cars.[242] In a 2017 survey of 4,135 US adults found that many Americans anticipated significant impacts from various automation technologies including the widespread adoption of automated vehicles.[243] In 2019, results from two opinion surveys of 54 and 187 US adults respectively were published. The questionnaire was termed the autonomous vehicle acceptance model (AVAM), including additional description to help respondents better understand the implications of various automation levels. Users were less accepting of high autonomy levels and displayed significantly lower intention to use autonomous vehicles. Additionally, partial autonomy (regardless of level) was perceived as requiring uniformly higher driver engagement (usage of hands, feet and eyes) than full autonomy.[244] SAE Level 2 features are available as part of the ADAS systems in many vehicles. In the US, 50% of new cars provide driver assistance for both steering and speed.[249] Ford started offering BlueCruise service on certain vehicles in 2022; the system is named ActiveGlide in Lincoln vehicles. The system provided features such as lane centering, street sign recognition, and hands-free highway driving on more than 130,000 miles of divided highways. The 2022 1.2 version added features including hands-free lane changing, in-lane repositioning, and predictive speed assist.[250][251] In April 2023 BlueCruise was approved in the UK for use on certain motorways, starting with 2023 models of Ford's electric Mustang Mach-E SUV.[252] Tesla's Autopilot and its Full Self-Driving (FSD) ADAS suites are available on all Tesla cars. FSD offers highway and street driving (without geofencing), navigation/turn management, steering, and dynamic cruise control, collision avoidance, lane-keeping/switching, emergency braking, obstacle avoidance, but still requires the driver to remain ready to control the vehicle at any moment. Its driver management system combines eye tracking with monitoring pressure on the steering wheel to ensure that drives are both hands on and eyes on.[253][254] General Motors is developing the \"Ultra Cruise\" ADAS system, that will be a dramatic improvement over their current \"Super Cruise\" system. Ultra Cruise will cover \"95 percent\" of driving scenarios on 2 million miles of roads in the US, according to the company. The system hardware in and around the car includes multiple cameras, short- and long-range radar, and a LiDAR sensor, and will be powered by the Qualcomm Snapdragon Ride Platform. The luxury Cadillac Celestiq electric vehicle will be one of the first vehicles to feature Ultra Cruise.[255] Tesla's FASD rewrite V12 (released in 2024) uses a single deep learning transformer model for all aspects of perception, monitoring, and control. It relies on its 8 cameras for its vision-only perception system, without use of LIDAR, radar, or ultrasound. As of January 2024, FSD V12 was undergoing testing in a limited number of customer vehicles. Tesla has not initiated requests for Level 3 status for its systems and has not disclosed its reason for not doing so.[254] As of 2023, three car manufacturers had registered Level 3 cars: Honda in Japan, Mercedes in Germany, Nevada and California[258] and BMW in Germany.[259] Mercedes Drive Pilot has been available on the EQS and S-class sedan in Germany since 2022, and in California and Nevada since 2023.[63] Drive Pilot can only be used when there is a vehicle in front, readable line markings, during the day, clear weather, and on freeways mapped by Mercedes down to the centimeter (100,000 miles in California).[63] Honda continued to enhance its Level 3 technology.[260][261] As of 2023, 80 vehicles with Level 3 support had been sold.[262] Mercedes-Benz received authorization in early 2023 to pilot its Level 3 software in Las Vegas.[263] California also authorized Drive Pilot in 2023.[264] BMW commercialized its AC in 2021.[265] In 2023 BMW stated that its Level-3 technology was nearing release. It would be the second manufacturer to deliver Level-3 technology, but the only one with a Level 3 technology which works in the dark.[266] In September 2021, Cruise, General Motors, and Honda started a joint testing programme, using Cruise AV.[282] In 2023, the Origin was put on indefinite hold following Cruise's loss of its operating permit.[283] In January 2023, Holon ann autonomous shuttle during the 2023 CES. The company claimed the vehicle is the world's first Level 4 shuttle built to automotive standard.[284] ^\"メルセデス・ベンツ日本に措置命令 事実と異なる記載 消費者庁\" [Administrative order to Mercedes-Benz Japan Co., Ltd. for the descriptions that are different from the fact – The Consumer Affairs Agency]. NHK, Japan (in Japanese). 10 December 2021. Retrieved 13 April 2022. ^\"A Tragic Loss\" (Press release). Tesla Motors. 30 June 2016. Retrieved 1 July 2016. This is the first known fatality in just over 130 million miles where Autopilot was activated. Among all vehicles in the US, there is a fatality every 94 million miles. Worldwide, there is a fatality approximately every 60 million miles. ^Hohenberger, C.; Spörrle, M.; Welpe, I. M. (2016). \"How and why do men and women differ in their willingness to use automated cars? The influence of emotions across different age groups\". Transportation Research Part A: Policy and Practice. 94: 374–385. doi:10.1016/j.tra.2016.09.022."}
{"url": "https://pubmed.ncbi.nlm.nih.gov/19935302/", "text": "Abstract Background: The condition of \"buried\" penis may arise from several factors. Although the pediatric form is a rare congenital disorder, it may become an acquired condition in adulthood, most commonly from obesity, radical circumcision, or penoscrotal lymphedema. As obesity has become a national epidemic, the incidence of this phenomenon will inevitably increase. The purpose of this article is to present current strategies in the management of this physically and psychologically debilitating condition. Methods: A literature review of the surgical management of buried penis was obtained mainly in the plastic surgery and urology literature (PubMed), from 1977 to 2007. Results: Several risk factors were identified in adult patients with buried penis, including morbid obesity and diabetes mellitus. Multiple techniques for release and reconstruction are described, including primary closure, Z-plasty, and skin resurfacing, all of which may or may not include a lipectomy. Recent publications focus on resurfacing with split-thickness skin grafts and negative-pressure dressings. These techniques have been successful in terms of graft survival and long-term cosmetic result. Conclusions: Buried penis is an unusual, difficult-to-treat condition that presents a unique challenge to the plastic surgeon and the urologist. Predisposing factors such as morbid obesity and diabetes mellitus are becoming increasingly prevalent, which suggests a potential increase in the incidence of this condition. Although no specific approach may be applicable to all patients, a combination of various techniques may be applied. In complicated and severe cases, a split-thickness skin graft to the penile shaft, reduction scrotoplasty, suction-assisted lipectomy, and/or surgical lipectomy, such as panniculectomy, may be indicated. Therapy adapted to the individual patient can result in high rates of successful reconstruction with acceptable cosmetic results."}
{"url": "https://en.m.wikipedia.org/wiki/Instrumentation_(computer_programming)", "text": "Instrumentation is limited by execution coverage. If the program never reaches a particular point of execution, then instrumentation at that point collects no data. For instance, if a word processor application is instrumented, but the user never activates the print feature, then the instrumentation can say nothing about the routines which are used exclusively by the printing feature. Some types of instrumentation may cause a dramatic increase in execution time. This can limit the application of instrumentation to debugging contexts. Hooking – range of techniques used to alter or augment the behavior of an operating system, of applications, or of other software components by intercepting function calls or messages or events passed between software components. DTrace – A comprehensive dynamic tracing framework for troubleshooting kernel and application problems on production systems in real time, implemented in Solaris, macOS, FreeBSD, and many other platforms and products."}
{"url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0203800", "text": "Abstract Background Current German and EU package leaflets (PLs) do not distinguish to what extent listed side effects are indeed side effects caused by drug intake or instead symptoms that occur regardless of drug use. We recently showed that most health professionals misinterpret the frequencies of listed side effects as solely caused by the drug. The present study investigated whether (1) these misinterpretations also prevail among laypeople and (2) alternative PLs reduce these misinterpretations. Methods In March 2017, 397 out of 400 laypeople approached completed an online survey. They were randomized to one of four PL formats: three alternative PLs (drug facts box with/without reading instruction, narrative format with numbers) and one standard PL. Each PL listed four side effects for a fictitious drug: two were presented as occurring more often, one as equally often, and one as less often with drug intake. The alternative formats (interventions) included information on frequencies with and without drug intake and included a statement on the causal relation. The standard PL (control) only included information on frequency ranges with drug intake. Questions were asked on general occurrence and causality of side effects. Results Participants randomized to the standard PL were unable to answer questions on causality. For side effects occurring more often (equally; less often) with drug intake, only 1.9% to 2.8% (equally: 1.9%; less often: 1.9%) provided correct responses about the causal nature of side effects, compared to 55.0% to 81.9% (equally: 23.8% to 70.5%; less often: 21.0% to 43.2%) of participants who received alternative PLs. It remains unclear whether one alternative format is superior to the others. Conclusion In conclusion, information on the frequency of side effects in current package leaflets is misleading. Comparative presentation of frequencies for side effects with and without drug intake including statements on the causal relation significantly improves understanding. Data Availability: The data underlying this study contain sensitive information and are protected by the Data Privacy Act. Interested researchers can submit data access requests to the ethics committee of the Max Planck Institute for Human Development (Dr. Uwe Czienskowski, sciencec@mpibberlin.mpg.de). Funding: Allowance for participants was paid from budgetary resources of the Center for Adaptive Behavior and Cognition at the Max Planck Institute for Human Development (Germany). There was no further funding of the study. Competing interests: The authors have declared that no competing interests exist. Introduction Informed decision making requires understandable evidence-based health information for patients. Yet studies document that patients are unlikely to receive such information on the Internet or in patient brochures. They are confronted with both a lack of evidence-based decision aids [1–3] and physicians who themselves are often insufficiently informed due to statistical illiteracy [4–9]. This situation may contribute to the findings of a recent survey of the European Medicines Agency, which showed that the majority of patients and healthcare professionals explicitly require greater amounts of unbiased and transparent information on the benefits and harms of medical interventions [10]. When requiring information about a drug’s side effects, up to date package leaflets (PLs) inserted in a drug’s package are the only source of information on drug side effects that patients may definitely receive. However, because PLs have to meet several legal standards, their content rather resembles a legal document than a tool for patient information. At the same time, article 56 of Directive 2001/83/EC of the European Commission requires PLs for medication to be “easily legible” and “clearly comprehensible” [11]. Yet, as the EU commission—on the basis of research from the Netherlands Institute for Health Services Research (NIVEL) and University of Leeds [12]—points out, “patients’ comprehension of the PL and its readability can be improved. The language used is often too complex and the design and lay-out are not always user-friendly. The elderly and those with low literate skills are particularly disadvantaged, but generally these problems hold for all patient groups.” Not only formal criteria such as readability and layout are insufficiently implemented, however. Current PLs also list “side effects” that occur equally or less often in people taking the drug as compared to people not taking the drug (or placebo in clinical trials). For instance, Barron et al. [13] discovered that 28 of 33 symptoms listed as side effects in PLs for patients with heart failure are not causally related to the intake of beta-blockers. Tan analyzed side effects in official and non-official drug information documents and compared them to the 20 most indicated symptoms in the general population, including back and joint pain, headache, and fatigue. He found that “nine of the 20 symptoms most commonly experienced in daily life are listed as adverse drug reactions in more than half of drug information documents, and 17 are listed by more than a third” [14]. However, German and EU PLs do not contain any placebo group information or reference that the side effects listed are not necessarily caused by drug intake with respective frequencies. The question of causality between drug intake and the occurrence of side effects therefore cannot be answered when consulting currently provided PLs. In a recent survey, we showed that even health professionals, who should be particularly skilled in understanding health information, misinterpreted the extent of side effects when presented with a current standard PL [15]. Over 80% of the health professionals erroneously expected a causal relation between the listed frequency of side effects in the PL and drug intake. Their understanding was only slightly improved when provided with a comparative data for a placebo group. In an attempt to provide physicians with risk information that are easier to approach for their counseling of patients, Barron et al. [13] developed a format that presented the “proportion of symptoms non-pharmacological” next to the “proportion of symptoms caused by the drug,” which corresponds to the proportion of each side effect not attributable or attributable to drug intake. Schwartz et al. had similar intentions when developing the drug facts box, which also displays numerical information on the benefits and harms of medical interventions for both people taking and not taking a drug. The drug facts box has been evaluated with patients and was found to improve understanding, even among those with lower education [16]. Based on these findings and others, a review commissioned by the FDA concluded that quantitative information about benefits and harms presented in terms of absolute numbers as well as in comparison between a drug and a placebo group supports consumers’ understanding of a drug’s efficacy [17]. Claims of benefits of a drug are heavily regulated in PLs, but side effects are not. An increased need for discussing adverse events in PLs was also requested by the NIVEL group [12]. Neither the literature nor stakeholders have reached a consensus on how to best achieve complete and transparent reporting of side effects in spite of this being essential to patients’ right to receive all information required for informed decision making. Not surprisingly, focus group interviews revealed that patients’ most prevailing response to the undifferentiated list of side effects in PLs is fear [18]. The aim of our exploratory study was twofold: First, we wanted to verify whether laypeople would also misinterpret side effects as we previously found health professionals to do. We expected that laypeople would also erroneously assume a causal relation between drug intake and side effects when presented with current forms of German and EU standard PLs. Second, we wanted to investigate whether alternative PLs in comparison to current standard PLs support laypeople in better understanding the causality of side effects due to drug intake. Methods A systematic search of the literature was carried out on PubMed, EMBASE, and PsycINFO [15] using the keywords “patient information leaflet,” “package insert,” “summary of product characteristics,” “direct to consumer advertising,” “instruction leaflet,” “product insert,” “enclosed label,” “drug labeling,” “product labeling,” “adverse effects,” “side effects,” “adverse reactions,” “drug-related side effect,” and respective combinations (PubMed search strategy, see S1 File). To keep the search as sensitive as possible, we did not restrict the keywords by the terms “causal interpretation” or “understanding.” The search was carried out in December 2014 and yielded 5,644 citations. We checked for updates for PubMed and EMBASE on a monthly basis until August 2018. The only inclusion criterion was the examination of understanding the causal relation between symptoms and adverse events in PLs. Fifty-seven articles were read in full text. Up to that time, we could not identify any research on the causal interpretation of adverse events in PLs. To investigate if alternative PLs potentially enhance people’s understanding of side effects in comparison to currently used standard PLs (control), we decided on three alternative formats (interventions), which have been suggested as a tool for communicating and summarizing findings from medical research: (1) drug facts box according to Schwartz et al. [16], (2) drug facts box according to Schwartz et al. [16] supplemented by reading instructions, and (3) narrative with numbers according to Barron et al. in a modified version [13]. The format of drug facts boxes (format 1)—developed to enhance people’s understanding of medical facts [16]—is a tabular visualization of benefits and harms for a group taking the drug and a group not taking the drug. All event rates for the benefit and harms are provided as absolute numbers, adjusted to the same denominator (e.g., X out of 100), and present the data for the group taking/not taking the drug in columns right next to each other for ease of comparison and to reduce the reader’s cognitive load [16, 19]. As format 2 in our study, we extended the facts box by additionally including reading instructions in order to investigate if it would further supports guidance of people’s interpretation—which to our knowledge has not yet been tested. For the third alternative format, we chose Barron’s et al. [13] approach of narrative with numbers, originally developed (but again not yet tested empirically) for physicians to use in patient communication. Because in its original version a same denominator can refer to different populations and frequencies, and information is presented in percentages and natural frequencies [20, 21], we modified the format by using absolute numbers referring to the same denominator throughout to ensure comparability between the three formats. That is, all three alternative formats included absolute numbers and the same denominators. However, the three formats varied in the extent of how much additional verbal information was given for guidance. Compared to the facts box formats (format 1, format 2) format 3 further did not display the numerical information for the groups taking/not taking the drug in columns next to each other. We decided to include three alternative formats instead of just one in order to explore for the first time if any of these provide a particular advantage over currently used standard PLs in fostering people’s understanding of the causal nature of side effects. To fill the different formats of a PL with real information, we used data from a systematic review on the side effects of beta-blockers [13]. To exclude the potential bias in response to our questions from people who may know or even take beta-blockers themselves and thus may hold a specific assumption about their side effects, we labeled the provided information on the side effects belonging to a fictitious drug called Suffia. Apart from the drug name, the content of all PLs and the format of the standard PL was not fictitious. Moreover, we took into account that a small survey by Sullivan et al. found that 100 participants were more likely to make direct comparisons between the efficacy of drug-taking and placebo groups when the term “without [drug name]” was used instead of “placebo” or “sugar pill” [22], and accordingly used the term “without Suffia” to describe placebo group results. Format 4 (standard PL) followed the current convention of how information on side effects is presented in the PLs of drugs, which served as the control condition in our study. For our fictitious drug, we listed four possible side effects: hyperglycemia (“increased blood glucose level”), bradycardia (“slow heart rate”), anemia, and depression. The four side effects were chosen from systematic review on the side effects of beta-blockers [13] by chance, with the only requirement that half of them had a causal relation to drug intake and the other half did not (see S1 Table). In the alternative PLs, hyperglycemia (16 of 100 patients who take Suffia vs. 13 of 100 patients who do not take Suffia) and bradycardia (5 of 100 patients who take Suffia vs. 2 of 100 patients who do not take Suffia) were presented as occurring more often in the group taking the drug. Anemia (4 of 100 people who take Suffia vs. 4 of 100 people who do not take Suffia) was depicted as occurring equally often in both groups and depression (9 of 100 patients who take Suffia vs. 12 of 100 patients who do not take Suffia) as occurring less often under drug intake. In each of the alternative formats, a summary statement was additionally given for each of the side effects on how many of these were due to drug intake (e.g., “Taking Suffia leads to 3 extra cases of increased blood glucose levels in 100 people”). We also provided a fictitious time frame within which patients would experience these side effects (“occurrence of undesired symptoms over 5 years”). Fig 1 provides an example of an alternative PL (format 2). For the standard PL (format 4), all information on the side effects was given as in currently distributed PLs and used verbal quantifiers (“very common”, “common”) together with a numerical range (“more than 10 cases in 100 people taking the drug”). That is, hyperglycemia (16 cases out of 100 in the group taking the drug) was labeled as “very common” (“affects more than 1 in 10 patients”), while bradycardia (5 cases out of 100 in the group taking the drug), anemia (4 cases out of 100 in the group taking the drug), and depression (9 cases out of 100 in the group taking the drug) were labeled as “common” (“affects up to 1 in 10 patients”) (see Fig 2). The remaining PLs can be seen in the Supporting Information (S1–S8 Figs). The online version survey was set up and published using Limesurvey, an open source survey software (Limesurvey GmbH, Hamburg, Germany; URL: http://www.limesurvey.org). Consent was recorded by participants ticking the box agreement of participation after reading information about the research goals and the data collection (consent form and introduction to the Online Survey, see S2 File). If participants did not agree to participate, they automatically exited the survey. Data were collected on 22 March 2017 using clickworker, an online survey panel provider (clickworker GmbH, Essen, Germany; URL: http://www.clickworker.de). Participants received the average reimbursement of 1 euro—as suggested by clickworker—for their time spent on participating in our survey. The study was conducted in German; all formats and survey questions in this publication are translations. Original phrasing of the PLs can be seen in the Supporting Information. Of the 400 participants who entered the survey, 397 provided their informed consent and were randomly assigned to one of the four formats of the PL. Randomization was achieved by means of a computer-based random number generation and neither participants nor investigators could foresee the random assignment. After reading the respective PL, participants were asked for each of the listed side effects 1) how many people out of 100 taking Suffia® would experience the respective side effects (general occurrence), and 2), for how many people out of 100 taking Suffia® the respective side effect is causally induced by the drug (causality). The order of these questions was randomized across participants. To best reflect a real-life situation, the respective PL remained visible the entire time while participants were answering the question. That is, participants were not required to recall the numbers of people experiencing side effects with and without drug intake when making their judgments. The general occurrence question tested whether participants were able to derive correct numbers from the table. The correct answer was the number of people experiencing the symptom among those taking the drug. The question regarding causality investigated whether the respective PLs enable people to correctly understand to what extent side effects listed in the PL are causally induced by drug intake. Here the correct answer was the difference in the event rate between the group taking and not talking the drug. The survey did not allow for item nonresponse; thus all 397 surveys were complete. For all formats of the PL, answers to the general occurrence question were rated correct according to the aforementioned frequencies. For the question on causality, the correct answers were “3 out of 100 people” for hyperglycemia and bradycardia and “0 out of 100 people” for anemia and depression for all formats of the PL. Analysis Taking into consideration our first study with health professionals [15] and the results of a randomized controlled trial on the effectiveness of a drug facts box on people’s understanding by Schwartz and Woloshin [23], we assumed a difference of at least 20 percent points between the standard PL and all alternative formats in the proportions of correct responses to the causality question (question 2). However, due to a lack of previous studies comparing differences in effectiveness between the three alternative formats we used in the study at hand, we were not able to estimate the potential differences between the alternative formats or to estimate the potential size of differences in supporting people’s understanding of medical facts. Thus, except for the difference between the standard PL and alternative PLs, all other analyses were only hypothesis generating. Data were analyzed by frequency, reporting for each group and side effect. Order effects were analyzed using the non-parametric Pearson Chi-square test. All data were stored and analyzed with IBM SPSS Statistics 24 (New York City, USA) and RStudio (RStudio Inc, Boston, USA). Graphics were produced with the RStudio package ggplot2 by H. Wickham (2009). Because analyses were not pre-defined but hypotheses generating we did not adjust for multiple testing or baseline imbalances between groups. Statistical analysis was not blinded. Ethical approval The study was approved by the Institutional Ethics Board of the Max Planck Institute for Human Development, Berlin (Germany). a) General occurrence of side effects during drug use Asked about the number of people who experience each of the side effects listed in the PL during intake of the drug Suffia®, between 68.1% and 75.8% of the participants presented with the drug facts box (format 1) gave a correct answer across the questions on the four listed side effects. For those presented with the drug facts box with reading instruction (format 2), between 52.6% and 55.8% responded correctly. Among people presented with the narrative including numbers (format 3), between 55.2% and 93.3% derived the correct number from the table. In contrast, participants receiving the standard PL (format 4) gave considerably fewer correct answers to the questions on general occurrence. Only between 0% and 11.3% of these participants were able to provide correct answers across the four questions. Detailed information on correct responses per format and listed side effects can be seen in Table 2. ExpandTable 2. Participants’ correct responses to the question on the general occurrence of side effects per format and per side effect. The formats also resulted in different variances of participant’s responses on the general occurrence of side effects. Participants receiving the standard PL (format 4) tended to have larger variance than did participants who received the alternative formats 1 to 3. For instance, for hyperglycemia (correct answer: 16), the responses of participants receiving the standard PL ranged from 1 to 92 out of 100 people with a median of 10 (Mresponses = 13.1; SD = 14.1) (Fig 3). The majority of participants who received the standard PL underestimated the general occurrence of side effects: 87.7% provided frequencies lower than the correct answer of 16. ExpandFig 3. Participants’ response distribution on the general occurrence of side effects per format. Although the majority of people receiving any of the alternative formats provided the correct answer regarding the general occurrence, a considerable number of participants provided the response that would have been correct for the question on the causality of side effects (correct response for hyperglycemia: 3). That number was provided by 23.1% of participants receiving the drug facts box (format 1) (range responses: 3–19, Mresponses = 13.1; SD = 5.7), 43.2% of participants receiving the drug facts box with reading instruction (format 2) (range responses: 3–19, Mresponses = 10.4; SD = 6.6), and 42.9% of those receiving narratives with numbers (format 3) (range responses: 2–16, Mresponses = 10.3; SD = 6.5). A similar pattern was observed for responses on the question for bradycardia, but not for anemia and depression. Information on the range, mean, and standard deviation of responses for each of the other three side effects can be found in the Supporting Information (see S2 Table). As mentioned before, the general occurrence question aimed at investigating whether participants were able to correctly draw numbers from a table. Because participants receiving the standard PL had to deal with changing denominators and imprecise numerical information (i.e., ranges instead of point estimates), they were generally at a disadvantage in arriving at a correct response to this question. If the responses to the general occurrence question are analyzed more liberally for this group, allowing for a ballpark of +/-5 out of 100, between 38.7% and 97.2% answered the question correctly. b) Causality between drug use and side effects Asked about how often each of the side effects listed in the PL is causally induced by the intake of the drug Suffia®, between 29.7% and 57.1% of people presented with the drug facts box (format 1) were able to provide the correct answers (see Table 3). Of the people presented with the drug facts box with reading instruction (format 2) between 43.2% and 70.5% responded correctly, and among people being presented with the narrative including numbers (format 3), between 21.0% and 81.9% provided correct estimates. Interpretation of the causal extent of the side effect depression—depicted as occurring less frequently in the group taking the drug—appeared challenging, however. Whereas over 50% of participants presented with any of the three alternative formats correctly understood the causal relation between drug intake and each of the other side effects, less than 50% did so for depression (see Table 3). Also, the alternative formats did not prevent some participants from confusing causality with general occurrence. For instance, for hyperglycemia, between 15.2% and 37.4% of the participants provided the correct response for general occurrence instead of causality. For the other side effects, see Supporting Information. ExpandTable 3. Participants with correct responses to the question on causality per format and per side effect. With the standard PL (format 4), a minority of participants correctly understood the extent of the causal relation between side effects and drug intake: Only between 1.9% and 2.8% estimated the correct answers to the respective questions. Table 3 shows the results for each format of the PL and each side effect in detail. The formats again resulted in different variances of participants’ responses. Participants receiving the standard PL (format 4) again tended to display a larger variance than participants who received the alternative formats 1 to 3. For instance, taking participants’ estimates of the causal extent of hyperglycemia due to drug intake (correct answer: 3), participants receiving the standard PL provided responses ranging from 0 to 90 (Mresponses = 11.2; SD = 9.4, medianresponses = 10) (see Fig 4). Now a majority of participants in this group overestimated the extent of side effects causally induced by the drug intake: 84.0% provided frequencies higher than the correct answer of 3. For the alternative formats, responses on the causal extent of hyperglycemia by participants receiving the drug facts box (format 1) ranged from 3–19 (Mresponses = 9.0; SD = 6.7), by participants receiving the drug facts box with reading instruction from 2–81 (Mresponses = 7.5; SD = 9.7, median responses: 3.0), and by participants receiving narratives including numbers (format 3) from 3–16 (Mresponses = 5.2; SD = 4.8), respectively (Fig 4). Information on the range, mean, and standard deviation of responses for each of the other three side effects can be found in the Supporting Information (see S3 Table). ExpandFig 4. Participants’ response distribution on the causal extent of side effects per format. Randomization of the questions on general occurrence and causality did not influence the proportion of correct response rates (results for order effects, see S4 and S5 Tables), except for the drug facts box (format 1) and the question on depression (p = 0.04), where participants who first viewed the question on causality provided more correct replies to the general occurrence question afterwards. Discussion In our study, we showed that currently used standard formats of PLs do not enable laypeople to distinguish which proportion of listed side effects are indeed side effects caused by drug intake or instead are symptoms occurring regardless of drug intake. Nearly all participants in the group receiving the standard PL were unable to draw correct conclusions on the causal extent of side effects and overestimated the frequency of all listed side effects. In contrast, all alternative formats improved understanding of the extent to which these side effects are causally induced by the drug. Particularly for the side effects presented as occurring more often in the group of people taking the drug as compared to not taking the drug, the majority of participants were able to correctly understand the causal link between drug intake and side effects when presented with the alternative format. Narratives with numbers and drug facts boxes with reading instructions might be particularly helpful in this context. Although the alternative formats, in contrast to the standard format, improved people’s understanding of the causality of all side effects, these formats could only partly prevent the misunderstanding of side effects that were depicted as occurring equally or less often under drug intake (anemia and depression). We assume that the word “side effect” triggers a certain anticipation. To find a listed side effect occurring less often within people taking a drug than within those not taking the drug may contradict the conventional wisdom of what “side effect” means. More neutral descriptions such as “unintended events” may have prevented some of the confusion we found for the question on the causal extent of depression due to drug intake. However, because “side effects” is the official term, well-known and commonly used, we decided to use it in our study. We can only speculate, however, why, among the three alternative formats, people receiving the narrative with numbers (format 3) particularly suffered from the aforementioned effect. In contrast to the facts box formats (formats 1 and 2), which displayed outcome information for the intervention and control group side-by-side, within the format “narrative with numbers” numbers had to be extracted from sentences, which might have contributed to the variability of our findings. Considering the lack of power for comparability between the alternative formats, variability might also be purely due to chance. We further found an order effect for the facts box (format 1) and depression, where participants who first viewed the question on causality provided more correct replies to the general occurrence question afterward. The order effect just gained significance and the respective effect size (0.22) was small. As can be seen in the S2 Table in the Supporting Information, for the other three side effects we also saw a trend toward the same order effect for the facts box format. Although again speculative, the reason for this finding might be that, in contrast to the other two alternative formats, the facts box in its pure form does not provide any additional guidance by reading instructions or verbalizations of the findings next to the provision of the numbers. Studying the effect of people’s interpretation on the causality of “side effects” occurring equally or less often under drug intake by using a different terminology for describing “side effects,” and further using larger sample sizes for studying differences in the interpretation on the nature of side effects caused by the alternative PLs within future research, will likely help shed more light on the underlying mechanisms. PLs have been in the focus of various research activities. Particular attention has been paid, for example, to the wording used in PLs [24] or to which layout (font size, color, bold print) best supports the consumer’s ability to find and understand important information contained in PLs [25]. Our study adds to the existing body of research in that it is the first to show that alternative formats of PLs, such as facts boxes, in comparison to currently used standard PLs, considerably improve laypeople’s understanding of the proportion of side effects that are causally related to drug use. The study was sufficiently powered to detect differences between currently used standard and alternative PLs. Further, to the best of our knowledge, the study is also the first to test the format “narratives with numbers”—originally designed by Barron et al. [13]—with laypeople, although in a modified form. Patient empowerment has undoubtedly taken a huge step forward that pharmaceutical companies are required to provide PLs with data on side effects. Our research should therefore not be misunderstood as a proposal to include only side effects with proven causal relations. Retrieving reliable data on the causality of side effects that occur very rarely (<1/1000) is often impossible. It would require an enormous number of participants to determine such rare events in clinical trials, which is why these are usually identified by post-marketing surveillance activities. However, informed decision making would already be greatly enhanced if information on causality for side effects occurring in 1% or more people were included in PLs. Already today the FDA recommends that symptoms for which solid evidence has shown that these are not caused by the drug should not be included in the PL. It might also be worthwhile to present different adverse events sections in the PL: one for those supported by high quality evidence and one for other adverse events. Way et al. [26] conducted a survey asking patients with chronic conditions and the general public whether they desired medical information that had not undergone scientific analysis or if they preferred to wait until safety information had been further investigated. 51% of the patient group and 63% of the general public said they preferred receiving information on potential side effects as soon as there is a sign of a safety problem. These findings may support the idea of including different sections that distinguish between already proven side effects and adverse events still under investigation. As our study documents, current standard PLs lead to a high degree of misperception about side effects and hence pose a considerable threat to informed choices. As findings from focus group interviews revealed, people fear taking drugs after reading currently used PLs [18], what may increase the risk for not adhering to their medication [27]. Further, verbal expressions like “very common” or “common”—consistently used in current standard PLs—influence expectations of the occurrence of side effects even in combination with frequency expressions, for instance, among people from ethnic minorities, people with less education, or people with negative beliefs about medicines [28]. Expectations from verbal suggestions are found to trigger the nocebo effects [29]. Alternative PLs may potentially help to reduce fear and foster informed choices. Listing symptoms that are “prevented” by drug intake should not, however, result in a form of hidden advertising, as has occurred for some drugs. For instance, during the German benefit assessment of pharmaceuticals in accordance with the German Social Code, Book Five (SGB V), for the antidiabetic drug dapagliflozin, positive side effects such as weight loss and lower blood pressure were strongly emphasized [30] while long-term data on cardiovascular outcomes were missing. The fact that current PLs do not allow conclusions to be drawn on the causality between drug intake and the frequency of side effects has further consequences on physicians’ and other health professionals’ workload. Lacking solid information on drugs’ effectiveness in current PLs, physicians dedicated to initiating informed decision making in their patients are forced to search the scientific literature themselves to identify the extent of side effects caused by the specific drug. But many databases for adverse events are invalid because of nonsystematic assessment [31] and expectations (from investigators and patients) in clinical trials [32], incomplete reporting, missing transparency of clinical trial results, and spontaneous reporting after market entry. Köhler et al. [33] compared the proportion of adverse events of new drugs reported in various publication types. Whereas journal publications report 40% of all adverse events, registry reports contain 53% and EPAR (European public assessment reports) only 20%. Reporting of serious or drug-specific adverse events was even more scarce. The practical consequence of the present study is to substantially increase regulatory efforts to change the presentation of numerical information on the occurrence of side effects in PL in the best interest of patients. However, current developments on this issue are not encouraging. Despite increasing evidence that alternative forms as compared to the current form of presenting medical risk information can improve patients’ understanding, the FDA does not see the need for new regulations. Their reasoning is “that the inclusion of quantitative information about the risks and benefits of prescription drugs in a single standardized format would not broadly improve health care decision making” [17]. The EMA recently announced work on improving the PL to better meet the need of patients and healthcare professionals [10]. Yet, they do not specify how they will go about this task and how changes should be implemented. From an ethical and patient-centered perspective, these objections are hard to understand. Risking that patients refrain from taking beneficial drugs after overestimating side effects is unjustifiable. Our data clearly demonstrate the high degree of misunderstanding induced by current PLs and that it is possible to improve the PL format and, consequently, patients’ understanding. PLs urgently need to be refined in the interest of informed decision making. It is now up to regulatory agencies to take action. Limitations Our study needs to be viewed in the light of limitations. First, the study was neither designed nor powered to detect differences between the three alternative formats. However, power calculation was not possible because previous research providing sufficient information on what differences to expect between these aforementioned conditions was lacking. Given that each of the three alternative approaches used in our study has been suggested to improve people’s understanding of medical research [16, 19], we nevertheless believe that our exploratory approach to study the relative effectiveness of each of the three alternative format in comparison to the current standard PL in just on study is justified. Whereas the present study therefore cannot ascertain whether one of the three alternative formats is superior to the others on communicating side effects occurring with different frequencies, it nevertheless provides first insights on the relative effectiveness of these different alternative approaches in communicating side effects over the currently used approach. Second, we did not test the wording of the questions used in our study in previous focus groups. A certain number of participants misinterpreted the general occurrence question as a question on causality. The format of an online survey makes it impossible to identify whether this occurred due to the wording of the question in the survey or because the effect of presenting a comparison between people taking or not taking the drug already strongly implies causality that asking for anything beyond the true (causal) extent of side effects did not seem reasonable. Asking both questions concurrently might have made it easier for the participants to detect the different intentions behind both questions. Third, compared to the general population, participants in our survey were educated above average and younger. Our sample is therefore not representative, but we can rule out bias due to a health professional background. Further, participants who were randomized to the standard PL differed in their levels of education compared to the other groups. However, because the proportion of correct responses for that format differed so substantially from all three alternative formats, we are confident that our main findings are primarily explained by the way information is presented in the different PLs rather than by the difference in education. Fourth, we used data from placebo groups to illustrate how often different symptoms occur without drug use. Placebo group data are the best available surrogate for the occurrence of symptoms without drug intake. However, adverse events reported under placebo can sometimes be similar to those expected for active treatment [32]. The fact that participants in clinical trials are aware of being in a trial, receiving special attention and a form of “treatment”—be it an active drug or a placebo—can lead to positive (placebo) or negative (nocebo) symptoms that might not occur under daily life conditions. Therefore, all values presented in our PLs are only rough estimates, without acknowledging placebo and nocebo effects. It is an unsolved problem that data without placebo or nocebo effects cannot be obtained. However, we refrained from describing the drug as a placebo or sugar pill in our PLs to keep the text easily understandable for laypeople, and to acknowledge the fact that our previous research with health professionals found only minor improvements in understanding when providing a placebo column without any additional explanation [15]. Fifth, estimating the number of adverse events caused by drug intake through calculating the difference between people experiencing the side effect while taking the drug and those experiencing these events while taking a placebo might seem too simplistic. In contrast to people participating in RCTs, those taking the drug in “real life” are often prone to multimorbidity and polypharmacy and might experience deviating rates. However, the aim of our study was not to establish evidence on the true size of side effects under real circumstances, but to investigate which format would likely support people’s understanding of side effects. Sixth, we presented only four adverse events within the PLs of our study in order to keep time load for working through the survey acceptable and ensure sufficient participation rates. Tan et al. [14] analyzed information on side effects for 15 prescription drugs in PLs and other sources and found that the median of side effects listed in these documents ranged from 26 to 74.5. Further research needs to assess whether alternative PLs still improve understanding when they contain a larger number of side effects than depicted in our study. Conclusion Laypeople commonly assume a causal relation between drug intake and the frequency of side effects when reading current PLs. With these PLs, it is impossible to find information on the causality of side effects induced by drug intake. Our study showed that a considerable number of people confronted with standard PL overestimate the extent of side effects. Considering that these PLs may leave a considerable number of people in fear and potentially affect their adherence to prescribed drugs, the current situation not only undermines informed decision making but asks for a profound change in how information is given in package leaflets. Our study demonstrated that alternative formats exist that would help people to better understand information on side effects. Yet most people are probably not even aware of the fact that substantial information is lacking in current PLs. Including all relevant information on side effects for both intervention and control group might require more space than available on current standard PLs; yet, we believe that thinking of new layout formats is more justified than withholding relevant information from patients. We now need the commitment of regulatory instances to change existing standard PLs and include comparative information on people taking and not taking the drug, in the best interest of patients and of health care. S1 Table. Excerpt from Barron et al.: The proportion of side-effects on beta-blocker that are caused by being on beta-blocker. doi:10.1371/journal.pone.0203800.s011 (PDF) S2 Table. Distribution of participants’ responses on the general occurrence of side effects during drug intake for each format. doi:10.1371/journal.pone.0203800.s012 (PDF) S3 Table. Distribution of participants’ responses on the causal relation between drug intake and side effects for each format. doi:10.1371/journal.pone.0203800.s013 (PDF) S4 Table. Effect of the order of the question of general occurrence on the proportion of participants’ correct responses in dependence on whether participants first received the question on general. doi:10.1371/journal.pone.0203800.s014 (PDF) S5 Table. Effect of the order of the question of causality on the proportion of participants’ correct responses in dependence on whether participants first received the question on causality and then the question on general occurrence or vice versa occurrence and then the question on causality or vice versa. doi:10.1371/journal.pone.0203800.s015 (PDF) Acknowledgments We would like to thank the working group “Package leaflet” of the Network for Evidence-based Medicine (www.ebm-netzwerk.de) for their intellectual support on this work. 11. European Parliament, Council of the European et al. Directive 2001/83/EC of the European Parliament and of the Council of 6 November 2001 on the Community code relating to medicinal products for human use. Official Journal of the European Communities."}
{"url": "https://en.m.wikipedia.org/wiki/Special:BookSources/978-1-55617-984-6", "text": "This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). If you arrived at this page by clicking an ISBN link in a Wikipedia page, you will find the full range of relevant search links for that specific book by scrolling to the find links below. To search for a different book, type that book's individual ISBN into this ISBN search box. Spaces and hyphens in the ISBN do not matter. Also, the number starts after the colon for \"ISBN-10:\" and \"ISBN-13:\" numbers. An ISBN identifies a specific edition of a book. Any given title may therefore have a number of different ISBNs. See #Find other editions below for finding other editions. An ISBN registration, even one corresponding to a book page on a major book distributor database, is not definite proof that such a book actually exists. A title may have been cancelled or postponed after the ISBN was assigned. Check to see if the book exists or not. Google Books and Amazon.com may be helpful if you want to verify citations in Wikipedia articles, because they often let you search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available). At the Open Library (part of the Internet Archive) you can borrow and read entire books online. Luxembourg Montenegro Netherlands Find this book in the Dutch-Union Catalogue that searches simultaneously in more than 400 Dutch electronic library systems (including regional libraries, university libraries, research libraries and the Royal Dutch library) Book-swapping websites Non-English book sources If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language."}
{"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3151731/", "text": "Share RESOURCES As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more: PMC Disclaimer | PMC Copyright Notice Associated Data Abstract BACKGROUND Specific dietary and other lifestyle behaviors may affect the success of the straightforward-sounding strategy “eat less and exercise more” for preventing long-term weight gain. METHODS We performed prospective investigations involving three separate cohorts that included 120,877 U.S. women and men who were free of chronic diseases and not obese at baseline, with follow-up periods from 1986 to 2006, 1991 to 2003, and 1986 to 2006. The relationships between changes in lifestyle factors and weight change were evaluated at 4-year intervals, with multivariable adjustments made for age, baseline body-mass index for each period, and all lifestyle factors simultaneously. Cohort-specific and sex-specific results were similar and were pooled with the use of an inverse-variance–weighted meta-analysis. CONCLUSIONS Specific dietary and lifestyle factors are independently associated with long-term weight gain, with a substantial aggregate effect and implications for strategies to prevent obesity. (Funded by the National Institutes of Health and others.) Because efforts to lose weight POSE tremendous challenges, primary prevention of weight gain is a global priority. Since weight stability requires a balance between calories consumed and calories expended, the advice to “eat less and exercise more” would seem to be straightforward. However, weight gain often occurs gradually over decades (about 1 lb per year), making it difficult for most people to perceive the specific causes. Weight-loss trials1–3 have typically enrolled obese or overweight persons who attempted substantial short-term weight loss on specialized diets, thus limiting the generalizability of the findings to nonobese populations and to the factors that determine long-term, gradual weight gain. Several lifestyle behaviors may influence whether or not a person can maintain energy balance over the long term. For instance, the consumption of sugar-sweetened beverages, sweets, and processed foods may make it harder to do so, whereas the consumption of whole grains, fruits, and vegetables might make it easier.4–10 Physical activity should also influence long-term weight gain, but evidence to support this expectation has been surprisingly inconsistent.11–14 In addition, the duration of television viewing and of sleep may influence energy consumption, energy expenditure, or both.15–19 Different lifestyle behaviors have often been evaluated separately, thus limiting relative comparisons or the quantification of combined effects. In addition, most studies of long-term weight gain have evaluated current behaviors, but changes in behavior over time may be more relevant in terms of both their biologic effects on long-term weight gain and their translation into prevention strategies. We investigated the relationship between multiple lifestyle changes, both independently and jointly, and long-term weight gain in nonobese women and men participating in three separate, prospective studies. METHODS STUDY DESIGN AND POPULATION The Nurses’ Health Study (NHS) is a prospective study of a cohort of 121,701 female registered nurses from 11 U.S. states who were enrolled in 1976. The Nurses’ Health Study II (NHS II) is a prospective study of a cohort of 116,686 younger female registered nurses from 14 states who were enrolled in 1989. The Health Professionals Follow-up Study (HPFS) is a prospective study of a cohort of 51,529 male health professionals from all 50 states, enrolled in 1986. Participants were followed with the use of biennial validated questionnaires concerning medical history, lifestyle, and health practices. For this analysis, the baseline year was the first year for which detailed information was available on diet, physical activity, and smoking habits — 1986 in the NHS and HPFS and 1991 in the NHS II. We excluded participants with obesity, diabetes, cancer, or cardiovascular, pulmonary, renal, or liver disease at baseline; those for whom baseline data on lifestyle habits were missing; those with an implausible energy intake (<900 or >3500 kcal per day); those with more than nine blank responses on the diet questionnaire; those who were newly pregnant during follow-up; and those who were over 65 years of age, given possible confounding due to age-related loss of lean muscle mass. The final analyses included 50,422 women in the NHS, 47,898 women in the NHS II, and 22,557 men in the HPFS, all of whom were free of obesity and chronic diseases and for whom data on weight and lifestyle habits at baseline were complete. Cohort members who were excluded because of missing data had characteristics similar to those included in the analysis (data not shown). The funders of this study had no role in its design or conduct; in the collection, management, analysis, or interpretation of the data; or in the preparation, review, or approval of the manuscript. LIFESTYLE ASSESSMENT Lifestyle habits of interest were physical activity, television watching, alcohol use, sleep duration, and diet, and cigarette smoking was a potential confounding factor (Table 1, and Tables 1 and 2 in the Supplementary Appendix, available with the full text of this article at NEJM.org). On the basis of their plausible biologic effects, the dietary factors we assessed included fruits, vegetables, whole grains, refined grains, potatoes (including boiled or mashed potatoes and french fries), potato chips, whole-fat dairy products, low-fat dairy products, sugar-sweetened beverages, sweets and desserts, processed meats, unprocessed red meats, fried foods, and trans fat (see Table 1 in the Supplementary Appendix). We also evaluated nuts, 100%-fruit juices, diet sodas, and subtypes of dairy products and potatoes. Different types of alcohol drinks were also evaluated. To assess aggregate dietary effects, changes in each dietary factor independently associated with weight gain were categorized in quintiles and assigned ascending values (1 to 5) or descending values (5 to 1) for habits inversely or positively associated with weight gain, respectively; these ordinal values were summed to generate an overall score for dietary change. Table 1 Baseline Characteristics and Average 4-Year Lifestyle Changes among 120,877 U.S. Women and Men in Three Prospective Cohorts.* *Plus–minus values are means ±SD. Data are based on 20 years of follow-up (1986–2006) in the Nurses’ Health Study (NHS), 12 years of follow-up (1991–2003) in the Nurses’ Health Study II (NHS II), and 20 years of follow-up (1986–2006) in the Health Professionals Follow-up Study (HPFS). Usual dietary habits and alcohol use were assessed every 4 years with the use of validated, semiquantitative food-frequency questionnaires. Alcohol included wine (5 oz), beer (1 glass, bottle, or can), and liquor (1 drink or shot). Other specific foods and beverages in each category are listed in Table 1 in the Supplementary Appendix. Results for smoking habits (assessed with the use of biennial questionnaires), trans fat, and fried foods can be found in Table 2 in the Supplementary Appendix. To convert pounds to kilograms, divide by 0.45. †Because serial assessments were limited, the 4-year changes could not be reliably quantified, and absolute levels at baseline were used for cohort-specific analyses. ‡The corresponding values for relative weight changes were 1.83% (5th to 95th percentile, −4.0 to 7.7) in the NHS, 3.60% (5th to 95th percentile, −1.80 to 10.7) in the NHS II, and 1.00% (5th to 95th percentile, −3.11 to 5.09) in the HPFS. §Physical activity was assessed by means of validated questionnaires every 2 years, with average energy expenditure (metabolic-equivalent [MET]-hours per week) for specific activities (e.g., walking, jogging, bicycling, swimming, racquet sports, and gardening). In the NHS II, physical activity levels in 1997, 2001, and 2005 were used to impute the levels in 1995, 1999, and 2003, respectively. ¶The average duration of sleep per 24-hour period was assessed in 1986, 2000, and 2002 in the NHS; in 2001 in the NHS II; and in 1987 and 2000 in the HPFS. ||The average number of hours per week spent watching television at home was assessed in 1992 and 2004 in the NHS; in 1991, 1997, and 2001 in the NHS II; and in 1998 and every 2 years thereafter in the HPFS. WEIGHT CHANGES Height and weight were assessed by questionnaire at enrollment, and weight was requested on each follow-up questionnaire. In a validation subsample, questionnaire-reported and staff-measured weights were highly correlated (r = 0.96; mean difference, 3.3 lb). Weight changes were evaluated every 4 years as both absolute changes (pounds) and relative changes (percentages). Anthropometric measurements and weight changes strongly predict disease outcomes in these cohorts.20–26 STATISTICAL ANALYSIS We assessed the independent relationships between changes in lifestyle behaviors and weight changes within 4-year periods over a period of 20 years in the NHS, 12 years in the NHS II, and 20 years in the HPFS (Table 1), using multivariable linear regression with robust variance and accounting for within-individual repeated measures. Lifestyle changes were assessed either as continuous variables, with censoring of data at the 0.5 and 99.5 percentiles to minimize the influence of outliers, or as indicator variables for categorical behaviors (e.g., smoking status). Potential nonlinear effects of decreases versus increases in each behavior were evaluated by modeling changes in indicator categories, with “no change” as the reference. Any missing lifestyle data during any follow-up period were coded as a missing indicator category for categorical variables (e.g., smoking status) and with carried-forward values for continuous variables. To minimize confounding from the loss of lean muscle mass at older ages or from loss of weight due to undiagnosed chronic disease, we censored data for participants after they reached 65 years of age or if they received a diagnosis of chronic pulmonary, renal, or liver disease or of cancer other than nonmelanoma skin cancer (3 to 4% of participants); data were censored 6 years before diagnosis to account for preclinical disease. Multivariable models were used to adjust for age, baseline body-mass index in each 4-year period, and all lifestyle factors simultaneously. Total energy intake, biologic factors (e.g., blood pressure), and medications were not included as covariables because such factors could be mediators (in causal pathways) or direct correlates of mediators of the effects of lifestyle on weight gain. Sensitivity analyses were performed to evaluate absolute physical activity levels at the start of each 4-year period rather than changes during the period, data among participants who never smoked, and data stratified according to age and baseline body-mass index. Findings across cohorts were pooled by means of inverse-variance–weighted, random-effects meta-analyses. Analyses were carried out with the use of SAS software, version 9.1 (SAS Institute), at a two-tailed alpha level of 0.05. RESULTS BASELINE CHARACTERISTICS AND WEIGHT GAIN Weight and lifestyle characteristics at baseline and changes during 1,570,808 person-years of follow-up are shown in Table 1. The mean weight gain for all the 4-year periods combined differed among the three cohorts, a finding that may have been related to cohort-specific differences in sex and age at baseline. The mean weight gains were as follows: 1.63 lb (5th to 95th percentile, −5.0 to 8.0) for the men in the HPFS (mean age, 50.8±7.5 years), 2.33 lb (5th to 95th percentile, −5.5 to 10.7) for the women in the NHS (mean age, 52.2±7.2), and 5.24 lb (5th to 95th percentile, −2.3 to 16.3) for the women in the NHS II (mean age, 37.5±4.1). The average weight gain across the cohorts was 3.35 lb (5th to 95th percentile, −4.1 to 12.4), or 2.4% of body weight (5th to 95th percentile, −3.0 to 8.4), during each 4-year period; this change corresponds to a weight gain of 16.8 lb over a period of 20 years. DIET AND LIFESTYLE CHANGES Although the mean changes in lifestyle in the overall study population were small, the between-individual changes were large (Table 1). In the NHS, for example, the difference between persons in the upper level of change and those in the lower level of change (95th percentile minus 5th percentile) was 3.1 servings per day for vegetable consumption, 25.3 metabolic equivalents (METs) per week for physical activity, and 0.66 drinks per day for alcohol consumption. Correlations between various lifestyle changes were generally small (r<0.05). Positive correlations in changes were largest for fruits and vegetables (r = 0.21) and for processed meats and unprocessed red meats (r = 0.21), whereas inverse correlations in changes were largest for whole-fat dairy products and low-fat dairy products (r = −0.08). RELATIONSHIPS BETWEEN DIETARY CHANGES AND WEIGHT CHANGES After multivariable adjustment, nearly every dietary factor was independently related to weight change (Table 2). Findings were similar, in direction and magnitude, for men and women and across the three cohorts (Fig. 1). (For additional detailed sex-specific and cohort-specific results, see Tables 3, 4, and 5 in the Supplementary Appendix.) The dietary factors with the largest positive associations with weight changes, per serving per day, were increases in the consumption of potato chips (1.69 lb), potatoes (1.28 lb), sugar-sweetened beverages (1.00 lb), unprocessed red meats (0.95 lb), and processed meats (0.93 lb). A secondary analysis of potato subtypes showed that weight changes were positively associated with increases in the consumption of french fries (3.35 lb) and of boiled, baked, or mashed potatoes (0.57 lb). Weight gain associated with increased consumption of refined grains (0.39 lb per serving per day) was similar to that for sweets and desserts (0.41 lb per serving per day). Inverse associations with weight gain, per serving per day, were seen for increased consumption of vegetables (−0.22 lb), whole grains (−0.37 lb), fruits (−0.49 lb), nuts (−0.57 lb), and yogurt (−0.82 lb). Relationships between Changes in Food and Beverage Consumption and Weight Changes Every 4 Years, According to Study Cohort Study participants included 50,422 women in the Nurses’ Health Study (NHS), followed for 20 years (1986 to 2006); 47,898 women in the Nurses’ Health Study II (NHS II), followed for 12 years (1991 to 2003); and 22,557 men in the Health Professionals Follow-up Study (HPFS), followed for 20 years (1986 to 2006). Weight changes are reported for each increase in the daily serving of the food or beverage; decreased intake would be associated with the inverse weight changes. There was little evidence of a significant interaction between diet and physical activity (P>0.10 for the interaction in each cohort). All weight changes were adjusted simultaneously for age, baseline body-mass index, sleep duration, and changes in smoking status, physical activity, television watching, alcohol use, and all of the dietary factors shown. The P value is less than 0.001 for all dietary factors with the exception of butter in the NHS II, cheese in the NHS and NHS II, low-fat or skim milk in the NHS and HPFS, diet soda in the NHS, and whole-fat milk in all three cohorts. Table 2 Pooled, Multivariable-Adjusted Results for the Relationships between Changes in Dietary Habits and Weight Change.* *Data are based on 20 years of follow-up (1986–2006) in the Nurses Health Study, 12 years of follow-up (1991–2003) in the Nurses Health Study II, and a 20 years of follow-up (1986–2006) in the Health Professionals Follow-up Study. Findings according to sex and within each study were generally similar to the pooled results (see the tables in the Supplementary Appendix). To convert pounds to kilograms, divide by 0.45. †The weight changes shown are for increased consumption; decreased consumption would be associated with the inverse of these weight changes. Increased consumption was defined as an increase in the number of servings per day for all items except trans fat (an increase in the percent of energy) and fried foods consumed at home or away from home (an increase in the number of servings per week). ‡Values were adjusted for age, baseline body-mass index at the beginning of each 4-year period, and sleep duration, as well as for changes in physical activity, alcohol use, television watching, smoking, and all the dietary factors in the table simultaneously. §For the categories of whole-fat dairy foods, low-fat dairy foods, and potatoes, subtypes were evaluated together in the full, multivariable-adjusted model in place of the overall food group (e.g., butter, cheese, and whole-fat milk were evaluated in place of total whole-fat dairy foods). ¶We were unable to evaluate separately the different types of yogurt (e.g., nonfat, low-fat, or whole-fat; sweetened or unsweetened). U.S. consumption patterns would suggest that most participants chose nonfat or low-fat yogurt; however, no inference could be made with regard to sweetened or unsweetened yogurt. ||Findings were similar when either total dietary fiber or cereal fiber was included in the analysis instead of whole grains. Categorical analyses of each dietary factor revealed similar linear relationships for increased versus decreased consumption (data not shown) — that is, for each dietary factor, the weight change with increased consumption was the inverse of that with decreased consumption. Thus, less weight gain occurred with decreased consumption of potato chips, processed meats, sugar-sweetened beverages, potatoes, or trans fat, and more weight gain occurred with decreased consumption of vegetables, whole grains, fruits, nuts, or yogurt. Aggregate dietary changes were robustly related to weight gain in a dose-dependent fashion, with a 3.93-lb greater weight gain across quintiles (Table 3). Table 3 Pooled, Multivariable-Adjusted Results for the Relationships between Changes in Lifestyle Habits and Weight Change.* *Data are based on a 20-year follow-up (1986–2006) in the Nurses’ Health Study (NHS), a 12-year follow-up (1991–2003) in the Nurses’ Health Study II (NHS II), and a 20-year follow-up (1986–2006) in the Health Professionals Follow-up Study (HPFS). Findings within each study and restricted to persons who never smoked were generally similar to these pooled results (see the tables in the Supplementary Appendix). To convert pounds to kilograms, divide by 0.45. †Values were adjusted for age, baseline body-mass index at the beginning of each 4-year period, and all the variables shown in the table simultaneously. ‡Changes in dietary habits associated with weight change were derived by totaling the ordinal values for the quintiles of change for each dietary habit in ascending order (1 to 5) or descending order (5 to 1) for habits that were inversely or positively associated with weight gain, respectively. Scores ranged from 17 to 85. ¶Data are based on absolute levels, owing to limited data on serial assessments of sleep for each cohort to assess change in each 4-year period. ||Differences in weight change are for increased television watching; the inverse difference would be associated with decreased watching. Values are based on data from the NHS II and the HPFS; limited serial assessments precluded an analysis of changes in television watching in the NHS. RELATIONSHIPS BETWEEN OTHER LIFESTYLE FACTORS AND WEIGHT CHANGES Other lifestyle behaviors were also independently related to weight change (Table 3), with similar findings in men and women and across the three cohorts. (For sex-specific and cohort-specific results, see Tables 6, 7, and 8 in the Supplementary Appendix.) Across quintiles, participants with greater increases in physical activity gained 1.76 fewer pounds within each 4-year period. Absolute levels of physical activity, rather than changes in these levels, were not associated with weight change (data not shown). Overall, increases in alcohol use (per drink per day) were positively associated with weight change (0.41 lb), but heterogeneity was evident with respect to both the beverage type and the size and direction of changes in use (see the figure in the Supplementary Appendix). Sleep duration had a U-shaped association with weight gain, with greater weight gain occurring with less than 6 hours or more than 8 hours of sleep per night. Increases in time spent watching television (per hour per day) were independently associated with weight gain (0.31 lb, P<0.001). As compared with persons who never smoked, those who had quit smoking within the previous 4 years had a weight gain of 5.17 lb (Table 3). Subsequent weight gain for former smokers was small (0.14 lb per 4-year period). Continued smoking was inversely associated with weight gain (−0.70 lb), a finding that may have been related to undiagnosed chronic disease. Initiation of smoking was not associated with weight change, but evaluation of this category was limited by its rarity in these populations (accounting for <0.1% of person-years). Findings for other lifestyle factors were similar when restricted to persons who never smoked (Table 3). ADDITIONAL ANALYSES We categorized dietary changes more finely to examine wider ranges of potential effects (Fig. 2). As compared with participants in the top decile, participants in each lower decile of dietary change had greater weight gain, in a dose-dependent fashion. Changes in diet and physical activity were independently associated with weight gain (Fig. 2). Findings for all lifestyle factors were generally similar in analyses stratified according to age or baseline body-mass index (the weight in kilograms divided by the square of the height in meters) (<25 or 25 to 30), although the magnitude of associated weight change was generally larger among overweight persons (Tables 9 and 10 in the Supplementary Appendix). All results were also similar when we evaluated relative (percent) weight changes rather than absolute weight changes (not shown). Relationships between Changes in Diet and Physical Activity and Weight Changes within Each 4-Year Period in the Three Cohorts In a multivariable-adjusted analysis, overall dietary changes among the 120,877 men and women in the three cohorts were based on the sum of changes in the intake of fruits, vegetables, whole grains, nuts, refined grains, potatoes or french fries, potato chips, butter, yogurt, sugar-sweetened beverages, 100%-fruit juice, sweets and desserts, processed meats, unprocessed red meats, trans fat, fried foods consumed at home, and fried foods consumed away from home. Panel A shows the relationship between deciles of dietary change and weight change per 4-year period in the three cohorts separately and combined. As compared with persons in the top decile, persons in the bottom decile had a 5.48-lb greater weight gain (95% confidence interval [CI], 4.02 to 6.94). Panel B shows the relationship between the cross-stratified quintiles of changes in both dietary habits and physical activity with weight changes per 4-year period for the combined cohorts. As compared with persons in the top quintiles of both dietary change and physical-activity change, persons in the lowest quintiles had a 5.93-lb greater weight gain (95% CI, 4.35 to 7.52). There was little evidence of a significant interaction between diet and physical activity (P>0.10 for the interaction in each cohort). All weight changes were adjusted for age, baseline body-mass index, sleep duration, and changes in smoking status, physical activity, television watching, and alcohol use. P<0.001 for all comparisons. DISCUSSION We found that multiple lifestyle changes were independently associated with long-term weight gain, including changes in the consumption of specific foods and beverages, physical activity, alcohol use, television watching, and smoking habits. Average long-term weight gain in nonobese populations is gradual — in the cohorts we studied, about 0.8 lb per year — but accumulated over time, even modest increases in weight have implications for long-term adiposity-related metabolic dysfunction, diabetes, cardiovascular disease, and cancer.21–24 Whereas weight changes associated with any single lifestyle factor were relatively modest in our three cohorts, in the aggregate, changes in diet and physical activity accounted for large differences in weight gain. The results were similar across the three separate cohorts, increasing our confidence in the validity and generalizability of the findings. All these relationships must be mediated by changes in energy intake, energy expenditure, or both. Total energy intake is not well estimated from dietary questionnaires, nor does it reflect energy balance, which is necessarily codetermined by energy expenditure. Thus, weight change is the best population metric of energy imbalance and at least partly captures energy intake after adjustment for determinants of expenditure (e.g., age, body-mass index, and physical activity). Eating more or less of any one food or beverage may change the total amount of energy consumed, but the magnitude of associated weight gain varied for specific foods and beverages. Differences in weight gain seen for specific foods and beverages could relate to varying portion sizes, patterns of eating, effects on satiety, or displacement of other foods or beverages. Strong positive associations with weight change were seen for starches, refined grains, and processed foods. These findings are consistent with those suggested by the results in limited short-term trials: consumption of starches and refined grains may be less satiating, increasing subsequent hunger signals and total caloric intake, as compared with equivalent numbers of calories obtained from less processed, higher-fiber foods that also contain healthy fats and protein.27 Consumption of processed foods that are higher in starches, refined grains, fats, and sugars can increase weight gain.28–30 Some foods — vegetables, nuts, fruits, and whole grains — were associated with less weight gain when consumption was actually increased. Obviously, such foods provide calories and cannot violate thermodynamic laws. Their inverse associations with weight gain suggest that the increase in their consumption reduced the intake of other foods to a greater (caloric) extent, decreasing the overall amount of energy consumed. Higher fiber content and slower digestion of these foods would augment satiety, and their increased consumption would also displace other, more highly processed foods in the diet, providing plausible biologic mechanisms whereby persons who eat more fruits, nuts, vegetables, and whole grains would gain less weight over time. Yogurt consumption was also associated with less weight gain in all three cohorts. Potential mechanisms for these findings are unclear; intriguing evidence suggests that changes in colonic bacteria might influence weight gain.31 It is also possible that there is an unmeasured confounding factor that tracks with yogurt consumption (e.g., people who change their yogurt consumption may have other weight-influencing behaviors that were not measured by our instruments). Our findings with regard to sugar-sweetened beverages are consistent with the results of prior observational studies and short-term interventions.7,32,33 Consumption of 100%-fruit juice was associated with weight gains of smaller magnitude, possibly because these beverages may be consumed in smaller servings than are sugar-sweetened beverages or in different patterns (i.e., single rather than multiple servings).33 Findings have been inconsistent in prior studies of alcohol use and weight gain.34–37 In a previous analysis of alcohol consumption in relation to weight change in the NHS II cohort over a period of 8 years, the smallest weight gain was seen among women who remained moderate drinkers.36 The present findings suggest that the relationship between alcohol use and weight change is complex, and further analyses are needed that address potential heterogeneity with respect to sex, beverage type, baseline intake, direction of change, and duration of follow-up. Short-term controlled trials suggest that liquids are less satiating than solid foods, increasing the total amount of energy consumed.38 Overall, our analysis showed that changes in the consumption of all liquids except milk were positively associated with weight gain; our findings for high-carbohydrate beverages were consistent with those for refined carbohydrates and starches consumed in foods. Temporal trends render our findings especially relevant: between 1965 and 2002, U.S. beverage consumption increased from 11.8 to 21.0% of all calories consumed — 222 more kilocalories per person per day — with sugar-sweetened beverages and alcohol accounting for 60% and 32% of the increase, respectively.39 Our analysis showed relatively neutral associations between change in the consumption of most dairy foods and weight gains. Few prior studies have evaluated these relationships. Prior analyses of HPFS data showed associations similar to ours for the overall categories of whole-fat and low-fat dairy products,40 but subtypes (e.g., milk, cheese, and butter) were not evaluated independently. Among Swedish women, higher intakes of whole milk and cheese were inversely associated with weight gain; as in our study, significant associations with weight gain were not seen for other dairy foods.41 In several long-term studies, inverse associations between dairy consumption and the risk of insulin resistance, the metabolic syndrome, or diabetes were observed,42,43 but potential mediating effects on weight change were not evaluated. Limited short-term studies of dairy foods and satiety or weight change have had inconsistent results.44,45 Overall, our analysis showed divergent relationships between specific foods or beverages and long-term weight gain, suggesting that dietary quality (the types of foods and beverages consumed) influences dietary quantity (total calories). Several dietary metrics that are currently emphasized, such as fat content, energy density, and added sugars, would not have reliably identified the dietary factors that we found to be associated with long-term weight gain. For example, most of the foods that were positively associated with weight gain were starches or refined carbohydrates; no significant differences were seen for low-fat and skim milk versus whole-fat milk, and the consumption of nuts was inversely associated with weight gain. Clear patterns were also not seen in the relationship between weight change and the energy density of dietary components (e.g., beverages of low energy density were strongly associated with weight gain). Foods that contained higher amounts of refined carbohydrates — whether these were added (e.g., in sweets and desserts) or were not added (e.g., in refined grains) — were associated with weight gain in similar ways, and potato products (which are low in sugars and high in starches) showed the strongest associations with weight gain. No single metric appears to capture these complexities. Our findings highlight gaps in our mechanistic understanding of how particular dietary characteristics alter energy balance, suggesting directions for future research regarding pathways involved in hunger, satiety, absorption, metabolism, and adipocyte growth or hyperplasia. In general, changes in the consumption of refined or processed foods and liquid carbohydrates or alcohol were positively associated with weight gain, whereas changes in the consumption of unprocessed foods such as whole grains, fruits, nuts, and vegetables were inversely associated with weight gain. These results suggest that future policies and research efforts to prevent obesity should consider food structure and processing as potentially relevant dietary metrics. Changes in physical activity were independently related to long-term changes in weight, supporting the biologic plausibility of our overall findings. Prior, smaller studies have shown inverse associations between activity changes and weight change.11,13 Prevalent (current) levels of physical activity are inconsistently related to weight change, with associations observed only for subgroups of persons14 or subtypes of activities.12 As seen in prior analyses of sugar-sweetened beverages,33 changes in lifestyle may be most relevant for weight gain. Persons may achieve a new steady-state weight within months after a change in regular physical activity, diet, or other lifestyle habits, highlighting the importance of repeated assessments of over time to discern long-term effects. Many prior studies of television watching and obesity have been cross-sectional, limiting the ability to make inferences about which came first.15 In controlled interventions, decreased television watching reduced weight gain in children,16,17 an effect that was mediated more by improvements in dietary habits than by a change in physical activity. Television watching appears to encourage snacking during viewing and also influences food choices both during viewing and at other times.46–52 Our long-term prospective evaluation provides evidence that both the duration of television viewing and changes in the duration of viewing influence weight gain in adults. Because these effects are probably mediated by changes in diet and physical activity, and may also be mediated by changes in sleep, the multivariable (mediator)–adjusted associations may underestimate the full effects of television watching. Decreases in sleep duration are concordant with the U.S. obesity epidemic.18,19,53 Data from cross-sectional studies and some prospective studies, including a prior analysis of NHS data, support the relationship of shorter sleep duration with obesity.18 In short-term trials, reduced sleep alters leptin, ghrelin, subjective hunger, and preferences for calorie-dense, refined-carbohydrate foods.19 Our results suggest that the association between sleep duration and long-term weight gain is characterized by a U-shaped curve — that is, weight gain is lowest among persons who sleep 6 to 8 hours a night and is higher among those who sleep less than 6 hours or more than 8 hours. Future studies should evaluate how changes in sleep over time are related to weight gain. Our long-term follow-up data confirm prior observations that smoking cessation results in weight gain initially but in little weight change thereafter. The health benefits of cessation exceed any potential adverse effects — that is, active smokers are at higher risk for cardiovascular diseases, cancer, and diabetes than are former smokers.54 Smoking may also adversely alter the distribution of body fat, promoting visceral rather than femoral or subcutaneous fat deposition; thus, even in the setting of lower total weight, active smoking has adverse metabolic consequences, as evidenced, for example, by its links to a higher risk of type 2 diabetes. 55 Any relative weight loss seen with active smoking should not be considered beneficial, nor should the relative weight gain soon after smoking cessation be considered harmful. Our study has some limitations. Although dietary questionnaires specified portion sizes, residual, unmeasured differences in portion sizes among participants might account for additional independent effects on energy balance. For example, an average, large baked potato contains 278 calories, as compared with 500 to 600 calories for a large serving of french fries.56 The typical portion size of a specific food or beverage may therefore partly mediate its effects on weight gain (i.e., both average portion sizes and biologic effects). As for lifestyle behaviors, each was measured with some degree of error, which, if random, would underestimate their true relationships with weight change. Lifestyle changes were self-selected, and residual confounding from other lifestyle behaviors is possible. However, in contrast to prevalent behaviors, changes in these behaviors were generally not strongly correlated (r< 0.05), which suggests that different behaviors are often changed relatively independently, thus minimizing potential confounding. A person’s weight change could lead to changes in lifestyle rather than vice versa. Such reverse causation would generally underestimate true effects. For example, persons who are gaining weight might plausibly either reduce their intake of sugar-sweetened beverages and sweets or increase their consumption of vegetables, leading to reverse bias with respect to the observed associations. As is the case with any biologic finding or medical intervention, our results represent the average population effect, and intraindividual variations exist. The cohorts studied here largely comprised white, educated U.S. adults, which potentially limits the generalizability of the findings. Conversely, the ranges of dietary intakes were broad and overlapped with national estimates. In addition, our findings were broadly consistent with cross-sectional national trends with respect to diet and obesity: between 1971 and 2004, the average dietary intake of calories in the United States increased by 22% among women and by 10% among men, primarily owing to the increased consumption of refined carbohydrates, starches, and sugar-sweetened beverages.39 Our findings were also consistent among the three cohorts and in analyses stratified according to smoking status, age, and baseline body-mass index, and it seems plausible that the biologic effects of many lifestyle factors would be qualitatively similar in other populations. A habitual energy imbalance of about 50 to 100 kcal per day may be sufficient to cause the gradual weight gain seen in most persons.57,58 This means that unintended weight gain occurs easily but also that modest, sustained changes in lifestyle could mitigate or reverse such an energy imbalance. Our findings suggest that both individual and population-based strategies to help people consume fewer calories may be most effective when particular foods and beverages are targeted for decreased (or increased) consumption. Aggregate dietary changes accounted for substantial differences in weight gain, with additional contributions from changes in physical activity and television watching, thus highlighting specific lifestyle changes that might be prioritized in obesity-prevention strategies. Supplementary Material Supplement1 Acknowledgments Supported by grants (DK46200, DK58845, HL085710, HL60712, HL35464, CA87969, CA50385, CA55075, and CA95589) from the National Institutes of Health and by the Searle Scholars Program. Dr. Mozaffarian reports receiving consulting fees from Nutrition Impact and Foodminds, lecture fees from Aramark, Unilever, and SPRIM, royalties from UpToDate, and grant support on behalf of Harvard Medical School from GlaxoSmithKline, Sigma-Tau, and Pronova and being listed as a coinventor on a provisional patent application filed by and assigned to Harvard University for the use of trans-palmitoleic acid to prevent and treat insulin resistance, type 2 diabetes, and related conditions. Dr. Hu reports receiving lecture fees from Amgen, Nutrition Impact, Unilever, and the Institute of Food Technologies and grant support on behalf of the Harvard School of Public Health from Merck and the California Walnut Commission. We thank Dr. Donna Spiegelman for statistical advice, Dr. Francine Laden for comments during the preparation of an early version of the manuscript, and Dr. Peilin Shi for assistance with statistical programming. Footnotes No other potential conflict of interest relevant to this article was reported. Disclosure forms provided by the authors are available with the full text of this article at NEJM.org. 48. Utter J, Scragg R, Schaaf D. Associations between television viewing and consumption of commonly advertised foods among New Zealand children and young adolescents. Public Health Nutr. 2006;9:606–12. [PubMed] [Google Scholar] 54. Office on Smoking and Health National Center for Chronic Disease Prevention and Health Promotion. Women and smoking: a report of the Surgeon General. Atlanta: Centers for Disease Control and Prevention; Mar, 2001. [Google Scholar]"}
{"url": "https://en.m.wikipedia.org/wiki/The_Open_University", "text": "The OU was established in 1969 and was initially based at Alexandra Palace, north London, using the television studios and editing facilities which had been vacated by the BBC. The first students enrolled in January 1971.[12] The university administration is now based at Walton Hall, but has administration centres in other parts of the United Kingdom. It also has a presence in other European countries. The university awards undergraduate and postgraduate degrees, as well as non-degree qualifications such as diplomas and certificates or continuing education units. It also offers unique Open Degrees, in which students may study any combination of modules across all subjects. With more than 208,308 students enrolled,[5] including around 34% of new undergraduates aged under 25[13] and more than 8,599 overseas students,[5] it is the largest academic institution in the United Kingdom (and one of the largest in Europe) by student number, and qualifies as one of the world's largest universities. Since it was founded, more than 2.3 million students have achieved their learning goals by studying with the Open University.[13] The Open University is one of only two[a] United Kingdom higher education institutions to gain accreditation in the United States of America by the Middle States Commission on Higher Education.[14] It also produces more CEOs than any other UK university.[15] Former UK Prime Minister Gordon Brown, astrophysicist Jocelyn Bell Burnell, broadcaster Anna Ford and actress Glenda Jackson are among those who have tutored for the OU.[16][17] The Open University was founded by the Labour government under Prime Minister Harold Wilson. Wilson was a strong advocate, using the vision of Michael Young. Planning commenced in 1965 under Minister of State for EducationJennie Lee, who established a model for the OU as one of widening access to the highest standards of scholarship in higher education, and set up a planning committee consisting of university vice-chancellors, educationalists and television broadcasters, chaired by Sir Peter Venables. The British Broadcasting Corporation (BBC) Assistant Director of Engineering at the time James Redmond, had obtained most of his qualifications at night school, and his natural enthusiasm for the project did much to overcome the technical difficulties of using television to broadcast teaching programmes. Queen Elizabeth II visits The Open University in 1979. Wilson envisaged The Open University as a major marker in the Labour Party's commitment to modernising British society. He believed that it would help build a more competitive economy while also promoting greater equality of opportunity and social mobility. The planned use of television and radio to broadcast its courses was also supposed to link The Open University to the technological revolution under way, which Wilson saw as a major ally of his modernisation schemes. However, from the start, Lee encountered widespread scepticism and even opposition from within and without the Labour Party, including senior officials in the Department of Education and Science (DES), her departmental head Anthony Crosland, the Treasury, ministerial colleagues, such as Richard Crossman and commercial broadcasters. The Open University was realised due to Lee's unflagging determination and tenacity in 1965–67, the steadfast support from Wilson, and the fact that the anticipated costs, as reported to Lee and Wilson by Arnold Goodman, seemed very modest. By the time the actual, much higher costs became apparent, it was too late to scrap the fledgling university.[18] The university was granted a royal charter by the Privy Council on 23 April 1969.[19] Walton Hall manor house, the vice-chancellor's office and the second-oldest building on the OU Campus The majority of staff are part-time associate lecturers and, as of the 2021–22 academic year, almost 5,000 work for the OU.[5] There are also 1,427 (mostly full-time) salaried academic employees (central academics based at Walton Hall and staff tutors based in a variety of regional locations) who are research active and responsible for the production and presentation of teaching materials, 2,502 who are academic-related and 1,905 support staff (including secretaries and technicians).[5] Salaries are the OU's main cost—over £598 million for the 2021–22 academic year.[5] In 2010 the OU became one of the Sunday Times' Best Places to Work in the Public Sector. In 2016, the university reorganised its departments and now operates with the Faculty of Arts & Social Sciences (FASS); the Faculty of Business and Law (FBL); the Faculty of Science, Technology, Engineering and Mathematics (STEM); and the Faculty of Wellbeing, Education and Language Studies (WELS). It also runs Open and Access programmes via PVC-Students, and programmes from the Institute of Educational Technology (IET) via WELS. In 1982, Open University offered a course titled, \"The Effective Manager\", developed by a team that was led by Charles Handy. After the reported success of the course, Derek S. Pugh proposed the establishment of a business school. In 1988, the Open University Business School (OUBS) was founded by the Faculty of Management department, for which professor Andrew Thomson was appointed to head. Thomson's main goal was the offering of an MBA programme, which was eventually funded through a grant from the DES. In 1989, the first class of MBA students were enrolled.[22] The OU Business School's MBA programme was ranked 13th in the Financial Times’ global rankings of online and distance learning MBA providers which featured five European schools, four of which were in the UK.[29] From 1992 to 2005, the Singapore Institute of Management (SIM) ran the Open University Degree Programme (OUDP), in collaboration with The Open University, United Kingdom (OUUK), which was renamed the Singapore Institute of Management's Open University Centre (SIM-OUC) as one of SIM's autonomous entity. In 2005, after SIM formed SIM University (UniSIM), it took over SIM-OUC students and granted those who graduated in 2006 a choice between a UniSIM or OUUK degree.[32] The OU uses a variety of methods for teaching, including written and audio materials, the Internet, disc-based software and television programmes on DVD. Course-based television broadcasts by the BBC, which started on 3 January 1971, ceased on 15 December 2006.[33] Materials comprise originally authored work by in-house and external academic contributors, and from third-party materials licensed for use by OU students. For most modules, students are supported by tutors (\"associate lecturers\") who provide feedback on their work and are generally available to them at face-to-face tutorials, by telephone, and/or on the Internet. A number of short courses worth ten credits are now available that do not have an assigned tutor but offer an online conferencing service (Internet forum) where help and advice are offered through conferencing \"moderators\". Perry C building in Open University Campus in Milton Keynes Some modules have mandatory day schools. Nevertheless, it is possible to be excused on the basis of ill health (or other extenuating circumstances) and many courses have no mandatory face-to-face component. Similarly, some modules have traditionally offered week-long summer schools offering an opportunity for students to remove themselves from the general distractions of their life and focus on their studies for a short time. The university has separated residential modules from full-time distance-taught modules. Exemption from attendance at residential schools, always as an Alternative Learning Experience (ALE), is sometimes available for disabled students and others who find it impossible to attend in person (See \"Qualifications-Undergraduate\" section.) For many years the OU produced television and radio programmes aimed at bringing learning to a wider audience. In its early years, most of these were in the form of documentaries or filmed lectures. Latterly, most OU-associated programming was mainstream and broadcast in peak hours, including series such as Rough Science and \"Battle of the Geeks\", while older-style programming was carried in the BBC Learning Zone. In 2004 the OU announced it was to stop its late-night programmes on BBC Two, and the last programme was broadcast at 5.30 am on 16 December 2006. The OU now plans to focus on semi-academic television programmes, such as many now broadcast on BBC Four. The Open University launched FutureLearn in December 2012 with a dozen UK university partners. The Quality Assurance Agency for Higher Education review published in December 2015 found five areas of good practice and made three recommendations for improvement.[34] The English national survey of student satisfaction has twice put the Open University in first place. In October 2006, the OU joined the open educational resources movement with the launch of OpenLearn. A growing selection of current and past distance learning course materials will be released for free access, including downloadable versions for educators to modify (under the Creative CommonsBY-NC-SA licence), plus free collaborative learning-support tools. In the early 2000s, the OU researched the use of virtual worlds in teaching and learning, and had two main islands in Second Life.[35][36] In May 2009 these regions formed the basis of a case study[37] by Linden Lab, the company which owns Second Life. In mid-2010, the university led the list of contributing universities in the number of downloads of its material from the educational resources site iTunes U, with downloads of over 20 million.[38] Open University continues to adopt Moodle as the Virtual Learning Environment (VLE) with their own team deploying custom plugins.[39][40] Open University modules are often assessed using an equal weighting of examinations and coursework. The coursework component normally takes the form of between two and seven tutor-marked assignments (TMAs), and may also include up to six multiple-choice or \"missing word\" 10-question interactive computer-marked assignments (iCMAs). The examinable component is usually an invigilated three-hour paper regardless of the size of the module (although on some modules it can be up to three three-hour papers),[b] but an increasing number of modules instead have an EMA (End of Module Assessment) which is similar to a TMA, in that it is completed at home, but is regarded as an exam for grading purposes. These grades can be weighted[41] according to their level, and combined to calculate the classification of a degree. An undergraduate degree will weigh level 3 modules twice as much as level 2, and in postgraduate programmes, all M-level modules are equally weighted. Open University modules have associated with them a number of Credit Accumulation and Transfer Scheme (CATS) credits – usually 30 or 60 – depending on the quantity of the material in the module and a level (1, 2, 3, or 4) corresponding to the complexity, with 120 credits roughly equating to the year of study for a full-time student. Walton Hall, Milton Keynes The OU offers a large number of undergraduate qualifications, including certificates, diplomas, and bachelor's degrees, based on both level and quantity of study. An OU undergraduate degree requires 300 (or 360 for honours) CATS credits. Students are generally advised not to undertake more than 60 credits per year, meaning that an undergraduate degree will take typically six years to complete. With the exception of some degrees in fast-moving areas (such as computing), there is generally no limit on the time that a student may take. Students need special permission to take more than 120 credits (equivalent to full-time study) at any time;[42] such permission is not usually granted.[citation needed] Originally the BA was the only undergraduate degree, and it was unnamed. The modern OU grants degrees of Bachelor of Arts (BA), Science (BSc), Laws (LLB) and Engineering (BEng); the BA and BSc may be named (following a specified syllabus) or unnamed (constructed of courses chosen by the student) degrees. Many OU faculties have now introduced short modules worth ten credits. Most of these modules are taught online and start at regular intervals throughout the year. They typically provide an introduction to a broader subject over a period of ten weeks, these are generally timed during vacations at conventional universities in order to take advantage of their facilities. Some science modules, which require only home study, are complemented by residential courses, in order to allow the student to gain practical laboratory experience in that field; typically, an award of a degree or diploma will require completion of both. Different modules are run at different times of the year, but, typically, a 30 or 60-credit undergraduate module will run from October to June, with some dual-presentation modules also running from February to October. Assessment is by both continual assessment (with, normally, between four and eight assignments during the year) and, for most, a major assignment or, on some modules, a final examination. As well as degrees in named subjects, the Open University also grants multidisciplinary \"Open\" degrees. Open degrees provide students with access to a wide variety of subjects to develop a personalised curriculum to meet their vocational needs and personal interests.[43] The Open degree may be awarded as a Bachelor of Arts Open, a Bachelor of Science Open (either with or without honours), a Master of Arts Open or a Master of Science Open.[44] The Open degree is the most popular qualification at the university,[45] followed by BSc (Hons) Psychology; Cert of HE in Psychology; Bachelor of Laws (Hons); and BA (Hons) Business Management. [5] Around 20,000 students are enrolled on the Open degree, which makes the Open University the UK's largest multidisciplinary education provider.[46] As of 2018, over 236,000 alumni have graduated with an Open degree,[43] and in 2019, the Open University celebrated its 50th anniversary; as did its flagship Open Programme.[47] The Open University provides the opportunity to study for a PhD on a part-time distance, or a full-time basis (on-site for science subjects and most social sciences, off-site with some supervisions on-site for arts) in a wide range of disciplines as well as an EdD for professionals in education. Since 2019 the Open University has also offered a professional doctorate for healthcare workers. The university offers a range of Master's levels modules such as the MBA and MPA, MSc, MA and MEd, and MRes, and a number of postgraduate diplomas and certificates including innovative practice-based modules and postgraduate computing qualifications for professionals. Postgraduate certificates are awarded for 120 credits of study on specified modules; postgraduate diplomas are awarded for 240 credits of study on specified modules. The university offers \"Advanced Diplomas\" that involve 60 credits at the undergraduate level and 60 credits at the postgraduate level – these are designed as \"bridges\" between undergraduate and postgraduate study. Its master's degrees in the field of engineering are accredited to support registration as a Chartered Engineer, the highest level of engineering professional registration in the United Kingdom.[48] The Open University holds its annual degree ceremony at The Barbican Centre in London. Unlike most United Kingdom universities, degree ceremonies at the Open University are not graduation ceremonies as such (the occasion on which degrees are formally conferred on those who have achieved substantive degrees)—although honours degrees are also normally conferred on these occasions. The Open University degree ceremony is officially known as a \"Presentation of Graduates\" at which those who have already had a degree bestowed on them are presented to the University Chancellor or his/her representative. Open University graduates normally graduate in absentia at a joint meeting of the university's council and senate (\"congregation\") which takes place at a meeting entirely separate from the degree ceremony. The university's degree ceremonies occur throughout the year at various prestigious auditorium venues located throughout England, as well as in Scotland, Wales, Northern Ireland, and the Republic of Ireland, including London, Manchester, Birmingham, Ely, Glasgow, Cardiff, Belfast and Dublin. In the year 2018 the OU held 29 degree ceremonies in total.[13] These ceremonies are presided over by a senior academic at the Pro-Vice-Chancellor level or higher, and have the normal formal rituals associated with a graduation ceremony, including academic dress, procession and university mace. Academic dress for the Open University is based on the colours blue and gold (yellow). No headwear is worn at degree ceremonies.[49] Open University academic dress Degree Gown Hood Doctor of Education Royal blue, 3-inch gold facings Full shape, gold Panama, lined light blue Doctor of Letters Royal blue, 5-inch gold facings Full shape, gold, lined royal blue Doctor of Philosophy Royal blue, 3-inch gold facings Full shape, royal blue, lined gold, edged 1-inch gold Doctor of Science Royal blue, 5-inch gold facings Full shape, gold, lined light blue Master of Philosophy Light blue Full shape, light blue, edged gold Master of Research Light blue Simple shape, royal blue, faced 3-inch golf Master of Science Light blue Full shape, dark blue, lined gold, edged 1/2-inch gold Master of Arts Light blue Full shape, dark blue, lined gold, edged 1/2-inch gold Master of Business Administration Light blue Full shape, dark blue, lined gold with a blue edge, edged 1-inch gold Master of Education Light blue Full shape, dark blue, lined gold with a 1-inch white edge on a cowl, edged 3/8-inch gold on cape Master of Engineering Master of Mathematics Light blue Full shape, gold, faced 3-inch inside light blue Bachelor of Arts Bachelor of Science Bachelor of Engineering Bachelor of Laws Dark blue Simple shape, light blue, faced 3-inch gold Foundation degree Dark blue Simple shape, light blue, faced 3-inch dark blue In the year 2000, the Open University was the first to host an online \"virtual\" graduation ceremony in the United Kingdom together with an audience at the OU's campus in Milton Keynes. Twenty-six students in eight countries, from the United States of America to Hong Kong, were presented for their master's degrees in online graduation, including, from the Massachusetts Institute of Technology (MIT) – Tim Berners-Lee, one of the founders of the World Wide Web, who was conferred an honorary doctorate.[50] Like other UK universities, the OU actively engages in research. The OU's Planetary and Space Sciences Research Institute has become particularly well known to the public through its involvement in space missions. In October 2006, the Cassini-Huygens mission including 15 people from the OU received the 2006 \"Laurels for Team Achievement Award\" from the International Academy of Astronautics (IAA). Cassini-Huygens' successful completion of its seven-year, two billion-mile journey in January 2005 to Saturn ended with Huygens landing farther away from Earth than any previous probe or craft in the history of space exploration. The first instrument to touch Saturn's moon Titan was the Surface Science Package containing nine sensors to investigate the physical properties of Titan's surface. It was built by a team at the OU led by ProfessorJohn Zarnecki. The OU employs over 500 people engaged in research in over 25 areas, and there are over 1,200 research students. It spends approximately £20 million each year on research, around £6 million from the Higher Education Funding Council for England, and the remainder from external funders. [citation needed] The Open University also runs the Open Research Online (ORO) website. ORO is a collection of over 40,000 open-access research outputs across a broad range of research areas.[60] The university operates a collection of telescopes and other instruments at the Observatorio del Teide, Tenerife. Its facilities compromise the COmpletely Autonomous Service Telescope (COAST), the Physics Innovations Robotic Telescope Explorer (PIRATE) and an associated weather station. In 2019/20, 99,834 students were from England, 14,903 were from Scotland, 6,668 from Wales, 3,667 from Northern Ireland and 4,900 from the European Union, with others elsewhere. 60% of undergraduates were female, with 53% of those taking postgraduate modules being male.[61] According to The Guardian, a cross-sector fall in the number of part-time students was accelerated in 2012 when tuition fees rose and there was limited financial support for part-time students. The Open University saw a 30% drop in part-time students between 2010–11 and 2015–16.[62] Enrollment numbers show a tremendous difference from 2009–2010 to 2016–2017.[63] While most of those studying are mature students, an increasingly large proportion of new undergraduates are aged between 17 and 25, to the extent that in 2010/11 the OU had more students in this age range than any other UK university.[64][65] In the 2003–2004 academic year around 20% of new undergraduates were under 25,[66] up from 12.5% in 1996–1997[66] (the year before top-up fees were announced). In 2010 approximately 55% of those under 25 were also in full-time employment.[67] In 2010, 29,000 undergraduates were in this age range.[67] By 2011, 32,000 undergraduates were under 25 years old,[64] representing around 25% of new students.[68] The majority of students in the 2015–16 academic year were aged between 25 and 34 years old, with the median age of new undergraduates being 28.[61] As of 2014, the OU's youngest graduate was a fifteen-year-old boy from Wales who gained a BSc with First Class Honours in 2014.[69] The OU works with some schools to introduce A-Level students to OU study and in 2009–10 3% of undergraduates were under 18 years old. [citation needed] Unlike other universities, where students register for a programme, OU students register separately for individual modules (which may be 30 or 60 CATS credits (and formerly available in 10, 15, or 20 credits), equivalent to 15 or 30 ECTS credits). These modules may then be linked to degree programmes. The most popular module during 2009–10 was DD101 An introduction to the social sciences (7,512 students), followed by AA100 The Arts Past and Present, B120 An Introduction to Business Studies, K101 An Introduction to Health and Social Care and Y163 Starting with Psychology.[70] Cintra House, Cambridge, the university's former base in the East of England 17,634 students received financial assistance for their studies in 2015–16.[61] The typical cost for United Kingdom-based students of a Bachelor's honours degree at the OU was between £3,780 and £5,130 in 2009–10. From September 2012 the Government reduced its funding for all students residing in England and fees went up to compensate. English students pay higher fees than those living in the rest of the United Kingdom. The average cost of one full-time year or 120 credits rose to £6,336 in 2021, bringing the cost of an average Bachelor's honours degree for an English student to £19,008. (European Union and international students pay more as the university does not receive government funding for them).[70] The most important revenue stream to the Open University is now academic fees paid by the students, which totalled about £157 million in 2009–10 and £248 million in 2015–16.[70][61] The university enrolled fewer than 50,000 students in the 1970–71 academic year, but it quickly exceeded that number by 1974–75.[70] By 1987–88 yearly enrolment had doubled to 100,000 students, passing 200,000 by 2001–02 and 250,000 in 2009–10.[70] Numbers fell when the fee regime changed. Cumulatively, by the end of 2009–10, the OU had educated more than 1.5 million students and awarded 819,564 qualifications after successful assessment.[70] The Open University Students Association is the Students' Union for Open University students and is a registered charity wholly funded by the Open University (OU). The Association is governed by a Board of Trustees, made up of internal student and external members, and a Student Leadership Team who are elected on a bi-yearly basis. The current team run from 2022-2024. Each student registered with the OU automatically becomes part of the Students Association unless they elect to formally opt out. It offers opportunities to meet up, volunteer, find information and access services to support learning along with a range of student clubs and societies typical of those found in other UK Universities. The current President is Margaret Greenaway and Deputy President is Mark Walker. The Open University has been featured in many films and television programmes. The plot of Educating Rita surrounds the working-class titular character aiming to \"improve\" herself by studying English literature. She attends private tutorials run by alcoholic lecturer Frank.[91] In autumn 2006, Lenny Henry was a star in Slings and Arrows, a one-off BBC television drama which he also wrote, about someone who falls in love while on an OU English Literature course. (Henry has himself completed an OU degree in English.)[92] In the 2006–07 TV series Life on Mars, Sam Tyler received messages from the real world via Open University programmes late at night. Dorian Green from Birds of a Feather announced she had been accepted by the Open University to do a degree in psychology and began studying with the university in series 3. In the TV series Bottom, specifically the episode Accident, Eddie, Spudgun, and Dave Hedgehog watch TV while playing hide-and-seek with Ritchie. They fall asleep, leaving Ritchie in a cupboard until they finally awaken to an OU lecture on 'Medieval population distribution patterns in Lower Saxony'. Through an agreement between the Ministry of Defence and the OU going back to the early 1970s, a wide range of courses is available to members of the British armed forces, with course materials supplied via the student's BFPO address. OU study centres have been established in Cyprus and Germany. Many have studied while on active service, even in conflict situations.[93] The Grand Union is an ESRC Doctoral Training Partnership uniting The Open University, the University of Oxford and Brunel University London. The partnership is committed to a student-centred approach to training researchers, increasing access to postgraduate study, and advancing disciplinary and interdisciplinary research.[95]"}
{"url": "https://en.m.wikipedia.org/wiki/Mining", "text": "Modern mining processes involve prospecting for ore bodies, analysis of the profit potential of a proposed mine, extraction of the desired materials, and final reclamation or restoration of the land after the mine is closed.[2] Mining materials are often obtained from ore bodies, lodes, veins, seams, reefs, or placer deposits. The exploitation of these deposits for raw materials is dependent on investment, labor, energy, refining, and transportation cost. Mining operations can create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact; however, the outsized role of mining in generating business for often rural, remote or economically depressed communities means that governments often fail to fully enforce such regulations. Work safety has long been a concern as well, and where enforced, modern practices have significantly improved safety in mines. Unregulated, poorly regulated or illegal mining, especially in developing economies, frequently contributes to local human rights violations and environmental conflicts. Mining can also perpetuate political instability through resource conflicts. Mining in Egypt occurred in the earliest dynasties. The gold mines of Nubia were among the largest and most extensive of any in Ancient Egypt. These mines are described by the Greek author Diodorus Siculus, who mentions fire-setting as one method used to break down the hard rock holding the gold. One of the complexes is shown in one of the earliest known mining maps.[11] The miners crushed the ore and ground it to a fine powder before washing the powder for the gold dust known as the dry and wet attachment processes.[12] Mining in Europe has a very long history. Examples include the silver mines of Laurium, which helped support the Greek city state of Athens. Although they had over 20,000 slaves working them, their technology was essentially identical to their Bronze Age predecessors.[13] At other mines, such as on the island of Thassos, marble was quarried by the Parians after they arrived in the 7th century BC.[14] The marble was shipped away and was later found by archaeologists to have been used in buildings including the tomb of Amphipolis. Philip II of Macedon, the father of Alexander the Great, captured the gold mines of Mount Pangeo in 357 BC to fund his military campaigns.[15] He also captured gold mines in Thrace for minting coinage, eventually producing 26 tons per year. However, it was the Romans who developed large-scale mining methods, especially the use of large volumes of water brought to the minehead by numerous aqueducts. The water was used for a variety of purposes, including removing overburden and rock debris, called hydraulic mining, as well as washing comminuted, or crushed, ores and driving simple machinery. The Romans used hydraulic mining methods on a large scale to prospect for the veins of ore, especially using a now-obsolete form of mining known as hushing. They built numerous aqueducts to supply water to the minehead, where the water was stored in large reservoirs and tanks. When a full tank was opened, the flood of water sluiced away the overburden to expose the bedrock underneath and any gold-bearing veins. The rock was then worked by fire-setting to heat the rock, which would be quenched with a stream of water. The resulting thermal shock cracked the rock, enabling it to be removed by further streams of water from the overhead tanks. The Roman miners used similar methods to work cassiterite deposits in Cornwall and lead ore in the Pennines. Roman techniques were not limited to surface mining. They followed the ore veins underground once opencast mining was no longer feasible. At Dolaucothi they stoped out the veins and drove adits through bare rock to drain the stopes. The same adits were also used to ventilate the workings, especially important when fire-setting was used. At other parts of the site, they penetrated the water table and dewatered the mines using several kinds of machines, especially reverse overshot water-wheels. These were used extensively in the copper mines at Rio Tinto in Spain, where one sequence comprised 16 such wheels arranged in pairs, and lifting water about 24 metres (79 ft). They were worked as treadmills with miners standing on the top slats. Many examples of such devices have been found in old Roman mines and some examples are now preserved in the British Museum and the National Museum of Wales.[18] Mining as an industry underwent dramatic changes in medieval Europe. The mining industry in the early Middle Ages was mainly focused on the extraction of copper and iron. Other precious metals were also used, mainly for gilding or coinage. Initially, many metals were obtained through open-pit mining, and ore was primarily extracted from shallow depths, rather than through deep mine shafts. Around the 14th century, the growing use of weapons, armour, stirrups, and horseshoes greatly increased the demand for iron. Medieval knights, for example, were often laden with up to 100 pounds (45 kg) of plate or chain link armour in addition to swords, lances and other weapons.[19] The overwhelming dependency on iron for military purposes spurred iron production and extraction processes. The silver crisis of 1465 occurred when all mines had reached depths at which the shafts could no longer be pumped dry with the available technology.[20] Although an increased use of banknotes, credit and coppercoins during this period did decrease the value of, and dependence on, precious metals, gold and silver still remained vital to the story of medieval mining. Due to differences in the social structure of society, the increasing extraction of mineral deposits spread from central Europe to England in the mid-sixteenth century. On the continent, mineral deposits belonged to the crown, and this regalian right was stoutly maintained. But in England, royal mining rights were restricted to gold and silver (of which England had virtually no deposits) by a judicial decision of 1568 and a law in 1688. England had iron, zinc, copper, lead, and tin ores. Landlords who owned the base metals and coal under their estates then had a strong inducement to extract these metals or to lease the deposits and collect royalties from mine operators. English, German, and Dutchcapital combined to finance extraction and refining. Hundreds of German technicians and skilled workers were brought over; in 1642 a colony of 4,000 foreigners was mining and smelting copper at Keswick in the northwestern mountains.[21] Use of water power in the form of water mills was extensive. The water mills were employed in crushing ore, raising ore from shafts, and ventilating galleries by powering giant bellows. Black powder was first used in mining in Selmecbánya, Kingdom of Hungary (now Banská Štiavnica, Slovakia) in 1627.[22] Black powder allowed blasting of rock and earth to loosen and reveal ore veins. Blasting was much faster than fire-setting and allowed the mining of previously impenetrable metals and ores.[23] In 1762, one of the world's first mining academies was established in the same town there. The widespread adoption of agricultural innovations such as the iron plowshare, as well as the growing use of metal as a building material, was also a driving force in the tremendous growth of the iron industry during this period. Inventions like the arrastra were often used by the Spanish to pulverize ore after being mined. This device was powered by animals and used the same principles used for grain threshing.[24] Much of the knowledge of medieval mining techniques comes from books such as Biringuccio's De la pirotechnia and probably most importantly from Georg Agricola's De re metallica (1556). These books detail many different mining methods used in German and Saxon mines. A prime issue in medieval mines, which Agricola explains in detail, was the removal of water from mining shafts. As miners dug deeper to access new veins, flooding became a very real obstacle. The mining industry became dramatically more efficient and prosperous with the invention of mechanically- and animal-driven pumps. Iron metallurgy in Africa dates back over four thousand years. Gold became an important commodity for Africa during the trans-Saharan gold trade from the 7th century to the 14th century. Gold was often traded to Mediterranean economies that demanded gold and could supply salt, even though much of Africa was abundant with salt due to the mines and resources in the Sahara desert. The trading of gold for salt was mostly used to promote trade between the different economies.[25] Since the Great Trek in the 19th century, after, gold and diamond mining in Southern Africa has had major political and economic impacts. The Democratic Republic of Congo is the largest producer of diamonds in Africa, with an estimated 12 million carats in 2019. Other types of mining reserves in Africa include cobalt, bauxite, iron ore, coal, and copper.[26] In Fiji, in 1934, the Emperor Gold Mining Company Ltd. established operations at Vatukoula, followed in 1935 by the Loloma Gold Mines, N.L., and then by Fiji Mines Development Ltd. (aka Dolphin Mines Ltd.). These developments ushered in a “mining boom”, with gold production rising more than a hundred-fold, from 931.4 oz in 1934 to 107,788.5 oz in 1939, an order of magnitude then comparable to the combined output of New Zealand and Australia's eastern states.[27] During prehistoric times, early Americans mined large amounts of copper along Lake Superior's Keweenaw Peninsula and in nearby Isle Royale; metallic copper was still present near the surface in colonial times.[28][29][30] Indigenous peoples used Lake Superior copper from at least 5,000 years ago;[28] copper tools, arrowheads, and other artifacts that were part of an extensive native trade-network have been discovered. In addition, obsidian, flint, and other minerals were mined, worked, and traded.[29] Early French explorers who encountered the sites[clarification needed] made no use of the metals due to the difficulties of transporting them,[29] but the copper was eventually[when?] traded throughout the continent along major river routes.[citation needed] In the early colonial history of the Americas, \"native gold and silver was quickly expropriated and sent back to Spain in fleets of gold- and silver-laden galleons\",[31] the gold and silver originating mostly from mines in Central and South America. Turquoise dated at 700 AD was mined in pre-Columbian America; in the Cerillos Mining District in New Mexico, an estimate of \"about 15,000 tons of rock had been removed from Mt. Chalchihuitl using stone tools before 1700.\"[32][33] In 1727 Louis Denys (Denis) (1675–1741), sieur de La Ronde – brother of Simon-Pierre Denys de Bonaventure and the son-in-law of René Chartier – took command of Fort La Pointe at Chequamegon Bay; where natives informed him of an island of copper. La Ronde obtained permission from the French crown to operate mines in 1733, becoming \"the first practical miner on Lake Superior\"; seven years later, mining was halted by an outbreak between Sioux and Chippewa tribes.[34] Mining in the United States became widespread in the 19th century, and the United States Congress passed the General Mining Act of 1872 to encourage mining of federal lands.[35] As with the California Gold Rush in the mid-19th century, mining for minerals and precious metals, along with ranching, became a driving factor in the U.S. Westward Expansion to the Pacific coast. With the exploration of the West, mining camps sprang up and \"expressed a distinctive spirit, an enduring legacy to the new nation\"; Gold Rushers would experience the same problems as the Land Rushers of the transient West that preceded them.[36] Aided by railroads, many people traveled West for work opportunities in mining. Western cities such as Denver and Sacramento originated as mining towns.[37] When new areas were explored, it was usually the gold (placer and then lode) and then silver that were taken into possession and extracted first. Other metals would often wait for railroads or canals, as coarse gold dust and nuggets do not require smelting and are easy to identify and transport.[30] In the early 20th century, the gold and silver rush to the western United States also stimulated mining for coal as well as base metals such as copper, lead, and iron. Areas in modern Montana, Utah, Arizona, and later Alaska became predominate suppliers of copper to the world, which was increasingly demanding copper for electrical and households goods.[38] Canada's mining industry grew more slowly than did the United States' due to limitations in transportation, capital, and U.S. competition; Ontario was the major producer of the early 20th century with nickel, copper, and gold.[38] Meanwhile, Australia experienced the Australian gold rushes and by the 1850s was producing 40% of the world's gold, followed by the establishment of large mines such as the Mount Morgan Mine, which ran for nearly a hundred years, Broken Hill ore deposit (one of the largest zinc-lead ore deposits), and the iron ore mines at Iron Knob. After declines in production, another boom in mining occurred in the 1960s. Now, in the early 21st century, Australia remains a major world mineral producer.[39] The process of mining from discovery of an ore body through extraction of minerals and finally to returning the land to its natural state consists of several distinct steps. The first is discovery of the ore body, which is carried out through prospecting or exploration to find and then define the extent, location and value of the ore body. This leads to a mathematical resource estimation to estimate the size and grade of the deposit. This estimation is used to conduct a pre-feasibility study to determine the theoretical economics of the ore deposit. This identifies, early on, whether further investment in estimation and engineering studies is warranted and identifies key risks and areas for further work. The next step is to conduct a feasibility study to evaluate the financial viability, the technical and financial risks, and the robustness of the project. This is when the mining company makes the decision whether to develop the mine or to walk away from the project. This includes mine planning to evaluate the economically recoverable portion of the deposit, the metallurgy and ore recoverability, marketability and payability of the ore concentrates, engineering concerns, milling and infrastructure costs, finance and equity requirements, and an analysis of the proposed mine from the initial excavation all the way through to reclamation. The proportion of a deposit that is economically recoverable is dependent on the enrichment factor of the ore in the area. To gain access to the mineral deposit within an area it is often necessary to mine through or remove waste material which is not of immediate interest to the miner. The total movement of ore and waste constitutes the mining process. Often more waste than ore is mined during the life of a mine, depending on the nature and location of the ore body. Waste removal and placement is a major cost to the mining operator, so a detailed characterization of the waste material forms an essential part of the geological exploration program for a mining operation. Once the analysis determines a given ore body is worth recovering, development begins to create access to the ore body. The mine buildings and processing plants are built, and any necessary equipment is obtained. The operation of the mine to recover the ore begins and continues as long as the company operating the mine finds it economical to do so. Once all the ore that the mine can produce profitably is recovered, reclamation can begin, to make the land used by the mine suitable for future use. Technical and economic challenges notwithstanding, successful mine development must also address human factors. Working conditions are paramount to success, especially with regard to exposures to dusts, radiation, noise, explosives hazards, and vibration, as well as illumination standards. Mining today increasingly must address environmental and community impacts, including psychological and sociological dimensions. Thus, mining educator Frank T. M. White (1909–1971), broadened the focus to the “total environment of mining”, including reference to community development around mining, and how mining is portrayed to an urban society, which depends on the industry, although seemingly unaware of this dependency. He stated, “[I]n the past, mining engineers have not been called upon to study the psychological, sociological and personal problems of their own industry – aspects that nowadays are assuming tremendous importance. The mining engineer must rapidly expand his knowledge and his influence into these newer fields.”[41] Mining techniques can be divided into two common excavation types: surface mining and sub-surface (underground) mining. Today, surface mining is much more common, and produces, for example, 85% of minerals (excluding petroleum and natural gas) in the United States, including 98% of metallic ores.[42] Targets are divided into two general categories of materials: placer deposits, consisting of valuable minerals contained within river gravels, beach sands, and other unconsolidated materials; and lode deposits, where valuable minerals are found in veins, in layers, or in mineral grains generally distributed throughout a mass of actual rock. Both types of ore deposit, placer or lode, are mined by both surface and underground methods.[citation needed] Explosives have been used in surface mining and sub-surface mining to blast out rock and ore intended for processing. The most common explosive used in mining is ammonium nitrate.[44] Between 1870 and 1920, in Queensland Australia, an increase in mining accidents lead to more safety measures surrounding the use of explosives for mining.[45] In the United States of America, between 1990 and 1999, about 22.3 billion kilograms of explosives were used in mining quarrying and other industries; Moreover \"coal mining used 66.4%, nonmetal mining and quarrying 13.5%, metal mining 10.4%, construction 7.1%, and all other users 2.6%\".[44] An artisanal miner or small-scale miner (ASM) is a subsistence miner who is not officially employed by a mining company, but works independently, mining minerals using their own resources, usually by hand.[46] Small-scale mining includes enterprises or individuals that employ workers for mining, but generally still using manually-intensive methods, working with hand tools. Interior of an artisanal mine near Low's Creek, Mpumalanga Province, South Africa. The human figures, exploring this mine, show the scale of tunnels driven entirely with hand tools (two-kilogram (4.4 lb) hammer and hand-forged scrap-steel chisel). Artisanal miners often undertake the activity of mining seasonally—for example crops are planted in the rainy season, and mining is pursued in the dry season. However, they also frequently travel to mining areas and work year-round. There are four broad types of ASM: permanent artisanal mining, seasonal (annually migrating during idle agriculture periods), rush-type (massive migration, pulled often by commodity price jumps), and shock-push (poverty-drive, following conflict or natural disasters).[47] ASM is an important socio-economic sector for the rural poor in many developing nations, many of whom have few other options for supporting their families. Over 90% of the world's mining workforce are ASM. There are an estimated 40.5 million men, women and children directly engaged in ASM, from over 80 countries in the global south. 20% of the global gold supply is produced by the ASM sector, as well as 80% of the global gemstone and 20% of global diamond supply, and 25% of global tin production.[48] More than 150 million depend on ASM for their livelihood. 70–80% of small-scale miners are informal, and approximately 30% are women, although this ranges in certain countries and commodities from 5% to 80%.[49] Surface mining is done by removing surface vegetation, dirt, and bedrock to reach buried ore deposits. Techniques of surface mining include: open-pit mining, which is the recovery of materials from an open pit in the ground; quarrying, identical to open-pit mining except that it refers to sand, stone and clay; strip mining, which consists of stripping surface layers off to reveal ore underneath; and mountaintop removal, commonly associated with coal mining, which involves taking the top of a mountain off to reach ore deposits at depth. Most placer deposits, because they are shallowly buried, are mined by surface methods. Finally, landfill mining involves sites where landfills are excavated and processed.[50] Landfill mining has been thought of as a long-term solution to methane emissions and local pollution.[51] High wall mining, which evolved from auger mining, is another form of surface mining. In high wall mining, the remaining part of a coal seam previously exploited by other surface-mining techniques has too much overburden to be removed but can still be profitably exploited from the side of the artificial cliff made by previous mining.[52] A typical cycle alternates sumping, which undercuts the seam, and shearing, which raises and lowers the cutter-head boom to cut the entire height of the coal seam. As the coal recovery cycle continues, the cutter-head is progressively launched further into the coal seam. High wall mining can produce thousands of tons of coal in contour-strip operations with narrow benches, previously mined areas, trench mine applications and steep-dip seams.[citation needed] Mantrip used for transporting miners within an underground mineCaterpillar Highwall Miner HW300 – Technology Bridging Underground and Open Pit Mining Sub-surface mining consists of digging tunnels or shafts into the earth to reach buried ore deposits. Ore, for processing, and waste rock, for disposal, are brought to the surface through the tunnels and shafts. Sub-surface mining can be classified by the type of access shafts used, and the extraction method or the technique used to reach the mineral deposit. Drift mining uses horizontal access tunnels, slope mining uses diagonally sloping access shafts, and shaft mining uses vertical access shafts. Mining in hard and soft rock formations requires different techniques.[53] Other methods include shrinkage stope mining, which is mining upward, creating a sloping underground room, long wall mining, which is grinding a long ore surface underground, and room and pillar mining, which is removing ore from rooms while leaving pillars in place to support the roof of the room. Room and pillar mining often leads to retreat mining, in which supporting pillars are removed as miners retreat, allowing the room to cave in, thereby loosening more ore. Additional sub-surface mining methods include hard rock mining, bore hole mining, drift and fill mining, long hole slope mining, sub level caving, and block caving.[citation needed] Heavy machinery is used in mining to explore and develop sites, to remove and stockpile overburden, to break and remove rocks of various hardness and toughness, to process the ore, and to carry out reclamation projects after the mine is closed. Bulldozers, drills, explosives and trucks are all necessary for excavating the land. In the case of placer mining, unconsolidated gravel, or alluvium, is fed into machinery consisting of a hopper and a shaking screen or trommel which frees the desired minerals from the waste gravel. The minerals are then concentrated using sluices or jigs.[citation needed] Large drills are used to sink shafts, excavate stopes, and obtain samples for analysis. Trams are used to transport miners, minerals and waste. Lifts carry miners into and out of mines, and move rock and ore out, and machinery in and out, of underground mines. Huge trucks, shovels and cranes are employed in surface mining to move large quantities of overburden and ore. Processing plants use large crushers, mills, reactors, roasters and other equipment to consolidate the mineral-rich material and extract the desired compounds and metals from the ore.[citation needed] Once the mineral is extracted, it is often then processed. The science of extractive metallurgy is a specialized area in the science of metallurgy that studies the extraction of valuable metals from their ores, especially through chemical or mechanical means.[54][55] Mineral processing (or mineral dressing) is a specialized area in the science of metallurgy that studies the mechanical means of crushing, grinding, and washing that enable the separation (extractive metallurgy) of valuable metals or minerals from their gangue (waste material). Processing of placer ore material consists of gravity-dependent methods of separation, such as sluice boxes. Only minor shaking or washing may be necessary to disaggregate (unclump) the sands or gravels before processing. Processing of ore from a lode mine, whether it is a surface or subsurface mine, requires that the rock ore be crushed and pulverized before extraction of the valuable minerals begins. After lode ore is crushed, recovery of the valuable minerals is done by one, or a combination of several, mechanical and chemical techniques.[56] Since most metals are present in ores as oxides or sulfides, the metal needs to be reduced to its metallic form. This can be accomplished through chemical means such as smelting or through electrolytic reduction, as in the case of aluminium. Geometallurgy combines the geologic sciences with extractive metallurgy and mining.[40] In 2018, led by Chemistry and Biochemistry professor Bradley D. Smith, University of Notre Dame researchers \"invented a new class of molecules whose shape and size enable them to capture and contain precious metal ions,\" reported in a study published by the Journal of the American Chemical Society. The new method \"converts gold-containing ore into chloroauric acid and extracts it using an industrial solvent. The container molecules are able to selectively separate the gold from the solvent without the use of water stripping.\" The newly developed molecules can eliminate water stripping, whereas mining traditionally \"relies on a 125-year-old method that treats gold-containing ore with large quantities of poisonous sodium cyanide... this new process has a milder environmental impact and that, besides gold, it can be used for capturing other metals such as platinum and palladium,\" and could also be used in urban mining processes that remove precious metals from wastewater streams.[57] Mining operations remain rigorous and intrusive, often resulting in significant environmental impacts on local ecosystems and broader implications for planetary environmental health.[60] To accommodate mines and associated infrastructure, land is cleared extensively, consuming significant energy and water resources, emitting air pollutants, and producing hazardous waste.[61] According to The World Counts page \"The amount of resources mined from Earth is up from 39.3 billion tons in 2002. A 55 percent increase in less than 20 years. This puts Earth’s natural resources under heavy pressure. We are already extracting 75 percent more than Earth can sustain in the long run.\"[62] For major mining companies and any company seeking international financing, there are a number of other mechanisms to enforce environmental standards. These generally relate to financing standards such as the Equator Principles, IFC environmental standards, and criteria for Socially responsible investing. Mining companies have used this oversight from the financial sector to argue for some level of industry self-regulation.[63] In 1992, a Draft Code of Conduct for Transnational Corporations was proposed at the Rio Earth Summit by the UN Centre for Transnational Corporations (UNCTC), but the Business Council for Sustainable Development (BCSD) together with the International Chamber of Commerce (ICC) argued successfully for self-regulation instead.[64] This was followed by the Global Mining Initiative which was begun by nine of the largest metals and mining companies and which led to the formation of the International Council on Mining and Metals, whose purpose was to \"act as a catalyst\" in an effort to improve social and environmental performance in the mining and metals industry internationally.[63] The mining industry has provided funding to various conservation groups, some of which have been working with conservation agendas that are at odds with an emerging acceptance of the rights of indigenous people – particularly the right to make land-use decisions.[65] Ore mills generate large amounts of waste, called tailings.[67] For example, 99 tons of waste is generated per ton of copper, with even higher ratios in gold mining – because only 5.3 g of gold is extracted per ton of ore, a ton of gold produces 200,000 tons of tailings.[68] (As time goes on and richer deposits are exhausted – and technology improves – this number is going down to .5 g and less.) These tailings can be toxic. Tailings, which are usually produced as a slurry, are most commonly dumped into ponds made from naturally existing valleys.[69] These ponds are secured by impoundments (dams or embankment dams).[69] In 2000 it was estimated that 3,500 tailings impoundments existed, and that every year, 2 to 5 major failures and 35 minor failures occurred.[70] For example, in the Marcopper mining disaster at least 2 million tons of tailings were released into a local river.[70] In 2015, Barrick Gold Corporation spilled over 1 million liters of cyanide into a total of five rivers in Argentina near their Veladero mine.[71] Since 2007 in central Finland, the Talvivaara Terrafame polymetal mine's waste effluent and leaks of saline mine water have resulted in ecological collapse of a nearby lake.[72] Subaqueous tailings disposal is another option.[69] The mining industry has argued that submarine tailings disposal (STD), which disposes of tailings in the sea, is ideal because it avoids the risks of tailings ponds. The practice is illegal in the United States and Canada, but it is used in the developing world.[73] The waste is classified as either sterile or mineralized, with acid generating potential, and the movement and storage of this material form a major part of the mine planning process. When the mineralised package is determined by an economic cut-off, the near-grade mineralised waste is usually dumped separately with view to later treatment should market conditions change and it becomes economically viable. Civil engineering design parameters are used in the design of the waste dumps, and special conditions apply to high-rainfall areas and to seismically active areas. Waste dump designs must meet all regulatory requirements of the country in whose jurisdiction the mine is located. It is also common practice to rehabilitate dumps to an internationally acceptable standard, which in some cases means that higher standards than the local regulatory standard are applied.[70] Mining exists in many countries. London is the headquarters for large miners such as Anglo American, BHP and Rio Tinto.[74] The US mining industry is also large, but it is dominated by extraction of coal and other nonmetal minerals (e.g., rock and sand), and various regulations have worked to reduce the significance of mining in the United States.[74] In 2007, the total market capitalization of mining companies was reported at US$962 billion, which compares to a total global market cap of publicly traded companies of about US$50 trillion in 2007.[75] In 2002, Chile and Peru were reportedly the major mining countries of South America.[76] The mineral industry of Africa includes the mining of various minerals; it produces relatively little of the industrial metals copper, lead, and zinc, but according to one estimate has as a percent of world reserves 40% of gold, 60% of cobalt, and 90% of the world's platinum group metals.[77]Mining in India is a significant part of that country's economy. In the developed world, mining in Australia, with BHP founded and headquartered in the country, and mining in Canada are particularly significant. For rare earth minerals mining, China reportedly controlled 95% of production in 2013.[78] While exploration and mining can be conducted by individual entrepreneurs or small businesses, most modern-day mines are large enterprises requiring large amounts of capital to establish. Consequently, the mining sector of the industry is dominated by large, often multinational, companies, most of them publicly listed. It can be argued that what is referred to as the 'mining industry' is actually two sectors, one specializing in exploration for new resources and the other in mining those resources. The exploration sector is typically made up of individuals and small mineral resource companies, called \"juniors\", which are dependent on venture capital. The mining sector is made up of large multinational companies that are sustained by production from their mining operations. Various other industries such as equipment manufacture, environmental testing, and metallurgy analysis rely on, and support, the mining industry throughout the world. Canadian stock exchanges have a particular focus on mining companies, particularly junior exploration companies through Toronto's TSX Venture Exchange; Canadian companies raise capital on these exchanges and then invest the money in exploration globally.[74] Some have argued that below juniors there exists a substantial sector of illegitimate companies primarily focused on manipulating stock prices.[74] Mining operations can be grouped into five major categories in terms of their respective resources. These are oil and gas extraction, coal mining, metal ore mining, nonmetallic mineral mining and quarrying, and mining support activities.[79] Of all of these categories, oil and gas extraction remains one of the largest in terms of its global economic importance. Prospecting potential mining sites, a vital area of concern for the mining industry, is now done using sophisticated new technologies such as seismic prospecting and remote-sensing satellites. Mining is heavily affected by the prices of the commodity minerals, which are often volatile. The 2000s commodities boom (\"commodities supercycle\") increased the prices of commodities, driving aggressive mining. In addition, the price of gold increased dramatically in the 2000s, which increased gold mining; for example, one study found that conversion of forest in the Amazon increased six-fold from the period 2003–2006 (292 ha/yr) to the period 2006–2009 (1,915 ha/yr), largely due to artisanal mining.[80] Mining companies can be classified based on their size and financial capabilities: Major companies are considered to have an adjusted annual mining-related revenue of more than US$500 million, with the financial capability to develop a major mine on its own. Intermediate companies have at least $50 million in annual revenue but less than $500 million. Junior companies rely on equity financing as their principal means of funding exploration. Juniors are mainly pure exploration companies, but may also produce minimally, and do not have a revenue exceeding US$50 million.[81] New regulations and a process of legislative reforms aim to improve the harmonization and stability of the mining sector in mineral-rich countries.[82] New legislation for mining industry in African countries still appears to be an issue, but has the potential to be solved, when a consensus is reached on the best approach.[83] By the beginning of the 21st century, the booming and increasingly complex mining sector in mineral-rich countries was providing only slight benefits to local communities, especially in given the sustainability issues. Increasing debate and influence by NGOs and local communities called for new approaches which would also include disadvantaged communities, and work towards sustainable development even after mine closure (including transparency and revenue management). By the early 2000s, community development issues and resettlements became mainstream concerns in World Bank mining projects.[83] Mining-industry expansion after mineral prices increased in 2003 and also potential fiscal revenues in those countries created an omission in the other economic sectors in terms of finances and development. Furthermore, this highlighted regional and local demand for mining revenues and an inability of sub-national governments to effectively use the revenues. The Fraser Institute (a Canadian think tank) has highlighted[clarification needed] the environmental protection laws in developing countries, as well as voluntary efforts by mining companies to improve their environmental impact.[84] In 2007, the Extractive Industries Transparency Initiative (EITI) was mainstreamed[clarification needed] in all countries cooperating with the World Bank in mining industry reform.[83] The EITI operates and was implemented with the support of the EITI multi-donor trust fund, managed by the World Bank.[85] The EITI aims to increase transparency in transactions between governments and companies in extractive industries[86] by monitoring the revenues and benefits between industries and recipient governments. The entrance process is voluntary for each country and is monitored by multiple stakeholders including governments, private companies and civil society representatives, responsible for disclosure and dissemination of the reconciliation report;[83] however, the competitive disadvantage of company-by-company public report is for some of the businesses in Ghana at least, the main constraint.[87] Therefore, the outcome assessment in terms of failure or success of the new EITI regulation does not only \"rest on the government's shoulders\" but also on civil society and companies.[88] However, implementation has issues; inclusion or exclusion of artisanal mining and small-scale mining (ASM) from the EITI and how to deal with \"non-cash\" payments made by companies to subnational governments. Furthermore, the disproportionate revenues the mining industry can bring to the comparatively small number of people that it employs,[89] causes other problems, like a lack of investment in other less lucrative sectors, leading to swings in government revenue because of volatility in the oil markets. Artisanal mining is clearly an issue in EITI Countries such as the Central African Republic, D.R. Congo, Guinea, Liberia and Sierra Leone – i.e. almost half of the mining countries implementing the EITI.[89] Among other things, limited scope of the EITI involving disparity in terms of knowledge of the industry and negotiation skills, thus far flexibility of the policy (e.g. liberty of the countries to expand beyond the minimum requirements and adapt it to their needs), creates another risk of unsuccessful implementation. Public awareness increase, where government should act as a bridge between public and initiative for a successful outcome of the policy is an important element to be considered.[90] In 1992, the World Bank began to push for privatization of government-owned mining companies with a new set of codes, beginning with its report The Strategy for African Mining. In 1997, Latin America's largest miner Companhia Vale do Rio Doce (CVRD) was privatized. These and other developments, such as the Philippines 1995 Mining Act, led the bank to publish a third report (Assistance for Minerals Sector Development and Reform in Member Countries) which endorsed mandatory environment impact assessments and attention to the concerns of the local population. The codes based on this report are influential in the legislation of developing nations. The new codes are intended to encourage development through tax holidays, zero custom duties, reduced income taxes, and related measures.[63]: 22 The results of these codes were analyzed by a group from the University of Quebec, which concluded that the codes promote foreign investment but \"fall very short of permitting sustainable development\".[93] The observed negative correlation between natural resources and economic development is known as the resource curse.[citation needed] There are numerous occupational hazards associated with mining, including exposure to rockdust which can lead to diseases such as silicosis, asbestosis, and pneumoconiosis. Gases in the mine can lead to asphyxiation and could also be ignited. Mining equipment can generate considerable noise, putting workers at risk for hearing loss. Cave-ins, rock falls, and exposure to excess heat are also known hazards. The current NIOSH Recommended Exposure Limit (REL) of noise is 85 dBA with a 3 dBA exchange rate and the MSHA Permissible Exposure Limit (PEL) is 90 dBA with a 5 dBA exchange rate as an 8-hour time-weighted average. NIOSH has found that 25% of noise-exposed workers in Mining, Quarrying, and Oil and Gas Extraction have hearing impairment.[99] The prevalence of hearing loss increased by 1% from 1991 to 2001 within these workers.[citation needed] In addition to the environmental impacts of mining processes, a prominent criticism pertaining to this form of extractive practice and of mining companies are the human rights abuses occurring within mining sites and communities close to them.[103] Frequently, despite being protected by International Labor rights, miners are not given appropriate equipment to provide them with protection from possible mine collapse or from harmful pollutants and chemicals expelled during the mining process, work in inhumane conditions spending numerous hours working in extreme heat, darkness and 14 hour workdays with no allocated time for breaks.[104] Included within the human rights abuses that occur during mining processes are instances of child labor. These instances are a cause for widespread criticism of mining cobalt, a mineral essential for powering modern technologies such as laptops, smartphones and electric vehicles. Many of these cases of child laborers are found in the Democratic Republic of Congo. Reports have risen of children carrying sacks of cobalt weighing 25 kg from small mines to local traders[105] being paid for their work only in food and accommodation. A number of companies such as Apple, Google, Microsoft and Tesla have been implicated in lawsuits brought by families whose children were severely injured or killed during mining activities in Congo.[106] In December 2019, 14 Congolese families filed a lawsuit against Glencore, a mining company which supplies the essential cobalt to these multinational corporations with allegations of negligence that led to the deaths of children or injuries such as broken spines, emotional distress and forced labor.[citation needed] There have also been instances of killings and evictions attributed to conflicts with mining companies. Almost a third of 227 murders in 2020 were of Indigenous peoples rights activists on the frontlines of climate change activism linked to logging, mining, large-scale agribusiness, hydroelectric dams, and other infrastructure, according to Global Witness.[107] The relationship between indigenous peoples and mining is defined by struggles over access to land. In Australia, the Aboriginal Bininj said mining posed a threat to their living culture and could damage sacred heritage sites.[108][109] In the Philippines, an anti-mining movement has raised concerns regarding \"the total disregard for [Indigenous communities'] ancestral land rights\".[110]Ifugao peoples' opposition to mining led a governor to proclaim a ban on mining operations in Mountain Province, Philippines.[110] In Brazil, more than 170 tribes organized a march to oppose controversial attempts to strip back indigenous land rights and open their territories to mining operations.[111] The United Nations Commission on Human Rights has called on Brazil's Supreme Court to uphold Indigenous land rights to prevent exploitation by mining groups and industrial agriculture.[112] The factual accuracy of parts of this article (those related to article) may be compromised due to out-of-date information. The reason given is: Deepest mine record was just changed and might need more detail. Please help update this article to reflect recent events or newly available information. Last update: 9th July 2022(July 2022) As of 2019, Mponeng is the world's deepest mine from ground level, reaching a depth of 4 km (2.5 mi) below ground level. The trip from the surface to the bottom of the mine takes over an hour. It is a gold mine in South Africa's Gauteng province. Previously known as Western Deep Levels #1 Shaft, the underground and surface works were commissioned in 1987. The mine is considered to be one of the most substantial gold mines in the world. The Moab Khutsong gold mine in North West Province (South Africa) has the world's longest winding steel wire rope, which is able to lower workers to 3,054 metres (10,020 ft) in one uninterrupted four-minute journey.[113] The deepest open-pit mine with respect to sea level is Tagebau Hambach in Germany, where the base of the pit is 299 metres (981 ft) below sea level.[118] The largest underground mine is Kiirunavaara Mine in Kiruna, Sweden. With 450 kilometres (280 mi) of roads, 40 million tonnes of annually produced ore, and a depth of 1,270 metres (4,170 ft), it is also one of the most modern underground mines. The deepest borehole in the world is Kola Superdeep Borehole at 12,262 metres (40,230 ft), but this is connected to scientific drilling, not mining.[119] During the 20th century, the variety of metals used in society grew rapidly. Today, the development of major nations such as China and India and advances in technologies are fueling an ever-greater demand. The result is that metal mining activities are expanding and more and more of the world's metal stocks are above ground in use rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the US rose from 73 kilograms (161 lb) to 238 kilograms (525 lb) per person.[120] The report's authors observed that the metal stocks in society can serve as huge anthropogenic mines above ground.[122] However, they warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars, and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.[citation needed] As recycling rates are low and so much metal has already been extracted, some landfills now contain higher concentrations of metal than mines themselves.[123] This is especially true of aluminum, used in cans, and precious metals, found in discarded electronics.[124] Furthermore, waste after 15 years has still not broken down, so less processing would be required when compared to mining ores. A study undertaken by Cranfield University has found £360 million of metals could be mined from just four landfill sites.[125] There is also up to 20 MJ/kg of energy in waste, potentially making the re-extraction more profitable.[126] However, although the first landfill mine opened in Tel Aviv, Israel in 1953, little work has followed due to the abundance of accessible ores.[127] Geological engineering – a discipline of engineering concerned with the application of geological science and engineering principles to fields, such as civil engineering, mining, environmental engineering, and forestry, among othersPages displaying wikidata descriptions as a fallback Woytinsky, W.S., and E.S. Woytinsky (1953). World Population and Production Trends and Outlooks, pp. 749–881; with many tables and maps on the worldwide mining industry in 1950, including coal, metals and minerals"}
{"url": "https://en.m.wikipedia.org/wiki/Diagram", "text": "Contents The term \"diagram\" in its commonly used sense can have a general or specific meaning: visual information device : Like the term \"illustration\", \"diagram\" is used as a collective term standing for the whole class of technical genres, including graphs, technical drawings and tables. specific kind of visual display : This is the genre that shows qualitative data with shapes that are connected by lines, arrows, or other visual links. In science the term is used in both ways. For example, Anderson (1997) stated more generally: \"diagrams are pictorial, yet abstract, representations of information, and maps, line graphs, bar charts, engineeringblueprints, and architects' sketches are all examples of diagrams, whereas photographs and video are not\".[2] On the other hand, Lowe (1993) defined diagrams as specifically \"abstract graphic portrayals of the subject matter they represent\".[3] a display that does not show quantitative data (numerical data), but rather relationships and abstract information with building blocks such as geometrical shapes connected by lines, arrows, or other visual links. Or in Hall's (1996) words \"diagrams are simplified figures, caricatures in a way, intended to convey essential meaning\".[5] These simplified figures are often based on a set of rules. The basic shape according to White (1984) can be characterized in terms of \"elegance, clarity, ease, pattern, simplicity, and validity\".[4] Elegance is basically determined by whether or not the diagram is \"the simplest and most fitting solution to a problem\".[6] Diagrammatology is the academic study of diagrams. Scholars note that while a diagram may look similar to the thing that it represents, this is not necessary. Rather a diagram may only have structural similarity to what it represents, an idea often attributed to Charles Sanders Peirce.[7]: 42 Structural similarity can be defined in terms of a mapping between parts of the diagram and parts of what the diagram represents and the properties of this mapping, such as maintaining relations between these parts and facts about these relations. This is related to the concept of isomorphism, or homomorphism in mathematics.[7]: 43 Sometimes certain geometric properties (such as which points are closer) of the diagram can be mapped to properties of the thing that a diagram represents. On the other hand, the representation of an object in a diagram may be overly specific and properties that are true in the diagram may not be true for the object the diagram represents.[7]: 48 A diagram may act as a means of cognitive extension allowing reasoning to take place on the diagram based on which constraints are similar.[7]: 50 Logical or conceptual diagrams, which take a collection of items and relationships between them, and express them by giving each item a 2D position, while the relationships are expressed as connections between the items or overlaps between the items, for example: ^Bert S. Hall (1996). \"The Didactic and the Elegant: Some Thoughts on Scientific and Technological Illustrations in the Middle Ages and Renaissance\". in: B. Braigie (ed.) Picturing knowledge: historical and philosophical problems concerning the use of art in science. Toronto: University of Toronto Press. p.9"}
{"url": "https://en.m.wikipedia.org/wiki/Electrical_relay", "text": "Relay A relay is an electrically operated switch. It consists of a set of input terminals for a single or multiple control signals, and a set of operating contact terminals. The switch may have any number of contacts in multiple contact forms, such as make contacts, break contacts, or combinations thereof. A relayElectromechanical relay principleElectromechanical relay schematic showing a control coil, four pairs of normally open and one pair of normally closed contactsAn automotive-style miniature relay with the dust cover taken off Relays are used where it is necessary to control a circuit by an independent low-power signal, or where several circuits must be controlled by one signal. Relays were first used in long-distance telegraph circuits as signal repeaters: they refresh the signal coming in from one circuit by transmitting it on another circuit. Relays were used extensively in telephone exchanges and early computers to perform logical operations. The traditional electromechanical form of a relay uses an electromagnet to close or open the contacts, but relays using other operating principles have also been invented, such as in solid-state relays which use semiconductor properties for control without relying on moving parts. Relays with calibrated operating characteristics and sometimes multiple operating coils are used to protect electrical circuits from overload or faults; in modern electric power systems these functions are performed by digital instruments still called protective relays or safety relays. Latching relays require only a single pulse of control power to operate the switch persistently. Another pulse applied to a second set of control terminals, or a pulse with opposite polarity, resets the switch, while repeated pulses of the same kind have no effects. Magnetic latching relays are useful in applications when interrupted power should not affect the circuits that the relay is controlling. However, an official patent was not issued until 1840 to Samuel Morse for his telegraph, which is now called a relay. The mechanism described acted as a digital amplifier, repeating the telegraph signal, and thus allowing signals to be propagated as far as desired.[5] The word relay appears in the context of electromagnetic operations from 1860 onwards.[6] Simple electromechanical relayOperation without flyback diode, arcing causes degradation of the switch contactsOperation with flyback diode, arcing in the control circuit is avoided A simple electromagnetic relay consists of a coil of wire wrapped around a soft iron core (a solenoid), an iron yoke which provides a low reluctance path for magnetic flux, a movable iron armature, and one or more sets of contacts (there are two contacts in the relay pictured). The armature is hinged to the yoke and mechanically linked to one or more sets of moving contacts. The armature is held in place by a spring so that when the relay is de-energized there is an air gap in the magnetic circuit. In this condition, one of the two sets of contacts in the relay pictured is closed, and the other set is open. Other relays may have more or fewer sets of contacts depending on their function. The relay in the picture also has a wire connecting the armature to the yoke. This ensures continuity of the circuit between the moving contacts on the armature, and the circuit track on the printed circuit board (PCB) via the yoke, which is soldered to the PCB. When an electric current is passed through the coil it generates a magnetic field that activates the armature, and the consequent movement of the movable contact(s) either makes or breaks (depending upon construction) a connection with a fixed contact. If the set of contacts was closed when the relay was de-energized, then the movement opens the contacts and breaks the connection, and vice versa if the contacts were open. When the current to the coil is switched off, the armature is returned by a force, approximately half as strong as the magnetic force, to its relaxed position. Usually this force is provided by a spring, but gravity is also used commonly in industrial motor starters. Most relays are manufactured to operate quickly. In a low-voltage application this reduces noise; in a high voltage or current application it reduces arcing. Operation of a 12 A relay When the coil is energized with direct current, a flyback diode or snubberresistor is often placed across the coil to dissipate the energy from the collapsing magnetic field (back EMF) at deactivation, which would otherwise generate a voltage spike dangerous to semiconductor circuit components. Such diodes were not widely used before the application of transistors as relay drivers, but soon became ubiquitous as early germanium transistors were easily destroyed by this surge. Some automotive relays include a diode inside the relay case. Resistors, while more durable than diodes, are less efficient at eliminating voltage spikes generated by relays[7] and therefore not as commonly used. A small cradle relay often used in electronics. The \"cradle\" term refers to the shape of the relay's armature If the relay is driving a large, or especially a reactive load, there may be a similar problem of surge currents around the relay output contacts. In this case a snubber circuit (a capacitor and resistor in series) across the contacts may absorb the surge. Suitably rated capacitors and the associated resistor are sold as a single packaged component for this commonplace use. If the coil is designed to be energized with alternating current (AC), some method is used to split the flux into two out-of-phase components which add together, increasing the minimum pull on the armature during the AC cycle. Typically this is done with a small copper \"shading ring\" crimped around a portion of the core that creates the delayed, out-of-phase component,[8] which holds the contacts during the zero crossings of the control voltage.[9] Contact materials for relays vary by application. Materials with low contact resistance may be oxidized by the air, or may tend to \"stick\" instead of cleanly parting when opening. Contact material may be optimized for low electrical resistance, high strength to withstand repeated operations, or high capacity to withstand the heat of an arc. Where very low resistance is required, or low thermally-induced voltages are desired, gold-plated contacts may be used, along with palladium and other non-oxidizing, semi-precious metals. Silver or silver-plated contacts are used for signal switching. Mercury-wetted relays make and break circuits using a thin, self-renewing film of liquid mercury. For higher-power relays switching many amperes, such as motor circuit contactors, contacts are made with a mixtures of silver and cadmium oxide, providing low contact resistance and high resistance to the heat of arcing. Contacts used in circuits carrying scores or hundreds of amperes may include additional structures for heat dissipation and management of the arc produced when interrupting the circuit.[10] Some relays have field-replaceable contacts, such as certain machine tool relays; these may be replaced when worn out, or changed between normally open and normally closed state, to allow for changes in the controlled circuit.[11] Circuit symbols of relays (C denotes the common terminal in SPDT and DPDT types.) Since relays are switches, the terminology applied to switches is also applied to relays; a relay switches one or more poles, each of whose contacts can be thrown by energizing the coil. Normally open (NO) contacts connect the circuit when the relay is activated; the circuit is disconnected when the relay is inactive. Normally closed (NC) contacts disconnect the circuit when the relay is activated; the circuit is connected when the relay is inactive. All of the contact forms involve combinations of NO and NC connections. The National Association of Relay Manufacturers and its successor, the Relay and Switch Industry Association define 23 distinct electrical contact forms found in relays and switches.[12] Of these, the following are commonly encountered: SPST-NO (Single-Pole Single-Throw, Normally-Open) relays have a single Form A or make contact. These have two terminals which can be connected or disconnected. Including two for the coil, such a relay has four terminals in total. SPST-NC (Single-Pole Single-Throw, Normally-Closed) relays have a single Form B or break contact. As with an SPST-NO relay, such a relay has four terminals in total. SPDT (Single-Pole Double-Throw) relays have a single set of Form C, break before make or transfer contacts. That is, a common terminal connects to either of two others, never connecting to both at the same time. Including two for the coil, such a relay has a total of five terminals. DPST – Double-Pole Single-Throw relays are equivalent to a pair of SPST switches or relays actuated by a single coil. Including two for the coil, such a relay has a total of six terminals. The poles may be Form A or Form B (or one of each; the designations NO and NC should be used to resolve the ambiguity). DPDT – Double-Pole Double-Throw relays have two sets of Form C contacts. These are equivalent to two SPDT switches or relays actuated by a single coil. Such a relay has eight terminals, including the coil The S (single) or D (double) designator for the pole count may be replaced with a number, indicating multiple contacts connected to a single actuator. For example, 4PDT indicates a four-pole double-throw relay that has 12 switching terminals. Where radio transmitters and receivers share one antenna, often a coaxial relay is used as a TR (transmit-receive) relay, which switches the antenna from the receiver to the transmitter. This protects the receiver from the high power of the transmitter. Such relays are often used in transceivers which combine transmitter and receiver in one unit. The relay contacts are designed not to reflect any radio frequency power back toward the source, and to provide very high isolation between receiver and transmitter terminals. The characteristic impedance of the relay is matched to the transmission line impedance of the system, for example, 50 ohms.[15] A contactor is a heavy-duty relay with higher current ratings,[16] used for switching electric motors and lighting loads. Continuous current ratings for common contactors range from 10 amps to several hundred amps. High-current contacts are made with alloys containing silver. The unavoidable arcing causes the contacts to oxidize; however, silver oxide is still a good conductor.[17] Contactors with overload protection devices are often used to start motors.[18] A force-guided contacts relay has relay contacts that are mechanically linked together, so that when the relay coil is energized or de-energized, all of the linked contacts move together. If one set of contacts in the relay becomes immobilized, no other contact of the same relay will be able to move. The function of force-guided contacts is to enable the safety circuit to check the status of the relay. Force-guided contacts are also known as \"positive-guided contacts\", \"captive contacts\", \"locked contacts\", \"mechanically linked contacts\", or \"safety relays\". These safety relays have to follow design rules and manufacturing rules that are defined in one main machinery standard EN 50205 : Relays with forcibly guided (mechanically linked) contacts. These rules for the safety design are the one defined in type B standards such as EN 13849-2 as Basic safety principles and Well-tried safety principles for machinery that applies to all machines. Force-guided contacts by themselves can not guarantee that all contacts are in the same state, however, they do guarantee, subject to no gross mechanical fault, that no contacts are in opposite states. Otherwise, a relay with several normally open (NO) contacts may stick when energized, with some contacts closed and others still slightly open, due to mechanical tolerances. Similarly, a relay with several normally closed (NC) contacts may stick to the unenergized position, so that when energized, the circuit through one set of contacts is broken, with a marginal gap, while the other remains closed. By introducing both NO and NC contacts, or more commonly, changeover contacts, on the same relay, it then becomes possible to guarantee that if any NC contact is closed, all NO contacts are open, and conversely, if any NO contact is closed, all NC contacts are open. It is not possible to reliably ensure that any particular contact is closed, except by potentially intrusive and safety-degrading sensing of its circuit conditions, however in safety systems it is usually the NO state that is most important, and as explained above, this is reliably verifiable by detecting the closure of a contact of opposite sense. Force-guided contact relays are made with different main contact sets, either NO, NC or changeover, and one or more auxiliary contact sets, often of reduced current or voltage rating, used for the monitoring system. Contacts may be all NO, all NC, changeover, or a mixture of these, for the monitoring contacts, so that the safety system designer can select the correct configuration for the particular application. Safety relays are used as part of an engineered safety system. A latching relay, also called impulse, bistable, keep, or stay relay, or simply latch, maintains either contact position indefinitely without power applied to the coil. The advantage is that one coil consumes power only for an instant while the relay is being switched, and the relay contacts retain this setting across a power outage. A latching relay allows remote control of building lighting without the hum that may be produced from a continuously (AC) energized coil. In one mechanism, two opposing coils with an over-center spring or permanent magnet hold the contacts in position after the coil is de-energized. A pulse to one coil turns the relay on, and a pulse to the opposite coil turns the relay off. This type is widely used where control is from simple switches or single-ended outputs of a control system, and such relays are found in avionics and numerous industrial applications. Another latching type has a remanent core that retains the contacts in the operated position by the remanent magnetism in the core. This type requires a current pulse of opposite polarity to release the contacts. A variation uses a permanent magnet that produces part of the force required to close the contact; the coil supplies sufficient force to move the contact open or closed by aiding or opposing the field of the permanent magnet.[19] A polarity controlled relay needs changeover switches or an H-bridge drive circuit to control it. The relay may be less expensive than other types, but this is partly offset by the increased costs in the external circuit. In another type, a ratchet relay has a ratchet mechanism that holds the contacts closed after the coil is momentarily energized. A second impulse, in the same or a separate coil, releases the contacts.[19] This type may be found in certain cars, for headlamp dipping and other functions where alternating operation on each switch actuation is needed. Some early computers used ordinary relays as a kind of latch—they store bits in ordinary wire-spring relays or reed relays by feeding an output wire back as an input, resulting in a feedback loop or sequential circuit. Such an electrically latching relay requires continuous power to maintain state, unlike magnetically latching relays or mechanically ratcheting relays. While (self-)holding circuits are often realized with relays they can also be implemented by other means. In computer memories, latching relays and other relays were replaced by delay-line memory, which in turn was replaced by a series of ever faster and ever smaller memory technologies. A machine tool relay is a type standardized for industrial control of machine tools, transfer machines, and other sequential control. They are characterized by a large number of contacts (sometimes extendable in the field) which are easily converted from normally open to normally closed status, easily replaceable coils, and a form factor that allows compactly installing many relays in a control panel. Although such relays once were the backbone of automation in such industries as automobile assembly, the programmable logic controller (PLC) mostly displaced the machine tool relay from sequential control applications. A relay allows circuits to be switched by electrical equipment: for example, a timer circuit with a relay could switch power at a preset time. For many years relays were the standard method of controlling industrial electronic systems. A number of relays could be used together to carry out complex functions (relay logic). The principle of relay logic is based on relays which energize and de-energize associated contacts. Relay logic is the predecessor of ladder logic, which is commonly used in programmable logic controllers. A mercury relay is a relay that uses mercury as the switching element. They are used where contact erosion would be a problem for conventional relay contacts. Owing to environmental considerations about significant amount of mercury used and modern alternatives, they are now comparatively uncommon. A mercury-wetted reed relay is a form of reed relay that employs a mercury switch, in which the contacts are wetted with mercury. Mercury reduces the contact resistance and mitigates the associated voltage drop. Surface contamination may result in poor conductivity for low-current signals. For high-speed applications, the mercury eliminates contact bounce, and provides virtually instantaneous circuit closure. Mercury wetted relays are position-sensitive and must be mounted according to the manufacturer's specifications. Because of the toxicity and expense of liquid mercury, these relays have increasingly fallen into disuse. The high speed of switching action of the mercury-wetted relay is a notable advantage. The mercury globules on each contact coalesce, and the current rise time through the contacts is generally considered to be a few picoseconds.[citation needed] However, in a practical circuit it may be limited by the inductance of the contacts and wiring. It was quite common, before restrictions on the use of mercury, to use a mercury-wetted relay in the laboratory as a convenient means of generating fast rise time pulses, however although the rise time may be picoseconds, the exact timing of the event is, like all other types of relay, subject to considerable jitter, possibly milliseconds, due to mechanical variations. The same coalescence process causes another effect, which is a nuisance in some applications. The contact resistance is not stable immediately after contact closure, and drifts, mostly downwards, for several seconds after closure, the change perhaps being 0.5 ohm.[citation needed] Multi-voltage relays are devices designed to work for wide voltage ranges such as 24 to 240 VAC and VDC and wide frequency ranges such as 0 to 300 Hz. They are indicated for use in installations that do not have stable supply voltages. Electric motors need overcurrent protection to prevent damage from over-loading the motor, or to protect against short circuits in connecting cables or internal faults in the motor windings.[20] The overload sensing devices are a form of heat operated relay where a coil heats a bimetallic strip, or where a solder pot melts, to operate auxiliary contacts. These auxiliary contacts are in series with the motor's contactor coil, so they turn off the motor when it overheats.[21] This thermal protection operates relatively slowly allowing the motor to draw higher starting currents before the protection relay will trip. Where the overload relay is exposed to the same ambient temperature as the motor, a useful though crude compensation for motor ambient temperature is provided.[22] The other common overload protection system uses an electromagnet coil in series with the motor circuit that directly operates contacts. This is similar to a control relay but requires a rather high fault current to operate the contacts. To prevent short over current spikes from causing nuisance triggering the armature movement is damped with a dashpot. The thermal and magnetic overload detections are typically used together in a motor protection relay.[citation needed] Electronic overload protection relays measure motor current and can estimate motor winding temperature using a \"thermal model\" of the motor armature system that can be set to provide more accurate motor protection. Some motor protection relays include temperature detector inputs for direct measurement from a thermocouple or resistance thermometer sensor embedded in the winding.[23] A polarized relay places the armature between the poles of a permanent magnet to increase sensitivity. Polarized relays were used in middle 20th Century telephone exchanges to detect faint pulses and correct telegraphic distortion. A reed relay is a reed switch enclosed in a solenoid. The switch has a set of contacts inside an evacuated or inert gas-filled glass tube that protects the contacts against atmospheric corrosion; the contacts are made of magnetic material that makes them move under the influence of the field of the enclosing solenoid or an external magnet. Reed relays can switch faster than larger relays and require very little power from the control circuit. However, they have relatively low switching current and voltage ratings. Though rare, the reeds can become magnetized over time, which makes them stick \"on\", even when no current is present; changing the orientation of the reeds or degaussing the switch with respect to the solenoid's magnetic field can resolve this problem. Sealed contacts with mercury-wetted contacts have longer operating lives and less contact chatter than any other kind of relay.[24] Safety relays are devices which generally implement protection functions. In the event of a hazard, the task of such a safety function is to use appropriate measures to reduce the existing risk to an acceptable level.[25] A solid-state contactor is a heavy-duty solid state relay, including the necessary heat sink, used where frequent on-off cycles are required, such as with electric heaters, small electric motors, and lighting loads. There are no moving parts to wear out and there is no contact bounce due to vibration. They are activated by AC control signals or DC control signals from programmable logic controllers (PLCs), PCs, transistor-transistor logic (TTL) sources, or other microprocessor and microcontroller controls. Timing relays are arranged for an intentional delay in operating their contacts. A very short (a fraction of a second) delay would use a copper disk between the armature and moving blade assembly. Current flowing in the disk maintains a magnetic field for a short time, lengthening release time. For a slightly longer (up to a minute) delay, a dashpot is used. A dashpot is a piston filled with fluid that is allowed to escape slowly; both air-filled and oil-filled dashpots are used. The time period can be varied by increasing or decreasing the flow rate. For longer time periods, a mechanical clockwork timer is installed. Relays may be arranged for a fixed timing period, or may be field-adjustable, or remotely set from a control panel. Modern microprocessor-based timing relays provide precision timing over a great range. Some relays are constructed with a kind of \"shock absorber\" mechanism attached to the armature, which prevents immediate, full motion when the coil is either energized or de-energized. This addition gives the relay the property of time-delay actuation. Time-delay relays can be constructed to delay armature motion on coil energization, de-energization, or both. Time-delay relay contacts must be specified not only as either normally open or normally closed, but whether the delay operates in the direction of closing or in the direction of opening. The following is a description of the four basic types of time-delay relay contacts. First, we have the normally open, timed-closed (NOTC) contact. This type of contact is normally open when the coil is unpowered (de-energized). The contact is closed by the application of power to the relay coil, but only after the coil has been continuously powered for the specified amount of time. In other words, the direction of the contact's motion (either to close or to open) is identical to a regular NO contact, but there is a delay in closing direction. Because the delay occurs in the direction of coil energization, this type of contact is alternatively known as a normally open, on-delay. A vacuum relay is a sensitive relay having its contacts mounted in an evacuated glass housing, to permit handling radio-frequency voltages [clarification needed] as high as 20,000 volts without flashover between contacts even though contact spacing is as low as a few hundredths of an inch when open. Relays are used wherever it is necessary to control a high power or high voltage circuit with a low power circuit, especially when galvanic isolation is desirable. The first application of relays was in long telegraph lines, whereas the weak signal received at an intermediate station could control a contact, regenerating the signal for further transmission. High-voltage or high-current devices can be controlled with small, low voltage wiring and pilots switches. Operators can be isolated from the high voltage circuit. Low power devices such as microprocessors can drive relays to control electrical loads beyond their direct drive capability. In an automobile, a starter relay allows the high current of the cranking motor to be controlled with small wiring and contacts in the ignition key. Electromechanical switching systems including Strowger and crossbar telephone exchanges made extensive use of relays in ancillary control circuits. The Relay Automatic Telephone Company also manufactured telephone exchanges based solely on relay switching techniques designed by Gotthilf Ansgarius Betulander. The first public relay based telephone exchange in the UK was installed in Fleetwood on 15 July 1922 and remained in service until 1959.[27][28] The use of relays for the logical control of complex switching systems like telephone exchanges was studied by Claude Shannon, who formalized the application of Boolean algebra to relay circuit design in A Symbolic Analysis of Relay and Switching Circuits. Relays can perform the basic operations of Boolean combinatorial logic. For example, the Boolean AND function is realised by connecting normally open relay contacts in series, the OR function by connecting normally open contacts in parallel. Inversion of a logical input can be done with a normally closed contact. Relays were used for control of automated systems for machine tools and production lines. The Ladder programming language is often used for designing relay logic networks. Because relays are much more resistant than semiconductors to nuclear radiation, they are widely used in safety-critical logic, such as the control panels of radioactive waste-handling machinery. Electromechanical protective relays are used to detect overload and other faults on electrical lines by opening and closing circuit breakers. For protection of electrical apparatus and transmission lines, electromechanical relays with accurate operating characteristics were used to detect overload, short-circuits, and other faults. While many such relays remain in use, digital protective relays now provide equivalent and more complex protective functions. Part of a relay interlocking using UK Q-style miniature plug-in relays Railway signalling relays are large considering the mostly small voltages (less than 120 V) and currents (perhaps 100 mA) that they switch. Contacts are widely spaced to prevent flashovers and short circuits over a lifetime that may exceed fifty years. Since rail signal circuits must be highly reliable, special techniques are used to detect and prevent failures in the relay system. To protect against false feeds, double switching relay contacts are often used on both the positive and negative side of a circuit, so that two false feeds are needed to cause a false signal. Not all relay circuits can be proved so there is reliance on construction features such as carbon to silver contacts to resist lightning induced contact welding and to provide AC immunity. Opto-isolators are also used in some instances with railway signalling, especially where only a single contact is to be switched. Contact sequence — \"make before break\" or \"break before make\". For example, the old style telephone exchanges required make-before-break so that the connection did not get dropped while dialing the number. Contact current rating — small relays switch a few amperes, large contactors are rated for up to 3000 amperes, alternating or direct current Operating lifetime, useful life — the number of times the relay can be expected to operate reliably. There is both a mechanical life and a contact life. The contact life is affected by the type of load switched. Breaking load current causes undesired arcing between the contacts, eventually leading to contacts that weld shut or contacts that fail due to erosion by the arc.[29] Coil current — Minimum current required for reliable operation and minimum holding current, as well as effects of power dissipation on coil temperature at various duty cycles. \"Sensitive\" relays operate on a few milliamperes. \"Dry\" contacts — when switching very low level signals, special contact materials may be needed such as gold-plated contacts Contact protection — suppress arcing in very inductive circuits Coil protection — suppress the surge voltage produced when switching the coil current Isolation between coil contacts Aerospace or radiation-resistant testing, special quality assurance Expected mechanical loads due to acceleration — some relays used in aerospace applications are designed to function in shock loads of 50 g, or more. Size — smaller relays often resist mechanical vibration and shock better than larger relays, because of the lower inertia of the moving parts and the higher natural frequencies of smaller parts.[24] Larger relays often handle higher voltage and current than smaller relays. Stray magnetic linkage between coils of adjacent relays on a printed circuit board. There are many considerations involved in the correct selection of a control relay for a particular application, including factors such as speed of operation, sensitivity, and hysteresis. Although typical control relays operate in the 5 ms to 20 ms range, relays with switching speeds as fast as 100 μs are available. Reed relays which are actuated by low currents and switch fast are suitable for controlling small currents. As with any switch, the contact current (unrelated to the coil current) must not exceed a given value to avoid damage. In high-inductance circuits such as motors, other issues must be addressed. When an inductance is connected to a power source, an input surge current or electromotor starting current larger than the steady-state current exists. When the circuit is broken, the current cannot change instantaneously, which creates a potentially damaging arc across the separating contacts. Consequently, for relays used to control inductive loads, we must specify the maximum current that may flow through the relay contacts when it actuates, the make rating; the continuous rating; and the break rating. The make rating may be several times larger than the continuous rating, which is larger than the break rating. Switching while \"wet\" (under load) causes undesired arcing between the contacts, eventually leading to contacts that weld shut or contacts that fail due to a buildup of surface damage caused by the destructive arc energy.[29] Without adequate contact protection, the occurrence of electric current arcing causes significant degradation of the contacts, which suffer significant and visible damage. Every time the relay contacts open or close under load, an electrical arc can occur between the contacts of the relay, either a break arc (when opening), or a make / bounce arc (when closing). In many situations, the break arc is more energetic and thus more destructive, in particular with inductive loads, but this can be mitigated by bridging the contacts with a snubber circuit. The inrush current of tungsten filament incandescent lamps is typically ten times the normal operating current. Thus, relays intended for tungsten loads may use special contact composition, or the relay may have lower contact ratings for tungsten loads than for purely resistive loads. An electrical arc across relay contacts can be very hot — thousands of degrees Fahrenheit — causing the metal on the contact surfaces to melt, pool, and migrate with the current. The extremely high temperature of the arc splits the surrounding gas molecules, creating ozone, carbon monoxide, and other compounds. Over time, the arc energy slowly destroys the contact metal, causing some material to escape into the air as fine particulate matter. This action causes the material in the contacts to degrade and coordination, resulting in device failure. This contact degradation drastically limits the overall life of a relay to a range of about 10,000 to 100,000 operations, a level far below the mechanical life of the device, which can be in excess of 20 million operations.[31] ^EN 50005:1976 \"Specification for low voltage switchgear and controlgear for industrial use. Terminal marking and distinctive number. General rules.\" (1976). In the UK published by BSI as BS 5472:1977."}
{"url": "https://en.m.wikipedia.org/wiki/World_Development_Report", "text": "World Development Report The World Development Report (WDR) is an annual report published since 1978 by the World Bank. Each WDR provides in-depth analysis of a specific aspect of economic development. Past reports have considered such topics as agriculture, youth, equity, public services delivery, the role of the state, transition economies, labour, infrastructure, health, the environment, risk management, and poverty. The reports are the Bank's best-known contribution to thinking about development.[1] World Development Report 2021: Data for Better Lives explores the tremendous potential of the changing data landscape to improve the lives of poor people, while also acknowledging its potential to open back doors that can harm individuals, businesses, and societies. It studies the various uses of data as a public good as well as harnessed by private players to enhance productivity. It explores the mechanism that could be put in place for ensuring the gainful and sustainable use of data . [2] The World Development Report 2019 studies the impact of technology on the nature of work. It is the most-downloaded World Development Report, with more than 2.5 million downloads, a third of which before its official publication. The study was led by Simeon Djankov and Federica Saliola.[3] A summary of the main arguments and data is provided in the Journal of International Affairs.[4] Fears that robots will take away jobs from people have dominated the discussion over the future of work, but the World Development Report 2019 finds that on balance this appears to be unfounded. Work is constantly reshaped by technological progress. Firms adopt new ways of production, markets expand, and societies evolve. The World Development Report 2014 Risk and Opportunity: Managing Risk for Development looked at risk management from a development perspective. It argued that managing risks such as job loss, crime, disease, disaster, social unrest, and financial and macroeconomic turbulence responsibly can save lives, avert damages, prevent development setbacks, and unleash opportunities. The report proposed a conceptual framework for thinking about risk and resilience, identified obstacles to better risk management, and recommended numerous avenues for better risk management that can be pursued by individuals, families, communities, enterprises, governments, and the international community.[5] The World Development Report 2011: Conflict, Security, and Development looked at conflict as a challenge to economic development. It analyzed the nature, causes and development consequences of modern violence and highlight lessons learned from efforts to prevent or recover from violence. The goal of this WDR was considered to promote new ways of preventing or addressing violent conflict. By drawing on insight and experiences from a host of past and present situations, the report identified promising national and regional initiatives as well as directions for change in international responses, and discuss how lessons can be applied in situations of vulnerability to violent conflict.[6] The World Development Report is published by the World Bank. The WDR 2010, on the theme of \"Development and Climate Change,\" explored how public policy can change to better help people cope with new or worsened risks, how land and water management must adapt to better protect a threatened natural environment while feeding an expanding and more prosperous population, and how energy systems will need to be transformed. The report was seen as a call for action, both for developing countries who are striving to ensure policies are adapted to the realities and dangers of a hotter planet, and for high-income countries who need to undertake ambitious mitigation while supporting developing countries' efforts. The WDR 2009 focused on the theme \"Reshaping Economic Geography\".[7] Rising densities of human settlements, migration and transport to reduce distances to market, and specialization and trade facilitated by fewer international divisions are central to economic development. The transformations along these three dimensions—density, distance, and division—are most noticeable in North America, Western Europe, and Japan, but countries in Asia and Eastern Europe are changing in ways similar in scope and speed. The report concludes that these spatial transformations are essential, and should be encouraged. The conclusion is not without controversy. Slum-dwellers now number a billion, but the rush to cities continues. Globalization is believed to benefit many, but not the billion people living in lagging areas of developing nations. High poverty and mortality persist among the world's \"bottom billion\", while others grow wealthier and live longer lives. Concern for these three billion often comes with the prescription that growth must be made spatially balanced. The WDR has a different message: economic growth is seldom balanced, and efforts to spread it out prematurely will jeopardize progress. The WDR 2008 addressed \"Agriculture for Development\", calling for greater investment in agriculture in developing countries. The report warned that the sector must be placed at the center of the development agenda if the goals of halving extreme poverty and hunger by 2015 are to be realized.[8] While 75 percent of the world's poor live in rural areas in developing countries, a mere 4 percent of official development assistance goes to agriculture. In Sub-Saharan Africa, a region heavily reliant on agriculture for overall growth, public spending for farming is also only 4 percent of total government spending and the sector is still taxed at relatively high levels. For the poorest people, GDP growth originating in agriculture is about four times more effective in raising incomes of extremely poor people than GDP growth originating outside the sector. \"A dynamic 'agriculture for development' agenda can benefit the estimated 900 million rural people in the Developing world who live on less than $1 a day, most of whom are engaged in agriculture\", said Robert B. Zoellick, World Bank Group President. \"We need to give agriculture more prominence across the board. At the global level, countries must deliver on vital reforms such as cutting distorting subsidies and opening markets, while civil society groups, especially farmer organizations, need more say in setting the agricultural agenda\". According to the report, agriculture can offer pathways out of poverty if efforts are made to increase productivity in the staple foods sector; connect smallholders to rapidly expanding high-value horticulture, poultry, aquaculture, as well as dairy markets; and generate jobs in the rural nonfarm economy. The WDR 2002 analyzes how to build effective institutions. In understanding what drives institutional change, the report emphasizes the importance of history, highlighting the need to ensure effective institutions through a design that complements existing institutions, human capabilities, and available technologies. The study was guided by Joseph Stiglitz and Roumeen Islam with principal authors Simeon Dyankov and Aart Kraay. Several background papers, including by Nobel Prize winners Robert Shiller, Amartya Sen and Gabriel García Márquez, were published in academic journals or books.[9][10][11][12][13] The World Development Report began as a series of annual publications in the year 1978 with its first report titled \"Prospects for Growth and Alleviation of Poverty.\" Since then, it has focused each year on a particular theme that is central to development and the reports present a detailed study of the relevant sectors, applications and toolkits developed. The reports and their titles are as follows:"}
{"url": "https://www.semanticscholar.org/paper/Robotic-Laboratory-Automation-Boyd/9770e16320fd04543ec636067d4249b5436b502c", "text": "The history and current state of the art in laboratory robotics is discussed, including separate modules for specimen centrifugation and aliquoting, specimen analysis, and post-analytical storage and retrieval. Growing the range of automation options suitable for research laboratories will require more flexible, modular and cheaper designs, and academic and commercial developers of automation will increasingly need to design with an environmental awareness and an understanding that large high-tech robotic solutions may not be appropriate for laboratories with constrained financial and spatial resources. The BioRobot platform not only provides flexibility in test process by carrying out various clinical tests simultaneously through multiple mobile agents, but also increases productivity by having controllable throughput according to amount of tests. The robotic platform, called “BioRobot Platform,” allows us to run a test process more flexibly than traditional sequential test procedures using conveyor belts by simultaneously conducting various clinical tests through multiple mobile agents. The robotic platform, called “BioRobot Platform,” allows us to run a test process more flexibly than traditional sequential test procedures using conveyor belts by simultaneously conducting various clinical tests through multiple mobile agents. A new robotic platform for clinical tests suitable for small or medium sized laboratories using mobile robots based on mobile agents that can control throughput according to the amount of tests is presented. The proposed robotic platform is composed of multiple mobile agents which deliver microplates from one site to another in the platform, a miniature robotic arm, microplate loaders, photometry scanner or other testing devices, and an incubator where mobile agents reside depending on various tests. Some of the current trends in laboratory automation in the pharmaceutical industry as they apply to research and development, including screening, sample management, combinatorial chemistry, ADME/Tox and pharmacokinetics are discussed. The current status of the development of prospective standards for laboratory automation by NCCLS is described and the relationship of those standards to those of other standards organizations is described. Laboratory automation is in its infancy, following a path parallel to the development of laboratory information systems in the late 1970s and early 1980s, and the trend in automation has moved from total laboratory automation to a modular approach."}
{"url": "https://en.m.wikipedia.org/wiki/Mobile_manipulator", "text": "Contents A mobile manipulation system combines the mobility offered by a mobile platform and dexterity offered by the manipulator. The mobile platform offers an extended workspace to the manipulator and more degrees of freedom to operate in. However, the operation of such a system is challenging because of the many degrees of freedom and the unstructured environment that it performs in. A system is generally composed of the mobile platform, the robotic manipulator arm, vision components, and tooling components. Mobile manipulation is a subject of focus in development and research environments. Mobile manipulators, either autonomous or remote operated, are used in many areas, such as space exploration, military operations, home care and health care. Within the industrial field, the implementation of mobile manipulators has been limited. The necessary technology entities (mobile platforms, robot manipulators, vision, and tooling) are, to a large extent, available from off-the-shelf components.[1] Few implementations of mobile robots in the industrial field have been reported due to the center of attention being drawn on optimization of the individual technologies, especially robot manipulators [2] and tooling,[3] while the integration, use, and application have been neglected in the field of industrial mobile manipulation. This means that few implementations of mobile robots, in production environments, have been reported – e.g.[4] and.[5]"}
{"url": "https://en.m.wikipedia.org/wiki/Donald_Routledge_Hill", "text": "Contents Born in London, after secondary school Hill served in the British army, in the Royal Engineers from 1941 to 1946. Two years he served in the Eighth Army in North Africa until he was wounded in action in Italy. Back in England he studied Engineering at the London University, obtaining his engineering degree in 1949. In 1964 he obtained his M.Litt in Islamic History at the University of Durham, and in 1970 his PhD from the University of London.[1] Late 1940s Hill started his career working for the Iraq Petroleum Company in Lebanon, Syria and Qatar. Back in England he worked for several petrochemical companies until his retirement in 1984.[1] Diagram of a hydro-powered perpetual flute from The Book of Knowledge of Ingenious Mechanical Devices by Ismail al-Jazari, 1206."}
{"url": "https://www.semanticscholar.org/paper/The-Double-Burden-of-Malnutrition-in-Countries-the-Prentice/1615e10aa752c17f9814d67d10653c599857e208", "text": "Although the prognosis looks threatening for many poor countries, they have the advantage of being able to learn from the mistakes made by other nations that have passed through the transition before them. 57 Citations The way forward would be to harness the large diversity in India’s food systems with the assistance of local governments and communities and nudging the individuals to a healthy diet and physical exercises using India‟s varied fare of traditional and modern options-this could also be in sync with the ongoing call for localness and selfreliance. For African countries to end all forms of malnutrition, there is a need for political commitment and increased financial investment in nutrition interventional programs, strengthening the evidence-base on key nutrition indicators is also important, and international and grassroots support for comprehensive evidence-informed nutritional interventions are needed. Results from a study of mother-child pairs sampled from Akwa Ibom State in the southern region of Nigeria suggest that more effort be placed on active nutrition surveillance to ascertain malnutrition prevalence and periodically reassess priority challenges. It is demonstrated that WFP activities in Ghana can serve as a platform on which to address the double burden, particularly by targeting the food access, food systems and socioeconomic disadvantage determinants of thedouble burden. Significant spatial patterns associated with undernutrition as identified through the hotspot and cold spot analysis do exist in Uganda and programs targeting to reduce the undernutrition of under-five children in Uganda should consider the spatial distribution of undernutrition and its determinants. It is concluded, pessimistically, that the pandemic of obesity will continue to spread for the foreseeable future, and that the governments and health services of poor countries will have few effective public health levers with which they can try to arrest the trend. An existing double burden of malnutrition in this setting is confirmed, characterized by a high prevalence of undernutrition particularly stunting early in life, with high levels of overweight/obesity in adulthood, particularly among women. It is estimated that childhood undernutrition may have its origins in the foetal period, suggesting a need to intervene early, ideally during pregnancy, with interventions known to reduce FGR and preterm birth. The rationale, design, and methods underlying the SHINE trial are described, which will test the independent and combined effects of protecting babies from fecal ingestion and optimizing nutritional adequacy of infant diet on length and hemoglobin at 18 months of age. Comparison of child growth patterns in 54 countries with WHO standards shows that growth faltering in early childhood is even more pronounced than suggested by previous analyses based on the National Center for Health Statistics reference, confirming the need to scale up interventions during the window of opportunity defined by pregnancy and the first 2 years of life. The objective of the WASH Benefits study is to help fill this knowledge gap by assessing improvements in water quality, sanitation, handwashing and child nutrition—alone and in combination—to rural households with pregnant women in Kenya and Bangladesh. It is proposed that the evidence for periconceptional effects on lifetime health is now so compelling that it calls for new guidance on parental preparation for pregnancy, beginning before conception, to protect the health of offspring."}
{"url": "https://en.m.wikipedia.org/wiki/Fan_death", "text": "Fan death Fan death is a misconception that people have died as a result of running an electric fan in a closed room with no open windows. While the supposed mechanics of fan death are impossible given how electric fans operate, belief in fan death persisted to the mid-2000s in South Korea,[1][2][3] and also to a lesser extent in Japan.[4][5][6] Fan death Electric fans sold in South Korea are equipped with timer knobs that turn them off after a set number of minutes. Contents Where the idea came from is unclear, but fears about electric fans date back to their introduction to Korea, with stories dating to the 1920s and 1930s warning of the risks of nausea, asphyxiation, and facial paralysis from the new technology.[7][8] Air movement will increase sweat evaporation, which cools the body. But in extreme heat and high humidity, sweat evaporation becomes ineffective, so the heat stress placed on the body increases, potentially speeding the onset of heat exhaustion and other detrimental conditions: The American Environmental Protection Agency (EPA) discourages people from using fans in closed rooms without ventilation when the heat index (a combination of temperature and humidity) is above 32 °C (89.6 °F). The EPA does, however, approve of using a fan if a window is open and it is cooler outside, or in a closed room when the heat index is lower.[10] It is alleged that fans may cause asphyxiation by oxygen displacement and carbon dioxide intoxication.[11][12][13][14] In the process of human respiration, inhaled fresh air is exhaled with a lower concentration of oxygen gas (O2) and higher concentration of carbon dioxide gas (CO2), causing a gradual reduction of O2 and buildup of CO2 in a completely unventilated room.[15] However, this is true of any room without ventilation, and a running fan will not greatly improve or worsen the problem. During the summer, mainstream South Korean news sources regularly report alleged cases of fan death. A typical example is this excerpt from the July 4, 2011, edition of The Korea Herald, an English-language newspaper: A man reportedly died on Monday morning after sleeping with an electric fan running. The 59-year-old victim, only known by his surname Min, was found dead with the fan fixed directly at him.[16] This article also noted there was \"no evidence\" the fan caused the death, however. University of Miami researcher Larry Kalkstein says a misunderstanding in translation resulted in his accidental endorsement of the fan death theory, which he denies is a real phenomenon.[17] Ken Jennings, writing for Slate, says that based on \"a recent email survey of contacts in Korea\", opinion seems to be shifting among younger Koreans: \"A decade of Internet skepticism seems to have accomplished what the preceding 75 years could not: convinced a nation that Korean fan death is probably hot air.\"[7] Philip Hiscock, when interviewed by The Star, suggested that fan death's prevalence in Korean beliefs and its potential as a euphemism contributed to the idea's continuation, \"Traditional fairy legends (or) contemporary UFO abductions are used for things that are either inadmissible or untellable in present company. The fact that fan death is well known in Korea (and) can be used to postpone explanations or cover up the truth is very interesting and a very traditional way of going about things.\"[18] If bodies are exposed to electric fans or air conditioners for too long, it causes [the] bodies to lose water and [causes] hypothermia. If directly in contact with [air current from] a fan, this could lead to death from [an] increase of carbon dioxide saturation concentration and decrease of oxygen concentration. The risks are higher for the elderly and patients with respiratory problems. From 2003 [to] 2005, a total of 20 cases were reported through the CISS [Consumer Injury Surveillance System] involving asphyxiations caused by leaving electric fans and air conditioners on while sleeping. To prevent asphyxiation, timers should be set, wind direction should be rotated, and doors should be left open. ^\"http://veritasest.egloos.com/2029688 Strange Harm From Electric Fans] Archived 2013-02-03 at the Wayback Machine\", Jungoe Ilbo (Domestic and International Daily), July 31, 1927, \"The rotating fan blades create a vacuum directly in front, and the intensity of the resulting air flow always results in an insufficient supply of oxygen to the lungs.\" (in Korean) ^Excessive Heat Events GuidebookArchived 2011-08-09 at the Wayback Machine, United States Environmental Protection Agency. \"Annex B: Use of Portable Electric Fans During Excessive Heat Events ... Don't Use a portable electric fan in a closed room without windows or doors open to the outside. ... Don't Use a portable electric fan to blow extremely hot air on yourself. This can accelerate the risk of heat exhaustion. ... Annex C: Excessive Heat Events Guidebook in Brief ... Don't direct the flow of portable electric fans toward yourself when room temperature is hotter than 90 °F.\""}
{"url": "https://web.archive.org/web/20180618235651/http://www.radiocollection.be/images/mble_img/mble_auto17.jpg", "text": "The Internet Archive discovers and captures web pages through many different web crawls. At any given time several distinct crawls are running, some for months, and some every day or longer. View the web archive through the Wayback Machine. Content crawled via the Wayback Machine Live Proxy mostly by the Save Page Now feature on web.archive.org. Liveweb proxy is a component of Internet Archive’s wayback machine project. The liveweb proxy captures the content of a web page in real time, archives it into a ARC or WARC file and returns the ARC/WARC record back to the wayback machine to process. The recorded ARC/WARC file becomes part of the wayback machine in due course of time."}
{"url": "https://pubmed.ncbi.nlm.nih.gov/7872581/", "text": "Affiliation Abstract Objective: To examine the relation between adult weight change and the risk for clinical diabetes mellitus among middle-aged women. Design: Prospective cohort study with follow-up from 1976 to 1990. Setting: 11 U.S. states. Participants: 114,281 female registered nurses aged 30 to 55 years who did not have diagnosed diabetes mellitus, coronary heart disease, stroke, or cancer in 1976. Outcome measures: Non-insulin-dependent diabetes mellitus. Results: 2204 cases of diabetes were diagnosed during 1.49 million person-years of follow-up. After adjustment for age, body mass index was the dominant predictor of risk for diabetes mellitus. Risk increased with greater body mass index, and even women with average weight (body mass index, 24.0 kg/m2) had an elevated risk. Compared with women with stable weight (those who gained or lost less than 5 kg between age 18 years and 1976) and after adjustment for age and body mass index at age 18 years, the relative risk for diabetes mellitus among women who had a weight gain of 5.0 to 7.9 kg was 1.9 (95% CI, 1.5 to 2.3). The corresponding relative risk for women who gained 8.0 to 10.9 kg was 2.7 (CI, 2.1 to 3.3). In contrast, women who lost more than 5.0 kg reduced their risk for diabetes mellitus by 50% or more. These results were independent of family history of diabetes. Conclusion: The excess risk for diabetes with even modest and typical adult weight gain is substantial. These findings support the importance of maintaining a constant body weight throughout adult life and suggest that the 1990 U.S. Department of Agriculture guidelines that allow a substantial weight gain after 35 years of age are misleading."}
{"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9347406/", "text": "Share RESOURCES As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more: PMC Disclaimer | PMC Copyright Notice This is an open access article under the terms of the http://creativecommons.org/licenses/by-nc/4.0/ License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes. Abstract The COVID‐19 crisis witnessed a major rise in investment in software for the digital organisation and rationalisation of work, while investment in robotics is continuously lagging behind expectations. This article argues that we can understand this development as the continuation of the rise of algorithmic management as a technological fix for profitability crises. Thus, in the face of falling wage rates and a structural overaccumulation of capital since the 1970s, algorithmic management has become an alternative to automation. The article reconstructs the history of algorithmic management in connection to economic crises. This allows for periodisation of the rise of algorithmic management from 'computer‐integrated manufacturing' to remote work in four waves. In times of crisis, algorithmic management functions as a substitute for investment in 'tangible capital' such as robots. Structural economic forces thus interact with labour conflicts at the company level, shaping the rise of algorithmic management. INTRODUCTION Digitalisation is probably the single most significant corporate response to the COVID‐19 crisis. Yet this trend may be further differentiated: there is a major rise in investment in software for the digital organisation and rationalisation of work (Amankwah‐Amoah et al., 2021; McKinsey, 2020), while investment in robotics is stagnating (IFR, 2020, 2021; Krzywdzinski et al., 2022). This article argues that this development is the continuation of the rise of algorithmic management as a technological fix for profitability crises and as an alternative to automation. Model‐based studies predict that the COVID‐19 pandemic will lead to an upsurge of automation (Blit, 2020; Chernoff & Warman, 2020). They carry forward a debate on the future of work, which—already before the pandemic—was very much focused on the substitution of work through automation (most prominently Acemoglu & Restrepo, 2019; Brynjolfsson & McAfee, 2014; Frey & Osborne, 2017; for a critical review, see Spencer et al., 2021). As a result, past predictions of a technological end of work were updated once again (e.g., Susskind, 2020). Yet, work shows no sign of vanishing (Spencer, 2018). That these predictions fail to come true is mainly due to their technological determinism, which largely ignores the politico‐economic preconditions of technology implementation (Wajcman, 2017). The decisive factor for implementation is not technological feasibility, but the expected profitability of the investment. Global surveys of managers show that the central motive for automation is cost reduction (Tulieres et al., 2019). If this is the case, there are important technological alternatives to automation. This article argues that in the face of falling wage rates and structural overaccumulation of capital since the 1970s, algorithmic management has turned into such an alternative, since it is increasingly attractive for capital. This article uses a broad definition of algorithmic management, which encompasses the digital direction, evaluation and optimisation of labour processes. In most cases, algorithmic management does not automate management completely but rather augments it. The corresponding technologies can be located at different levels of management, from the control of individual workers to resource planning systems spanning a whole company (Schaupp, 2021c). Yet the latter systems will be considered here only insofar as they affect workers. Several studies have already outlined how algorithmic management transforms the world of work (e.g., Heiland, 2021; Joyce et al., 2019; Lehdonvirta, 2016). However, these studies are very much focused on the so‐called platform or gig economy, while algorithmic management is not limited to this sector. This article will show, by contrast, that algorithmic management in a broad sense originated in manufacturing and is continuously spreading across sectors. The present article makes three main contributions: First, it elaborates the framework of crisis history, which can help identify the agency of capital and labour (and its limits) to shape the dissemination of algorithmic management. This approach also allows for a periodisation of the development of algorithmic management in four waves. All waves are characterised by a new crisis constellation, from the recession of the 1970s to the global coronavirus pandemic. Among the reactions to all of these crises were specific new sociotechnical innovations that aimed to restore profitability. Second, the article demonstrates how these crises are connected to concrete conflicts on the company level, all of which shaped the rise of algorithmic management. Third, it concludes that structural overaccumulation of capital and falling wages contribute to making algorithmic management an attractive alternative to automation—with important implications for the future of work. The first section of the article elaborates the analytical framework of 'crisis history', while each of the following sections deals with one of the four waves of the dissemination of algorithmic management. This process arguably took off with the rise of the idea of 'computer integrated manufacturing' (CIM) in the mid‐1970s. Only a few years later and along with economic financialization, the second wave of algorithmic management followed; it was characterised by the idea of 'lean production'. Since 2010, the emergence of the 'Internet of Things' provided the possibility of equipping all means of production with networked sensor technology, thus initiating the third wave of algorithmic management. The fourth and current wave began with the coronavirus pandemic and its requirement of 'social distancing'. This led to further acceleration in the algorithmic remote control of work, especially in telework and the delivery sector. The final section of the article recapitulates the analysis to elaborate the relationship between economic crises and the agency of capital and labour in the rise of algorithmic work control. Technology and crisis Since the beginning of industrialised work, attempts have been made continuously not only to rationalise work control (by different forms of organisation) but also to automate it. Computers drastically expanded the automation and augmentation of management in algorithmic work control (Zuboff, 1988). The dissemination of digital technology, however, does not follow a strictly technical logic but is bound to continuous crises in capital accumulation (Schiller, 2014). With this in mind, the current article will examine the connection between the rise of algorithmic management and economic crises from 1970 to 2020.1 The emergence of algorithmic management in industry coincides with a global economic turning point: since the mid‐1970s, the rate of reinvestment of profits in fixed capital has been decreasing steadily (Durand & Légé, 2014). In light of this, Aaron Benanav (2020) has shown that economic indicators point toward a decline rather than an increase of investment into automation technology. A surge in automation would be visible in the growth rates of capital stock and productivity. Yet both are declining continually in developed economies (see Figures 1 and 2). Critical political economy has attributed this lack of investment to a crisis of capital overaccumulation (Harvey, 1990; Nachtwey, 2018). This means that while present‐day profits may be high, the realisation of future profits appears uncertain, which keeps firms from expanding their infrastructures—a tendency that gets proportionately stronger with a growing concentration of capital. This phenomenon has turned into a structural crisis of capital overaccumulation across all core capitalist countries, albeit in different phases (Bischoff et al., 2018). David Harvey argues that the roots of overaccumulation cannot be eliminated within the capitalist order: 'The only question, therefore, is how the overaccumulation tendency can be expressed, contained, absorbed, or managed in ways that do not threaten the capitalist social order. We here encounter the heroic side of bourgeois life and politics, in which real choices have to be made if the social order is not to dissolve into chaos' (Harvey, 1990, p. 181). One of those choices is investment in 'intangible capital'.2 This segment of capital has been an exemption from the general decline of investment and, subsequently, has been growing much faster than tangible capital, that is, machines and facilities (Crouzet & Eberly, 2019). Haskel and Westlake (2018, p. 26) have shown that in the European Union and the United States, intangible investment overtook tangible investment during the financial crisis. The gap has continued to widen since then. This development has been attributed to the fact that intangible is more easily scalable and is less excludable than tangible capital (Crouzet & Eberly, 2019, p. 4). Thus, the use of intangible assets enables firms to increase market power and profitability without generating a corresponding increase in fixed capital investment (Orhangazi, 2019). The economic discussion on intangibles is vast. Here it will be limited to one important part of this larger category: business software. While algorithmic management is always connected to physical devices, its genuine innovations are mostly within the realm of software. Haskel and Westlake (2018, p. 18) even claim that 'This is not so much innovation, but innervation—the process of a body part being supplied with nerves, making it sensate, orderly, and controllable'. Because algorithmic rationalisation of the processes of production is cheaper and more flexible than advanced automation infrastructure, it aligns with the requirements of avoiding too much capital fixation and increasing profitability. Accordingly, the proportion of investment in intangibles correlates with capital concentration, suggesting that intangible capital functions as a substitute for tangible capital (Crouzet & Eberly, 2019). These data indicate that the growing importance of algorithmic management over automation may be a symptom of overaccumulation. While some theories of overaccumulation merely deduce concrete economic ruptures from a general law, this article develops a framework that conceptualises a complex relationship between global economic crises and the collective agencies of capital and labour. Richard Edwards (1979) developed a new classical approach that posits the appropriation practices of workers as an important factor in the development of workplace technologies. In his understanding, these are shaped by both the nature and outcome of the workers' struggles: 'The workers' militance created a crisis of control within the firm, a crisis that revealed the flaws in the existing organisation of work. Even though the workers were defeated, the corporations took notice, for after the immediate crisis had passed, corporate capitalists had to deal with the chronic causes underlying the crisis' (Edwards, 1979, p. 49). Edwards sees 'crises of control' as one of the most important motors of the development of new management technologies. In this sense, empirical studies have shown how a successful implementation of new forms of managerial and technological work control depends on the strength of labour unions (e.g., Link & Siegel, 2002). Silver (2003) has transferred this approach to the macro‐level. She argues that labour disputes which lead to less favourable conditions for the utilisation of capital can turn into a viable crisis factor. The reactions of the capital to such crises are an essential motor for the continuous transformation of the sphere of production. Silver calls these reactions 'fixes', and distinguishes among them 'product', 'financial' and 'technological' types (Silver, 2003, p. 39). Although the various fixes are usually intertwined, the technological fix is, of course, central here. In Silver's case, the term refers to both technical innovations (such as the assembly line, which allows mass production and easier control of work at the same time) and organisational transformations (such as the management system of lean production). On the one hand, Silver's schema indicates that technical and organisational measures form a unit in the rationalisation of production as well as the control of labour. On the other hand, it underscores that technical developments in capitalist production are always tied to further dynamics of crises and conflicts, which is why the design and dissemination of technology are historically contingent (see also Noble, 2011). Edwards and Silver conceptualise innovations of control as direct reactions to crises of profitability. However, as we will see in the following sections, the relationship between crisis and technology dissemination is usually not necessarily a direct one. While economic crises are the indirect result of dispersed accumulation strategies, they cannot be addressed by collective actors on the side of capital or labour. Instead, to them, they are external forces, which cannot be controlled but may initiate certain strategic actions. Thus, the technological fixes described in this article do not resolve any crises; they do not even aim to address them. This is because economic crises relate to a separate level of society, which is incommensurate with the company level as the main arena of negotiating the implementation of technology. This article, therefore, proposes that there is an indirect relationship between economic crises and the agency of capital and labour in shaping technology at the workplace. Such a perspective integrates an emphasis on structural forces such as overaccumulation and the collective agency of concrete fractions of capital and labour, which cannot be deduced from these structures. Although this article will refer to various concrete examples of technological projects, the aim is not so much to reconstruct individual motivations behind technology‐related decision making, but rather to examine the relationship between these technological projects and the power relations between capital and labour. This focus on the capital‐labour relation, in turn, is not to deny that digitalisation is also influenced by other factors. Part of the rationale of the latter stems from the capital–capital relation, that is, capital's aim to increase flexibility to reach new markets, to add digital services to products, and so forth (Haskel & Westlake, 2018). Furthermore, the state is a very important actor in the development and dissemination of technology (Mazzucato, 2015) but it will be addressed here only peripherally. Instead of identifying a singular driving force behind the development of algorithmic management, the goal of this heuristic is to raise theoretical awareness of the connection between this development and conflicting socioeconomic dynamics. It will serve as an analytical framework for the following sections, which take a more detailed look at the development of algorithmic management. Computer‐integrated manufacturing (ICM) and the threat of automation In the course of the global oil crisis of 1974 and the subsequent stagflation, economies all over the world experienced the worst recession in postwar history. The world's 24 richest countries saw their growth rates drop from 5% to 0% (Moody, 2007, p. 12). The crisis was followed by a massive boost in the technical rationalisation of production, which entailed especially a surge in automation, achieved by using early forms of robotics and digital production control. This linked automation and computerisation, and the computer became an organisational technology in the production process (Zuboff, 1988). Thus, the vision of CIM was developed, according to which not only the material production was to be taken over by robots, but also the control of plants was to be automated and digitalised. By the mid‐1980s, a genuine CIM‐euphoria had spread to continental Europe. At its centre was the vision of a completely controllable factory—a radical automation of both planning and manufacturing within a deterministic system of hierarchical control. This hierarchy of algorithmic control was formalised in the 'automation pyramid' (Siepmann & Graef, 2016, pp. 49–53). At the top of this pyramid is the enterprise resource planning system (ERP), through which the use of personnel, material, machines and the like is planned. Incoming orders are divided automatically into smaller orders and requirements, which are in turn passed on to the subjacent control level of the manufacturing execution systems (MES). These process‐related control systems are directly linked to the plant control system and thus enable real‐time managing and controlling of production. The lowest level of the pyramid is supervisory control and data acquisition (SCADA)—the direct control of individual production processes. It includes direct execution commands to a production machine, but also technical control of individual human workers (Schaupp & Diab, 2020). With this model, the advocates of CIM (mainly management consultancies) promised the elimination of a range of uncertainties in production. Besides reducing machine downtime or errors, the main aim was to eliminate human labour as a source of disruption—by removing it from the production process. Misbehaviour and disobedience of individual workers always posed a threat to highly rationalised production processes (Ackroyd & Thompson, 2022). But in the 1970s, a global wave of mass strikes by organised industrial labour endangered capital accumulation across this sector (Cowie, 2010). In the Western European automotive industry especially, demand could regularly not be met due to production downtime caused by strikes (Hessler, 2014, p. 63). Consequently, those workers who could not be replaced were subjected to a regime of continuous computer control (Ebers & Lieb, 1989). On the level of the labour process, production machines became computer‐controlled. On the managerial level, ERP systems were used to coordinate and optimise the production process. Empirical studies imply that this contributed to a polarisation in the organisation of work (Goos & Manning, 2007): while computer‐controlled machines were designed to substitute for skilled labour, engineers would act as 'technical virtuosos,' watching over the system, further developing it and correcting its errors. In this sense, the empirical reality of CIM was characterised as 'computer‐aided neo‐Taylorism' (Lutz, 1990). However, a decade after the beginning of the CIM euphoria, the grave disappointment of equal magnitude manifested itself. Even as many companies had invested huge sums in new computer infrastructure, the leaps in productivity it promised had not materialised. The complete digital representation of production processes proved impossible, especially since the networking of the various systems could not yet be realised (Dolata, 1988). Above all, resistance began to emerge from the labour unions in a variety of ways against the job cuts associated with automation (Ebel, 1990; Hessler, 2014). The combination of these factors led to CIM being considered a 'miserable failure' (Brödner, 2015, p. 239) according to the assessment of actors involved at the time. In terms of the relationship between crises and the agency of capital and labour in the implementation of technology, this first wave of algorithmic management provides an important lesson. While the stagflation of the 1970s was the result of capitalist accumulation strategies, its causes were clearly beyond the control of individual capitalists. The 'technological fixes' described in this section thus transferred the problem to the level of capital‐labour relations by responding to the gains of industrial workers. However, when CIM had to prove itself in practice, visions of full automation were soon abandoned as the rate of investment in fixed capital began to revert to tendential decline after a peak in 1974 (Brenner, 2006; Durand & Légé, 2014). From this time on, the direct connection between automation and algorithmic management was severed. It was not the automatic factory itself—which had survived the demise of CIM as a managerial fashion—but rather computer‐controlled machines and ERP systems which would grow in importance, as we will see below. Financialization and lean production The technological fixes of the 1970s could not restore profits to precrisis levels. Therefore, they were complemented by increasing attacks on the social conditions of the Fordist production model, the complex institutional regulation of the economy combined with a network of welfare state security mechanisms. These institutions were now perceived as an obstacle to capital accumulation and were therefore undermined in the period of the global triumph of neoliberalism (Jessop, 1991). The erosion of the Fordist production model led to a steady decrease in wage rates as well as a demise of social security systems. Since the 1980s, wages in all OECD countries shrank in relation to labour productivity. In Germany and the United States particularly, this ratio is extreme (Uguccioni & Sharpe, 2016). Taken together, these developments caused a slump in consumer demand in the Western world, which contributed to making the future amount of product sales uncertain. Such uncertainty resulted in a continuous worldwide decline in the so‐called gross investment ratio, that is, the proportion of profits that are reinvested in capital. In a sense, there was an excess of capital (Brenner, 2006; Nachtwey, 2018). In the face of falling profits in production, surplus capital was increasingly invested in financial markets instead of the expansion of productive technology (Orhangazi, 2008). In the 1990s, the global financial economy grew to unprecedented dimensions. The scope, but above all, the speed of financial transactions, increased enormously. This required a global digital infrastructure that could shrink space and time as much as possible, which in turn created one of the most important initial sources of the demand for a digital infrastructure (Schiller, 1999). Financialization also led to the restructuring of corporations themselves; a large part of turnover and profits shifted to financial markets. This development also affected production. A focus on shareholder value was widely enforced, which above all meant a shift of market boundaries into the companies (Brinkmann, 2011). The central transmission mechanism of this shift was the paradigm of 'lean production'. Originating from the Toyota production model, lean production was intended to enhance product quality and production flexibility (Bertagnolli, 2018; Womack et al., 1990). Efficiency gains were to be realised through the elimination of 'waste' in the dimensions of time, material and labour. Silver (2003) has identified lean production as both a technological and a spatial fix. In technological terms, through the analysis of labour processes, 'dead times' were to be eliminated, while simultaneously, workers were made accountable for the continuous improvement of the production process (Moody, 1997). Since the 1990s, most companies introduced new digital technologies to measure the labour process by means of 'key performance indicators' (KPIs), which are used to gather data on workers (Taylor, 2013). These numeric performance indicators made it possible to transfer investor goals onto workers (Ezzamel et al., 2008). For the labour process, this meant an intensification of work in both quantitative and qualitative terms, as work was not only sped up but tacit skills and knowledge were increasingly demanded (Thompson, 2013). Even though the lean paradigm originated in the automotive industry, its technologies of performance measurement quickly spread to other sectors. They became especially formative for the labour process in call centres and from there spread to nearly all areas of white‐collar work (Bain et al., 2002). Today, 'lean methods' are employed in the administrative sector, the related 'Kanban principle' is used in fast‐food restaurants, 'store floor management' is operative in banks, 'Obeya' used for merchandise trade, 'fast set‐up' common on construction sites, 'value stream analysis' is ubiquitous in zoos, and so‐called 'cardboard engineering' is common in hospitals (Bertagnolli, 2018, p. 5). The term 'lean' was rapidly adapted for use in the description of any kind of continuous process of optimisation and flexibilization. The central organisational instrument for achieving flexibilization was self‐organised teamwork. In this way, mutual support—but also mutual control—via teams promised to replace elements of middle management. Where responsibilities and processes were once previously defined by hierarchical positions and departmental boundaries, now flexibility potential was being sought in teamwork and independent company units or 'profit centres' (Pongratz & Voß, 1997). Empirically, the functional flexibility demanded by the lean production model had a powerful impact on industrial work organisation. Most companies reacted to this requirement, as Toyota had already done, first by reducing staff and second by using instruments of 'flexible' personnel deployment. In concrete terms, this usually meant an increase in temporary work. Thus, production on demand was followed by work on demand (Moody, 1997). For production infrastructure, flexibilization meant a shift away from automation systems spanning a whole plant toward one of decentralised layouts. This was a countermovement against the aim of 'integration' of all processes into a unified computer‐controlled system, which had been central for CIM. In some cases, flexibilization also appears to have been used to reduce the growing disruptive capacity of workers in highly vulnerable integrated automation systems. For example, in 1986, workers at a John Deere tractor plant in Waterloo, United States, claimed redundancy protection in the face of CIM. A strike in one part of the plant led to a shutdown of all production due to the high degree of technical integration. The event led management to restructure the plant along the lines of decentralised 'cellular manufacturing' (Ebel, 1990, p. 68). Such technological fixes were tightly connected to the spatial fix. According to the just‐in‐time (JIT) imperative, products were only to be manufactured or transported when there was a concrete demand. The removal of the product at the end of the process chain was supposed to automatically trigger the information to reorder the same product at the beginning of the chain ('pull'). The development of ERP systems consequently changed from managing a single plant (as in CIM) toward a systemic rationalisation of the entire supply chain (Altmann & Deiß, 1998). This was the technological basis for the development of JIT as an important component of the lean paradigm. Based on the Internet, value chain analysis began to encompass production processes all over the globe (Bruun & Mefford, 2004). Subsequently, ERP systems continued to evolve, combining ever more data to synchronise and optimise processes along the value chain. Such global resource planning and value chain analysis technologies created new opportunities to outsource individual work steps and were used to distribute production processes globally. The intensified competition among workers it detonated has contributed to the falling wage rates at a world scale (Huws, 2003). Yet neither the technological nor the spatial fix succeeded in themselves in weakening the bargaining power of workers. Lean supply chains also resulted in new vulnerabilities. The JIT imperative eliminated all resource buffers to reduce storage costs and reaction times in light of market volatilities. This largely increased the disruptive power of workers, because a strike at a specific 'choke point' could hold up the supply chain in its entirety (Alimahomed‐Wilson & Ness, 2018; Silver, 2003). In sum, lean production brought forward a second wave of algorithmic management insofar as it relied on certain types of digital technologies: digital performance measurement systems at the level of the labour process, and expanded ERP systems for global value stream analysis at the managerial level. Like the former, this second wave of algorithmic management demonstrates a complex relationship between the dissemination of technologies, economic crises and the agency of capital and labour. We have seen that in light of overaccumulation, the capital shifted from production to finance. At the company level, control innovations in performance management and ERP systems functioned as transmission mechanisms of shareholder value in production. Internal and supply‐chain‐based flexibilization also functioned as a technological and special fix to reduce the bargaining power of workers. However, JIT production supported by digital value chain analysis did not eliminate but rather expanded the disruptive capacity to whole supply chains. Postgrowth capitalism and new algorithmic work control In the financial economy, derivatives and other instruments of speculation made it possible to profit without actually producing and selling goods. In a sense, capital was betting on the risks of its own cycles, which created the illusion that value could be generated without labour. This wager was proved wrong in dramatic fashion when the subprime bubble burst in 2008 and set off a crisis that shook the entire world economy (Dyer‐Witheford et al., 2019, p. 143). Even in the aftermath of this global economic crisis, the ongoing overaccumulation crisis of capital still constrained investment in the material production of goods. On the one hand, equipping production facilities with new technologies, especially robotics, is rather expensive; on the other hand, in contrast to the financial markets, invested capital is fixed for a long time and is, therefore, particularly prone to crisis. In view of permanently low growth rates, many companies continued to refrain from major industrial investments. Thus, after the global financial crisis, the major countries were not able to significantly boost economic growth. Even in phases of economic boom, GDP growth in highly developed capitalist economies ranged between 1.5% and 1.8%—with a trend toward a secular stagnation (Galbraith, 2015), which led to the diagnosis of a 'postgrowth capitalism' (Nachtwey, 2018). Salvation was once again seen in digitalisation. Thus, in the mid‐2010s, nearly all high‐tech economies launched state‐sponsored programs to promote digital rationalisation. A few years later, these programs were relaunched under the now more popular term of 'artificial intelligence' (AI). In the United States, the National Artificial Intelligence R&D Strategic Plan was drafted in 2016. China announced the 'Next Generation Artificial Intelligence Development Plan' in 2017. One year later, the EU announced its plan to increase investment in AI research by at least €20 billion by 2020 (Dyer‐Witheford et al., 2019, p. 40). These programs may be interpreted as techno‐political responses to the financial crisis on the state level (Frey & Schaupp, 2020). The technological innovation behind this third wave of algorithmic management was the paradigm of the 'industrial internet of things' (IIoT). This paradigm aims to link all corporate 'things' and processes as well as entire supply chains and connect them to control systems. These control systems are then linked to global monitoring and management systems. The main innovation is that the hierarchical control model of the automation pyramid, which was characteristic for CIM, is turned into a multi‐directional feedback system. Thereby, the previously separate control levels now form an integrated network. However, contrary to the integration model of CIM, the IIoT paradigm aims for flexible adaptability in the sense of cybernetic self‐organisation (Schaupp & Diab, 2020). The microlevel of such a system is constituted by the control of an individual machine or worker. Traditionally, work control systems consist of a screen through which detailed visual instructions are given to workers. Further developments are now partially replacing this interface with technologies that are closer to the body, so‐called 'wearables' (Moore, 2017). The IIoT allows to connect such work control systems to meso‐level control systems to create a feedback loop between resource planning and labour process (Schaupp & Diab, 2020). An example for a meso‐level system of algorithmic management would be KapaflexCy, a digital system for personnel resource planning, which semiautomatically allocates workers to specific tasks based on the data available on them.3 A report by the research team which developed this system (Bauer & Gerlach, 2015) justifies its necessity by explicit reference to stagnating growth rates after 2008. The authors indicates, furthermore, that they expect an increase in market volatility as a result of the trend toward individualised production in the digital economy. KapaflexCy promises to address these problems by offering additional flexibility in personnel deployment. This is a prime example of Silver's technological fix to profitability crises. On the macrolevel of algorithmic management, ERP systems have evolved into cloud platforms, which consolidate process optimisation along the whole supply chain, down to the individual machine. For example, Volkswagen (2020), together with Amazon and Siemens, has developed an industrial cloud, which integrates data of all production systems from every VW factory around the world. Moreover, the corporation's suppliers will be able to integrate their own data via platform interface to synchronise and optimise the whole value chain. Yet there are various accounts of 'algoactivism' (Kellogg et al., 2020) or 'technopolitics from below' (Schaupp, 2021c) in which workers contest the new forms of algorithmic control. In some cases, this leads to the alteration or even abandonment of projects. Thus, while automation is featured prominently in the discourse of 'Industry 4.0', most of its genuine innovations are intangible. Consequently, the growth of the investment share in intangible capital has accelerated since the financial crisis (Haskel & Westlake, 2018), while the share of tangible capital, including robotics, has tended to decline (Moody, 2018). This dynamic is also reflected in the promise of digitalisation initiatives in manufacturing to 'reshore' jobs from low‐wage countries (Coons, 2019). In Germany, the 'Industrie 4.0' initiative has explicitly turned away from the visions of full automation, which were still dominant in CIM. This can be partly attributed to the fact that German trade unions have been involved in the implementation of 'Industrie 4.0' from the beginning (Haipeter, 2020). Thus, the third wave of algorithmic management did not emphasise automation but instead the rationalisation or even reintegration of human labour as algorithmically controlled low‐skilled labour. In this sense, managers in manufacturing explicitly point to algorithmic management as an alternative to automation (Schaupp, 2021b). Haskel and Westlake (2018, p. 191) conclude that 'part of the reason for the perhaps unexpected growth in […] very nonautonomous work is that the intangibles of organisational development and software enable more and more effective monitoring'. The current instantiation of algorithmic management has been enabled by a prior global employers' offensive, which reduced labour costs by means of precarious employment contracts and low wages (Standing, 2011). In many cases, human labour is actually now cheaper than robots and also more flexible. Such flexibility entails both the possibility of simply firing temporary workers when they are no longer needed as well as the option of moving human labour between different workplaces when products change or when other innovations occur. Robots could not adapt to such developments. Studies have shown that the IIoT paradigm radicalises primarily the principles of lean production (Butollo et al., 2019) and that higher adoption levels of Industry 4.0 were more likely in companies that had extensively implemented lean production practices before (Rossini et al., 2019). The JIT imperative especially was radicalised as so‐called 'just in sequence' (JIS): parts are to be delivered not only at the time they are needed but also in the exact order in which they are used. This protocol has also, however, increased the capacity of workers to disrupt tightly integrated supply chains. One example of this is the 1‐week strike at a Volkswagen plant in Györ, Hungary, in January 2019. Due to heavy supply‐chain integration, the strike stopped production in several automobile plants and won the workers an 18% rise in basic wages (Horváth, 2019). Nevertheless, technological fixes were soon applied. They aimed to keep workers at a distance from companies while maintaining close control over them. Coronavirus and remote control of work COVID‐19, the World Bank (2021, p. 3) stated, 'caused a global recession whose depth was surpassed only by the two World Wars and the Great Depression over the past century and a half'. The recession was accompanied by losses of 8.8 percent of global working hours in 2020 relative to the fourth quarter of 2019, the equivalent of 255 million full‐time jobs, and quadruple the losses of the 2008 crisis. Global labour income declined by 8.3% (ILO, 2021). The crisis also worsened the effects of the pre‐existing secular stagnation as described above. According to the World Bank (2021, p. 128), the global contraction in investment in 2020 was 'considerably sharper than during the global financial crisis'. Over the year 2019, investment in industrial robotics had already dropped by 12% globally in the face of economic downturn and trade tensions (IFR, 2020). For the first half of 2020, year‐to‐date numbers show a decline in robotics investment by 18 percent (Demaitre, 2020; see also Xiao, 2020). For the whole year of 2020, the growth rate of robot installations was below 0.5% with 71% of all newly deployed robots installed in Asia (IFR, 2021). A study of five leading companies in the global apparel industry found none of the companies embracing automation as a result of Covid. All respondents noted the reluctance of suppliers to make big investments in technology and their fear of decreasing flexibility. Interviewees in all companies agreed, however, that significant job losses in production are unlikely (Barcia de Mattos et al., 2021). For the US context, a model‐based projection has concluded that in the long term, Covid will decrease the rate of workforce automation (Shutters, 2021). While it is too early to draw definite conclusions, predictions of a new wave of automation in the wake of the pandemic (Blit, 2020; Chernoff & Warman, 2020) do not seem to match empirical data. Already in the decades before the pandemic, the decrease of investment in productive capital did not only contribute to financialization but also to a shift of jobs toward the service sector. It was specifically this sector, which was hit the hardest by the virus and the subsequent lockdowns. This meant that in high‐tech economies such as the United States, a more than a third of the workforce employed before the pandemic was shifted to remote work with office workers being the most likely to experience such a shift (Brynjolfsson et al., 2020). While some managers feared losing control over their staff in home office, others announced that they wanted to keep a high level of remote work after the pandemic, assuming reduced infrastructure costs as well as constant or even increased work productivity (McKinsey, 2020). On the side of labour, some European trade unions have also argued for a right to remote work (Meyer, 2020). An international survey among home office workers shows that most of them believe the pandemic to accelerate the digital transformation and that they will work exclusively 'digitally' even after the pandemic (Nagel, 2020). The effects of this development cannot yet be fully evaluated. Research from before the pandemic suggests that while remote working is associated with higher satisfaction and job‐related well‐being, it also comes with work intensification and a greater inability to 'switch off' (Felstead & Henseke, 2017). The technological basis of this shift to remote work is a fourth wave of algorithmic management, which promises to close the control gap in remote work, mainly via digital surveillance (Faraj et al., 2021). A recent global survey has shown that the demand for employee monitoring software increased by 87% in April 2020 compared with the monthly average before the pandemic. Long‐term demand is expected to stabilise at 58% above its prepandemic level (Migliano, 2021). Generally, the consensus seems to be that accelerated digitalisation is the central reaction to the crisis (Amankwah‐Amoah et al., 2021) with 'digital collaboration' and supply chain management the most important areas of investment (Barcia de Mattos et al., 2021; McKinsey, 2020). Yet the bulk of investment takes place in companies that are already heavily digitalised while the distance of those lagging behind is growing (Krzywdzinski et al., 2022). Software such as Zoom, Slack or Microsoft Teams, is predominantly designed to allow for digital collaboration, yet it also facilitates advances in algorithmic management. This is due to the fact that all communications in these channels are documented in meta‐data. Virtual private network login times reveal if employees have started work late. Server‐side timestamps detect if someone spends an unusual amount of time in a specific portal, and second‐counts of total talk time per speaker in Zoom show who was the quietest in a meeting. These data can be combined with other sets such as employee turnover or performance measures to inform managers: 'Although many companies were ready to analyze data at scale, they didn't have enough of it to analyze because most employees' actions were not recorded and stored. COVID‐19 solved that problem' (Leonardi, 2020, p. 3). As international and national privacy regulations prohibit the automated analysis of individual performance profiles in many regions, such analysis primarily takes place at the team level. In contrast to most other sectors, platform companies profited from the crisis. In the United Kingdom, the food delivery service Deliveroo almost doubled its customer base by delivering consumer goods from supermarkets. In the United States, Amazon aimed to hire 100,000 new delivery and warehouse workers to cope with the exploding demand, but also to disrupt a growing wave of strikes. In the locked‐down cities of Paris and Milan, bike couriers for the food delivery services were often the only people left in the empty streets (Altenried et al., 2020). The pandemic thus accelerated a pre‐existing tendency of platform work to outsource and commodify domestic labour such as cooking (Huws, 2019). Much has been written about the platform economy and its deteriorating effects on working conditions (e.g., Heiland, 2021; Joyce et al., 2019; Woodcock & Graham, 2020). The most important aspect for the purpose of this article is that the central technological basis of its business model is algorithmic work control: only the possibility of the 'remote control' of workers enables the spatial and organisational decoupling of workers from the company. In the delivery sector, this takes the form of navigation systems, which direct workers through the space of the city or the warehouse and track their behaviour at the same time (Heiland, 2021). This remote control may be referred to as 'delocalisation' (Lehdonvirta, 2016). It seems to be exactly this aspect of the delocalisation of work that makes delivery platforms so successful in periods of 'social distancing'. In contrast to automation, new forms of algorithmic management do not only aim at the expulsion of human labour—in the form of qualitative and quantitative rationalisation—but also at its devalued reintegration (Schaupp, 2021b). Detailed digital instructions allow for the hiring of unskilled labour. Many platform companies rely especially on employing migrants, who do not have to speak the local language because they are directed via algorithmic management (Schaupp, 2021a). This makes it possible to reduce labour costs, which in turn makes highly labour‐intensive business models—such as food delivery via bicycle—economically feasible in the first place. Overall, companies have reacted to the Covid crisis with a major shift to remote work. The process of delocalisation this set in train created a crisis of control, which in turn contributed to the further dissemination of algorithmic management. While the applied technologies have all been available for some time, the Covid crisis dramatically accelerated their dissemination, and precipitated a fourth wave of algorithmic management—even before the third wave had been fully rolled out. Yet, in light of the previous sections, employees should be expected to react to the new forms of control, and the practical use of the new digital infrastructures is unlikely to mirror managerial intentions exactly (Faraj et al., 2021). If the financial crisis of 2008 appeared to be beyond the control of all individual or collective actor, the Covid crisis—as 'natural disaster'—was characterised even more definitively by this quality. However, the velocity and severity of the pandemic has been in part due to dangerous practices of organising work: companies' refusal of health measures and their heavily globalised business networks (Bapuji et al., 2020). These managerial practices sparked a global series of workers' struggles for which new 'fixes' are likely to follow in response (Azzellini, 2021). CONCLUSION While the pandemic has once again provoked speculation about an upsurge of automation, the arguments presented here point in another direction: algorithmic management gained importance as a central means of increasing profitability, especially when compared to automation technology. This article has argued that the increase in the relative importance of algorithmic management is to be explained structurally by as part of the crisis of overaccumulation which has afflicted the global economy in various forms since the 1970s. While global rates of investment in fixed capital as a whole are continuously declining, the investment ratio of intangible capital—among them software systems of algorithmic management—has grown since the turn of the millennium and especially since the 2008 financial crisis. Software‐based rationalisation also increases profitability but it is cheaper than investment in advanced production infrastructures and can be applied more flexibly.4 Algorithmic management thus serves as an alternative to automation, especially in places of high capital concentration. This does not mean, however, that the two are mutually exclusive. A considerable part of the enterprise of software development responds to the need to steer more complex automated production systems. Thus, automation does have effects on labour markets, but the theoretical considerations laid out here imply that the future of work in the digital era will not be characterised by an end of work. Algorithmic management especially serves as the technological basis for new, highly labour‐intensive production processes, as in delivery platforms. By overcoming language barriers and simplifying labour processes, such platforms extend the pool of available labour and put downward pressure on wages. Their use makes automation uneconomical by comparison in many cases—and will likely thwart further investment in robotics. Yet the rise of algorithmic management is not only shaped by politico‐economic structures. Macrolevel phenomena, such as the trend toward financialization and its crises after the burst of the subprime bubble, or the global coronavirus pandemic, interact with conflicts on the corporate level in giving rise to specific technological fixes. We have seen how the drive to automation and digital control in CIM served as a means for eliminating contingencies in labour. Later, however, organised labour was seen as a threat even and especially in highly automated production processes, and informed the switch to a more decentralised and flexible production model associated with the lean paradigm. The digitally managed JIT supply chains of the lean model in turn created choke points at which strike action could halt production beyond a single plant. The potential for work stoppage was expanded once supply chains became more integrated via the global IIoT. Finally, the coronavirus pandemic was accompanied by a further rationalisation of work by means of algorithmic remote control of labour processes, which in turn has contributed to a global wave of labour disputes. Economic crises are important hinges between the dissemination of technology and the social relations of production. Through various mediations, they both shape and are shaped by the agency of capital and labour. The interaction of macroeconomic developments and concrete social conflicts—not mere technological potentials—will shape the future of work. CONFLICTS OF INTEREST The author declares no conflicts of interest. ACKNOWLEDGMENTS The author thanks Oliver Nachtwey, Philipp Frey, Simon Joyce and the anonymous reviewers for their helpful comments on different stages of this article. Open access funding provided by Universitat Basel. Notes Schaupp, S. (2022) COVID‐19, economic crises and digitalization: how algorithmic management became an alternative to automation. New Technology, Work and Employment, 1–19. 10.1111/ntwe.12246 [CrossRef] Footnotes 1Such a crisis history must necessarily adopt an international perspective, as neither global economic crises nor the development of means of production can be isolated in a single country. The focus of this article is on the early‐industrialised economies. Felstead, A. & Henseke, G. (2017) Assessing the growth of remote working and its consequences for effort, well‐being and work‐life balance. New Technology, Work and Employment, 32(3), 195–212. Available at: 10.1111/ntwe.12097 [CrossRef] [Google Scholar] Nagel, L. (2020) The influence of the COVID‐19 pandemic on the digital transformation of work. International Journal of Sociology and Social Policy, 40(9/10), 861–875. Available at: 10.1108/IJSSP-07-2020-0323 [CrossRef] [Google Scholar] Noble, D.F. (2011) Forces of production: a social history of industrial automation. New Brunswick: Taylor & Francis. [Google Scholar] Orhangazi, Ö. (2008) Financialization and capital accumulation in the non‐financial corporate sector: a theoretical and empirical investigation on the US economy, 1973–2003. Cambridge Journal of Economics, 32(6), 863–86. [Google Scholar] Spencer, D. (2018) Fear and hope in an age of mass automation: debating the future of work. New Technology, Work and Employment, 33(1), 1–12. Available at: 10.1111/ntwe.12105 [CrossRef] [Google Scholar] Susskind, D. (2020) World without work: technology, automation, and how we should respond. New York: Macmillan. [Google Scholar] Taylor, P. (2013) Performance management and the new workplace tyranny: a report for the scottish trades union congress. Available at: https://strathprints.strath.ac.uk/57598/ [Accessed 27th February 2021]."}
{"url": "https://web.archive.org/web/20180619110302/http://www.radiocollection.be/images/mble_img/mble_auto15.jpg", "text": "The Internet Archive discovers and captures web pages through many different web crawls. At any given time several distinct crawls are running, some for months, and some every day or longer. View the web archive through the Wayback Machine. Content crawled via the Wayback Machine Live Proxy mostly by the Save Page Now feature on web.archive.org. Liveweb proxy is a component of Internet Archive’s wayback machine project. The liveweb proxy captures the content of a web page in real time, archives it into a ARC or WARC file and returns the ARC/WARC record back to the wayback machine to process. The recorded ARC/WARC file becomes part of the wayback machine in due course of time."}
{"url": "https://en.m.wikipedia.org/wiki/Democracy", "text": "Democracy (from Ancient Greek: δημοκρατία, romanized: dēmokratía, dēmos 'people' and kratos 'rule')[1] is a system of government in which state power is vested in the people or the general population of a state.[2] Under a minimalist definition of democracy, rulers are elected through competitive elections while more expansive definitions link democracy to guarantees of civil liberties and human rights in addition to competitive elections.[3][4] The notion of democracy has evolved over time considerably. Throughout history, one can find evidence of direct democracy, in which communities make decisions through popular assembly. Today, the dominant form of democracy is representative democracy, where citizens elect government officials to govern on their behalf such as in a parliamentary or presidential democracy. Most democracies apply in most cases majority rule,[5][6] but in some cases plurality rule, supermajority rule (e.g. constitution) or consensus rule (e.g. Switzerland) are applied. They serve the crucial purpose of inclusiveness and broader legitimacy on sensitive issues—counterbalancing majoritarianism—and therefore mostly take precedence on a constitutional level. In the common variant of liberal democracy, the powers of the majority are exercised within the framework of a representative democracy, but a constitution and supreme court limit the majority and protect the minority—usually through securing the enjoyment by all of certain individual rights, such as freedom of speech or freedom of association.[7][8] The term appeared in the 5th century BC in Greek city-states, notably Classical Athens, to mean \"rule of the people\", in contrast to aristocracy (ἀριστοκρατία, aristokratía), meaning \"rule of an elite\".[9]Western democracy, as distinct from that which existed in antiquity, is generally considered to have originated in city-states such as those in Classical Athens and the Roman Republic, where various degrees of enfranchisement of the free male population were observed. In virtually all democratic governments throughout ancient and modern history, democratic citizenship was initially restricted to an elite class, which was later extended to all adult citizens. In most modern democracies, this was achieved through the suffrage movements of the 19th and 20th centuries. Although democracy is generally understood to be defined by voting,[1][8] no consensus exists on a precise definition of democracy.[13]Karl Popper says that the \"classical\" view of democracy is, \"in brief, the theory that democracy is the rule of the people, and that the people have a right to rule\".[14] One study identified 2,234 adjectives used to describe democracy in the English language.[15] One theory holds that democracy requires three fundamental principles: upward control (sovereignty residing at the lowest levels of authority), political equality, and social norms by which individuals and institutions only consider acceptable acts that reflect the first two principles of upward control and political equality.[20]Legal equality, political freedom and rule of law[21] are often identified by commentators as foundational characteristics for a well-functioning democracy.[13] It has also been suggested that a basic feature of democracy is the capacity of all voters to participate freely and fully in the life of their society.[27] With its emphasis on notions of social contract and the collective will of all the voters, democracy can also be characterised as a form of political collectivism because it is defined as a form of government in which all eligible citizens have an equal say in lawmaking.[28] Democratic assemblies are as old as the human species and are found throughout human history,[33] but up until the nineteenth century, major political figures have largely opposed democracy.[34] Republican theorists linked democracy to small size: as political units grew in size, the likelihood increased that the government would turn despotic.[35][36] At the same time, small political units were vulnerable to conquest.[35]Montesquieu wrote, \"If a republic be small, it is destroyed by a foreign force; if it be large, it is ruined by an internal imperfection.\"[37] According to Johns Hopkins University political scientist Daniel Deudney, the creation of the United States, with its large size and its system of checks and balances, was a solution to the dual problems of size.[35][pages needed] Retrospectively different polities, outside of declared democracies, have been described as proto-democratic. The term democracy first appeared in ancient Greek political and philosophical thought in the city-state of Athens during classical antiquity.[43][44] The word comes from dêmos '(common) people' and krátos 'force/might'.[45] Under Cleisthenes, what is generally held as the first example of a type of democracy in 508–507 BC was established in Athens. Cleisthenes is referred to as \"the father of Athenian democracy\".[46] The first attested use of the word democracy is found in prose works of the 430s BC, such as Herodotus' Histories, but its usage was older by several decades, as two Athenians born in the 470s were named Democrates, a new political name—likely in support of democracy—given at a time of debates over constitutional issues in Athens. Aeschylus also strongly alludes to the word in his play The Suppliants, staged in c.463 BC, where he mentions \"the demos's ruling hand\" [demou kratousa cheir]. Before that time, the word used to define the new political system of Cleisthenes was probably isonomia, meaning political equality.[47] Athenian democracy took the form of a direct democracy, and it had two distinguishing features: the random selection of ordinary citizens to fill the few existing government administrative and judicial offices,[48] and a legislative assembly consisting of all Athenian citizens.[49] All eligible citizens were allowed to speak and vote in the assembly, which set the laws of the city state. However, Athenian citizenship excluded women, slaves, foreigners (μέτοικοι / métoikoi), and youths below the age of military service.[50][51][contradictory] Effectively, only 1 in 4 residents in Athens qualified as citizens. Owning land was not a requirement for citizenship.[52] The exclusion of large parts of the population from the citizen body is closely related to the ancient understanding of citizenship. In most of antiquity the benefit of citizenship was tied to the obligation to fight war campaigns.[53] Athenian democracy was not only direct in the sense that decisions were made by the assembled people, but also the most direct in the sense that the people through the assembly, boule and courts of law controlled the entire political process and a large proportion of citizens were involved constantly in the public business.[54] Even though the rights of the individual were not secured by the Athenian constitution in the modern sense (the ancient Greeks had no word for \"rights\"[55]), those who were citizens of Athens enjoyed their liberties not in opposition to the government but by living in a city that was not subject to another power and by not being subjects themselves to the rule of another person.[56] Range voting appeared in Sparta as early as 700 BC. The Spartan ecclesia was an assembly of the people, held once a month, in which every male citizen of at least 20 years of age could participate. In the assembly, Spartans elected leaders and cast votes by range voting and shouting (the vote is then decided on how loudly the crowd shouts). Aristotle called this \"childish\", as compared with the stone voting ballots used by the Athenian citizenry. Sparta adopted it because of its simplicity, and to prevent any biased voting, buying, or cheating that was predominant in the early democratic elections.[57] Even though the Roman Republic contributed significantly to many aspects of democracy, only a minority of Romans were citizens with votes in elections for representatives. The votes of the powerful were given more weight through a system of weighted voting, so most high officials, including members of the Senate, came from a few wealthy and noble families.[58] In addition, the overthrow of the Roman Kingdom was the first case in the Western world of a polity being formed with the explicit purpose of being a republic, although it didn't have much of a democracy. The Roman model of governance inspired many political thinkers over the centuries.[59] Other cultures, such as the Iroquois Nation in the Americas also developed a form of democratic society between 1450 and 1660 (and possibly in 1142[60]), well before contact with the Europeans. This democracy continues to the present day and is the world's oldest standing representative democracy.[61][62] While most regions in Europe during the Middle Ages were ruled by clergy or feudal lords, there existed various systems involving elections or assemblies, although often only involving a small part of the population. In Scandinavia, bodies known as things consisted of freemen presided by a lawspeaker. These deliberative bodies were responsible for settling political questions, and variants included the Althing in Iceland and the Løgting in the Faeroe Islands.[63][64] The veche, found in Eastern Europe, was a similar body to the Scandinavian thing. In the Roman Catholic Church, the pope has been elected by a papal conclave composed of cardinals since 1059. The first documented parliamentary body in Europe was the Cortes of León. Established by Alfonso IX in 1188, the Cortes had authority over setting taxation, foreign affairs and legislating, though the exact nature of its role remains disputed.[65] The Republic of Ragusa, established in 1358 and centered around the city of Dubrovnik, provided representation and voting rights to its male aristocracy only. Various Italian city-states and polities had republic forms of government. For instance, the Republic of Florence, established in 1115, was led by the Signoria whose members were chosen by sortition. In 10th–15th century Frisia, a distinctly non-feudal society, the right to vote on local matters and on county officials was based on land size. The Kouroukan Fouga divided the Mali Empire into ruling clans (lineages) that were represented at a great assembly called the Gbara. However, the charter made Mali more similar to a constitutional monarchy than a democratic republic. The Parliament of England had its roots in the restrictions on the power of kings written into Magna Carta (1215), which explicitly protected certain rights of the King's subjects and implicitly supported what became the English writ of habeas corpus, safeguarding individual freedom against unlawful imprisonment with right to appeal.[66][67] The first representative national assembly in England was Simon de Montfort's Parliament in 1265.[68][69] The emergence of petitioning is some of the earliest evidence of parliament being used as a forum to address the general grievances of ordinary people. However, the power to call parliament remained at the pleasure of the monarch.[70] Studies have linked the emergence of parliamentary institutions in Europe during the medieval period to urban agglomeration and the creation of new classes, such as artisans,[71] as well as the presence of nobility and religious elites.[72] Scholars have also linked the emergence of representative government to Europe's relative political fragmentation.[73] Political scientist David Stasavage links the fragmentation of Europe, and its subsequent democratization, to the manner in which the Roman Empire collapsed: Roman territory was conquered by small fragmented groups of Germanic tribes, thus leading to the creation of small political units where rulers were relatively weak and needed the consent of the governed to ward off foreign threats.[74] In Poland, noble democracy was characterized by an increase in the activity of the middle nobility, which wanted to increase their share in exercising power at the expense of the magnates. Magnates dominated the most important offices in the state (secular and ecclesiastical) and sat on the royal council, later the senate. The growing importance of the middle nobility had an impact on the establishment of the institution of the land sejmik (local assembly), which subsequently obtained more rights. During the fifteenth and first half of the sixteenth century, sejmiks received more and more powers and became the most important institutions of local power. In 1454, Casimir IV Jagiellon granted the sejmiks the right to decide on taxes and to convene a mass mobilization in the Nieszawa Statutes. He also pledged not to create new laws without their consent.[75] In 17th century England, there was renewed interest in Magna Carta.[76] The Parliament of England passed the Petition of Right in 1628 which established certain liberties for subjects. The English Civil War (1642–1651) was fought between the King and an oligarchic but elected Parliament,[77][78] during which the idea of a political party took form with groups debating rights to political representation during the Putney Debates of 1647.[79] Subsequently, the Protectorate (1653–59) and the English Restoration (1660) restored more autocratic rule, although Parliament passed the Habeas Corpus Act in 1679 which strengthened the convention that forbade detention lacking sufficient cause or evidence. After the Glorious Revolution of 1688, the Bill of Rights was enacted in 1689 which codified certain rights and liberties and is still in effect. The Bill set out the requirement for regular elections, rules for freedom of speech in Parliament and limited the power of the monarch, ensuring that, unlike much of Europe at the time, royal absolutism would not prevail.[80][81] Economic historians Douglass North and Barry Weingast have characterized the institutions implemented in the Glorious Revolution as a resounding success in terms of restraining the government and ensuring protection for property rights.[82] Renewed interest in the Magna Carta, the English Civil War, and the Glorious Revolution in the 17th century prompted the growth of political philosophy on the British Isles. Thomas Hobbes was the first philosopher to articulate a detailed social contract theory. Writing in the Leviathan (1651), Hobbes theorized that individuals living in the state of nature led lives that were \"solitary, poor, nasty, brutish and short\" and constantly waged a war of all against all. In order to prevent the occurrence of an anarchic state of nature, Hobbes reasoned that individuals ceded their rights to a strong, authoritarian power. In other words, Hobbes advocated for an absolute monarchy which, in his opinion, was the best form of government. Later, philosopher and physician John Locke would posit a different interpretation of social contract theory. Writing in his Two Treatises of Government (1689), Locke posited that all individuals possessed the inalienable rights to life, liberty and estate (property).[83] According to Locke, individuals would voluntarily come together to form a state for the purposes of defending their rights. Particularly important for Locke were property rights, whose protection Locke deemed to be a government's primary purpose.[84] Furthermore, Locke asserted that governments were legitimate only if they held the consent of the governed. For Locke, citizens had the right to revolt against a government that acted against their interest or became tyrannical. Although they were not widely read during his lifetime, Locke's works are considered the founding documents of liberal thought and profoundly influenced the leaders of the American Revolution and later the French Revolution.[85] His liberal democratic framework of governance remains the preeminent form of democracy in the world. In the Cossack republics of Ukraine in the 16th and 17th centuries, the Cossack Hetmanate and Zaporizhian Sich, the holder of the highest post of Hetman was elected by the representatives from the country's districts. In North America, representative government began in Jamestown, Virginia, with the election of the House of Burgesses (forerunner of the Virginia General Assembly) in 1619. English Puritans who migrated from 1620 established colonies in New England whose local governance was democratic;[86] although these local assemblies had some small amounts of devolved power, the ultimate authority was held by the Crown and the English Parliament. The Puritans (Pilgrim Fathers), Baptists, and Quakers who founded these colonies applied the democratic organisation of their congregations also to the administration of their communities in worldly matters.[87][88][89] During the Age of Liberty in Sweden (1718–1772), civil rights were expanded and power shifted from the monarch to parliament.[94] The taxed peasantry was represented in parliament, although with little influence, but commoners without taxed property had no suffrage. Colonial America had similar property qualifications as Britain, and in the period before 1776 the abundance and availability of land meant that large numbers of colonists met such requirements with at least 60 per cent of adult white males able to vote.[96] The great majority of white men were farmers who met the property ownership or taxpaying requirements. With few exceptions no blacks or women could vote. Vermont, which, on declaring independence of Great Britain in 1777, adopted a constitution modelled on Pennsylvania's with citizenship and democratic suffrage for males with or without property.[97] The United States Constitution of 1787 is the oldest surviving, still active, governmental codified constitution. The Constitution provided for an elected government and protected civil rights and liberties, but did not end slavery nor extend voting rights in the United States, instead leaving the issue of suffrage to the individual states.[98] Generally, states limited suffrage to white male property owners and taxpayers.[99] At the time of the first Presidential election in 1789, about 6% of the population was eligible to vote.[100] The Naturalization Act of 1790 limited U.S. citizenship to whites only.[101] The Bill of Rights in 1791 set limits on government power to protect personal freedoms but had little impact on judgements by the courts for the first 130 years after ratification.[102] In 1876 the Ottoman Empire transitioned from an absolute monarchy to a constitutional one, and held two elections the next year to elect members to her newly formed parliament.[117] Provisional Electoral Regulations were issued, stating that the elected members of the Provincial Administrative Councils would elect members to the first Parliament. Later that year, a new constitution was promulgated, which provided for a bicameral Parliament with a Senate appointed by the Sultan and a popularly elected Chamber of Deputies. Only men above the age of 30 who were competent in Turkish and had full civil rights were allowed to stand for election. Reasons for disqualification included holding dual citizenship, being employed by a foreign government, being bankrupt, employed as a servant, or having \"notoriety for ill deeds\". Full universal suffrage was achieved in 1934.[118] In 1893 the self-governing colony New Zealand became the first country in the world (except for the short-lived 18th-century Corsican Republic) to establish active universal suffrage by recognizing women as having the right to vote.[119] The number of nations 1800–2003 scoring 8 or higher on Polity IV scale, another widely used measure of democracy[needs update] 20th-century transitions to liberal democracy have come in successive \"waves of democracy\", variously resulting from wars, revolutions, decolonisation, and religious and economic circumstances.[120] Global waves of \"democratic regression\" reversing democratization, have also occurred in the 1920s and 30s, in the 1960s and 1970s, and in the 2010s.[121][122] The war was followed by decolonisation, and again most of the new independent states had nominally democratic constitutions. India emerged as the world's largest democracy and continues to be so.[125] Countries that were once part of the British Empire often adopted the British Westminster system.[126][127] By 1960, the vast majority of country-states were nominally democracies, although most of the world's populations lived in nominal democracies that experienced sham elections, and other forms of subterfuge (particularly in \"Communist\" states and the former colonies.) A subsequent wave of democratisation brought substantial gains toward true liberal democracy for many states, dubbed \"third wave of democracy\". Portugal, Spain, and several of the military dictatorships in South America returned to civilian rule in the 1970s and 1980s.[nb 2] This was followed by countries in East and South Asia by the mid-to-late 1980s. Economic malaise in the 1980s, along with resentment of Soviet oppression, contributed to the collapse of the Soviet Union, the associated end of the Cold War, and the democratisation and liberalisation of the former Eastern bloc countries. The most successful of the new democracies were those geographically and culturally closest to western Europe, and they are now either part of the European Union or candidate states. In 1986, after the toppling of the most prominent Asian dictatorship, the only democratic state of its kind at the time emerged in the Philippines with the rise of Corazon Aquino, who would later be known as the Mother of Asian Democracy. Corazon Aquino taking the Oath of Office, becoming the first female president in Asia According to Freedom House, in 2007 there were 123 electoral democracies (up from 40 in 1972).[129] According to World Forum on Democracy, electoral democracies now represent 120 of the 192 existing countries and constitute 58.2 per cent of the world's population. At the same time liberal democracies i.e. countries Freedom House regards as free and respectful of basic human rights and the rule of law are 85 in number and represent 38 per cent of the global population.[130] Also in 2007 the United Nations declared 15 September the International Day of Democracy.[131] Many countries reduced their voting age to 18 years; the major democracies began to do so in the 1970s starting in Western Europe and North America.[132][failed verification][133][134] Most electoral democracies continue to exclude those younger than 18 from voting.[135] The voting age has been lowered to 16 for national elections in a number of countries, including Brazil, Austria, Cuba, and Nicaragua. In California, a 2004 proposal to permit a quarter vote at 14 and a half vote at 16 was ultimately defeated. In 2008, the German parliament proposed but shelved a bill that would grant the vote to each citizen at birth, to be used by a parent until the child claims it for themselves. \"Democratic backsliding\" in the 2010s were attributed to economic inequality and social discontent,[141] personalism,[142] poor government's management of the COVID-19 pandemic,[143][144] as well as other factors such as manipulation of civil society, \"toxic polarization\", foreign disinformation campaigns,[145] racism and nativism, excessive executive power,[146][147][148] and decreased power of the opposition.[149] Within English-speaking Western democracies, \"protection-based\" attitudes combining cultural conservatism and leftist economic attitudes were the strongest predictor of support for authoritarian modes of governance.[150] A common view among early and renaissance Republican theorists was that democracy could only survive in small political communities.[153] Heeding the lessons of the Roman Republic's shift to monarchism as it grew larger or smaller, these Republican theorists held that the expansion of territory and population inevitably led to tyranny.[153] Democracy was therefore highly fragile and rare historically, as it could only survive in small political units, which due to their size were vulnerable to conquest by larger political units.[153]Montesquieu famously said, \"if a republic is small, it is destroyed by an outside force; if it is large, it is destroyed by an internal vice.\"[153]Rousseau asserted, \"It is, therefore the natural property of small states to be governed as a republic, of middling ones to be subject to a monarch, and of large empires to be swayed by a despotic prince.\"[153] The theory of aggregative democracy claims that the aim of the democratic processes is to solicit citizens' preferences and aggregate them together to determine what social policies society should adopt. Therefore, proponents of this view hold that democratic participation should primarily focus on voting, where the policy with the most votes gets implemented. Different variants of aggregative democracy exist. Under minimalism, democracy is a system of government in which citizens have given teams of political leaders the right to rule in periodic elections. According to this minimalist conception, citizens cannot and should not \"rule\" because, for example, on most issues, most of the time, they have no clear views or their views are not well-founded. Joseph Schumpeter articulated this view most famously in his book Capitalism, Socialism, and Democracy.[155] Contemporary proponents of minimalism include William H. Riker, Adam Przeworski, Richard Posner. According to the theory of direct democracy, on the other hand, citizens should vote directly, not through their representatives, on legislative proposals. Proponents of direct democracy offer varied reasons to support this view. Political activity can be valuable in itself, it socialises and educates citizens, and popular participation can check powerful elites. Most importantly, citizens do not rule themselves unless they directly decide laws and policies. Governments will tend to produce laws and policies that are close to the views of the median voter—with half to their left and the other half to their right. This is not a desirable outcome as it represents the action of self-interested and somewhat unaccountable political elites competing for votes. Anthony Downs suggests that ideological political parties are necessary to act as a mediating broker between individual and governments. Downs laid out this view in his 1957 book An Economic Theory of Democracy.[156] Robert A. Dahl argues that the fundamental democratic principle is that, when it comes to binding collective decisions, each person in a political community is entitled to have his/her interests be given equal consideration (not necessarily that all people are equally satisfied by the collective decision). He uses the term polyarchy to refer to societies in which there exists a certain set of institutions and procedures which are perceived as leading to such democracy. First and foremost among these institutions is the regular occurrence of free and open elections which are used to select representatives who then manage all or most of the public policy of the society. However, these polyarchic procedures may not create a full democracy if, for example, poverty prevents political participation.[157] Similarly, Ronald Dworkin argues that \"democracy is a substantive, not a merely procedural, ideal.\"[158] Deliberative democracy is based on the notion that democracy is government by deliberation. Unlike aggregative democracy, deliberative democracy holds that, for a democratic decision to be legitimate, it must be preceded by authentic deliberation, not merely the aggregation of preferences that occurs in voting. Authentic deliberation is deliberation among decision-makers that is free from distortions of unequal political power, such as power a decision-maker obtained through economic wealth or the support of interest groups.[159][160][161] If the decision-makers cannot reach consensus after authentically deliberating on a proposal, then they vote on the proposal using a form of majority rule. Citizens assemblies are considered by many scholars as practical examples of deliberative democracy,[162][163][164] with a recent OECD report identifying citizens assemblies as an increasingly popular mechanism to involve citizens in governmental decision-making.[165] Radical democracy is based on the idea that there are hierarchical and oppressive power relations that exist in society. Democracy's role is to make visible and challenge those relations by allowing for difference, dissent and antagonisms in decision-making processes. The democracies indices differ in whether they are categorical, such as classifying countries into democracies, hybrid regimes, and autocracies,[168][169] or continuous values.[170] The qualitative nature of democracy indices enables data analytical approaches for studying causal mechanisms of regime transformation processes. Because democracy is an overarching concept that includes the functioning of diverse institutions which are not easy to measure, limitations exist in quantifying and econometrically measuring the potential effects of democracy or its relationship with other phenomena—whether inequality, poverty, education etc.[175] Given the constraints in acquiring reliable data with within-country variations on aspects of democracy, academics have largely studied cross-country variations, yet variations in democratic institutions can be large within countries. Another way of conceiving the difficulties in measuring democracy is through the debate between minimalist versus maximalist definitions of democracy. A minimalist conception of democracy defines democracy by primarily considering the essence of democracy; such as electoral procedures.[176] A maximalist definition of democracy can include outcomes, such as economic or administrative efficiency, into measures of democracy.[177] Some aspects of democracy, such as responsiveness[178] or accountability, are generally not included in democracy indices due to the difficulty measuring these aspects. Other aspects, such as judicial independence or quality of the electoral system, are included in some democracy indices but not in others. Democracy has taken a number of forms, both in theory and practice. Some varieties of democracy provide better representation and more freedom for their citizens than others.[179][180] However, if any democracy is not structured to prohibit the government from excluding the people from the legislative process, or any branch of government from altering the separation of powers in its favour, then a branch of the system can accumulate too much power and destroy the democracy.[181][182][183] Several variants of democracy exist, but there are two basic forms, both of which concern how the whole body of all eligible citizens executes its will. One form of democracy is direct democracy, in which all eligible citizens have active participation in the political decision making, for example voting on policy initiatives directly.[184] In most modern democracies, the whole body of eligible citizens remain the sovereign power but political power is exercised indirectly through elected representatives; this is called a representative democracy. A Landsgemeinde (in 2009) of the canton of Glarus, an example of direct democracy in SwitzerlandIn Switzerland, without needing to register, every citizen receives ballot papers and information brochures for each vote (and can send it back by post). Switzerland has a direct democracy system and votes (and elections) are organised about four times a year; here, to Berne's citizen in November 2008 about 5 national, 2 cantonal, 4 municipal referendums, and 2 elections (government and parliament of the City of Berne) to take care of at the same time. Direct democracy is a political system where the citizens participate in the decision-making personally, contrary to relying on intermediaries or representatives. A direct democracy gives the voting population the power to: Some modern democracies that are predominantly representative in nature also heavily rely upon forms of political action that are directly democratic. These democracies, which combine elements of representative democracy and direct democracy, are termed semi-direct democracies or participatory democracies. Examples include Switzerland and some U.S. states, where frequent use is made of referendums and initiatives. The Swiss confederation is a semi-direct democracy.[186] At the federal level, citizens can propose changes to the constitution (federal popular initiative) or ask for a referendum to be held on any law voted by the parliament.[186] Between January 1995 and June 2005, Swiss citizens voted 31 times, to answer 103 questions (during the same period, French citizens participated in only two referendums).[186] Although in the past 120 years less than 250 initiatives have been put to referendum.[191] Examples include the extensive use of referendums in the US state of California, which is a state that has more than 20 million voters.[192] In New England, town meetings are often used, especially in rural areas, to manage local government. This creates a hybrid form of government, with a local direct democracy and a representative state government. For example, most Vermont towns hold annual town meetings in March in which town officers are elected, budgets for the town and schools are voted on, and citizens have the opportunity to speak and be heard on political matters.[193] The use of a lot system, a characteristic of Athenian democracy, is a feature of some versions of direct democracies. In this system, important governmental and administrative tasks are performed by citizens picked from a lottery.[194] Representative democracy involves the election of government officials by the people being represented. If the head of state is also democratically elected then it is called a democratic republic.[195] The most common mechanisms involve election of the candidate with a majority or a plurality of the votes. Most western countries have representative systems.[186] Representatives may be elected or become diplomatic representatives by a particular district (or constituency), or represent the entire electorate through proportional systems, with some using a combination of the two. Some representative democracies also incorporate elements of direct democracy, such as referendums.[196] A characteristic of representative democracy is that while the representatives are elected by the people to act in the people's interest, they retain the freedom to exercise their own judgement as how best to do so. Such reasons have driven criticism upon representative democracy,[197][198] pointing out the contradictions of representation mechanisms with democracy[199][200] Parliamentary democracy is a representative democracy where government is appointed by or can be dismissed by, representatives as opposed to a \"presidential rule\" wherein the president is both head of state and the head of government and is elected by the voters. Under a parliamentary democracy, government is exercised by delegation to an executive ministry and subject to ongoing review, checks and balances by the legislative parliament elected by the people.[201][202][203][204] In a parliamentary system, the Prime Minister may be dismissed by the legislature at any point in time for not meeting the expectations of the legislature. This is done through a Vote of No Confidence where the legislature decides whether or not to remove the Prime Minister from office with majority support for dismissal.[205] In some countries, the Prime Minister can also call an election at any point in time, typically when the Prime Minister believes that they are in good favour with the public as to get re-elected. In other parliamentary democracies, extra elections are virtually never held, a minority government being preferred until the next ordinary elections. An important feature of the parliamentary democracy is the concept of the \"loyal opposition\". The essence of the concept is that the second largest political party (or opposition) opposes the governing party (or coalition), while still remaining loyal to the state and its democratic principles. Presidential Democracy is a system where the public elects the president through an election. The president serves as both the head of state and head of government controlling most of the executive powers. The president serves for a specific term and cannot exceed that amount of time. The legislature often has limited ability to remove a president from office. Elections typically have a fixed date and aren't easily changed. The president has direct control over the cabinet, specifically appointing the cabinet members.[205] The executive usually has the responsibility to execute or implement legislation and may have the limited legislative powers, such as a veto. However, a legislative branch passes legislation and budgets. This provides some measure of separation of powers. In consequence, however, the president and the legislature may end up in the control of separate parties, allowing one to block the other and thereby interfere with the orderly operation of the state. This may be the reason why presidential democracy is not very common outside the Americas, Africa, and Central and Southeast Asia.[205] A semi-presidential system is a system of democracy in which the government includes both a prime minister and a president. The particular powers held by the prime minister and president vary by country.[205] Elite upper houses of legislatures, which often had lifetime or hereditary tenure, were common in many states. Over time, these either had their powers limited (as with the British House of Lords) or else became elective and remained powerful (as with the Australian Senate). The term republic has many different meanings, but today often refers to a representative democracy with an elected head of state, such as a president, serving for a limited term, in contrast to states with a hereditary monarch as a head of state, even if these states also are representative democracies with an elected or appointed head of government such as a prime minister.[207] The Founding Fathers of the United States often criticised direct democracy, which in their view often came without the protection of a constitution enshrining inalienable rights; James Madison argued, especially in The Federalist No. 10, that what distinguished a direct democracy from a republic was that the former became weaker as it got larger and suffered more violently from the effects of faction, whereas a republic could get stronger as it got larger and combats faction by its very structure.[208] Professors Richard Ellis of Willamette University and Michael Nelson of Rhodes College argue that much constitutional thought, from Madison to Lincoln and beyond, has focused on \"the problem of majority tyranny\". They conclude, \"The principles of republican government embedded in the Constitution represent an effort by the framers to ensure that the inalienable rights of life, liberty, and the pursuit of happiness would not be trampled by majorities.\"[209] What was critical to American values, John Adams insisted,[210] was that the government be \"bound by fixed laws, which the people have a voice in making, and a right to defend.\" As Benjamin Franklin was exiting after writing the U.S. constitution, Elizabeth Willing Powel[211] asked him \"Well, Doctor, what have we got—a republic or a monarchy?\". He replied \"A republic—if you can keep it.\"[212] A liberal democracy is a representative democracy in which the ability of the elected representatives to exercise decision-making power is subject to the rule of law, and moderated by a constitution or laws that emphasise the protection of the rights and freedoms of individuals, and which places constraints on the leaders and on the extent to which the will of the majority can be exercised against the rights of minorities (see civil liberties). In a liberal democracy, it is possible for some large-scale decisions to emerge from the many individual decisions that citizens are free to make. In other words, citizens can \"vote with their feet\" or \"vote with their dollars\", resulting in significant informal government-by-the-masses that exercises many \"powers\" associated with formal government elsewhere. Within Marxist orthodoxy there is a hostility to what is commonly called \"liberal democracy\", which is referred to as parliamentary democracy because of its centralised nature. Because of orthodox Marxists' desire to eliminate the political elitism they see in capitalism, Marxists, Leninists and Trotskyists believe in direct democracy implemented through a system of communes (which are sometimes called soviets). This system can begin with workplace democracy and ultimately manifests itself as council democracy. Some anarcho-communists oppose the majoritarian nature of direct democracy, feeling that it can impede individual liberty and opt-in favour of a non-majoritarian form of consensus democracy, similar to Proudhon's position on direct democracy.[216] Consociational democracy, also called consociationalism, is a form of democracy based on power-sharing formula between elites representing the social groups within the society. In 1969, Arendt Lijphart argued this would stabilize democracies with factions.[218] A consociational democracy allows for simultaneous majority votes in two or more ethno-religious constituencies, and policies are enacted only if they gain majority support from both or all of them. The Qualified majority voting rule in European Council of Ministers is a consociational democracy approach for supranational democracies. This system in Treaty of Rome allocates votes to member states in part according to their population, but heavily weighted in favour of the smaller states. A consociational democracy requires consensus of representatives, while consensus democracy requires consensus of electorate.[needs update] Inclusive democracy is a political theory and political project that aims for direct democracy in all fields of social life: political democracy in the form of face-to-face assemblies which are confederated, economic democracy in a stateless, moneyless and marketless economy, democracy in the social realm, i.e. self-management in places of work and education, and ecological democracy which aims to reintegrate society and nature. The theoretical project of inclusive democracy emerged from the work of political philosopher Takis Fotopoulos in \"Towards An Inclusive Democracy\" and was further developed in the journal Democracy & Nature and its successor The International Journal of Inclusive Democracy. A Parpolity or Participatory Polity is a theoretical form of democracy that is ruled by a Nested Council structure. The guiding philosophy is that people should have decision-making power in proportion to how much they are affected by the decision. Local councils of 25–50 people are completely autonomous on issues that affect only them, and these councils send delegates to higher level councils who are again autonomous regarding issues that affect only the population affected by that council. A council court of randomly chosen citizens serves as a check on the tyranny of the majority, and rules on which body gets to vote on which issue. Delegates may vote differently from how their sending council might wish but are mandated to communicate the wishes of their sending council. Delegates are recallable at any time. Referendums are possible at any time via votes of lower-level councils, however, not everything is a referendum as this is most likely a waste of time. A parpolity is meant to work in tandem with a participatory economy. Cosmopolitan democracy, also known as Global democracy or World Federalism, is a political system in which democracy is implemented on a global scale, either directly or through representatives. An important justification for this kind of system is that the decisions made in national or regional democracies often affect people outside the constituency who, by definition, cannot vote. By contrast, in a cosmopolitan democracy, the people who are affected by decisions also have a say in them.[222] According to its supporters, any attempt to solve global problems is undemocratic without some form of cosmopolitan democracy. The general principle of cosmopolitan democracy is to expand some or all of the values and norms of democracy, including the rule of law; the non-violent resolution of conflicts; and equality among citizens, beyond the limits of the state. To be fully implemented, this would require reforming existing international organisations, e.g., the United Nations, as well as the creation of new institutions such as a World Parliament, which ideally would enhance public control over, and accountability in, international politics. Creative Democracy is advocated by American philosopher John Dewey. The main idea about Creative Democracy is that democracy encourages individual capacity building and the interaction among the society. Dewey argues that democracy is a way of life in his work of \"Creative Democracy: The Task Before Us\"[225] and an experience built on faith in human nature, faith in human beings, and faith in working with others. Democracy, in Dewey's view, is a moral ideal requiring actual effort and work by people; it is not an institutional concept that exists outside of ourselves. \"The task of democracy\", Dewey concludes, \"is forever that of creation of a freer and more humane experience in which all share and to which all contribute\". Guided democracy is a form of democracy that incorporates regular popular elections, but which often carefully \"guides\" the choices offered to the electorate in a manner that may reduce the ability of the electorate to truly determine the type of government exercised over them. Such democracies typically have only one central authority which is often not subject to meaningful public review by any other governmental authority. Russian-style democracy has often been referred to as a \"Guided democracy\".[226] Russian politicians have referred to their government as having only one center of power/ authority, as opposed to most other forms of democracy which usually attempt to incorporate two or more naturally competing sources of authority within the same government.[227] Shareholder democracy is a concept relating to the governance of corporations by their shareholders. In the United States, shareholders are typically granted voting rights according to the one share, one vote principle. Shareholders may vote annually to elect the company's board of directors, who themselves may choose the company's executives. The shareholder democracy framework may be inaccurate for companies which have different classes of stock that further alter the distribution of voting rights. Condorcet's jury theorem is logical proof that if each decision-maker has a better than chance probability of making the right decision, then having the largest number of decision-makers, i.e. a democracy, will result in the best decisions. This has also been argued by theories of the wisdom of the crowd. A 2019 study by Acemoglu and others estimated that countries switching to democratic from authoritarian rule had on average a 20% higher GDP after 25 years than if they had remained authoritarian. The study examined 122 transitions to democracy and 71 transitions to authoritarian rule, occurring from 1960 to 2010.[230] Acemoglu said this was because democracies tended to invest more in health care and human capital, and reduce special treatment of regime allies.[231] Peacekeeping is conducive to democracy promotion and building in the developing world. Here, facilitator and former MICAH Police Commissioner Yves Bouchard shares mission experience with senior military and police officials in mission management to contribute to African Union peacekeeping missions. The Planification Avancée des Missions Intégrées (APIM), or Advanced Mission Planning Course, was held by the Pearson Centre at Bamako's Ecole de maintien de la paix. Democracy promotion, also referred to as democracy building, can be domestic policy to increase the quality of already existing democracy or a strand of foreign policy adopted by governments and international organizations that seek to support the spread of democracy as a system of government. Among the reasons for supporting democracy include the belief that countries with a democratic system of governance are less likely to go to war, are likely to be economically better off and socially more harmonious.[232] In democracy building, the process includes the building and strengthening of democracy, in particular the consolidation of democratic institutions, including courts of law, police forces, and constitutions.[233] Some critics have argued that the United States has used democracy promotion to justify military intervention abroad.[234][235] Much experience was gained after the Revolutions of 1989 resulted in the fall of the Iron Curtain and a wave of democratic transitions in former Communist states, particularly in Central and Eastern Europe. According to Freedom House, the number of democracies increased from 41 of 150 existing states in 1974 to 123 of 192 states in 2006.[236] The pace of transition slowed considerably since the beginning of the twenty-first century, which encouraged discussion of whether democracy was under threat.[237] In the early twenty-first century, a democratic deficit was noticed in countries where democratic systems already existed, including Britain, the US and the European Union.[238] In the financial sense, democracy promotion grew from 2% of aid in 1990 to nearly 20% in 2005.[239] An open question for democracy promotion around the world, both in countries where it is already at the core of the system of governance and in those where it is not, is defining the terminology of promoting, supporting or assisting democracy in the post-Cold War situation.[240] Several philosophers and researchers have outlined historical and social factors seen as supporting the evolution of democracy. Other commentators have mentioned the influence of economic development.[251] In a related theory, Ronald Inglehart suggests that improved living-standards in modern developed countries can convince people that they can take their basic survival for granted, leading to increased emphasis on self-expression values, which correlates closely with democracy.[252][253] Douglas M. Gibler and Andrew Owsiak in their study argued about the importance of peace and stable borders for the development of democracy. It has often been assumed that democracy causes peace, but this study shows that, historically, peace has almost always predated the establishment of democracy.[254] Carroll Quigley concludes that the characteristics of weapons are the main predictor of democracy:[255][256] Democracy—this scenario—tends to emerge only when the best weapons available are easy for individuals to obtain and use.[257] By the 1800s, guns were the best personal weapons available, and in the United States of America (already nominally democratic), almost everyone could afford to buy a gun, and could learn how to use it fairly easily. Governments could not do any better: it became the age of mass armies of citizen soldiers with guns.[257] Similarly, Periclean Greece was an age of the citizen soldier and democracy.[258] Evidence consistent with conventional theories of why democracy emerges and is sustained has been hard to come by. Statistical analyses have challenged modernisation theory by demonstrating that there is no reliable evidence for the claim that democracy is more likely to emerge when countries become wealthier, more educated, or less unequal.[262] In fact, empirical evidence shows that economic growth and education may not lead to increased demand for democratization as modernization theory suggests: historically, most countries attained high levels of access to primary education well before transitioning to democracy.[263] Rather than acting as a catalyst for democratization, in some situations education provision may instead be used by non-democratic regimes to indoctrinate their subjects and strengthen their power.[263] The assumed link between education and economic growth is called into question when analyzing empirical evidence. Across different countries, the correlation between education attainment and math test scores is very weak (.07). A similarly weak relationship exists between per-pupil expenditures and math competency (.26). Additionally, historical evidence suggests that average human capital (measured using literacy rates) of the masses does not explain the onset of industrialization in France from 1750 to 1850 despite arguments to the contrary.[264] Together, these findings show that education does not always promote human capital and economic growth as is generally argued to be the case. Instead, the evidence implies that education provision often falls short of its expressed goals, or, alternatively, that political actors use education to promote goals other than economic growth and development. Some scholars have searched for the \"deep\" determinants of contemporary political institutions, be they geographical or demographic.[265][266] An example of this is the disease environment. Places with different mortality rates had different populations and productivity levels around the world. For example, in Africa, the tsetse fly—which afflicts humans and livestock—reduced the ability of Africans to plough the land. This made Africa less settled. As a consequence, political power was less concentrated.[267] This also affected the colonial institutions European countries established in Africa.[268] Whether colonial settlers could live or not in a place made them develop different institutions which led to different economic and social paths. This also affected the distribution of power and the collective actions people could take. As a result, some African countries ended up having democracies and others autocracies. An example of geographical determinants for democracy is having access to coastal areas and rivers. This natural endowment has a positive relation with economic development thanks to the benefits of trade.[269] Trade brought economic development, which in turn, broadened power. Rulers wanting to increase revenues had to protect property-rights to create incentives for people to invest. As more people had more power, more concessions had to be made by the ruler and in many[quantify] places this process lead to democracy. These determinants defined the structure of the society moving the balance of political power.[270] Robert Michels asserts that although democracy can never be fully realised, democracy may be developed automatically in the act of striving for democracy: The peasant in the fable, when on his deathbed, tells his sons that a treasure is buried in the field. After the old man's death the sons dig everywhere in order to discover the treasure. They do not find it. But their indefatigable labor improves the soil and secures for them a comparative well-being. The treasure in the fable may well symbolise democracy.[271] Democracy in modern times has almost always faced opposition from the previously existing government, and many times it has faced opposition from social elites. The implementation of a democratic government from a non-democratic state is typically brought by peaceful or violent democratic revolution. Criticism of democracy, or debate on democracy and the different aspects of how to implement democracy best have been widely discussed. There are both internal critics (those who call upon the constitutional regime to be true to its own highest principles) and external ones who reject the values promoted by constitutional democracy.[288] Criticism of democracy has been a key part of democracy, its functions, and its development throughout history. Plato famously opposed democracy, arguing for a 'government of the best qualified'; James Madison extensively studied the historic attempts at and arguments on democracy in his preparation for the Constitutional Convention; and Winston Churchill remarked that \"No one pretends that democracy is perfect or all-wise. Indeed, it has been said that democracy is the worst form of government except all those other forms that have been tried from time to time.\"[289] The theory of democracy relies on the implicit assumption that voters are well informed about social issues, policies, and candidates so that they can make a truly informed decision. Since the late 20'th century there has been a growing concern that voters may be poorly informed because the news media are focusing more on entertainment and gossip and less on serious journalistic research on political issues.[293][294] The media professors Michael Gurevitch and Jay Blumler have proposed a number of functions that the mass media are expected to fulfill in a democracy:[295] Mechanisms for holding officials to account for how they have exercised power Incentives for citizens to learn, choose, and become involved A principled resistance to the efforts of forces outside the media to subvert their independence, integrity, and ability to serve the audience A sense of respect for the audience member, as potentially concerned and able to make sense of his or her political environment This proposal has inspired a lot of discussions over whether the news media are actually fulfilling the requirements that a well functioning democracy requires.[296] Commercial mass media are generally not accountable to anybody but their owners, and they have no obligation to serve a democratic function.[296][297] They are controlled mainly by economic market forces. Fierce economic competition may force the mass media to divert themselves from any democratic ideals and focus entirely on how to survive the competition.[298][299] The tabloidization and popularization of the news media is seen in an increasing focus on human examples rather than statistics and principles. There is more focus on politicians as personalities and less focus on political issues in the popular media. Election campaigns are covered more as horse races and less as debates about ideologies and issues. The dominating media focus on spin, conflict, and competitive strategies has made voters perceive the politicians as egoists rather than idealists. This fosters mistrust and a cynical attitude to politics, less civic engagement, and less interest in voting.[300][301][302] The ability to find effective political solutions to social problems is hampered when problems tend to be blamed on individuals rather than on structural causes.[301] This person-centered focus may have far-reaching consequences not only for domestic problems but also for foreign policy when international conflicts are blamed on foreign heads of state rather than on political and economic structures.[303][304] A strong media focus on fear and terrorism has allowed military logic to penetrate public institutions, leading to increased surveillance and the erosion of civil rights.[305] The responsiveness[306] and accountability of the democratic system is compromised when lack of access to substantive, diverse, and undistorted information is handicapping the citizens' capability of evaluating the political process.[297][302] The fast pace and trivialization in the competitive news media is dumbing down the political debate. Thorough and balanced investigation of complex political issues does not fit into this format. The political communication is characterized by short time horizons, short slogans, simple explanations, and simple solutions. This is conducive to political populism rather than serious deliberation.[297][305] Commercial mass media are often differentiated along the political spectrum so that people can hear mainly opinions that they already agree with. Too much controversy and diverse opinions are not always profitable for the commercial news media.[307]Political polarization is emerging when different people read different news and watch different TV channels. This polarization has been worsened by the emergence of the social media that allow people to communicate mainly with groups of like-minded people, the so-called echo chambers.[308] Extreme political polarization may undermine the trust in democratic institutions, leading to erosion of civil rights and free speech and in some cases even reversion to autocracy.[309] Many media scholars have discussed non-commercial news media with public service obligations as a means to improve the democratic process by providing the kind of political contents that a free market does not provide.[310][311] The World Bank has recommended public service broadcasting in order to strengthen democracy in developing countries. These broadcasting services should be accountable to an independent regulatory body that is adequately protected from interference from political and economic interests.[312] Public service media have an obligation to provide reliable information to voters. Many countries have publicly funded radio and television stations with public service obligations, especially in Europe and Japan,[313] while such media are weak or non-existent in other countries including the US.[314] Several studies have shown that the stronger the dominance of commercial broadcast media over public service media, the less the amount of policy-relevant information in the media and the more focus on horse race journalism, personalities, and the pecadillos of politicians. Public service broadcasters are characterized by more policy-relevant information and more respect for journalistic norms and impartiality than the commercial media. However, the trend of deregulation has put the public service model under increased pressure from competition with commercial media.[313][315][316] The emergence of the internet and the social media has profoundly altered the conditions for political communication. The social media have given ordinary citizens easy access to voice their opinion and share information while bypassing the filters of the large news media. This is often seen as an advantage for democracy.[317] The new possibilities for communication have fundamentally changed the way social movements and protest movements operate and organize. The internet and social media have provided powerful new tools for democracy movements in developing countries and emerging democracies, enabling them to bypass censorship, voice their opinions, and organize protests.[318][319] A serious problem with the social media is that they have no truth filters. The established news media have to guard their reputation as trustworthy, while ordinary citizens may post unreliable information.[318] In fact, studies show that false stories are going more viral than true stories.[320][321] The proliferation of false stories and conspiracy theories may undermine public trust in the political system and public officials.[321][309] Reliable information sources are essential for the democratic process. Less democratic governments rely heavily on censorship, propaganda, and misinformation in order to stay in power, while independent sources of information are able to undermine their legitimacy.[322] ^Economic Intelligence Unit Democracy Index, 2022, p. 4: \"According to our measure of democracy, less than half (45.7%) of the world's population now live in a democracy of some sort, a significant decline from 2020 (49.4%).\" ^ abStaff writer (22 August 2007). \"Liberty and justice for some\". The Economist. Economist Group. Democracy can be seen as a set of practices and principles that institutionalise and thus ultimately protect freedom. Even if a consensus on precise definitions has proved elusive, most observers today would agree that, at a minimum, the fundamental features of a democracy include government based on majority rule and the consent of the governed, the existence of free and fair elections, the protection of minorities and respect for basic human rights. Democracy presupposes equality before the law, due process and political pluralism. ^\"Magna Carta: an introduction\". The British Library. Archived from the original on 23 April 2021. Retrieved 28 January 2015. Magna Carta is sometimes regarded as the foundation of democracy in England. ...Revised versions of Magna Carta were issued by King Henry III (in 1216, 1217 and 1225), and the text of the 1225 version was entered onto the statute roll in 1297. ...The 1225 version of Magna Carta had been granted explicitly in return for a payment of tax by the whole kingdom, and this paved the way for the first summons of Parliament in 1265, to approve the granting of taxation. ^ ab\"Britain's unwritten constitution\". British Library. Archived from the original on 8 December 2015. Retrieved 27 November 2015. The key landmark is the Bill of Rights (1689), which established the supremacy of Parliament over the Crown.... The Bill of Rights (1689) then settled the primacy of Parliament over the monarch's prerogatives, providing for the regular meeting of Parliament, free elections to the Commons, free speech in parliamentary debates, and some basic human rights, most famously freedom from 'cruel or unusual punishment'. ^\"Constitutionalism: America & Beyond\". Bureau of International Information Programs (IIP), U.S. Department of State. Archived from the original on 24 October 2014. Retrieved 30 October 2014. The earliest, and perhaps greatest, victory for liberalism was achieved in England. The rising commercial class that had supported the Tudor monarchy in the 16th century led the revolutionary battle in the 17th and succeeded in establishing the supremacy of Parliament and, eventually, of the House of Commons. What emerged as the distinctive feature of modern constitutionalism was not the insistence on the idea that the king is subject to law (although this concept is an essential attribute of all constitutionalism). This notion was already well established in the Middle Ages. What was distinctive was the establishment of effective means of political control whereby the rule of law might be enforced. Modern constitutionalism was born with the political requirement that representative government depended upon the consent of citizen subjects... However, as can be seen through provisions in the 1689 Bill of Rights, the English Revolution was fought not just to protect the rights of property (in the narrow sense) but to establish those liberties which liberals believed essential to human dignity and moral worth. The \"rights of man\" enumerated in the English Bill of Rights gradually were proclaimed beyond the boundaries of England, notably in the American Declaration of Independence of 1776 and in the French Declaration of the Rights of Man in 1789. ^Gregory, Desmond (1985). The ungovernable rock: a history of the Anglo-Corsican Kingdom and its role in Britain's Mediterranean strategy during the Revolutionary War, 1793–1797. London: Fairleigh Dickinson University Press. p. 31. ISBN978-0-8386-3225-3. ^Saskia Sassen, Globalisation, the State and Democratic Deficit, Open Democracy, 18 July 2007, [2]Archived November 13, 2009, at the Wayback Machine; Patrice de Beer, France and Europe: the Democratic Deficit Exposed, Open Democracy, 4 June 2006, [3]Archived August 31, 2009, at the Wayback Machine ^Compare: Rindermann, H (2008). \"Relevance of education and intelligence for the political development of nations: Democracy, rule of law and political liberty\". Intelligence. 36 (4): 306–22. doi:10.1016/j.intell.2007.09.003. Political theory has described a positive linkage between education, cognitive ability and democracy. This assumption is confirmed by positive correlations between education, cognitive ability, and positively valued political conditions (N = 183–130). [...] It is shown that in the second half of the 20th century, education and intelligence had a strong positive impact on democracy, rule of law and political liberty independent from wealth (GDP) and chosen country sample. One possible mediator of these relationships is the attainment of higher stages of moral judgment fostered by cognitive ability, which is necessary for the function of democratic rules in society. The other mediators for citizens as well as for leaders could be the increased competence and willingness to process and seek information necessary for political decisions due to greater cognitive ability. There are also weaker and less stable reverse effects of the rule of law and political freedom on cognitive ability. ^Buckley, Steve; Duer, Kreszentia; Mendel, Toby; Siochrú, Seán Ó (2008). Broadcasting, voice, and accountability: A public interest approach to policy, law, and regulation. World Bank and University of Michigan Press. ^ abGunther, Richard; Mugham, Anthony (2000). \"The Political Impact of the Media: A Reassessment\". In Gunther, Richard; Mugham, Anthony (eds.). Democracy and the Media: A Comparative Perspective. Cambridge University Press. pp. 402–448. ^Pickard, Victor (2020). \"The Public Media Option: Confronting Policy Failure in an Age of Misinformation\". In Bennett, W. Lance; Livingston, Steven (eds.). The Disinformation Age: Politics, Technology, and Disruptive Communication in the United States. Cambridge University Press. pp. 238–258."}
{"url": "https://en.m.wikipedia.org/wiki/Special:BookSources/978-0-253-21682-3", "text": "This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). If you arrived at this page by clicking an ISBN link in a Wikipedia page, you will find the full range of relevant search links for that specific book by scrolling to the find links below. To search for a different book, type that book's individual ISBN into this ISBN search box. Spaces and hyphens in the ISBN do not matter. Also, the number starts after the colon for \"ISBN-10:\" and \"ISBN-13:\" numbers. An ISBN identifies a specific edition of a book. Any given title may therefore have a number of different ISBNs. See #Find other editions below for finding other editions. An ISBN registration, even one corresponding to a book page on a major book distributor database, is not definite proof that such a book actually exists. A title may have been cancelled or postponed after the ISBN was assigned. Check to see if the book exists or not. Google Books and Amazon.com may be helpful if you want to verify citations in Wikipedia articles, because they often let you search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available). At the Open Library (part of the Internet Archive) you can borrow and read entire books online. Luxembourg Montenegro Netherlands Find this book in the Dutch-Union Catalogue that searches simultaneously in more than 400 Dutch electronic library systems (including regional libraries, university libraries, research libraries and the Royal Dutch library) Book-swapping websites Non-English book sources If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language."}
{"url": "https://en.m.wikipedia.org/wiki/Special:BookSources/978-1-86197-580-5", "text": "This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). If you arrived at this page by clicking an ISBN link in a Wikipedia page, you will find the full range of relevant search links for that specific book by scrolling to the find links below. To search for a different book, type that book's individual ISBN into this ISBN search box. Spaces and hyphens in the ISBN do not matter. Also, the number starts after the colon for \"ISBN-10:\" and \"ISBN-13:\" numbers. An ISBN identifies a specific edition of a book. Any given title may therefore have a number of different ISBNs. See #Find other editions below for finding other editions. An ISBN registration, even one corresponding to a book page on a major book distributor database, is not definite proof that such a book actually exists. A title may have been cancelled or postponed after the ISBN was assigned. Check to see if the book exists or not. Google Books and Amazon.com may be helpful if you want to verify citations in Wikipedia articles, because they often let you search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available). At the Open Library (part of the Internet Archive) you can borrow and read entire books online. Luxembourg Montenegro Netherlands Find this book in the Dutch-Union Catalogue that searches simultaneously in more than 400 Dutch electronic library systems (including regional libraries, university libraries, research libraries and the Royal Dutch library) Book-swapping websites Non-English book sources If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language."}
{"url": "https://www.semanticscholar.org/author/R.-Sanusi/108616758", "text": "Having good nutrition knowledge or practice did not directly determine athletic performance, however, there is the need for nutrition education interventions, to improve athlete’s performance by promoting adequate energy intake, lean muscle mass and appropriate weight gain. Assessment of knowledge and practice of nursing mothers concerning breastfeeding in the selected Local Government Area of Lagos State found high awareness of EBF and adequate knowledge of breastfeeding did not translate into practice. Investigation of the impact of adequate intake energy and retinol on pregnancy outcome among selected pregnant women in Osun State Southwest Nigeria finds that emphasis on dietary sources of energy iron vitamin A and folic acid during pregnancy should be the key discussion during antenatal visits. About half of the private university students in Ota, Ogun State, Nigeria had normal BMI however, their food habit indicated regular skipping of breakfast, and Nutrition education is recommended as a general study program of the university. Underweight, stunting and overweight/obesity have a significant impact on cognitive neurodevelopment and multi-dimensional approaches with various stakeholders should address all issues simultaneously. Consumption of cassava and vitamin A intake was high among women and children in Akwa Ibom with a prevalence of vitamin A deficiency ranging from moderate in children to non-existent among women, which should focus dissemination in states where red palm oil is not widely consumed."}
{"url": "https://en.m.wikipedia.org/wiki/Special:BookSources/978-0-674-01004-8", "text": "This page links to catalogs of libraries, booksellers, and other book sources where you will be able to search for the book by its International Standard Book Number (ISBN). If you arrived at this page by clicking an ISBN link in a Wikipedia page, you will find the full range of relevant search links for that specific book by scrolling to the find links below. To search for a different book, type that book's individual ISBN into this ISBN search box. Spaces and hyphens in the ISBN do not matter. Also, the number starts after the colon for \"ISBN-10:\" and \"ISBN-13:\" numbers. An ISBN identifies a specific edition of a book. Any given title may therefore have a number of different ISBNs. See #Find other editions below for finding other editions. An ISBN registration, even one corresponding to a book page on a major book distributor database, is not definite proof that such a book actually exists. A title may have been cancelled or postponed after the ISBN was assigned. Check to see if the book exists or not. Google Books and Amazon.com may be helpful if you want to verify citations in Wikipedia articles, because they often let you search an online version of the book for specific words or phrases, or you can browse through the book (although for copyright reasons the entire book is usually not available). At the Open Library (part of the Internet Archive) you can borrow and read entire books online. Luxembourg Montenegro Netherlands Find this book in the Dutch-Union Catalogue that searches simultaneously in more than 400 Dutch electronic library systems (including regional libraries, university libraries, research libraries and the Royal Dutch library) Book-swapping websites Non-English book sources If the book you are looking for is in a language other than English, you might find it helpful to look at the equivalent pages on other Wikipedias, linked below – they are more likely to have sources appropriate for that language."}
{"url": "https://web.archive.org/web/20211031061608/https://jonestown.sdsu.edu/?page_id=29475", "text": "Jonestown As a Reflection of American Society by Nora Woods The development and destruction of Peoples Temple posed a potential threat to how citizens of the United States understood themselves and the forces at work within American society. The development and destruction of Peoples Temple demonstrated how American society can regard differences with contempt, demonize victims, and pursue individualism and competition in such a way that communities can be destroyed and individuals left without a meaningful way to interact with others. Accordingly, within the United States, there has been little acknowledgement that those who died in Jonestown might have had anything worthwhile to contribute to public discourse. Instead, Jonestown has been appropriated by the so-called anti-cult movement which seeks to demonize and dismantle organizations which attempt to subvert the normative social order. In this paper, those who died in Jonestown will be re-humanized. Drawing upon the theories of Emilé Durkheim, Max Weber, David Chidester, and John Hall, those who died in Jonestown will be considered as sane and rational, making choices which were congruent with their social and religious identity. In this context, the impact of society sidestepping accountability will be touched upon. Finally, I will consider that an unwillingness to confront one’s own moral failings results in a diminished capacity to make dramatic social change. The United States of America’s tendency to demonize those who are not members of the dominant class serves a distinct, important purpose for capitalist society. Capitalism requires a large reserve of poor, disenfranchised people. Furthermore, in order for those who are part of the ruling class to view themselves as moral beings, the poor must be constructed as being responsible for their own poverty. Ergo, when groups of demonized, oppressed people do not live down to the standards of the oppressors, the façade is threatened and those in power are forced to either face their own immorality or destroy the group that is rising up. When a group reflects the depravity of a society, the society must either demonize that group or accept its own shortcomings and change them. Peoples Temple and Jonestown were critiques of the American capitalist system in life as well as in death. Rather than examine the uncomfortable truth that Peoples Temple revealed, we have focused on the pornographic. In order to avoid the truth, authors from all directions have systematically reduced the people of Jonestown to evil, crazy, brainwashed, or so completely “other” that they are barely recognizable as human. It is not possible to know how many people laid down their lives willingly and how many were forced. Given that nearly one third of those who died were children, it is impossible to classify all of the deaths as willing suicide. The widespread loss of human life, particularly young life, is certainly tragic. It is impossible to fully imagine the sort of pain and fury the families of those who died must feel. In attempting to examine and discuss Peoples Temple as a sane group whose members deliberately and rationally chose an intentional and meaningful death, I am not denying the legitimacy of those feelings, nor am I advocating for suicide or any violent action as a way to send a message. Rather, I am examining the suicidal impulse of the collective whole (which, of course, does not take into consideration all the subtleties, all the horror, all the tragedy of each individual’s death). I believe that we ought to examine the Jonestown event, not as a singular phenomenon, but as an example of the United States’ tendency to make human life and dignity secondary to capital gain. This is but one in a number of perspectives, but I believe that it is one that is necessary. My hope is that, through this study, I will find meaning where there seems to be none, and cultivate growth in the wake of destruction. In this paper I attempt to demonstrate that Peoples Temple was a reflection of its time and society, that the deaths in Jonestown were the expressions of a fully human group’s social and religious identity, and that the rejection and othering of those who died is reflective of a capitalistic, American trend that hinders the moral progression of our society. Initially Peoples Temple reflected American aspirations that emerged during the 1960s. It sought to create a social order in which the individual was defined in terms of the community; race and economic status would no longer hold significance, and all people would be responsible to and for one another. Jonestown was meant to be a place where the sacred ideals of a religious community could be realized, a place outside of the United States which was seen, in the eyes of Peoples Temple, as irrevocably tainted by capitalism, racism, individualism, greed, and intolerance. The death was meant to be revolutionary, to demonstrate to the world that one could place ones ideals ahead of oneself. Peoples Temple was formed in 1955 in Indianapolis, Indiana and became an official part of the Disciples of Christ in 1960. Peoples Temple services were similar to those of Pentecostals, featuring healings, speaking in tongues, and a biblical message. It was founded by Jim Jones, a charismatic preacher who began his pastoral career as a student pastor in a Methodist Church. Jones left the Methodist Church because of his strong belief in racial equality and integration. His primary messages were about social and racial equality, justice, and communalism. Jones ran the Temple with the help of his wife, Marceline and recruited Archie Ijames, a black Temple member, to be the assistant pastor. The membership was composed primarily of African-Americans and working class whites, many of whom were initially attracted by Jones’ miraculous healings. The Temple experienced significant growth and success in its first decade and, by 1960, opened a free restaurant and social service center, offered drug and alcohol counseling, and operated nursing homes. The substantial humanitarian works of Peoples Temple were recognized by city officials who, in 1961, invited Jim Jones to be director of the Indianapolis Human Rights Commission, a position he used to aggressively fight segregation. Jones’ integrationist politics made him and his family the victims of persistent harassment, which ultimately contributed to a mental breakdown requiring hospitalization. Following his release from the hospital, Jones and his family traveled to Guyana, Cuba, and Hawaii before moving to Brazil for two years. While in Brazil, Jones had a vision of a nuclear holocaust, a vision he became increasingly concerned with over the next few decades. In Jones’ absence, Peoples Temple was run by associate pastors Russell Winberg, Ross Case, Jack Beam and Archie Ijames, but without its principal leader, Peoples Temple began to fall apart, financially and organizationally. Toward the end of 1963 Jones left Brazil to return to Indianapolis with an intensified commitment to communalism. In 1965 Jones decided to move Peoples Temple to California, where he felt it would be safe from any potential nuclear attacks. Although only about 150 members of the Indiana church followed Jones to California, the membership quickly grew, reaching 3,000 in the mid-seventies.[1] One of the most significant new members to join in California was Timothy Stoen, the Assistant District Attorney of Mendocino County whom Jones met while serving as the foreman of the Mendocino County Grand Jury.[2] In 1968 Peoples Temple was officially recognized by the Disciples of Christ Church and in 1969 Peoples Temple Redwood Valley complex opened. In 1970 Peoples Temple began branching out into San Francisco and, in 1972 they purchased a building for services. Peoples Temple began busing its members along the coast of California for services in Redwood Valley, San Francisco, and Los Angeles. The headquarters remained in Redwood Valley until 1976, when it moved to San Francisco. The Los Angeles church had its own ministers and church leaders. From 1972 until 1978 Peoples Temple was primarily active in San Francisco where its members made a substantial impact on the political scene. In 1974, a small group of Temple members went to Guyana to begin building a settlement that would ultimately become Jonestown. From 1970 until 1975 Peoples Temple was primarily concerned with social service. They practiced the social gospel by living communally and caring for the oppressed of society. The Temple “addressed issues of drug rehabilitation, medical care, child care, and feeding the hungry.”[3] While in San Francisco, Peoples Temple was similar to a Pentecostal church with emphasis on the social gospel, faith healings, and charismatic services. However, Jim Jones himself was still presented as a prophet of social justice and equality. Peoples Temple was Christian only insofar as they viewed Jesus as a heroic figure, but there was a clear departure from traditional Christian dogma. This was evidenced by Jones’ publication of “The Letter Killeth,” a booklet which reviewed the Bible in exhaustive detail and pointed out inconsistencies. “The Letter Killeth” also noted the Biblical justification for slavery, violence, and inequity. However, even though this booklet was a critique of the Bible, its subtitle calls for readers to “Exalt the Name of Jesus, and Not the King James of England!!” Moreover, although most of the booklet was devoted to pointing out flaws in the Bible, the first section made the point that the words themselves are useless without a Prophet through whom to interpret them. The first section explained why a prophet is needed and the second explained how one can know that Jones is that prophet. Then, prior to the Biblical criticism, Jones articulated the truths that the Bible does communicate.[4] In 1975 Peoples Temple took its first step into the public forum. During this year Peoples Temple became involved with the mayoral race between liberal George Moscone and conservative John Barbagelata. This involvement marked Peoples Temple “turn from a progressive church in the Disciples of Christ to an activist political organization.”[5] While planning the campaign, someone from the Moscone camp suggested that Peoples Temple volunteers help with the campaign. They succeeded, Moscone won and, “[s]oon the Temple was being bandied about as one of the community groups needed to pull together a winning liberal coalition.”[6] Following this event Peoples Temple began to focus heavily on political action and activism, although they still continued to address human needs. The Temple not only engaged in Democratic Party events but also in revolutionary politics.[7] The election of Moscone led to Jones’ appointment to the San Francisco Housing Authority. He quickly became its chairman. Peoples Temple’s social services also continued to attract attention. Jones was named as one of the nation’s one hundred outstanding clergyman by Religion in Life magazine in 1975; he was recognized as the “Humanitarian of the Year” by the Los Angeles Herald in 1976; and he won the “Martian Luther King Jr., Humanitarian of the Year” award in 1977.[8] Peoples Temple members embodied their ideals in their day-to-day life. They lived in an egalitarian society. The communal living situation included public confession and humiliation as forms of constructing a collective conscious.[9] These meetings, known as catharsis, were a carryover from Indianapolis. One member, Patricia Cartmell, who had joined in Indiana, described catharsis meetings as such: [E]ach member of the body was encouraged to stand and get off his chest everything that was in any way a hindrance to fellowship between himself and another member or between himself and the group, or the leader even, Jim in his utter honesty not desiring nor seeking immunity from the exposure of his own faults. We are reminded that catharsis is not a new approach to the solution of human problems, there being an old but seldom obeyed biblical injunction, “Confess your faults to one another and pray one for another that ye might be healed.”[10] Catharsis meetings often involved fairly harsh punishments including violence. However, there were also positive methods of social reinforcement. Senior citizens were fully integrated into Temple life and played important roles in the organizations. Medical staff provided excellent medical care. Furthermore, the “communal living apartments, as well as state-licensed senior care homes, not only provided a source of income for the Temple and room and board for senior citizens but also offered a meaningful intergenerational experience for the society’s elders.”[11] The Temple also provided foster care and guardianship for children who had been otherwise discarded by society. Finally, members were encouraged to go communal by turning over paychecks and property to Peoples Temple in return for room and board, and a small allowance. Although this has been seen as a form of manipulation and control by many outsiders, to members of the Temple it was “the first step toward transforming self-centered, destructive, capitalist individuals into other-centered, constructive, family members.”[12] Peoples Temple was an organization which sought to embody the ideals of the 1960s. The 1960’s were a tumultuous time full of hope, optimism, and a sense that individuals could change the world. At the end of the 60’s, when the Temple began to grow exponentially, the United States was facing a number of social and political reversals that left many people feeling disappointed. While much of the country was sending the message that one ought to give up on social change, Peoples Temple was still striving for it; while many progressive organizations were falling apart, the Temple was prospering. Peoples Temple offered hope for an integrated, equitable society, it allowed members to believe that they could be part of a solution to the immense problems facing the country, and it presented an opportunity to live out ideals with a community who shared them. As Allen J. Matusow explains, during “a few short years, optimism vanished, fundamental differences in values emerged to divide the country, social cohesion rapidly declined and the unraveling of America began.”[13] When the counter-cultural, social revolution era of the 1960s ended, many were primed for a message like the one of Peoples Temple. Peoples Temple’s primary offer was a chance to be involved in something greater than one’s self. For members of all races, the communalist society was an answer to the displacement and isolation in many ways characteristic of United States society. Deborah Layton, a high ranking member who left Jonestown prior to Congressman Ryan’s visit,[14] describes how she became involved in Peoples Temple in her book Seductive Poison. Deborah, like many other young people in the 1960s, felt a deep desire to help the world coupled with a paralyzing sense of loneliness. A poem she wrote while in high school aptly captures these mixed emotions: Once and for all I push away the cloud from my eyes. I can see misery and pain all about me. Suddenly I am where I began, Still too weak to help the underprivileged of our world. My responsibility and what am I doing? Naught! [15] It was this combination of feelings that led Deborah and many others to Peoples Temple. For her elderly mother, Lisa, who ultimately died in Jonestown, the Temple was attractive not only because it had helped her daughter mature and grow, but also because it “might be a safe place to make her own flight away from a life of frustration and sorrow.”[16] Lisa was born in Germany to non-religious, Jewish parents. In May of 1938, in order to escape the Nazis, Lisa left her family and friends for New York.[17] Deborah and her mother Lisa both joined Peoples Temple because it gave them an opportunity to be fully immersed in a community that emphasized the inherent worth of all people. In Jonestown: The Life and Death of Peoples Temple, a documentary directed by Stanley Nelson, many former members describe their attraction to the Temple as being similar to Deborah Layton’s.[18] Bryan Kravitz remembered his decision to join Peoples Temple after he decided not to go to Vietnam and I was at the point of what am I going to do with myself. I listened to him, I was impressed by such an interracial group and people were really happy. I heard Jim Jones talking about equality among races, what its like living in entry and all of a sudden the answer was there. Neva Sly Hargrave joined because Jones echoed her concerns: “What he spoke about were things that were in our hearts; the government was not taking care of the people, there were too many poor people out there, there were poor children.” For black members, Jones’ race did not serve as a deterrent. Juanell Smart recalled that, “[w]hen people heard Jim, they didn’t look on him as being a white preacher. People didn’t look at Jim as being white. He was not white, he was just their preacher.” Jones’ adopted, black son, Jim Jones Jr. agreed. “He talked black, he really understood it, he understood how it was to be treated differently.” There were a number of social, political, and economic events that particularly influenced Peoples Temple. It should be noted that these events not only pushed people towards groups like the Temple, but also shaped much of the Temple’s psyche over the course of its existence. The majority of Temple members were black and many joined because of the radical integration preached and practiced by Jim Jones. Of primary importance to delaying political action that would make equal treatment of blacks a lived reality was the capitalist structure of American society. Ergo, it is not surprising that Jones’ anti-capitalist message held substantial weight with those who believed in integration. President Lyndon Johnson did not utilize Title VII of the 1964 Civil Rights act in order to combat discriminatory employment practices “not because he was indifferent to fair employment, but because he would not antagonize corporations and unions.”[19] Despite the climb of some blacks into the lower middle class over the course of the 1960s, there “remained a vast black underclass. still mired in the decaying heart of the ghetto, primarily the blacks who … found themselves increasingly irrelevant in the complex modern economy.”[20] Generally, those who were in need of welfare were “relatively powerless individuals” left to “confront bureaucratic demands on their own.”[21] However, within Peoples Temple, cases were handled on a collective basis, thereby freeing individuals “from deferring to the often confusing and conflicting demands of external authorities. The individual received relief from the often degrading experience of being ‘processed’.”[22] By the end of the 1960s, the Civil Rights Movement was coming to an end for a number of reasons. First, the Civil Rights Movement had “succeeded in achieving its legislative agenda.”[23]Second, the assassinations of Martin Luther King Jr. and Malcolm X deprived the movement of its primary leaders. Third, the Black Panther Party had been “effectively neutralized through police subversion and repression.”[24] Finally, the movement “simply ran out of steam” having “drained the energy and resources of the black community for some fifteen years.”[25] In 1976, the California State Senate passed a resolution commending Jim Jones and Peoples Temple for their “steadfast support of the Constitution and encouragement of responsible citizenship” demonstrated by their support of organizations such as the American Cancer Society, Big Brothers of America, the Mendocino County Heart Association and others.[26] However, once defectors from the Temple began to share their stories it became apparent that Peoples Temple was building an alternative social order, and then the attacks began. One major attack came in August of 1977 in the form of an article in New West magazine by Marshall Kilduff and Phil Tracy. Peoples Temple members attempted to stop the article from being published, but in so doing increased the attention paid to it. On June 17th, 1977 New West reported a break-in which they believed to be the work of Temple members who were trying to destroy the article. Ultimately the police discovered that the supposed break-in was the work of a New West editor who had locked himself out, however, in the public imagination it was Temple members. This served to further sensationalize the article. The article, “Inside Peoples Temple” certainly attacks Peoples Temple, but with no substantiated or truly disturbing claims.[27] The article relies entirely on the testimonies of ten defectors, none of whom make any particularly disturbing accusations against the Temple. One woman, Birdie Marable, left after the conditions on a Temple trip were too crowded, Elmer and Deanna Mertle left because of the physical punishments, three other members left because they felt the healings were not real and the other five left for various reasons involving finance. The authors themselves note that these defectors are not necessarily the most reliable source of information. “Obviously they all had their biases. So we checked the verifiable facts of their accounts – the property transfers, the nursing and foster home records, political campaign contributions and other matters of public record. The details of their stories checked out.”[28] However, the accusations being made by the defectors were related to internal Temple dealings, none of which could have been verified by these records.[29] Moreover, although presented in such a way that one is left with a sense of unease, at their core the accusations against the Temple and Jones are primarily trivial. Kilduff and Tracy devote the last page of their article to a section entitled “Why Jim Jones Should Be Investigated.” Although there is brief mention of the physical punishment that Deanna and Elmer Mertle[30] cited as one of their reasons for leaving, the bulk of this section is dedicated to concerns about the “30 pieces of property. transferred from individuals to the temple during the years 1968 to 1976,” some of which where apparently “signed or recorded improperly.”[31] However, the improper recording is revealed to have little to do with the intention of the people who were deeding their houses to the Temple and more to do with minor clerical problems. The only clear, consistent accusation is that the Temple was attempting to be a communal organization, and it would seem that this was the most disturbing to the American psyche. The New West article also served as one of the elements that brought the Concerned Relatives into existence. From 1976-1977 the Mills met informally with other ex-members including the Purifoy family, Joyce Shaw, and Grace Stoen. In 1977 the group became formalized because of the New West article and the arrival of Tim Stoen.[32] It was Tim Stoen’s legal genius that organized the Concerned Relatives “into a group that could mount an intensive and successful campaign against Jones and the Temple.”[33] Prior to this attack, the Temple had already begun to consider a move to Guyana. In 1972, Lester Kinsolving began the first negative news article on Peoples Temple for the San Francisco Examiner. This article “attacked Jones’ messianic pretensions, his claims to have raised forty-three people from the dead, and began to explore the authoritarian structure of Peoples Temple.”[34] This was followed by the first significant defection of members in 1973 when eight Temple members left. This group, who later became known as the Eight Revolutionaries, left because they felt the Temple’s political action was not radical enough, You said the revolutionary focal point at present is in the black people. There is no potential in the white population according to you. Yet where is the black leadership, where is the black staff and black attitude?[35] After this first major defection Jones raised the possibility of revolutionary suicide as a way to avoid persecution, but it was instead decided that the Temple ought to move to Guyana.[36] In 1974 a small number of Temple members began clearing land in Guyana for what would eventually become Jonestown. Fearing reprisal, following the publication of the New West article, Jim Jones decided that the migration to Guyana would be expedited and by the beginning of 1978 one thousand members had relocated. Although the New West article served as one of the catalysts for the Temple’s relocation, the disenchantment with the United States ran deeper than that. In a response to the article, Peoples Temple states their belief that they are being attacked because of their concern for the welfare of the oppressed. The charges made against us are an inevitable result of the success of the work the Temple is engaged in. We present the power of ordinary people to come together and “do for self.” We represent the antithesis of the profit/greed system that thrives on ruthless exploitation of the poor. We represent an answer that the real power-brokers do not want. Because if it succeeds, they fear that their power will be taken out of their hands and returned to the people.[37] Given the assassinations of Martin Luther King Jr., Malcolm X, and John F. Kennedy in preceding years, this belief on the behalf of Peoples Temple seems far from irrational. The move also reflected fears that the “IRS might freeze the Temple’s assets. that the results of a child custody battle might remove one of the children from the community. that a fascist takeover was imminent in the United States.”[38] Moreover, people believed that a better society awaited them in Jonestown. Once relocated to Guyana, the Temple underwent a shift from a progressive, activist organization to a utopian community free from the sexism, racism, and classism inherent in American society. Nevertheless there were significant problems with the large influx of people into Jonestown. For example, there were only 50 houses built in 1977. Although this was impressive progress, it was hardly enough to accommodate the hundreds who were rapidly moving in. Don Beck, one of the original settlers, described Jonestown in the beginning as “almost a paradise. Peaceful, beautiful, a place of mutual respect and support. Just working for yourself and your community. A good feeling. Safe. Safer in the tropical jungle than in the urban jungle.”[39] Life in Jonestown was hard. The goal of building a self-sustaining community required long hours and immense commitment. The reports regarding the amount of food available to residents are mixed. Some, like Deborah Layton, claim that food was “woefully inadequate.”[40] whereas others, such as Odell Rhodes (a Jonestown resident who survived by fleeing into the jungle) say the opposite.[41] The diaries of Edith Roller indicate that although food was not abundant, neither was it inadequate. There were extensive farming and construction work, nightly meetings, and classes on socialism and Russian language. Sleep and relaxation were not plentiful. Despite the harsh conditions in Jonestown many residents felt that the importance of what they were doing outweighed the hardship. In a letter home to her mother and father, Carolyn Moore Layton wrote, “It is interesting that with a simpler lifestyle people have more time for deep thinking and analysis than they do in a fast-paced urban setting and one can really feel like one is building something for a change.”[42] Another letter, from Pat Grunnet to Jonathan Kozol in September of 1978, states, We’re free from the pointless, irrelevant public education there and into the practical, relevant process here. Seniors share their experiences under the slavery they have known with the kids who will never know those heartaches.[43] Despite difficulties, it was not the material situation in Jonestown that led to the deaths in November of 1978. The struggle between the Concerned Relatives and Peoples Temple “quickly overshadowed daily life in Jonestown.”[44] Within Jonestown there was a distinct feeling of persecution, a feeling that was by no means without merit. The Concerned Relatives succeeded in “arousing relatives and the general public to a powerful and emotional issue: whether people had been taken to Jonestown against their will, and, in the case of children, without permission of parents or guardians.”[45] Families of Temple members argued in some cases that they had granted permission for their children to go to Jonestown under the false pretense that the trip would last only two weeks. These charges were investigated by the San Francisco district attorney, who found that “in no case in which we had the name of such a child was the allegation borne out.”[46] Despite this finding, a number of families hired a private detective named Joe Mazor who promised he could return their loved ones. Mazor contacted U.S. embassy consul Richard McCoy in Georgetown to request checks on twelve children in Jonestown. McCoy visited Jonestown three separate times searching for signs of mistreatment and offering assistance in leaving Jonestown, each offer was rejected.[47] The custody battle over John Victor Stoen was central to the sense of oppression and impending attack in Jonestown. John Victor was the son of Grace Stoen and Tim Stoen, however, when John Victor was two weeks old Tim signed an affidavit that he “was unable to sire a child himself, and that he had asked Jones to impregnate Grace Stoen.”[48] When Grace Stoen left Peoples Temple, she left John Victor with Tim, and when Tim defected, he left the boy in Jonestown. Although the affidavit was untrue, and Tim was in fact John Victor’s natural father, most Temple members believed that John Victor was Jones’ son. “The boy became a symbol for the community, and attempts to wrest him away from his Jonestown family were understood as part of a larger plot to break up other families and the community itself.”[49] After the bouncing back and forth between California and Guyana courts, a California court awarded Grace physical custody; this meant that should Jones return to California without John Victor, he would face arrest. Ultimately, John Victor died in Jonestown, but his parents succeeded in getting 100 members of Congress to contact the State Department regarding the custody case by January, 1978.[50] Along with the custody battle over John Victor, the Concerned Relatives also launched a number of other lawsuits most of which “could be classified as nuisances rather than real threats,” but they were nevertheless viewed by Temple members and leaders as “evidence of a conspiracy against it, organized and led by one of its own: Tim Stoen.”[51] Frustrated by the lack of progress, the Concerned Relatives began attempting to contact their loved ones in Jonestown through the embassy in Guyana. Unfortunately, “[e]fforts by relatives to talk with people at Jonestown and learn of their welfare through the embassy consul widened the gulf between the two opposed camps.”[52] The media also played a key role in the persecution of Jonestown. One reporter, Kathy Hunter of the Ukiah Daily Journal traveled to Guyana to visit Jonestown, but was denied entry. Hunter claimed that “Temple members interrogated andthreatened her in Georgetown,” and that “Guyanese police posted a guard outside her hotel room.”[53] Hunter’s account is questionable at best, given that she also asserted that Guyana had a pro-Chinese government, indicated by a portrait of Mao Tse-Tung that she apparently saw on the wall in a Guyanese government building; the portrait that was actually that of Arthur Chung, the president of Guyana.[54] Despite her dubious credentials, Hunter’s article about Guyana and Jonestown was picked up by a number of news outlets. Hunter continued to write a series of stories critical of Peoples Temple. “Its dealings with Kathy Hunter proved to the Temple that it could not get a fair deal from the media,” a feeling that was further proved by “a major expose planned by the National Enquirer at the instigation of Tim Stoen.”[55] Gordon Lindsay, the reporter for the National Enquirer attempted to enlist the help of John and Barbara Moore, the parents of Carolyn and Annie Moore, in his article. John Moore, who talked to the reporter for an hour and a half, said that “the reporter repeatedly tried to put words into his mouth.”[56] Lindsay’s article claimed that Jones was “a mixture of ‘Moon and Manson'” – referring to Unification Church leader Rev. Sun Myung Moon and a notorious murderer named Charles Manson – and included “salacious details about the Temple.”[57] Public criticism of Peoples Temple increased. The first official appearance of the Concerned Relatives took place on April 11, 1978 when they delivered an “Accusation of Human Rights Violations by Rev. James Warren Jones Against Our Children and Relatives at the Peoples Temple Jungle Encampment in Guyana, South America” to Temple members still in San Francisco.[58] This document accuses exclusively accuses Jones of human rights violations, one of which is: Making the following threat calculated to cause alarm for the lives of our relatives: “I can say without hesitation that we are devoted to a decision that it is better even to die than to be constantly harassed from one continent to the next.”[59] The document further alleged that Jonestown residents were “denied freedom of speech, freedom of movement, and freedom to send and receive mail without censorship.” The relatives were attempting to “disconnect the leader from his people.” However this strategy was “bound to fail. because members of Peoples Temple felt a deep loyalty to Jones as well as to the movement. In addition they identified with Jones: what happened to him would happen to them.”[60] Therefore, the Concerned Relatives only served to further alienate their loved ones in Jonestown. Jonestown and Peoples Temple attempted to respond to the attacks being leveled against them with logorrheic press releases rebutting charges by the Concerned Relatives, with “campaign mail” letters to the State Department and members of Congress, with well-argued essays by house intellectuals, with slurs about defectors “leaked” to the newspapers, with Freedom of Information Act requests to the government, and with court strategies to deal with the lawsuits.[61] However, when the press did carry “Temple statements at all, they often focused on flamboyant assertions” so that the “hyperbolic style of the Temple press releases often overshadowed their content.”[62] The New West article resulted in a number of investigations of the Temple, government “investigations … seemed to feed on one another as bureaucrats took up Kilduff and Tracy’s call for official inquiries.”[63] Public investigations “never translated into any significant criminal prosecution.” Indeed, even if “substantiated, abuses by Temple notary publics would amount to misdemeanors.”[64] However, Temple staff still sensed that there was more going on. Although the U.S. Treasury Department’s Customs Service, in response to a letter from Peoples Temple asking whether or not they were under investigation, denied “any activity in this Region of the Customs Service which would be in the nature of a ‘fishing expedition,'” this was simply untrue.[65] Responding to tips from members of the Concerned Relatives, seven Customs officials searched ninety Temple crates being shipped to Guyana on August 17, 1977 (they found nothing). Customs told Peoples Temple attorney Charles Garry that such searches were routine, although they were not. Customs Service informed Interpol which in turned informed the Guyana police that they suspected Peoples Temple of shipping weapons or unreported currency to Jonestown. The Guyana police showed a copy of the Interpol report to a Temple official which confirmed their belief that “defectors had met with a government agent.” Furthermore, the Interpol report “alluded to use of secret codes in Temple radio traffic. thus showing that Temple communications were being monitored.”[66] It is worth noting that the opponents of the Temple were “not above the sort of embellishment Jones practiced in his sermons, even in making allegations to government agents.”[67] The Interpol report confirmed for Jonestown leaders that they were indeed under attack by “a network of opposition that included defectors, reporters, and government agencies.”[68] Documents released under the Freedom of Information Act reveal that the CIA was also involved in monitoring activities in Jonestown. “It is likely that several embassy staff. were CIA operatives” and since “the embassy knew of the visits Jonestown leaders had made to Communist embassies,” the CIA was “keenly aware of events in Jonestown.”[69] Aside from the CIA, the IRS began “investigating the Temple’s unrelated business income in the spring of 1977.”[70] If the IRS decided that proceeds from the Temple’s care facilities, property rentals, and nursing homes could be taxed, the amount of money that supported the community in Jonestown would be substantially diminished.[71] By themselves, none of these threats would truly serve to dismantle the Jonestown community. Taken together, however, the threat was quite real. Although I have not addressed all the ways in which these threats were posed, Jonestown’s mode of communication, source of income, and ability to import goods were all being jeopardized. From within Jonestown, the future looked grim, and in their minds, “the existence of an organized opposition that had poisoned public opinion about the Temple and was attempting to dismantle the community in Jonestown was nothing less than a full-blown conspiracy.”[72] Overall, the Concerned Relatives’ campaign succeeded in winning in the court of public opinion and created “an atmosphere of fear and desperation in Jonestown.”[73] It is important to bear this in mind when considering Congressman Leo Ryan’s fateful visit. The atmosphere and desperation in Jonestown resulted in an interest in relocating to a communist country, an obsession with loyalty, and an increased obsession with martyrdom. The interest to relocate to a communist country, in particular the Soviet Union, was demonstrated in the learning of Russian taking place as well as by documents discovered in Jonestown. Eugene Chaikin, a Temple member and legal advisor, wrote that, “So long as we have to cover our ass, so long as P.R. has priority over production, so long as we are not free to invest and use our money in [George]town, we will not make it here. Unfortunately, time is very much against us now.”[74] The defections of Tim Stoen and Deborah Layton were both major blows to the Jonestown community. Discounting Jones himself, Stoen was the highest leader in the Temple, and the primary leader of the crusade against it. Layton had been one member responsible for the Temple’s finances, so after her defection the Temple had to change bank accounts. Both were traitors in the minds of Temple members. “It should not be surprising then, that an obsession with loyalty and hatred of defectors – real or potential – developed in Jonestown.”[75] Therefore, all who expressed any desire to return to the United States was seen as a potential threat to the entire community. The rhetoric of martyrdom referred to revolutionary suicide rather than resistant violence because the community did not want to engage in conflict with the Guyanese who were black.[76] The ritual of White Nights, which had occurred in Ukiah and San Francisco, emerged in Jonestown with increased intensity. White Nights were called in response to perceived threats, and involved the entire community gathering in the central pavilion. During White Nights there were extensive discussions of what course of action ought to be taken in the face of varying scenarios. Hunger strikes and other forms of protest were discussed during these meetings, as was the idea of escaping further into the jungle. However, the idea of revolutionary suicide became increasingly prevalent. One woman said, “I don’t mind dying for the cause, because I believe in it,” a sentiment that seemed to be held by many, as indicated by the applause following this statement.[77] Other residents in Jonestown wrote letters to Jones expressing their interest in committing revolutionary suicide.[78] For residents in Jonestown, Congressman Ryan’s visit was the culmination of the oppression they felt they had been experiencing at the hands of the United States. Ryan’s visit marked an American invasion of their constructed space. “People joined Peoples Temple and moved to Jonestown because they believed they could bring about a new society based on liberty, justice, and racial equality,” and just as “they viewed Jonestown as an ideal society, they also believed that the United States was corrupt.”[79]Congressman Ryan, a representative of all that was corrupt in the United States to Jonestown residents, announced his visit to Jim Jones on the first of November. Rather than requesting an invitation, Ryan simply declared that he would visit with Guyana government officials and then travel to Jonestown. This, combined with Ryan’s bringing of reporters and relatives, showed “a lack of impartiality” to the Jonestown residents. John Burke, the U.S. Ambassador to Guyana, reported to the Secretary of State that: PT seemed convinced that Codel [Congressional Delegation] was hostile, would be arriving with well-developed prejudices against PT and merely wanted an on-the-spot visit to enable Codel to return to U.S. and reiterate prejudiced view of People’s Temple community with more authority than before. PT officials had apparently cited to [Guyana Ambassador Laurence] Mann coincident visit by NBC camera team as proof-positive of Codel’s bad faith.[80] Jones announced the visit to the residents by stating “We may have the invasion, not with guns, but with hostile racists.”[81] Prior to Ryan’s visit, 800 community members signed a document stating that they did not wish to see any of those in Ryan’s delegation. Despite the impending visit, Jonestown continued to look into the future; the “basketball team was looking forward to its scheduled match in Georgetown” and the “Jonestown Express, the community’s showcase musical group, was rehearsing for an upcoming concert in Georgetown.”[82] It would seem that the final decision to die had not yet been made. Congressman Ryan, his aides, members of the Concerned Relatives, and an NBC news team arrived in Guyana on Wednesday morning November 15, 1978. On Friday the 17th, they flew to the Port Kaituma airstrip, six miles from Jonestown, where they were told that the media and relatives would not be allowed to enter. Ryan, his aide Jackie Speier, and an Information Officer from Guyana, were given a tour by Marceline Jones before meeting with Jim Jones himself. During the meeting, Jones was convinced that allowing the press in would do more good than harm, and so they were allowed to enter, along with the Concerned Relatives delegation. The Jonestown leaders had attempted to prepare for the visit. Jones announced “that everyone should behave so that the Ryan party could visit without incident and leave as soon as possible.”[83] Initially the visit went fairly smoothly; dinner was served (an upgraded version), there was a performance of the program Jonestown was planning to put on in Georgetown, and Ryan himself gave a speech. Ryan said, “From what I have seen there are some people here who believe this is the best thing that has happened in their lives.” Following this statement the Jonestown community erupted into a prolonged bout of cheering. Vernon Gosney and Monica Bagby, two residents who wanted to leave, slipped a note to NBC reporter Don Harris that said “Help us get out of Jonestown.” While reporters were interviewing Jones, Ryan mingled with the crowd and asked Gosney if he wanted to leave. Gosney replied affirmatively, but also warned Ryan that his life was in danger. The next morning, November 18th, the reporters returned after having spent the night discussing the note. On the tour given by Marceline Jones, the reporters became aggressive in their demands to see the inside of the cabins. While this was going on, the Parks and Bogue families disclosed their desire to leave with Ryan. Although Jones tried to talk them in to staying, they left with Ryan and the reporters. Larry Layton also left with Ryan, although he was not truly interested in defecting. Ryan planned to stay in Jonestown in order to ensure the safety of any other who wanted to leave, but Don Sly attacked him with a knife, and so he decided to leave. The defectors, Layton, the newsmen, Ryan and his aides all waited at the Port Kaituma airstrip. While they waited, a tractor pulling a trailer drove to the far end of the airstrip. In the trailer were gunmen who opened fire as those on the airstrip attempted to board the plane. Richard Dwyer, who was with Congressman Ryan, and survived, described the attacks: The firing continued for several minutes and then there was a short pause before the firing recommenced. It seemed to me that one or more of the assailants with shotguns was proceeding amongst the wounded, firing a blast at each of them. the truck and tractor were heard to drive away and after a few moments those who had not been wounded and the ambulatory wounded began to get to their feet. I went over to the Congressman, who had been badly hit. It was clear he was dead.[84] Five people were killed on the airstrip: Leo Ryan; Bob Brown, an NBC cameraman; Don Harris, an NBC reporter; Greg Robinson, a photographer from the San Francisco Examiner; and Patricia Parks, a defector who was shot as she attempted to board the plane. Ryan’s aide, Jackie Speier was shot four times at point-blank range, but survived. Larry Layton also began shooting inside the airplane once the firing outside began, but after he wounded two people, his gun jammed, and Dale Parks disarmed him. Back in Jonestown, the entire community gathered at the pavilion.[85] There is an audio recording of the meeting that night. Jones reported to the group that someone was going to shoot the pilot of one of the planes which, he predicted, will lead to a full scale invasion of the community. Jones asked for any dissenting opinions, but only Christine Miller raised any objection. Miller, a sixty-year-old black woman, asked Jones if going to Russia was still an option, but Jones replied that they would no longer be accepted into Russia because of the stigma they would acquire following the deaths of the congressman and others. Miller argued with Jones for some time, but was ultimately shouted down by the rest of the group. Interestingly enough, it seemed that Jones was encouraging Miller’s dissension. One of the surviving eye witnesses, Odell Rhodes, recalled that two nurses brought out the vat of Flavor-Aid and cyanide. Rhodes then described how one woman, Ruletta Paul, walked up to the vat with her infant: She just poured it down the baby’s throat. And then she took the rest herself. It didn’t take them right away. She had time to walk down outside. I watched her go, and the next woman, Michelle Wilson, she came up with her baby and did the same thing.[86] Even as some were dying, others can be heard on the tape stepping up to the microphone, expressing their own commitment to the revolutionary suicide. One man can be heard saying, “We’re all ready to go. If you tell us we have to give our lives now, we’re ready – at least the rest of the sisters and brothers are with me.” A woman stood and says, I’ve been here ah, one year and nine months. And I never felt better in my life. Not in San Francisco, but until I came to Jonestown. I had a very good life. I had a beautiful life. And I don’t see nothing that I could be sorry about. We should be happy. At least I am. That’s all I’m gonna say. Another man thanked Jones for giving them life and giving them death. The tape drags on and in the background one can hear the noise of the crying children. The tape ends as the adults began to take the poison. Odell Rhodes escaped by pretending to go retrieve a stethoscope for a doctor. Stanley Clayton also survived, after watching his girlfriend and her family die, by pretending he was going to say goodbye to friends on the other side of the pavilion and escaping from there. Tim Carter, his brother Mike, and Mike Prokes also survived. All three of them witnessed the deaths before they were sent away with guns and suitcases filled with cash; they were meant to go to the Soviet Embassy in Georgetown, but were detained in Port Kaituma. Two Temple attorneys, Mark Lane and Charles Garry also survived by promising a guard that they would stay alive and tell the truth about Jonestown. Only two people in Jonestown were killed by gunshot: Jim Jones and Annie Moore. Four other Peoples Temple members died later that day when the message, pre-determined in order to inform those not at Jonestown when it was time to die, reached Sharon Amos. Amos, who was at Lamaha Gardens, took her three children with her into the bathroom and slit the two younger children’s throats, and then she and her older daughter slit each other’s throats. Jones also issued the message to the Temple members at the San Francisco Temple, but Stephan Jones – Jones’ biological son – who was on the basketball team in Georgetown, quickly sent a message urging them not to proceed. Over 900 people died in Jonestown, leaving behind only three suicide notes. Tish Leroy, a 48-year-old white woman, wrote Dad I see no way out- I agree with your decision- I fear only that without you the world may not make it to communism- Tish For my part- I am more than tired of this wretched, merciless planet & the hell it holds for so many masses of beautiful people- Thank you for the only life I’ve ever known[87] Annie Moore wrote: I am 24 years of age right now and don’t expect to live through the end of this book. I thought I should at least make some attempt to let the world know what Jim Jones and Peoples Temple is – OR WAS – all about. It seems that some people and perhaps the majority of people would like to destroy the best thing that ever happened to the 1,200 or so of us who have followed Jim. I am at a point right now sow embittered against the world that I don’t know why I am writing this. Where can I begin – JONESTOWN – the most peaceful, loving community that ever existed. JIM JONES – the one who made this paradise possible – much to the contrary of the lies stated about Jim Jones being a power-hungry, sadistic, mean person. I want you to read this and know Jim was the most honest, loving, caring, concerned person whom I ever met and knew. Jim Jones showed us all this – that we could live together with our differences, that we are all the same human beings. Luckily, we are more fortunate than the starving babies of Ethiopia, than the starving babies in the United States, What a beautiful place this was. The children loved the jungle, learned about animals and plants. There were no cars to run over them; no child-molesters to molest them; nobody to hurt them. They were the freest, most intelligent children I had ever known. The final note, though unsigned, is presumed to have been written by Richard Tropp, it was entitled “The Last Day of Peoples Temple” and addressed “To Whomever Finds This Note”: Collect all the tapes, all the writing, all the history. The story of this movement, this action, must be examined over and over. It must be understood in all of its incredible dimensions. Words fail. We have pledged our lives to this great cause. We are proud to have something to die for. We do not fear death. We hope the world will someday realize the ideals of brotherhood, justice and equality that Jim Jones has lived and died for. We have all chosen to die for this cause. We know there is no way we can avoid misinterpretation. But Jim Jones and this movement were born too soon. The world was not ready to let us live I am sorry there is no eloquence as I write these final words. We are resolved, but grieved that we cannot make the truth of our witness clear. This is the last day of our lives. May the world find a way to a new birth of social justice. If there is any way that our lives and the life of Jim Jones can ever help that take place, we will not have lived in vain. Please try to understand. Lot at all. Look at all in perspective. Look at Jonestown, see what we have tried to do. This was a monument to life, to the renewal of the human spirit, broken by capitalism, by a system of exploitation & injustice. Look at all that was built by a beleaguered people. We did not want this kind of ending – we wanted to live, to shine, to bring light to a world that is dying for a little bit of love. To those left behind of our loved ones, many of whom will not understand, who never knew this truth, grieve not, we are grateful for this opportunity to bear witness – a bitter witness – history has chosen our destiny in spite of our own desires to forge our own. We were at a cross purpose with history. But we are calm in this hour of our collective leave taking. If nobody understands, it matters not. I am ready to die now. Darkness settles over Jonestown on its last day on earth.[89] From these notes, and the tape of the final hours, is it clear that a substantial portion of Jonestown residents did intentionally lay down their lives. The question that must be answered in order to examine anything else is, how could sane people do this? Emilé Durkheim notes that there are sometimes “periods in history when, under the influence of some great collective shock, social interactions have become much more frequent and active.”[90] The 1960’s clearly seems to be one such period. Fractured by the dissolution of the values of the 1960s, people turned to Jim Jones because he addressed the concerns of racism, sexism, and capitalism that plagued them. Given the shock to American culture of the Vietnam War, the explosion of racial tensions, and the upheaval of the political system, individuals were looking for new ways to make meaning of the world. The United States’ capitalist system encourages competition and individualism, so Peoples Temple provided the welcome alternative of communalism. Many of the communities that one traditionally used to define oneself had been eroded. New communities became necessary. Once members became involved in the intensified social actions of Peoples Temple, the role of societal morality began to figure in the consciousness of the individual. As the moral or ideal plays a greater role, so too does the need to make sense of its force. Durkheim constitutes the totem as the representation in ones mind of the “collective and anonymous force of the clan.”[91] The totem is the concrete object upon which “sentiments experienced fix themselves”; it is the object by which “the emotions experienced are perpetually sustained and revived,” and it is the seeming source for everything that happens.[92] Traditionally, totems are either animals or inanimate objects; however I contend that the personhood of Jim Jones served as a totem for Peoples Temple. I also believe Jim Jones became increasingly important as a totem of Peoples Temple. Jones was understood as the manifestation of the ideal of the group. In one Peoples Temple publication, “A True Follower of This Activist Christian Ministry,”[93] the ideals of the movement are laid out in such a way that Jones is constructed as their embodiment. It could even be further said that, by presenting Jones as an incarnation of Jesus, he serves as a totem of the Christ. Jim Jones was central to Peoples Temple, not only because he was the founder of the church and the head pastor, he was primarily important as a prophet. According to Max Weber, the prophet is an “individual bearer of charisma, who by virtue of his mission proclaims a religious doctrine or divine commandment.”[94] The prophet’s authority is exclusively the result of his or her personal revelation and charisma. The prophet’s primary goal is the declaration of said doctrine or commandment, but in order to legitimate his or her authority, the prophet frequently uses magic, such as healings and divination. Weber notes that a prophet rarely succeeds in “establishing his authority without charismatic authentication, which in practice meant magic.”[95] Jones’ primary goal was to create a world in which discrimination no longer existed and all people were equal. However, in order to gain the power needed to create such a world, he convinced people that he was able to heal the sick, see the future, and read minds. Jones once explained the magical shows in services to Deborah Layton, saying that it was necessary, “so that I can bring them forward to the message that is so important for all of us today and that is activism … I need to speak on each person’s level.”[96] As both totem and prophet, Jones is not significant as an individual human, but as a symbolic figure of the movement. Jones was constructed by the society of Peoples Temple as a salvific god and, therefore, as a manifestation of the group. The passion with which Peoples Temple members were loyal to Jones is then, essentially, the same as their loyalty to one another and even themselves. When individuals participate in such movements, “the passions moving them are of such an intensity that they cannot be satisfied except by violent and unrestrained action.”[97] The collective suicidal impulse of Peoples Temple arose from a number of interacting factors, not the least of which was the United States’ invasion of their space. David Chidester argues that the civil space of the United States was disrupted during the 1960’s and 70’s, thereby leading “alternative religious movements that emerged” during that period to either establish “a place within American society by appropriating the symbols of the ‘center in here’ and claiming them as their own” or to direct their attention to “powerful, sacred symbols of a “center out there,’ beyond the territorial boundaries of the United States.”[98] Peoples Temple opted for the “center out there” model, similar to American Catholics, Jews, and the National of Islam. For many groups who chose to locate their sacred geographic center outside the space of the United States, the “civil space of American society may be experienced as an oppressive space in which the group occupies a position of ‘existential outsideness,’ while the group itself may be perceived, or may perceive itself, as a subversive space.”[99] Peoples Temple’s migration to Guyana therefore may be understood as a pilgrimage out from the oppressive, polluted, prison like space of the United States. Catherine Wessinger, a Religious Studies professor at Loyola University, describes “catastrophic millennialists” as groups who “expect far-reaching changes to come after a major catastrophe or series of disasters” and those that “anticipate gradual change and expect improvements to arrive step-by-step over long period of time” as “progressive millennialists.”[100] Members of Peoples Temple were progressive millennialists in that they believed that through their move to Jonestown, they could create a new, just society. However, they were catastrophic millennialists in that they expected that the United States would become a fascist dictatorship in which African Americans and those who supported them to be arrested tortured, and killed.[101] Wessinger notes that If members of a catastrophic millennial group perceive themselves as being persecuted by outside cultural opponents, and furthermore, perceive that they are failing to achieve their ultimate concern, this will be a group that is likely to commit violent acts in order to preserve its ultimate concern.[102] The attacks aimed at Jim Jones were designed to minimize the alienation of Peoples Temple members, but an attack on Jones was in many ways an attack on the group’s ultimate concern. The collective suicidal impulse of the group served a number of functions for the group as a whole. In his work Suicide, Durkheim presents altruistic suicide as one ideal-type. Durkheim notes that just as “excessive individuation leads to suicide, insufficient individuation has the same effects.”[103] From the expressions of individuals as they died, it seems clear that the individual body was insignificant in comparison to the communal body. This was also demonstrated by some of those who survived the final White Night. Mike Prokes, who survived Jonestown, called a press conference on March 13, 1979 in a motel room in Modesto, California, during which he announced “I can’t disassociate myself from the people who died, nor do I want to. The people weren’t brainwashed fanatics or cultists; the Temple was not a cult.” He then went into the bathroom and shot himself in the head.[104] Bea Orsot, another survivor who had been in Georgetown for a dental appointment when the deaths occurred, stated in an interview a year after the Jonestown event that her eight years with Peoples Temple and one year in Jonestown had been the happiest of her life. “If I had been there, I would have been the first one to stand in that line and take that poison and I would have been proud to take it,” she said. She wished she could have died with her community, “I wanted to die with my friends. I wanted to do whatever they wanted to do – be alive or dead.”[105] Durkheim notes that in order for “the individual to occupy so little place in collective life, he must be almost completely absorbed in the group and. very highly integrated.”[106] Altruistic suicide occurs when the ego is no longer the property of the individual, but the property of the collective whole, where it is “blended with something not itself, where the goal of conduct is exterior to itself, that is, in one of the groups in which it participates.”[107] In this mode, suicide is either “imposed by society as a duty, or some question of honor was involved. But it even happens that the individual kills himself purely for the joy of sacrifice, because. renunciation in itself is considered praiseworthy.”[108] The martyrdom that was presented in Jonestown constructed the death as an opportunity for “the individual … to strip himself of his personal being in order to be engulfed in something which he regards as his true essence. he feels that he exists in it. and strives so violently to blend himself with it in order to have being.”[109] This is congruent with the idea presented by David Chidester that part of Peoples Temple’s appeal to the poor, the elderly, blacks, and women was that it offered them an opportunity to be recognized as fully human. From the beginning of Peoples Temple, there was an emphasis on what Chidester identifies as the subclassification of people in American society. Jones argued that this subclassification was legitimated by the King James Bible. “The classification of persons in the worldview of Peoples Temple responded to the prevalent experience of subclassification in America shared by blacks, Indians, Mexicans, women, and the poor.”[110] Peoples Temple located a space of human potential that promised to be realized through socialism. That was a space filled with the inherent goodness of human nature, the healing of mind, body, and social relations, and a humanitarian ethic of reciprocal sharing, concern and service to others. This was what it meant to be a fully human person within the Peoples Temple.[111] Thus, reintroduction into the polluted space of the United States would have meant an entrance into a space where many members felt they would be systematically sub-humanized. The communal suicide offered an opportunity for what was understood as a fully human death. The deaths in Jonestown quickly attained a prominent position if the American consciousness. By February, 1978, 98% of Americans had heard of the Jonestown tragedy according to a Gallup Poll.[112] Despite this extremely high level of awareness, the news stories produced in the aftermath were far from factual or impartial. Hall claims that the mass media attention in Jonestown created myths which has left historical analysis with the burden of wading through those myths. Hall argues that, in the case of Jonestown there is not a compelling cultural demand to know. the causes of the deaths. The horror could never be understood in historical terms, for history has an uneven relation to the moral distinction between good and evil. Thus, the task of myth is to close the curtain on a tragedy steeped in stigma so as to reaffirm the normal social world.[113] However, it was not only myth that was necessary to reaffirm the normal social order. Both the Guyanese and United States governments attempted to distance themselves from Jonestown. A Guyanese Public Health Officer, along with two American medical officers, recommended that the bodies be buried on site in Jonestown, but the Guyanese government would not allow it and insisted that the bodies be removed from Guyana as soon as possible.[114] The transportation of the bodies was undertaken by the United States military. The bodies were transported from Jonestown to Georgetown and on to Dover Air Force Base in Dover, Maryland. The bodies remained at the base while relatives attempted to retrieve their loved ones’ remains “from a government who seemed indifferent to their grief. Some bodies never could be identified, and some people of Jonestown had no concerned relatives willing to claim their bodies.”[115] Although in many cases no one was willing to pay for the burial of the Jonestown community, regarding the Temples “fantastic monetary assets” there was “no shortage of claimants.”[116] These claimants appeared before the dead had even been buried. Walter and Charlotte Baldwin, Marcie Jones’ parents, attempted to bury Jim and Marcie at the Earlham Cemetery, in Richmond Indiana, but the town of Richmond would not allow it. Jim was cremated, his ashes spread at sea. The competition between a variety of interest groups “for control of in the initial aftermath. resulted in the dehumanization of the dead.”[117] Government agencies, surviving Peoples Temple members, families, religious associations, the media, and so-called cult experts all attempted to control the construction of the Jonestown narrative. Family members and surviving Temple members were attempting to deal with their grief and bury the dead. Religious organizations were anxious to emphasize the “differences between good religion and bad religion” although the “voices heard loudest in the media belonged to critics of non-traditional religions.”[118] The media descended on Jonestown in droves and news coverage tended to focus on the sensational: Jones’ sexual history, the modes of punishment used in Peoples Temple, the specifics of how the death took place. News outlets looked to “cult experts – and only to cult experts – for analysis. These experts knew next to nothing about Peoples Temple, and exaggerated the differences between Temple members and ordinary members of society.”[119] The efforts to demonize the Jonestown dead were part of a ritual of exclusion, as Chidester calls it. In The Elementary Forms of Religious Life, Durkheim discusses the positive cult in contrast to the negative cult. By classifying Peoples Temple as a negative cult in contrast with itself, society establishes itself as the positive cult. Durkheim argues that it through the rejection of the negative cult that an individual enters into the positive cult.[120] However, this “symbolic differentiation cannot be. accomplished when what is socially defined as evil emerges within the sanctified centers of a culture.”[121] Therefore, a mad dash by Guyanese officials, the American embassy officers, the Disciples of Christ, and elected officials in California to distance themselves from Peoples Temple ensued. Jones and his movement were defined as evil because of their goal, as much as the methods used to achieve it. There was “a particular ideological judgment about the relative values of communal versus individualist approaches to life.”[122] There was also a focus on Temple methods: healings, intimidation, punishment, worship of a prophet, political manipulations, and public relations. But, “this auto-da-fe can proceed only by the device of projecting onto Jones the burden of carrying evils that are widespread and sometimes institutionalized practices in the wider society.”[123] The effort to demonize Jones and his followers was evidenced in the treatment of the bodies at the Dover Air Base as well. U.S. officials claimed that no U.S. entity was responsible for doing autopsies of those who had died abroad, but as Rebecca Moore notes following an airplane crash in the Canary Islands where American citizens had died, the bodies had been shipped to Dover where autopsies and identification had taken place. Rather than engaging in careful autopsies, the forensic scientists at Dover conducted autopsies on Jim Jones at the request of the U.S. government, and Laurence Schacht, Maria Katsaris, Carolyn Layton, and Ann Moore, at the request of the families. Only two other bodies were autopsied at random from the remaining 909 Temple members.[124] According to Dr. Cyril Wecht, Past President of the American College of Forensic Sciences, the deaths in Jonestown “failed to arouse the sensitive interests and pragmatic concern of the people in charge because the victims were perceived as ‘cultists.’ The unspoken attitude was something like: ‘what did you expect from such lunatics?’ ‘they got what they deserved.'”[125] The rejection of the bodies of Jonestown residents was part of an American process during the late 70’s. This process was one in which black Americans who attempted to attain equal status – not just equal opportunity – were rejected by society at large. Although Jones was its leader, Peoples Temple was constituted by the individuals who gave it life. Those individuals were primarily black and searching for a real alternative to the economic and social oppression they experienced in the United States. Peoples Temple was attempting to point out that racism was entrenched in the economic policies of the United States. By going communal, Peoples Temple members in many ways addressed the enduring cause of racial discrepancies: differences in accumulated wealth. The attack on capitalism made by Peoples Temple was an attack on the civil religion of America. There is no evidence of an orchestrated effort to prevent the message of Peoples Temple from being communicated. I don’t believe that the bodies were consciously mistreated in a grand conspiracy to keep Jonestown residents silent. Rather I think it was the response of a country which, when potentially confronted with its own immorality, opted to shift the blame. As Charles Silberman, in his book Crisis in Black and White, stated: The tragedy of race relations in the United States is that there is no American dilemma White Americans are not torn and tortured by the conflict between their devotion to the American creed and their actual behavior. They are upset by the current state of race relations, to be sure. But what troubles them is not that justice is being denied, but that their peaces is being shattered and their business interrupted.[126] Had the American involvement in the Jonestown deaths dominated the news in 1978, perhaps the Jonestown events might have served as a wake-up call regarding the inequities of capitalism. However, part of effectively maintaining a capitalist society is the continued belief that the system is inherently just, that those who opt to leave it are crazy, and that those who fail within it failed themselves. As Ernest van den Haag states, “people will tolerate a social or economic system. only if they perceive it as just.”[127] The race riots that followed the Civil Right era were used as justification for white flight from American cities. Although these riots reflected the enduring oppression of black people, they were constructed in the white psyche as self-destructive, insane acts which allowed white people to comfortably distance themselves from black people. The deaths in Jonestown were constructed and treated in much the same way: as depraved, crazy acts. Therefore, no serious consideration was given to the notion that Peoples Temple’s fate was reflective of the larger injustices of American society. In 1935, following a riot in the Harlem neighborhood of New York City, Mayor Fiorello La Guardia appointed a biracial commission to study its causes. The commission’s report, The Negro Problem in Harlem: A Report on the Social and Economic Conditions Responsible for the Outbreak of March 19, 1935, noted that the riot was caused primarily by economic factors, particularly the poor job opportunities available to blacks.[128] However, “white society, including the academic establishment and the liberal foundations, were not ready to contemplate, much less embrace, the simple truths enunciated” in the report, and so its policy recommendations were largely ignored.[129] In 1968 the Report of the National Advisory Commission on Civil Disorders, also known as the Kerner Report was published. This commission was appointed by President Johnson as a response to the riots that began in Newark and Detroit. The Kerner Report found that “white racism is essentially responsible for the explosive mixture which has been accumulating in our cities since the end of World War II.”[130] Similar to the 1935 report, the Kerner Report’s policy recommendations were primarily focused on job creation. Although the Kerner Report received wider circulation, it never “became the basis for legislative action or other public policy initiatives.”[131] Throughout the 1960s there was an outpouring of publications that sought to confront white American society with its culpability in the situation of black people. Unfortunately, as quickly as this period began, it was repressed. In his book, Racial Oppression in America, a collection of essays, Robert Blauner presents a new paradigm with which to consider the situation of black Americans. Blauner presented African Americans are an internal colony and the riots as forms of revolt.[132] His “colonial model” did away with notions that the oppression of African Americans was comparable to the oppression of other immigrant populations and “portrayed blacks as a minority of a different kind: a permanent minority, an oppressed people, a ‘colonized group.'”[133] Blauner’s model challenged many of the basic assumptions of liberals and called for an entirely new method for eradicating racial difference in the United States. However, it did not take long for Blauner to be contested. Nathan Glazer published an essay in which he rejected Blauner’s colonial model and once again shifted the blame for the black situation in American back to blacks themselves. Glazer claimed that racial ghettos “are not a product of institutionalized racism, but merely an expression of ethnic cohesion and voluntary self-segregation”; that blacks have not had enough time in northern cities for the process of “mobility and assimilation to play itself out”; that the lack of black economic and political control over their own communities is reflective of the “economic enterprise that was lacking among blacks”; and that “much of the blame for economic stagnation has to do with blacks themselves, specifically with cultural deficiencies that prevent them from following the footsteps of earlier waves of immigrants.”[134] Glazer’s 1971 refutation of Blauner’s argument marked the beginning of the academic backlash against scholarship that places the burden of black oppression on the doorstep of white America. Richard Nixon won the 1968 presidential election largely because of his Southern strategy that appealed to white backlash. Following his election, Nixon “lost no time in turning the clock back on the civil rights revolution.”[135] Nixon shifted “responsibility for school desegregation to the federal judiciary, and then nominated two Southerners with pro-segregationist records to the Supreme Court,” and allowed the Justice Depart to work with the FBI “in a covert war against the Black Panther Party.”[136] However, Nixon also laid the groundwork for affirmative action through his expansion of the Equal Employment Opportunity Commission, resolution of the AT&T-EEO Consent Decree, and resurrection of the Philadelphia Plan. These policies were not part of a “preconceived design” but were “a byproduct of a series of administrative decisions and compromises that, in their net effect, formed the basis for affirmative action as we know it.”[137] These policies were essentially the compromises of the political establishment that were necessary to maintain order.[138] The white withdrawal from racial issues was the result of a number of factors. Once desegregation came to Northern cities, whites realized that “the Negro was not just an abstraction, and not just a Southern problem.”[139] Furthermore, in 1965, when notions of reparations and compensation began to present themselves in political discourse, “white liberals were beginning to display their disquiet with this troublesome turn of events.”[140] The realization that legislative change was not sufficient to achieve equality for black Americans came in the wake of the riots in the Watts neighborhood of los Angeles in 1965, although these ideas had existed before its outbreak. In 1965, “equality of opportunity – in name at least – had been achieved” and it was following this period that “an overt phase of racial oppression ended in the United States and was replaced by economic subordination.”[141] This economic subordination was based on a lack of equality of condition, something that reparations and compensation were designed to combat. However, “white state and corporate power is the product in part of black powerlessness” and the “income mobility for the few is rooted in the income stasis for the many.”[142] Therefore, the true equalizing of condition would result in an upheaval of the economic status quo. Liberal whites also retreated from race by claiming that “America is too racist to support programs targeted specifically for blacks, especially if these involve any form of preference. highlighting racial issues, therefore, only serves to drive a wedge in the liberal coalition. and is ultimately self defeating.”[143] Following the Watts riots in California, Governor Brown appointed a commission to investigate its causes. This commission, known as the McCone Commission, released a report that characterized the riots as the meaningless outburst of marginalized people. The report claimed that the rioters were marginal people. because they were a small and unrepresentative fraction of the Negro population, namely, the unemployed, ill-educated, juvenile, delinquent, and uprooted. What provoked them to riot were not conditions endemic to Negro ghettos. but rather problems peculiar to immigrant groups. and irresponsible agitation by Negro leaders.[144] This report was, of course, factually incorrect. This report, however, was so incorrect because it was a reflection of typical white attitude towards riots of this type. Critics of the McCone Commission examined the reasons for the inaccuracies of the report and found that the “commissioners were not altogether unsympathetic to the plight of Negroes in the south central ghetto, nor were they unintelligent or irresponsible.” Instead they were “representative of upper-middle-class whites in Los Angeles and other American cities.”[145] As such, they brought with them “assorted preconceptions about violence, law enforcement, ghettos, and slums, preconceptions which they shared with others of their class and race,” it was these preconceptions that allowed them to “draw conclusions based on the flimsiest material while ignoring the more substantial but less reassuring data.”[146] The violence of the race riots allowed whites to distance themselves from American blacks. Over and over again white Americans were presented with evidence that racial oppression was inextricably linked to economic oppression. The only moments that whites were interested in examining this data was in the face of violent black uprisings. Then the changes made were aimed at merely placating urban blacks, rather than to attempting any substantial change. Today substantial differences in opportunities available to whites and those available to people of color persist. The educational system of the United States is arguably as segregated today as it was prior to legislative reform.[147] These inequalities are based, on a system of segregation by race, poverty, and, increasingly, language, in which most black and Latino students never receive similar opportunities, similar peer groups, or any real chance to connect with and learn how to operate comfortably in middle class white institutions and networks. Many are in high schools where there is no real path to college because there are not enough teachers credentialed and experienced in key subjects and not enough fellow students ready to enroll in strong pre-collegiate courses taught at an appropriate level. For those students, there is no way to get the right preparation in their school regardless of their personal talent and motivation.[148] In his article “Education and Crime,” Lance Lochner articulates how “an increase in educational attainment significantly reduces subsequent violent and property crime yielding sizeable social benefits.”[149]Is it any wonder then that gang membership balloons and violent crimes increase among populations who are systematically disenfranchised and dehumanized? According the US Bureau of Census from 1977-1995, The United States increased the prison budget by 823% but only increased spending on higher education by 374%. So rather than attempt to improve educational opportunities, the United States pours money into the prison systems, in essence, expecting those who have been disenfranchised by the American system to pay for their own abuse. This system brings to mind the infamous Kristallnacht, after which Jews were billed for the destruction of their own property. Of course, the problem here is more complicated, but the essential contradiction is the same. Prisons are disproportionately populated by people of color, yet rather than confronting the fact that this is because of the difference in opportunity presented to them, American society supports prisons in order to maintain the illusion that disenfranchised people are disenfranchised because of their own criminal inclinations rather than the situation handed to them. Furthermore, the imprisonment of those whom the American system has failed allows those who benefit from the oppression of poor people, and people of color, to feel justified. Although dominant American society seems to expect those they oppress to involve themselves in positive organizations, those organizations are only acceptable insofar as they adhere to greater societal norms. Organizations of oppressed people that build solidarity across racial, ethnic, and socio-economic lines by their very nature threaten the status quo and are therefore treated with suspicion and contempt. Peoples Temple, an organization which sought to connect and equalize people regardless of race or economic status was thus treated with hostility. Initially, the Temple was acceptable to society at large because it appeared to be operating within the prescribed social order; however, once it became clear that it was in fact reconstituting what it means to be human, it was no longer acceptable and was attacked with increasing ferocity. [13] Allen J. Matusow, The Unraveling of America: a History of Liberalism in the 1960s (New York: Harper & Row, 1984) xiv. [14] Larry Layton, Deborah’s brother, was the only member of Peoples Temple arrested and tried for the deaths in Jonestown. Larry pretended to join other defectors who were leaving with Congressman Ryan and then opened fire once inside the airplane. [29] In fact, public record might even have contradicted Walter Jones’ accusation that the Temple did not provide enough money for him to take care of the emotionally disturbed boys in his care. There is substantial indication that Peoples Temple homes were in fact far better then the facilities provided by the government. Evidence of this can be found on page 82 of John Hall’s Gone from the Promised Land. [30] Deborah and Elmer Mertle later changed their names to Al and Jeanne Mills and will be referred to as such throughout the rest of the paper. [59] “Accusation of Human Rights Violations by Rev. James Warren Jones against Our Children and Relatives at the Peoples Temple Jungle Encampment in Guyana, South America” (11 April, 1978), Alternative Considerations at http://jonestown.sdsu.edu/?page_id=13080 [80] State Department Cable, 5 November 1978, From U.S. Embassy to Secretary of State, in The Assassination of Representative Leo J. Ryan and The Jonestown, Guyana Tragedy: Report of a Staff Investigative Group to the Committee on Foreign Affairs, U.S. House of Representatives, House Document No. 96-223 (15 May 1979), 96th Congress, 1rst Session (Washington D.C.: U.S. Government Printing Office, 1979), 51."}
{"url": "https://scienceline.org/2006/07/ask-cosier-skunk/", "text": "Nothing is quite as jarring as the smell of a skunk, especially if you’re the one who’s been sprayed. But contrary to folklore, bathing in tomato sauce or juice won’t wash away the stench. Tomato juice simply masks the skunk smell. It does not eliminate it. People are very sensitive to the sulfur compounds in skunk spray, called thiols, and we can detect them at very low concentrations (10 parts per billion). Anyone who is sprayed by a skunk eventually gets used to the smell because of olfactory fatigue, which is when your nose is bombarded with a scent for so long that it stops detecting it. If you are sprayed, a shower is your best first defense. If your pet was sprayed, a better solution is to mix a quart of three percent hydrogen peroxide with a quarter cup of baking soda and a teaspoon or two of liquid detergent, according William Wood, a chemistry professor at Humboldt State University. This will neutralize the smell, Wood says, which is much better than masking it. Don’t store this magic potion, however, because the oxygen buildup could blow off the top of the container, and it could also bleach your dog’s fur. In days gone by, people believed that tomato juice could neutralize skunk stink. But since then, chemists have found that in order to deodorize you or your dog, you must change the chemical composition of the malodorous compound. Skunk spray, which is also known as skunk musk, can be harmful to an animal or person at high concentrations. People who are sprayed may experience burning eyes or nausea. If you weren’t sprayed, but find that after washing a sprayed pet, the clothes you wore absorbed some of the smell, a regular wash with a dash of ammonia should get rid of any lingering odors. Until, that is, your dog pursues his curiosity again. Maybe you should remind him of what killed the cat. Filed To Share About the Author Always interested in multiple aspects of the environment, Susan decided to try writing about them. She is now pursuing a career investigating science and environmental topics from yet another perspective. Discussion 14 Comments thanks very much for answering my queston. You also did what I was hoping for, you gave an actual treatment for a skunk spary. The only other thing I would ask, and Im not sure if you would want to take this site in quite that direction, is to give the reason the treatment works. Like the chemical reaction that is neutralizing the smell. I dont know if that would be appreciated by all, I am a biochemist and so would love to hear the answer, but i would certinly understand if that would not be what you want. Again thank you for answering my question. The reason that the treatment works is because the solution destroys the thiols and hydrogen sulfide found in skunk musk. The mixture oxidizes the thiols to sulfonic acids, which don’t reek of skunk. Other solutions can also oxidize the compounds, reports Wood. If you’d like more specifics, just let us know. My dog, Maxie, has been sprayed by a skunk I think 3 or 4 times now. It is not a fun experience. I tried the tomato juice the first time and it turned my blond lab into a pizza! And it only masked the odor a little bit but I could still smell the skunk odor. It also made a HUGE mess. I have also tried the other treatment in the past but I did not work so well for me. It did reduce the smell but it still lingered for about a month afterward. I have also looked at many other sites about this mixture and it says to avoid your dogs face area, while was were my dog has been sprayed pretty much every time. (Why don’t they ever learn?!) I also had Maxie run all over my house after being skunked and that is a hard job trying to get rid of the smell around the house as well. After the second time she was sprayed, I found a product called BioWorld Odor Neutralizer that worked the best. I just added it to Maxie’s bath water and shampoo and it actually got rid of ALL the odors! After talking to the company, they told me I could also add it to my carpet shampooer and laundry and that worked great as well. That was what really sold me on the product. It solved all my problems! There is nothing worse than smelling Skunk for a month, let me tell you. I didn’t even have to smell it for more than a day using their product. I have learned my lesson and I am sure that Maxie will have many more encounters with skunks so I now keep this product around my house for when it happens again (she is only 4 now so she has a lot of life to live!) It also comes in handy with other odors around the house. I just thought I would share the information because I wish I knew what worked and what didn’t the first time it happened to Maxie. It would have saved me a big headache! ha I like that there is an alternative way to get rid of the skunk musk, rather than just bathing in tomato juice. Is there a way too completly get rid of the smell? What do you do when the stench is in your house? After my dog & cat were sprayed (at different times) I got right to it and searched the internet to find a remedy for that horrible hellish stink. I put this concoction from a few ideas. First I put on an old tee shirt that I could throw away, and a towel too, I had a deep sink which was so much easier on the back, but a bathtub works fine too. Then have a box of baking soda, dawn dish soap( no substitutes) and lemon juice. First on the dry fir, sprinkle the baking soda all over & work in the fir down to the skin, work in in for about 3-5 min. Then wet the fir with warm water. Then take a good squirt of the dish soap in about 2 cups warm water & careful not to get in their eyes, pour over coat and work in to the fir, now comes the exciting part…take about 1 cup of lemon or lime juice and gently pour over fur, it will foam up big time, then work this into the fir. For the face you can wet a corner of a wash cloth and dip this mix and be very careful not to get in their eyes, and rub the areas sprayed. Rinse off with clear warm water. You might have to repeat, but I found I only needed 1 treatment and it worked beautifully! Throw the tee & towel away, and voila, stink all gone! Yay! Edge Shave Gel actually works wonders for skunk smell, can be stored so readily available, and does not bleach the fur. I have used this many times for dogs having been sprayed, including a direct spray so close that it was dripping off of my dog. After the Edge treatment, there was no slunk odor even when he was wet. We are battling the smell right now in our entire house. A neighbor’s dog either chased or found a skunk in our crawl space, and our house, clothes, crawl space, furniture,etc now smell like skunk – big time! So how do you neutralize the smell in a house? Any suggestions appreciated. Sometime soon there will likely be human colonies through out this photovoltaic system, on moons connected with planets and within the planets independently. There isn’t a doubt there are likewise human"}
{"url": "https://www.semanticscholar.org/author/C.-Kyobutungi/145382428", "text": "The study indicates poor adherence to WHO recommendations for breastfeeding and infant feeding practices in Nairobi slums and interventions should pay attention to factors such as cultural practices, access to and utilization of health care facilities, child feeding education, and family planning. A literature search on PubMed on a broad range of topics regarding hypertension in Africa, including data collection from related documents from World Health Organization and other relevant organizations that are available, finds that hypertension is the number one risk factor for CVD in Africa. Slum residents’ ability to seek healthcare for non-COVID-19 conditions has been reduced during lockdowns and clear communication is needed about what is available and whether infection control is in place to encourage healthcare seeking."}
{"url": "https://www.academia.edu/418498/The_Place_of_Greenland_In_Medieval_Icelandic_Saga_Narrative", "text": "The Place of Greenland In Medieval Icelandic Saga Narrative The Place of Greenland In Medieval Icelandic Saga Narrative 2009, Norse Greenland: Selected Papers of the Hvalsey Conference 2008 (Journal of the North Atlantic, Special Volume 2), pp. 30–51 This paper explores the accounts of Norse Greenland in the medieval Icelandic sagas, looking past the Vínland sagas to examine ways in which Greenlandic settings are employed in the 'post-classical' saga-tradition and other texts. The style and content of these tales varied over time, but the recurrence of certain conventional patterns indicates that stories set in Greenland retained important thematic continuities for Icelandic saga audiences. From as early as the 12th century, Icelandic writers identified Greenland as a peripheral space in the Norse world, connected with Iceland, but markedly distinct and remote. This marginalization is evident in the Vínland sagas and developed further in the post-classical tradition, which made Greenland a place of exile in which Icelandic heroes were tested by extreme adversity in the settlements and wilderness. Embodying the preoccupations of Icelandic writers and audiences, these writings tell us little about historical realities in Norse Greenland; but they do show how details of geographical and historical lore were subsumed and transformed in the Icelandic narrative tradition. New Norse Studies, edited by Jeffrey Turco, gathers twelve original essays engaging aspects of Old Norse–Icelandic literature that continue to kindle the scholarly imagination in the twenty-first century. The assembled authors examine the arrière-scène of saga literature; the nexus of skaldic poetry and saga narrative; medieval and post-medieval gender roles; and other manifestations of language, time, and place as preserved in Old Norse–Icelandic texts. This volume will be welcomed not only by the specialist and by scholars in adjacent fields but also by the avid general reader, drawn in ever-increasing number to the Icelandic sagas and their world. Table of Contents Preface; Jeffrey Turco, volume editor: Introduction; Andy Orchard: Hereward and Grettir: Brothers from Another Mother?; Richard L. Harris: “Jafnan segir inn ríkri ráð”: Proverbial Allusion and the Implied Proverb in Fóstbrœðra saga; Torfi H. Tulinius: Seeking Death in Njáls saga; Guðrún Nordal: Skaldic Poetics and the Making of the Sagas of Icelanders; Russell Poole: Identity Poetics among the Icelandic Skalds; Jeffrey Turco: Loki, Sneglu-Halla þáttr, and the Case for a Skaldic Prosaics; Thomas D. Hill: Beer, Vomit, Blood and Poetry: Egils saga, Chapters 44-45; Shaun F. D. Hughes: The Old Norse Exempla as Arbiters of Gender Roles in Medieval Iceland; Paul Acker: Performing Gender in the Icelandic Ballads; Joseph Harris: The Rök Inscription, Line 20; Sarah Harlan-Haughey: A Landscape of Conflict: Three Stories of the Faroe Conversions; Kirsten Wolf: Non-Basic Color Terms in Old Norse-Icelandic The Icelandic Family Sagas – Old-Norse prose narratives written during the 1200s – inscribe in retrospect a process by which the unknown terrain of late ninth-century settlement Iceland is ‘mapped’ through association with human story. Space begs history: family sagas locate past deeds in a present landscape. At the most evident level, sagas explain how places received their names by reference to the people who had lived there. Another layer of meaning is created by the movement of stories and journeys over this named geography. Furthermore, the saga landscape thus constructed is shown to have continuing relevance: the sagas link past and present, with physical evidence of saga action still evident in thirteenthor even twentieth-century Iceland. Yet family sagas do not claim that all responsibility for this construction of landscape lay with the early settlers. The land too is shown to have had agency, so choosing its people and history. Based on a literary and historical perspective, this talk will explore what in actuality are the famous “Viking Sagas” in a way that is fitting both for complete newcomers and connoisseurs of Norse culture. The origin of the various subgenera of sagas such as kings’ sagas, legendary sagas and chivalric sagas will all be discussed, alongside the transmission of skaldic poetry, the historicity of saga narratives and much more. In addition, a special focus will be put on the place of supernatural and Heathen motives in this literary tradition and how much faith can modern Pagans and Heathens have in these age-old tales. The 9 essays collected in this volume are the result of a workshop for international doctoral and postdoctoral researchers in Old Norse-Icelandic Saga Studies held at the Institute for Nordic Philology (LMU) in Munich in December 2018. The contributors focus on ›unwanted‹, illicit, neglected, and marginalised elements in saga literature and research on it. The chapters cover a wide range of intra-textual phenomena, narrative strategies, and understudied aspects of individual texts and subgenres. The analyses demonstrate the importance of deviance and transgression as literary characteristics of saga narration, as well as the discursive parameters that have been dominant in Saga Studies. The aim of this collection is to highlight the productiveness of developing modified methodological approaches to the sagas and their study, with a starting point in narratological considerations. Andreas Schmidt and Daniela Hahn are postdoctoral researchers, reading and teaching Old Icelandic literature from narratological perspectives. Both completed their PhDs in Scandinavian Studies at the Ludwig-Maximilians-Universität in Munich. Following Bad Boys and Wicked Women (Münchner Nordistische Studien 27), this is their second co-edited collection of essays. Scholars have traditionally viewed the post-classical sagas and þaettir of Icelanders, written between the second half of the 13th century and the early 15th century, as inferior to the classical sagas and þaettir in terms of narrative form and social relevance. In the present study we argue that the post-classical texts show an innovative approach to the concept of narrative space, and that they are reflective of the various narrative modes in a way that allows the narratives to become more varied and multi-layered than the classical sagas. We also argue that the increased use of supernatural motifs is not a sign of disinterest in the social concerns, because such motifs contribute to the conceptualization of the social issues that had changed after the end of the Sturlung Age, but they had not become less significant. Masters Thesis: The reputation and importance of Ari hinn fróði in the development of the Icelandic literary corpus is evident and widely recognized, but nevertheless, the importance of Íslendingabók in the development of the Íslendingasögur has not been investigated in detailed. Showing the importance of the narrative structure and fundamental argumentation of Ari Þorgilsson in the genesis of an Icelandic historical ethos that allowed Icelanders to recover, reshaped and made use of their pagan ancestry is important to understand the close relation that Icelanders kept with their past in the centuries following the conversion. Thus, it will be investigated how Íslendingabók represents an emerging ethos in Icelandic scholar tradition, that re-appropriates the past as praise-worthy but looks forward to a Christian future. In Íslendingabók, Ari chose an array of historical events, that became in the Íslendingasögur creating a Christian narrative that, like the one of many Íslendigasögur, evolves from one pole towards the final resolution. Ari’s narrative gravitates around the Christian origins of the land, and its final rising of an independent Church in the land. By doing this, Ari created a new cultural ethos in Iceland, that wrote in vernacular rather than Latin, and influenced the narrative structure of the sagas while also rising the indigenous narratives to an scholar production. A Face recognition system is an application of computer vision which is capable of performing two major tasks identifying and verifying a person from given data base. The objective of this paper is to design an effective attendance system which is based on facial recognition and intend to reduce the manual efforts of the teacher. In the conventional attendance system there are several issue like fake attendance, time consumption, manipulation of attendance. The algorithm used is named fisher face algorithm, which is already in use but it gives an accuracy of 5-6% and the amount of faces it can detect is comparatively less, Here we intend to use fisher face algorithm with the help of support vector machine(SVM). The system is trained with database faces. The data gets updated in the portal which is accessed by the faculty and the students. This paper is a speculative model of attendance management system using facial recognition. The state of a cutting tool is an important factor in any metal cutting process as additional costs in terms of scrapped components, machine tool breakage and unscheduled downtime result from worn tool usage. Therefore, tool wear prediction plays an important role in industry automation for higher productivity and acceptable product quality. Therefore, in order to increase the productivity of turning process, various researches have been made recently for tool wear estimation and classification in turning process. Chip form is one of the most important factors commonly considered in evaluating the performance of machining process. On account of the effect of the progressive tool wear on the shape and geometrical features of produced chip, it is possible to predict some measurable machining outputs such as crater wear. According to experimentally performed researches, cutting speed and cutting time are two extremely effective parameters which contribute to the development of the crat... The aim of the article is to place Andrzej Wajda’s work from the early 1970s in the late-moderist trend developing in Polish film at that time. The best example of this is The Wedding (1972), which implements modernist strategies of subjectification, self-reflection and stylisation at the artistic and narrative level. The presence of ghosts in this film especially leads to a theoretical discussion on the spectral nature of the medium of film. To synthesize published literature on noncommunicable disease (NCD) behavior change communication (BCC) interventions in sub-Saharan Africa (SSA) among persons living with HIV (PLHIV) and in the general population to inform efforts to adopt similar HIV and NCD BCC intervention activities. We conducted a literature review of NCD BCC interventions and included 20 SSA-based studies. Inclusion criteria entailed describing a BCC intervention targeting any four priority NCDs (cardiovascular disease, type 2 diabetes, cervical cancer, and depression) or both HIV and any of the NCDs. The RE-AIM (Reach, Effectiveness, Adoption, Implementation, and Maintenance) framework was used to assess potential public health impact of these studies. We also solicited expert opinions from 10 key informants on the topic of HIV/NCD health promotion in five SSA countries. The BCC interventions reviewed targeted multiple parts of the HIV and NCD continuum at both individual and community levels. Various strate... The many machine learning and data mining techniques produced over the last decades can prove invaluable assets in diverse fields, but choosing the most appropriate for a given application may be very difficult for a non-expert. Our objective is thus to provide modelling assistance using a meta-learning approach based on an evolutionary metaheuristic. We present the intended workflow of such modelling assistant and the expected challenges along our line of work."}
{"url": "https://en.m.wikipedia.org/wiki/Sparta", "text": "Hollow Lacedaemon. Site of the Menelaion, the ancient shrine to Helen and Menelaus constructed in the Bronze Age city that stood on the hill of Therapne on the left bank of the Eurotas River overlooking the future site of Dorian Sparta. Across the valley the successive ridges of Mount Taygetus are in evidence. Sparta had a double effect on Greek thought: through the reality, and through the myth.... The reality enabled the Spartans to defeat Athens in war; the myth influenced Plato's political theory, and that of countless subsequent writers.... [The] ideals that it favors had a great part in framing the doctrines of Rousseau, Nietzsche, and National Socialism.[5] Names The ancient Greeks used one of three words to refer to the Spartan city-state and its location. First, \"Sparta\" refers primarily to the main cluster of settlements in the valley of the Eurotas River.[6] The second word, \"Lacedaemon\" (Λακεδαίμων),[7] was often used as an adjective and is the name referenced in the works of Homer and the historians Herodotus and Thucydides. The third term, \"Laconice\" (Λακωνική), referred to the immediate area around the town of Sparta, the plateau east of the Taygetos mountains,[8] and sometimes to all the regions under direct Spartan control, including Messenia. The earliest attested term referring to Lacedaemon is the Mycenaean Greek𐀨𐀐𐀅𐀖𐀛𐀍, ra-ke-da-mi-ni-jo, \"Lakedaimonian\", written in Linear B syllabic script,[9][n 1] the equivalent of the later GreekΛακεδαιμόνιος, Lakedaimonios (Latin: Lacedaemonius).[15][16] Herodotus seems to use \"Lacedaemon\" for the Mycenaean Greek citadel at Therapne, in contrast to the lower town of Sparta. This term could be used synonymously with Sparta, but typically it denoted the terrain in which the city was located.[17] In Homer it is typically combined with epithets of the countryside: wide, lovely, shining and most often hollow and broken (full of ravines),[18] suggesting the Eurotas Valley. \"Sparta\" on the other hand is described as \"the country of lovely women\", an epithet for people. The residents of Sparta were often called Lacedaemonians. This epithet utilized the plural of the adjective Lacedaemonius (Greek: Λακεδαιμόνιοι; Latin: Lacedaemonii, but also Lacedaemones). The ancients sometimes used a back-formation, referring to the land of Lacedaemon as Lacedaemonian country. As most words for \"country\" were feminine, the adjective was in the feminine: Lacedaemonia (Λακεδαιμονία, Lakedaimonia). Eventually, the adjective came to be used alone. \"Lacedaemonia\" was not in general use during the classical period and before. It does occur in Greek as an equivalent of Laconia and Messenia during the Roman and early Byzantine periods, mostly in ethnographers and lexica of place names. For example, Hesychius of Alexandria's Lexicon (5th century AD) defines Agiadae as a \"place in Lacedaemonia\" named after Agis.[19] The actual transition may be captured by Isidore of Seville's Etymologiae (7th century AD), an etymological dictionary. Isidore relied heavily on Orosius' Historiarum Adversum Paganos (5th century AD) and Eusebius of Caesarea's Chronicon (early 5th century AD), as did Orosius. The latter defines Sparta to be Lacedaemonia Civitas,[20] but Isidore defines Lacedaemonia as founded by Lacedaemon, son of Semele, which is consistent with Eusebius' explanation.[21] There is a rare use, perhaps the earliest of \"Lacedaemonia\", in Diodorus Siculus' The Library of History,[22] but probably with Χώρα (‘’chōra’’, \"country\") suppressed. Geography Antique Map of Classical City of Sparta (based on ancient sources and not archaeology). Sparta is located in the region of Laconia, in the south-eastern Peloponnese. Ancient Sparta was built on the banks of the Eurotas, the largest river of Laconia, which provided it with a source of fresh water. The Eurotas valley was a natural fortress, bounded to the west by Mt. Taygetus (2,407 m) and to the east by Mt. Parnon (1,935 m). To the north, Laconia is separated from Arcadia by hilly uplands reaching 1000 m in altitude. These natural defenses worked to Sparta's advantage and protected it from sacking and invasion. Though landlocked, Sparta had a vassal harbor, Gytheio, on the Laconian Gulf. Tyrtaeus, an archaic era Spartan writer, is the earliest source to connect the origin myth of the Spartans to the lineage of the hero Heracles; later authors, such as Diodorus Siculus, Herodotus, and Apollodorus, also made mention of Spartans understanding themselves to be descendants of Heracles.[24][25][26][27] Archaeology of the classical period Suppose the city of Sparta to be deserted, and nothing left but the temples and the ground-plan, distant ages would be very unwilling to believe that the power of the Lacedaemonians was at all equal to their fame. Their city is not built continuously, and has no splendid temples or other edifices; it rather resembles a group of villages, like the ancient towns of Hellas, and would therefore make a poor show.[28][29] Until the early 20th century, the chief ancient buildings at Sparta were the theatre, of which, however, little showed above ground except portions of the retaining walls; the so-called Tomb of Leonidas, a quadrangular building, perhaps a temple, constructed of immense blocks of stone and containing two chambers; the foundation of an ancient bridge over the Eurotas; the ruins of a circular structure; some remains of late Roman fortifications; several brick buildings and mosaic pavements.[28] The remaining archaeological wealth consisted of inscriptions, sculptures, and other objects collected in the local museum, founded by Stamatakis in 1872 and enlarged in 1907. Partial excavation of the round building was undertaken in 1892 and 1893 by the American School at Athens. The structure has been since found to be a semicircular retaining wall of Hellenic origin that was partly restored during the Roman period.[28] A \"small circus\" (as described by Leake) proved to be a theatre-like building constructed soon after 200 AD around the altar and in front of the Temple of Artemis Orthia. It is believed that musical and gymnastic contests took place here, as well as the famous flogging ordeal administered to Spartan boys (diamastigosis). The temple, which can be dated to the 2nd century BC, rests on the foundation of an older temple of the 6th century, and close beside it were found the remains of a yet earlier temple, dating from the 9th or even the 10th century. The votive offerings in clay, amber, bronze, ivory and lead dating from the 9th to the 4th centuries BC, which were found in great profusion within the precinct range, supply invaluable information about early Spartan art.[28] Remaining section of wall that surrounded ancient Sparta In 1907, the location of the sanctuary of Athena \"of the Brazen House\" (Χαλκίοικος, Chalkioikos) was determined to be on the acropolis immediately above the theatre. Though the actual temple is almost completely destroyed, the site has produced the longest extant archaic inscription in Laconia, numerous bronze nails and plates, and a considerable number of votive offerings. The city-wall, built in successive stages from the 4th to the 2nd century, was traced for a great part of its circuit, which measured 48 stades or nearly 10 km (6 miles) (Polyb. 1X. 21). The late Roman wall enclosing the acropolis, part of which probably dates from the years following the Gothic raid of 262 AD, was also investigated. Besides the actual buildings discovered, a number of points were situated and mapped in a general study of Spartan topography, based upon the description of Pausanias.[28] In terms of domestic archaeology, little is known about Spartan houses and villages before the Archaic period, but the best evidence comes from excavations at Nichoria in Messenia where postholes have been found. These villages were open and consisted of small and simple houses built with stone foundations and clay walls.[30] Menelaion The Menelaion is a shrine associated with Menelaus, located east of Sparta, by the river Eurotas, on the hill Profitis Ilias (Coordinates: 37°03′57″N22°27′13″E﻿ / ﻿37.0659°N 22.4536°E﻿ / 37.0659; 22.4536). Built around the early 8th century BC, the Spartans believed it had been the former residence of Menelaus. In 1970, the British School in Athens started excavations around the Menelaion in an attempt to locate Mycenaean remains in the area. Among other findings, they uncovered the remains of two Mycenaean mansions and found the first offerings dedicated to Helen and Menelaus. These mansions were destroyed by earthquake and fire, and archaeologists consider them the possible palace of Menelaus himself.[31][better source needed] Excavations made from the early 1990s to the present suggest that the area around the Menelaion in the southern part of the Eurotas valley seems to have been the center of Mycenaean Laconia.[32] The Mycenaean settlement was roughly triangular in shape, with its apex pointed towards the north. Its area was approximately equal to that of the \"newer\" Sparta, but denudation has wreaked havoc with its buildings and nothing is left of its original structures save for ruined foundations and broken potsherds.[28] History Prehistory, \"dark age\" and archaic period The prehistory of Sparta is difficult to reconstruct because the literary evidence was written far later than the events it describes and is distorted by oral tradition.[33] The earliest certain evidence of human settlement in the region of Sparta consists of pottery dating from the Middle Neolithic period, found in the vicinity of Kouphovouno some two kilometres (1.2 miles) south-southwest of Sparta.[34] This civilization seems to have fallen into decline by the late Bronze Age, when, according to Herodotus, Macedonian tribes from the north (called Dorians by those they conquered) marched into the Peloponnese and, subjugating the local tribes, settled there.[33] The Dorians seem to have set about expanding the frontiers of Spartan territory almost before they had established their own state.[35] They fought against the Argive Dorians to the east and southeast, and also the Arcadian Achaeans to the northwest. The evidence suggests that Sparta, relatively inaccessible because of the topography of the Taygetan plain, was secure from early on: it was never fortified.[35] Nothing distinctive in the archaeology of the Eurotas River Valley identifies the Dorians or the Dorian Spartan state. The prehistory of the Neolithic, the Bronze Age and the Dark Age (the Early Iron Age) at this moment must be treated apart from the stream of Dorian Spartan history.[citation needed] The legendary period of Spartan history is believed to fall into the Dark Age. It treats the mythic heroes such as the Heraclids and the Perseids, offering a view of the occupation of the Peloponnesus that contains both fantastic and possibly historical elements. The subsequent proto-historic period, combining both legend and historical fragments, offers the first credible history. Between the 8th and 7th centuries BC the Spartans experienced a period of lawlessness and civil strife, later attested by both Herodotus and Thucydides.[36] As a result, they carried out a series of political and social reforms of their own society which they later attributed to a semi-mythical lawgiver, Lycurgus.[37] Several writers throughout antiquity, including Herodotus, Xenophon, and Plutarch have attempted to explain Spartan exceptionalism as a result of the so-called Lycurgan Reforms.Xenophon, Constitution of the Lacedaimonians, chapter 1[38][39][40] Classical Sparta In the Second Messenian War, Sparta established itself as a local power in the Peloponnesus and the rest of Greece. During the following centuries, Sparta's reputation as a land-fighting force was unequalled.[41] At its peak around 500 BC, Sparta had some 20,000–35,000 citizens, plus numerous helots and perioikoi. The likely total of 40,000–50,000 made Sparta one of the larger Greek city-states;[42][43] however, according to Thucydides, the population of Athens in 431 BC was 360,000–610,000, making it much larger.[n 2] In 480 BC, a small force led by King Leonidas (about 300 full Spartiates, 700 Thespians, and 400 Thebans, although these numbers were lessened by earlier casualties[45]) made a legendary last stand at the Battle of Thermopylae against the massive Persian army, led by Xerxes.[46] The Spartans received advance warning of the Persian invasion from their deposed king Demaratus, which prompted them to consult the Delphic oracle. According to Herodotus, the Pythia proclaimed that either one of the kings of Sparta had to die or Sparta would be destroyed.[47] This prophecy was fulfilled after king Leonidas died in the battle. The superior weaponry, strategy, and bronze armour of the Greek hoplites and their phalanx fighting formation again proved their worth one year later when Sparta assembled its full strength and led a Greek alliance against the Persians at the Battle of Plataea in 479 BC. Ancient Sparta. The decisive Greek victory at Plataea put an end to the Greco-Persian War along with Persian ambitions to expand into Europe. Even though this war was won by a pan-Greek army, credit was given to Sparta, who besides providing the leading forces at Thermopylae and Plataea, had been the de facto leader of the entire Greek expedition.[48] In 464 BC, a violent earthquake occurred along the Sparta faultline destroying much of what was Sparta and many other city-states in ancient Greece. This earthquake is marked by scholars as one of the key events that led to the First Peloponnesian War. In later Classical times, Sparta along with Athens, Thebes, and Persia were the main powers fighting for supremacy in the northeastern Mediterranean. In the course of the Peloponnesian War, Sparta, a traditional land power, acquired a navy which managed to overpower the previously dominant flotilla of Athens, ending the Athenian Empire. At the peak of its power in the early 4th century BC, Sparta had subdued many of the main Greek states and even invaded the Persian provinces in Anatolia (modern day Turkey), a period known as the Spartan hegemony. During the Corinthian War, Sparta faced a coalition of the leading Greek states: Thebes, Athens, Corinth, and Argos. The alliance was initially backed by Persia, which feared further Spartan expansion into Asia.[49] Sparta achieved a series of land victories, but many of her ships were destroyed at the Battle of Cnidus by a Greek-Phoenician mercenary fleet that Persia had provided to Athens. The event severely damaged Sparta's naval power but did not end its aspirations of invading further into Persia, until Conon the Athenian ravaged the Spartan coastline and provoked the old Spartan fear of a helot revolt.[50] After a few more years of fighting, in 387 BC the Peace of Antalcidas was established, according to which all Greek cities of Ionia would return to Persian control, and Persia's Asian border would be free of the Spartan threat.[50] The effects of the war were to reaffirm Persia's ability to interfere successfully in Greek politics and to affirm Sparta's weakened hegemonic position in the Greek political system.[51] Sparta entered its long-term decline after a severe military defeat to Epaminondas of Thebes at the Battle of Leuctra. This was the first time that a full strength Spartan army lost a land battle. As Spartan citizenship was inherited by blood, Sparta increasingly faced a helot population that vastly outnumbered its citizens. The alarming decline of Spartan citizens was commented on by Aristotle. Hellenistic and Roman Sparta Sparta never fully recovered from its losses at Leuctra in 371 BC and the subsequent helot revolts. In 338, Philip II invaded and devastated much of Laconia, turning the Spartans out, though he did not seize Sparta itself.[52] Even during its decline, Sparta never forgot its claim to be the \"defender of Hellenism\" and its Laconic wit. An anecdote has it that when Philip II sent a message to Sparta saying \"If I invade Laconia, I shall turn you out.\"[53], the Spartans responded with the single, terse reply: αἴκα, \"if\".[54][55][56] When Philip created the League of Corinth on the pretext of unifying Greece against Persia, the Spartans chose not to join, since they had no interest in joining a pan-Greek expedition unless it were under Spartan leadership. Thus, upon defeating the Persians at the Battle of the Granicus, Alexander the Great sent to Athens 300 suits of Persian armour with the following inscription: \"Alexander, son of Philip, and all the Greeks except the Spartans, give these offerings taken from the foreigners who live in Asia\". Sparta continued to be one of the Peloponesian powers until its eventual loss of independence in 192 BC. During Alexander's campaigns in the east, the Spartan king Agis III sent a force to Crete in 333 BC to secure the island for the Persian interest.[57][58] Agis next took command of allied Greek forces against Macedon, gaining early successes, before laying siege to Megalopolis in 331 BC. A large Macedonian army under general Antipater marched to its relief and defeated the Spartan-led force in a pitched battle.[59] More than 5,300 of the Spartans and their allies were killed in battle, and 3,500 of Antipater's troops.[60] Agis, now wounded and unable to stand, ordered his men to leave him behind to face the advancing Macedonian army so that he could buy them time to retreat. On his knees, the Spartan king slew several enemy soldiers before being finally killed by a javelin.[61] Alexander was merciful, and he only forced the Spartans to join the League of Corinth, which they had previously refused.[62] During the Punic Wars, Sparta was an ally of the Roman Republic. Spartan political independence was put to an end when it was eventually forced into the Achaean League after its defeat in the decisive Laconian War by a coalition of other Greek city-states and Rome, and the resultant overthrow of its final king Nabis, in 192 BC. Sparta played no active part in the Achaean War in 146 BC when the Achaean League was defeated by the Roman general Lucius Mummius. Subsequently, Sparta became a free city under Roman rule, some of the institutions of Lycurgus were restored,[63] and the city became a tourist attraction for the Roman elite who came to observe exotic Spartan customs.[n 3] Areus king of the Lacedemonians to Onias the high priest, greeting: It is found in writing, that the Lacedemonians and Jews are brethren, and that they are of the stock of Abraham: Now therefore, since this is come to our knowledge, ye shall do well to write unto us of your prosperity. We do write back again to you, that your cattle and goods are ours, and ours are yours. The letters are reproduced in a variant form by Josephus.[69] Jewish historian Uriel Rappaport notes that the relationship between the Jews and the Spartans expressed in this correspondence has \"intrigued many scholars, and various explanations have been suggested for the problems raised ... including the historicity of the Jewish leader and high priestJonathan's letter to the Spartans, the authenticity of the letter of Arius to Onias, cited in Jonathan's letter, and the supposed 'brotherhood' of the Jews and the Spartans.\" Rappaport is clear that \"the authenticity of [the reply] letter of Arius is based on even less firm foundations than the letter of Jonathan\".[70] Spartans long spurned the idea of building a defensive wall around their city, believing they made the city's men soft in terms of their warrior abilities. A wall was finally erected after 184 BCE, after the peak of the city-state's power had come and gone.[71] The duties of the kings were primarily religious, judicial, and military. As chief priests of the state, they maintained communication with the Delphian sanctuary, whose pronouncements exercised great authority in Spartan politics. In the time of Herodotus c. 450 BC, their judicial functions had been restricted to cases dealing with heiresses (epikleroi), adoptions and the public roads (the meaning of the last term is unclear in Herodotus' text and has been interpreted in a number of ways). Aristotle describes the kingship at Sparta as \"a kind of unlimited and perpetual generalship\" (Pol. iii. 1285a), while Isocrates refers to the Spartans as \"subject to an oligarchy at home, to a kingship on campaign\" (iii. 24).[28] Civil and criminal cases were decided by a group of officials known as the ephors, as well as a council of elders known as the Gerousia. The Gerousia consisted of 28 elders over the age of 60, elected for life and usually part of the royal households, and the two kings.[76] High state decisions were discussed by this council, who could then propose policies to the damos, the collective body of Spartan citizenry, who would select one of the alternatives by vote.[77][78] Royal prerogatives were curtailed over time. From the period of the Persian wars, the king lost the right to declare war and was accompanied in the field by two ephors. He was supplanted by the ephors also in the control of foreign policy. Over time, the kings became mere figureheads except in their capacity as generals. Political power was transferred to the ephors and Gerousia.[28] An assembly of citizens called the Ekklesia was responsible for electing men to the Gerousia for life. Citizenship The Spartan education process known as the agoge was essential for full citizenship. However, usually the only boys eligible for the agoge were Spartiates, those who could trace their ancestry to the original inhabitants of the city. There were two exceptions. trophimoi or \"foster sons\" were foreign students invited to study. The Athenian general Xenophon, for example, sent his two sons to Sparta as trophimoi. Also, the son of a helot could be enrolled as a syntrophos[79] if a Spartiate formally adopted him and paid his way; if he did exceptionally well in training, he might be sponsored to become a Spartiate.[80] Spartans who could not afford to pay the expenses of the agoge could lose their citizenship. These laws meant that Sparta could not readily replace citizens lost in battle or otherwise, which eventually proved near fatal as citizens became greatly outnumbered by non-citizens, and even more dangerously by helots. Non citizens The other classes were the perioikoi, free inhabitants who were non-citizens, and the helots,[81] state-owned serfs. Descendants of non-Spartan citizens were forbidden the agoge. Helots The Spartans were a minority of the Lakonian population. The largest class of inhabitants were the helots (in Classical GreekΕἵλωτες / Heílôtes).[82][83] The helots were originally free Greeks from the areas of Messenia and Lakonia whom the Spartans had defeated in battle and subsequently enslaved.[84] In contrast to populations conquered by other Greek cities (e.g. the Athenian treatment of Melos), the male population was not exterminated and the women and children turned into chattel slaves. Instead, the helots were given a subordinate position in society more comparable to serfs in medieval Europe than chattel slaves in the rest of Greece.[citation needed] The Spartan helots were not only agricultural workers, but were also household servants, both male and female would be assigned domestic duties, such as wool-working.[85] However, the helots were not the private property of individual Spartan citizens, regardless of their household duties, and were instead owned by the state through the kleros system.[86] Helots did not have voting or political rights. The Spartan poet Tyrtaios refers to Helots being allowed to marry and retaining 50% of the fruits of their labor.[87] They also seem to have been allowed to practice religious rites and, according to Thucydides, own a limited amount of personal property.[88] Initially, helots couldn't be freed but during the middle Hellenistic period, some 6,000 helots accumulated enough wealth to buy their freedom, for example, in 227 BC. In other Greek city-states, free citizens were part-time soldiers who, when not at war, carried on other trades. Since Spartan men were full-time soldiers, they were not available to carry out manual labour.[89] The helots were used as unskilled serfs, tilling Spartan land. Helot women were often used as wet nurses. Helots also travelled with the Spartan army as non-combatant serfs. At the last stand of the Battle of Thermopylae, the Greek dead included not just the legendary three hundred Spartan soldiers but also several hundred Thespian and Theban troops and a number of helots.[90] There was at least one helot revolt (c. 465–460 BC) that led to prolonged conflict. By the tenth year of this war the Spartans and Messenians had reached an agreement in which Messenian rebels were allowed to leave the Peloponnese.[91] They were given safe passage under the terms that they would be re-enslaved if they tried to return. This agreement ended the most serious incursion into Spartan territory since their expansion in the seventh and eighth centuries BC.[92] Thucydides remarked that \"Spartan policy is always mainly governed by the necessity of taking precautions against the helots.\"[93][94] On the other hand, the Spartans trusted their helots enough in 479 BC to take a force of 35,000 with them to Plataea, something they could not have risked if they feared the helots would attack them or run away. Slave revolts occurred elsewhere in the Greek world, and in 413 BC 20,000 Athenian slaves ran away to join the Spartan forces occupying Attica.[95] What made Sparta's relations with her slave population unique was that the helots, precisely because they enjoyed privileges such as family and property, retained their identity as a conquered people (the Messenians) and also had effective kinship groups that could be used to organize rebellion.[citation needed] As the Spartiate population declined and the helot population continued to grow, the imbalance of power caused increasing tension. According to Myron of Priene[96] of the middle 3rd century BC: They assign to the Helots every shameful task leading to disgrace. For they ordained that each one of them must wear a dogskin cap (κυνῆ / kunễ) and wrap himself in skins (διφθέρα / diphthéra) and receive a stipulated number of beatings every year regardless of any wrongdoing, so that they would never forget they were slaves. Moreover, if any exceeded the vigour proper to a slave's condition, they made death the penalty; and they allotted a punishment to those controlling them if they failed to rebuke those who were growing fat.[97] Plutarch also states that Spartans treated the helots \"harshly and cruelly\": they compelled them to drink pure wine (which was considered dangerous – wine usually being cut with water) \"...and to lead them in that condition into their public halls, that the children might see what a sight a drunken man is; they made them to dance low dances, and sing ridiculous songs...\" during syssitia (obligatory banquets).[98] Each year when the Ephors took office, they ritually declared war on the helots, allowing Spartans to kill them without risk of ritual pollution.[99] This fight seems to have been carried out by kryptai (sing. κρύπτης kryptēs), graduates of the agoge who took part in the mysterious institution known as the Krypteia.[100] Thucydides states: The helots were invited by a proclamation to pick out those of their number who claimed to have most distinguished themselves against the enemy, in order that they might receive their freedom; the object being to test them, as it was thought that the first to claim their freedom would be the most high spirited and the most apt to rebel. As many as two thousand were selected accordingly, who crowned themselves and went round the temples, rejoicing in their new freedom. The Spartans, however, soon afterwards did away with them, and no one ever knew how each of them perished.[101][102] Perioikoi The Perioikoi came from similar origins as the helots but occupied a significantly different position in Spartan society. Although they did not enjoy full citizen-rights, they were free and not subjected to the same restrictions as the helots. The exact nature of their subjection to the Spartans is not clear, but they seem to have served partly as a kind of military reserve, partly as skilled craftsmen and partly as agents of foreign trade.[103] Perioikoic hoplites served increasingly with the Spartan army, explicitly at the Battle of Plataea, and although they may also have fulfilled functions such as the manufacture and repair of armour and weapons,[104] they were increasingly integrated into the combat units of the Spartan army as the Spartiate population declined.[105] Economy Full citizen Spartiates were barred by law from trade or manufacture, which consequently rested in the hands of the Perioikoi.[28] This lucrative monopoly, in a fertile territory with a good harbors, ensured the loyalty of the perioikoi.[106] Despite the prohibition on menial labor or trade, there is evidence of Spartan sculptors,[107] and Spartans were certainly poets, magistrates, ambassadors, and governors as well as soldiers. Allegedly, Spartans were prohibited from possessing gold and silver coins, and according to legend Spartan currency consisted of iron bars to discourage hoarding.[108][109] It was not until the 260s or 250s BC that Sparta began to mint its own coins.[110] Though the conspicuous display of wealth appears to have been discouraged, this did not preclude the production of very fine decorated bronze, ivory and wooden works of art as well as exquisite jewellery, attested in archaeology.[111] Allegedly as part of the Lycurgan Reforms in the mid-8th century BC, a massive land reform had divided property into 9,000 equal portions. Each citizen received one estate, a kleros, which was expected to provide his living.[112] The land was worked by helots who retained half the yield. From the other half, the Spartiate was expected to pay his mess (syssitia) fees, and the agoge fees for his children. However, nothing is known of matters of wealth such as how land was bought, sold, and inherited, or whether daughters received dowries.[113] However, from early on there were marked differences of wealth within the state, and these became more serious after the law of Epitadeus some time after the Peloponnesian War, which removed the legal prohibition on the gift or bequest of land.[28][114] By the mid-5th century, land had become concentrated in the hands of a tiny elite, and the notion that all Spartan citizens were equals had become an empty pretence. By Aristotle's day (384–322 BC) citizenship had been reduced from 9,000 to less than 1,000, then further decreased to 700 at the accession of Agis IV in 244 BC. Attempts were made to remedy this by imposing legal penalties upon bachelors,[28] but this could not reverse the trend. Life in Classical Sparta Birth and death Sparta was above all a militarist state, and emphasis on military fitness began virtually at birth. According to Plutarch after birth, a mother would bathe her child in wine to see whether the child was strong. If the child survived it was brought before the Gerousia by the child's father. The Gerousia then decided whether it was to be reared or not.[28] It is commonly stated that if they considered it \"puny and deformed\", the baby was thrown into a chasm on Mount Taygetos known euphemistically as the Apothetae (Gr., ἀποθέται, \"Deposits\").[115][116] This was, in effect, a primitive form of eugenics.[115] Plutarch is the sole historical source for the Spartan practice of systemic infanticide motivated by eugenics.[117] Sparta is often viewed as being unique in this regard, however, anthropologist Laila Williamson notes: \"Infanticide has been practiced on every continent and by people on every level of cultural complexity, from hunter gatherers to high civilizations. Rather than being an exception, then, it has been the rule.\"[118] There is controversy about the matter in Sparta, since excavations in the chasm only uncovered adult remains, likely belonging to criminals[119] and Greek sources contemporary to Sparta does not mention systemic infanticide motivated solely by eugenics.[120] Never do his [the war-dead's] name and good fame perish, But even though he is beneath the earth he is immortal, Young and old alike mourn him, All the city is distressed by the painful loss, and his tomb and children are pointed out among the people, and his children’s children and his line after them. When Spartans died, marked headstones would only be granted to soldiers who died in combat during a victorious campaign or women who died either in service of a divine office or in childbirth.[122] These headstones likely acted as memorials, rather than as grave markers. Evidence of Spartan burials is provided by the Tomb of the Lacedaimonians in Athens.[citation needed] Excavations at the cemetery of classical Sparta, uncovered ritually pierced kantharoid-like ceramic vessels, the ritual slaughter of horses, and specific burial enclosures alongside individual 'plots'. Some of the graves were reused over time.[123][124] In the Hellenistic Period, grander, two-storey monumental tombs are found at Sparta. Ten of these have been found for this period.[124] Education When male Spartans began military training at age seven, they would enter the agoge system. The agoge was designed to encourage discipline and physical toughness and to emphasize the importance of the Spartan state. Boys lived in communal messes and, according to Xenophon, whose sons attended the agoge, the boys were fed \"just the right amount for them never to become sluggish through being too full, while also giving them a taste of what it is not to have enough.\"[125] In addition, they were trained to survive in times of privation, even if it meant stealing.[126] Besides physical and weapons training, boys studied reading, writing, music and dancing. Special punishments were imposed if boys failed to answer questions sufficiently \"laconically\" (i.e. briefly and wittily).[127] Spartan boys were expected to take an older male mentor, usually an unmarried young man. According to some sources, the older man was expected to function as a kind of substitute father and role model to his junior partner; however, others believe it was reasonably certain that they had sexual relations (the exact nature of Spartan pederasty is not entirely clear). Xenophon, an admirer of the Spartan educational system whose sons attended the agoge, explicitly denies the sexual nature of the relationship.[128][125] Some Spartan youth apparently became members of an irregular unit known as the Krypteia. The immediate objective of this unit was to seek out and kill vulnerable helot Laconians as part of the larger program of terrorising and intimidating the helot population.[129] Less information is available about the education of Spartan girls, but they seem to have gone through a fairly extensive formal educational cycle, broadly similar to that of the boys but with less emphasis on military training. Spartan girls received an education known as mousikē. This included music, dancing, singing and poetry. Choral dancing was taught so Spartan girls could participate in ritual activities, including the cults of Helen and Artemis.[130] In this respect, classical Sparta was unique in ancient Greece. In no other city-state did women receive any kind of formal education.[131] Military life At age 20, the Spartan citizen began his membership in one of the syssitia (dining messes or clubs), composed of about fifteen members each, of which every citizen was required to be a member.[28] Here each group learned how to bond and rely on one another. The Spartans were not eligible for election for public office until the age of 30. Only native Spartans were considered full citizens and were obliged to undergo the training as prescribed by law, as well as participate in and contribute financially to one of the syssitia.[132] Sparta is thought to be the first city to practice athletic nudity, and some scholars claim that it was also the first to formalize pederasty.[133] According to these sources, the Spartans believed that the love of an older, accomplished aristocrat for an adolescent was essential to his formation as a free citizen. The agoge, the education of the ruling class, was, they claim, founded on pederastic relationships required of each citizen,[134] with the lover responsible for the boy's training. However, other scholars question this interpretation. Xenophon explicitly denies it,[125] but not Plutarch.[135] Spartan men remained in the active reserve until age 60. Men were encouraged to marry at age 20 but could not live with their families until they left their active military service at age 30. They called themselves \"homoioi\" (equals), pointing to their common lifestyle and the discipline of the phalanx, which demanded that no soldier be superior to his comrades.[136] Insofar as hoplite warfare could be perfected, the Spartans did so.[137] Thucydides reports that when a Spartan man went to war, his wife (or another woman of some significance) would customarily present him with his aspis (shield) and say: \"With this, or upon this\" (Ἢ τὰν ἢ ἐπὶ τᾶς, Èi tàn èi èpì tàs), meaning that true Spartans could only return to Sparta either victorious (with their shield in hand) or dead (carried upon it).[138] This is almost certainly propaganda. Spartans buried their battle dead on or near the battle field; corpses were not brought back on their shield.[139] Nevertheless, it is fair to say that it was less of a disgrace for a soldier to lose his helmet, breastplate or greaves than his shield, since the former were designed to protect one man, whereas the shield also protected the man on his left. Thus, the shield was symbolic of the individual soldier's subordination to his unit, his integral part in its success, and his solemn responsibility to his comrades in arms – messmates and friends, often close blood relations. According to Aristotle, the Spartan military culture was actually short-sighted and ineffective. He observed: It is the standards of civilized men not of beasts that must be kept in mind, for it is good men not beasts who are capable of real courage. Those like the Spartans who concentrate on the one and ignore the other in their education turn men into machines and in devoting themselves to one single aspect of city's life, end up making them inferior even in that.[140] One of the most persistent myths about Sparta that has no basis in fact is the notion that Spartan mothers were without feelings toward their off-spring and helped enforce a militaristic lifestyle on their sons and husbands.[141][142] The myth can be traced back to Plutarch, who includes no less than 17 \"sayings\" of \"Spartan women\", all of which paraphrase or elaborate on the theme that Spartan mothers rejected their own offspring if they showed any kind of cowardice. In some of these sayings, mothers revile their sons in insulting language merely for surviving a battle. These sayings purporting to be from Spartan women were far more likely to be of Athenian origin and designed to portray Spartan women as unnatural and so undeserving of pity.[139] Agriculture, food, and diet Sparta's agriculture consisted mainly of barley, wine, cheese, grain, and figs. These items were grown locally on each Spartan citizen's kleros and were tended to by helots. Spartan citizens were required to donate a certain amount of what they yielded from their kleros to their syssitia, or mess. These donations to the syssitia were a requirement for every Spartan citizen. All the donated food was then redistributed to feed the Spartan population of that syssitia.[143] The helots who tended to the lands were fed using a portion of what they harvested.[144] Marriage The custom was to capture women for marriage... The so-called 'bridesmaid' took charge of the captured girl. She first shaved her head to the scalp, then dressed her in a man's cloak and sandals, and laid her down alone on a mattress in the dark. The bridegroom – who was not drunk and thus not impotent, but was sober as always – first had dinner in the messes, then would slip in, undo her belt, lift her and carry her to the bed.[145] The husband continued to visit his wife in secret for some time after the marriage. These customs, unique to the Spartans, have been interpreted in various ways. One of them decidedly supports the need to disguise the bride as a man in order to help the bridegroom consummate the marriage, so unaccustomed were men to women's looks at the time of their first intercourse. The \"abduction\" may have served to ward off the evil eye, and the cutting of the wife's hair was perhaps part of a rite of passage that signaled her entrance into a new life.[146] Role of women Political, social, and economic equality Spartan women, of the citizenry class, enjoyed a status, power, and respect that was unknown in the rest of the classical world. The higher status of females in Spartan society started at birth; unlike Athens, Spartan girls were fed the same food as their brothers.[147] Nor were they confined to their father's house and prevented from exercising or getting fresh air as in Athens, but exercised and even competed in sports.[147] Most important, rather than being married off at the age of 12 or 13, Spartan law forbade the marriage of a girl until she was in her late teens or early 20s. The reasons for delaying marriage were to ensure the birth of healthy children, but the effect was to spare Spartan women the hazards and lasting health damage associated with pregnancy among adolescents. Spartan women, better fed from childhood and fit from exercise, stood a far better chance of reaching old age than their sisters in other Greek cities, where the median age for death was 34.6 years or roughly 10 years below that of men.[148] Unlike Athenian women who wore heavy, concealing clothes and were rarely seen outside the house, Spartan women wore dresses (peplos) slit up the side to allow freer movement and moved freely about the city, either walking or driving chariots. Girls as well as boys exercised, possibly in the nude, and young women as well as young men may have participated in the Gymnopaedia (\"Festival of Nude Youths\").[149][150] Another practice that was mentioned by many visitors to Sparta was the practice of \"wife-sharing\". In accordance with the Spartan belief that breeding should be between the most physically fit parents, many older men allowed younger, more fit men, to impregnate their wives. Other unmarried or childless men might even request another man's wife to bear his children if she had previously been a strong child bearer.[151] For this reason many considered Spartan women polygamous or polyandrous.[152] This practice was encouraged in order that women bear as many strong-bodied children as they could. The Spartan population was hard to maintain due to the constant absence and loss of the men in battle and the intense physical inspection of newborns.[153] Spartan women were also literate and numerate, a rarity in the ancient world. Furthermore, as a result of their education and the fact that they moved freely in society engaging with their fellow (male) citizens, they were notorious for speaking their minds even in public.[154] Plato, in the middle of the fourth century, described women's curriculum in Sparta as consisting of gymnastics and mousike (music and arts). Plato praised Spartan women's ability when it came to philosophical discussion.[155] Most importantly, Spartan women had economic power because they controlled their own properties, and those of their husbands. It is estimated that in later Classical Sparta, when the male population was in serious decline, women were the sole owners of at least 35% of all land and property in Sparta.[156] The laws regarding a divorce were the same for both men and women. Unlike women in Athens, if a Spartan woman became the heiress of her father because she had no living brothers to inherit (an epikleros), the woman was not required to divorce her current spouse in order to marry her nearest paternal relative.[156] Historic women Many women played a significant role in the history of Sparta.[157]Queen Gorgo, heiress to the throne and the wife of Leonidas I, was an influential and well-documented figure. Herodotus records that as a small girl she advised her father Cleomenes to resist a bribe. She was later said to be responsible for decoding a warning that the Persian forces were about to invade Greece; after Spartan generals could not decode a wooden tablet covered in wax, she ordered them to clear the wax, revealing the warning.[158] Plutarch's Moralia contains a collection of \"Sayings of Spartan Women\", including a laconic quip attributed to Gorgo: when asked by a woman from Attica why Spartan women were the only women in the world who could rule men, she replied \"Because we are the only women who are mothers of men\".[159] In 396, Cynisca, sister of the Eurypontid king Agesilaos II, became the first woman in Greece to win an Olympic chariot race. She won again in 392, and dedicated two monuments to commemorate her victory, these being an inscription in Sparta and a set of bronze equestrian statues at the Olympic temple of Zeus.[160][161] Laconophilia Laconophilia is love or admiration of Sparta and its culture or constitution. Sparta was subject of considerable admiration in its day, even in rival Athens. In ancient times \"Many of the noblest and best of the Athenians always considered the Spartan state nearly as an ideal theory realised in practice.\"[162] Many Greek philosophers, especially Platonists, would often describe Sparta as an ideal state, strong, brave, and free from the corruptions of commerce and money. The French classicist François Ollier in his 1933 book Le mirage spartiate (The Spartan Mirage) warned that a major scholarly problem is that all surviving accounts of Sparta were by non-Spartans who often excessively idealized their subject.[163] No accounts survive by the Spartans themselves, if such were ever written. With the revival of classical learning in Renaissance Europe, Laconophilia re-appeared, for example in the writings of Machiavelli. The Elizabethan English constitutionalist John Aylmer compared the mixed government of Tudor England to the Spartan republic, stating that \"Lacedemonia [was] the noblest and best city governed that ever was\". He commended it as a model for England. The philosopher Jean-Jacques Rousseau contrasted Sparta favourably with Athens in his Discourse on the Arts and Sciences, arguing that its austere constitution was preferable to the more sophisticated Athenian life. Sparta was also used as a model of austere purity by Revolutionary and Napoleonic France.[164] A German racist strain of Laconophilia was initiated by Karl Otfried Müller, who linked Spartan ideals to the supposed racial superiority of the Dorians, the ethnic sub-group of the Greeks to which the Spartans belonged. In the 20th century, this developed into Fascist admiration of Spartan ideals. Adolf Hitler praised the Spartans, recommending in 1928 that Germany should imitate them by limiting \"the number allowed to live\". He added that \"The Spartans were once capable of such a wise measure... The subjugation of 350,000 Helots by 6,000 Spartans was only possible because of the racial superiority of the Spartans.\" The Spartans had created \"the first racialist state\".[165] Following the invasion of theUSSR, Hitler viewed citizens of the USSR as like the helots under the Spartans: \"They [the Spartans] came as conquerors, and they took everything\", and so should the Germans. A Nazi officer specified that \"the Germans would have to assume the position of the Spartiates, while... the Russians were the Helots.\"[165] Certain early Zionists, and particularly the founders of Kibbutz movement in Israel, were influenced by Spartan ideals, particularly in education. Tabenkin, a founding father of the Kibbutz movement and the Palmach strikeforce, prescribed that education for warfare \"should begin from the nursery\", that children should from kindergarten be taken to \"spend nights in the mountains and valleys\".[166][167] In modern times, the adjective \"Spartan\" means simple, frugal, avoiding luxury and comfort.[168] The term \"laconic phrase\" describes the very terse and direct speech characteristic of the Spartans. See also Notes ^Found on the following tablets: TH Fq 229, TH Fq 258, TH Fq 275, TH Fq 253, TH Fq 284, TH Fq 325, TH Fq 339, TH Fq 382.[10] There are also words like 𐀨𐀐𐀅𐀖𐀛𐀍𐀄𐀍, ra-ke-da-mo-ni-jo-u-jo – found on the TH Gp 227 tablet[10] – that could perhaps mean \"son of the Spartan\".[11][12] Moreover, the attested words 𐀨𐀐𐀅𐀜 , ra-ke-da-no and 𐀨𐀐𐀅𐀜𐀩, ra-ke-da-no-re could possibly be Linear B forms of Lacedaemon itself; the latter, found on the MY Ge 604 tablet, is considered to be the dative case form of the former which is found on the MY Ge 603 tablet. It is considered much more probable though that ra-ke-da-no and ra-ke-da-no-re correspond to the anthroponymΛακεδάνωρ, Lakedanor, though the latter is thought to be related etymologically to Lacedaemon.[10][13][14] ^According to Thucydides, the Athenian citizens at the beginning of the Peloponnesian War (5th century BC) numbered 40,000, making a total of 140,000 people when including their families. The metics, i.e. those who did not have citizen rights and paid for the right to reside in Athens, numbered a further 70,000, whilst slaves were estimated at between 150,000 to 400,000.[44] ^Especially the Diamastigosis at the Sanctuary of Artemis Orthia, Limnai outside Sparta. There an amphitheatre was built in the 3rd century AD to observe the ritual whipping of Spartan youths.[64][65] Visiting Romans came to see Sparta as having degraded to a disgusting cult of fetish brutality.[66][67] ^Cartledge 2002, p. 273 \"Philip laid Lakonia waste as far south as Gytheion and formally deprived Sparta of Dentheliatis (and apparently the territory on the Messenian Gulf as far as the Little Pamisos river), Belminatis, the territory of Karyai and the east Parnon foreland.\" ^Plutarch, Life of Lycurgus 27.2–3. However this may be conflating later practice with that of the classical period. See Not the Classical Ideal: Athens and the Construction of the Other in Greek Art ed. Beth Cohen, p. 263, note 33, 2000, Brill. ^ abChristesen, P. (2018). The typology and topography of Spartan burials from the Protogeometric to the Hellenistic period: rethinking Spartan exceptionalism and the ostensible cessation of adult intramural burials in the Greek world. Annual of the British School at Athens, 113, 307-363."}
{"url": "https://en.m.wikipedia.org/wiki/BookFinder.com", "text": "BookFinder.com BookFinder.com is a vertical searchwebsite that helps readers buy books online. The site's meta-search engine scans the inventories of over 100,000 booksellers located around the world. Among the books from sellers whose inventories are indexed, users can find the lowest price for a book of their choice from over 150 million volumes available for sale, and purchase titles directly from the bookseller, without a markup.[2][3] The search engine is focused primarily on Dutch, English, French, German, Italian, and Spanish language titles. BookFinder.com was founded in 1997 by Anirvan Chatterjee, then a student at the University of California, Berkeley; it was one of the earliest vertical search engines for books online.[4] Originally known as MX BookFinder,[4] it was relaunched as BookFinder.com in 1998 and established as a standalone company based in Berkeley, California in 1999."}
{"url": "https://library.deakin.edu.au:443/search~S1?/i9781861975805&Search=Search/i9781861975805/1%2C1%2C2%2CE/frameset&FF=i9781861975805&2%2C%2C2", "text": "Copies Following an introduction entitled 'Joy of Economics', which explains what economics is about, its strengths and shortcomings and the challenges facing economists today, the bulk of the book is an expansive A-Z with several hundred entries that explain the essentials of economics"}
{"url": "https://web.archive.org/web/20211030173802/https://jonestown.sdsu.edu/?page_id=51", "text": "What’s New? This page lists articles, tapes, documents and resources which have been added to this website within the past 15 months. Previous pages with substantive revisions are listed in italics. (Note: This does not include entries from the jonestown report, an annual publication featuring articles, remembrances, personal reflections, and artistic interpretations of Peoples Temple.)"}
{"url": "https://www.deakin.edu.au/library", "text": "Help and contact us Connect with us We acknowledge the Traditional Custodians of the unceded lands and waterways on which Deakin University does business. We pay our deep respect to the Ancestors and Elders of Wadawurrung Country, Eastern Maar Country and Wurundjeri Country where our physical campuses are located. We also acknowledge all First Nations Peoples that make contributions to our learning communities."}
{"url": "https://web.archive.org/web/20120307110815/http://www.odi.org.uk/resources/details.asp?id=1344&title=international-democracy-assistance-lessons-learned-can-donors-better-support-democratic-processes", "text": "The Internet Archive discovers and captures web pages through many different web crawls. At any given time several distinct crawls are running, some for months, and some every day or longer. View the web archive through the Wayback Machine. Assessing international democracy assistance and lessons learned: how can donors better support democratic processes? Prepared for the Wilton Park Conference on Democracy and Development, 23-25 October 2007. This Background Note aims to provide a broad overview of international democracy assistance, understood as conscious, practical international efforts to encourage, support or influence democratic change and political reform in other countries."}
{"url": "https://es.m.wikipedia.org/wiki/Rep%C3%BAblica_de_las_Dos_Naciones", "text": "↑The death of Sigismund II Augustus in 1572 was followed by a three-year Interregnum during which adjustments were made in the constitutional system. The lower nobility was now included in the selection process, and the power of the monarch was further circumscribed in favor of the expanded noble class. From that point, the king was effectively a partner with the noble class and constantly supervised by a group of senators. «The Elective Monarchy». Poland – The Historical Setting. Federal Research Division of the Library of Congress. 1992. Archivado desde el original el 4 de junio de 2011. Consultado el 15 de julio de 2011. ↑Dyer, Thomas Henry (1861). The History of Modern Europe. From the Fall of Constantinople, in 1453, to the War in the Crimea, in 1857. Volume 2. London: J. Murray. p. 504. Consultado el 20 de febrero de 2021."}
{"url": "https://es.m.wikipedia.org/wiki/Anexo:Definiciones_de_la_democracia", "text": "↑Platón (1892). http://oll.libertyfund.org/?option=com_staticxt&staticfile=show.php%3Ftitle=768&chapter=93842&layout=html&Itemid=27|urlcapítulo= sin título (ayuda). Statesman. Plato, Dialogues, Parmenides, Theaetetus, Sophist, Statesman, Philebus4. Oxford University Press. «There are three chief forms of government; monarchy, the rule of the few, and democracy; these expand into five by the division of monarchy into royalty and tyranny, and of the government of the few into aristocracy and oligarchy (291).(...) Str.: Is not the third form of government the rule of the multitude, which is called by the name of democracy? Y. Soc.: Certainly (291).(...) Democracy is the best of lawless and the worst of lawful governments (303).» ↑La famosa frase de Churchill sobre democracia suele ser difundida con diferentes variantes. La frase textual es: «Democracy is the worst form of government, except for all those other forms that have been tried from time to time». Corresponde a un discurso en la Casa de los Comunes pronunciado el 11 de noviembre de 1947. Winston Churchill, when asked to define democracy, replied: \"Democracy means to know that when somebody rings at the door of your house in the morning it is the milkman.\" (Rolandis, Debate en el Consejo de Europa, pg. 219, 29 de abril de 1982)."}
{"url": "https://en.m.wikisource.org/wiki/1911_Encyclop%C3%A6dia_Britannica/Sparta", "text": "​SPARTA (Gr. Σπάρτη or Λακεδαίμον), an ancient city in Greece, the capital of Laconia and the most powerful state of the Peloponnese. The city lay at the northern end of the central Laconian plain, on the right bank of the river Eurotas, a little south of the point where it is joined by its largest tributary, the Oenus (mod. Kelefína). The site is admirably fitted by nature to guard the only routes by which an army can penetrate Laconia from the land side, the Oenus and Eurotas valleys leading from Arcadia, its northern neighbour, and the Langáda Pass over Mt Taygetus connecting Laconia and Messenia. At the same time its distance from the sea—Sparta is 27 m. from its seaport, Gythium—made it invulnerable to a maritime attack. I.—History Prehistoric Period.—Tradition relates that Sparta was founded by Lacedaemon, son of Zeus and Taygete, who called the city after the name of his wife, the daughter of Eurotas. But Amyclae and Therapne (Therapnae) seem to have been in early times of greater importance than Sparta, the former a Minyan foundation a few miles to the south of Sparta, the latter probably the Achaean capital of Laconia and the seat of Menelaus, Agamemnon's younger brother. Eighty years after the Trojan War, according to the traditional chronology, the Dorian migration took place. A band of Dorians (q.v.) united with a body of Aetolians to cross the Corinthian Dorian Invasion. Gulf and invade the Peloponnese from the north-west. The Aetolians settled in Elis, the Dorians pushed up to the headwaters of the Alpheus, where they divided into two forces, one of which under Cresphontes invaded and later subdued Messenia, while the other, led by Aristodemus or, according to another version, by his twin sons Eurysthenes and Procles, made its way down the Eurotas valley and gained Sparta, which became the Dorian capital ​of Laconia. In reality this Dorian immigration probably consisted of a series of inroads and settlements rather than a single great expedition, as depicted by legend, and was aided by the Minyan elements in the population, owing to their dislike of the Achaean yoke. The newly founded state did not at once become powerful: it was weakened by internal dissension and lacked the stability of a united and well-organized community. The turning-point is marked by the legislation of Lycurgus (q.v.), who effected the unification of the state and instituted that training which was its distinguishing feature and the source of its greatness. Nowhere else in the Greek world was the pleasure of the individual so thoroughly subordinated to the interest of the state. The whole education of the Spartan was designed to make him an efficient soldier. Obedience, endurance, military success—these were the aims constantly kept in view, and beside these all other ends took a secondary place. Never, perhaps, in the world's history has a state so clearly set a definite ideal before itself or striven so consistently to reach it. But it was solely in this consistency and steadfastness that the greatness of Sparta lay. Her ideal was a narrow and unworthy one, and was pursued with a calculating selfishness and a total disregard for the rights of others, which robbed it of the moral worth it might otherwise have possessed. Nevertheless, it is not probable that without the training introduced by Lycurgus the Spartans would have been successful in securing their supremacy in Laconia, much less in the Peloponnese, for they formed a small immigrant band face to face with a large and powerful Achaean and autochthonous population. The Expansion of Sparta.—We cannot trace in detail the process by which Sparta subjugated the whole of Laconia, but apparently the first step, taken in the reign of Archelaus and Charillus, was to secure the upper Eurotas valley, conquering the border territory of Aegys. Archelaus' son Teleclus is said to have taken Amyclae, Pharis and Geronthrae, thus mastering the central Laconian plain and the eastern plateau which lies between the Eurotas and Mt Parnon: his son, Alcamenes, by the subjugation of Helos brought the lower Eurotas plain under Spartan rule. About this time, probably, the Argives, whose territory included the whole east coast of the Peloponnese and the island of Cythera (Herod, i. 82), were driven back, and the whole of Laconia was thus incorporated in the Spartan state. It was not long before a further extension took place. Under Alcamenes and Theopompus a war broke out between the Spartans and the Messenians, their Messenian Wars. neighbours on the west, which, after a struggle lasting for twenty years, ended in the capture of the stronghold of Ithome and the subjection of the Messenians, who were forced to pay half the produce of the soil as tribute to their Spartan overlords. An attempt to throw off the yoke resulted in a second war, conducted by the Messenian hero Aristomenes (q.v.); but Spartan tenacity broke down the resistance of the insurgents, and Messenia was made Spartan territory, just as Laconia had been, its inhabitants being reduced to the status of helots, save those who, as perioeci, inhabited the towns on the sea-coast and a few settlements inland. This extension of Sparta's territory was viewed with apprehension by her neighbours in the Peloponnese. Arcadia and Argos had vigorously aided the Messenians in their two struggles, and help was also sent by the Sicyonians, Pisatans and Triphylians: only the Corinthians appear to have supported the Spartans, doubtless on account of their jealousy of their powerful neighbours, the Argives. At the close of the second Messenian War, i.e. by the wyear 631 at latest, no power could hope to cope with that of Sparta save Arcadia and Argos. Early in the 6th century the Spartan kings Leon and Agasicles made a vigorous attack on Tegea, the most powerful of the Arcadian cities, but it was not until the reign of Anaxandridas and Ariston, about the middle of the century, that the attack was successful and Tegea was forced to acknowledge Spartan overlordship, though retaining its independence. The final struggle for Peloponnesian supremacy was with Argos, which had at an early period been the most powerful state of the peninsula, and even now, though its territory had been curtailed, was a serious rival of Sparta. But Argos was now no longer at the height of its Argive Wars. power: its league had begun to break up early in the century, and it could not in the impending struggle count on the assistance of its old allies, Arcadia and Messenia, since the latter had been crushed and robbed of its independence and the former had acknowledged Spartan supremacy. A victory won about 546 B.C., when the Lydian Empire fell before Cyrus of Persia, made the Spartans masters of the Cynuria, the borderland between Laconia and Argolis, for which there had been an age-long struggle. The final blow was struck by King Cleomenes I. (q.v.), who maimed for many years to come the Argive power and left Sparta without a rival in the Peloponnese. In fact, by the middle of the 6th century, and increasingly down to the period of the Persian Wars, Sparta had come to be acknowledged as the leading state of Hellas and the champion of Hellenism. Croesus of Lydia had formed an alliance with her. Scythian envoys sought her aid to stem the invasion of Darius; to her the Greeks of Asia Minor appealed to withstand the Persian advance and to aid the Ionian revolt; Plataea asked for her protection; Megara acknowledged her supremacy; and at the time of the Persian invasion under Xerxes no state questioned her right to lead the Greek forces on land and sea. Of such a position Sparta proved herself wholly unworthy. As an ally she was ineffective, nor could she ever rid herself of her narrowly Peloponnesian outlook sufficiently to throw herself heartily into the affairs of the greater Hellas that lay beyond the isthmus and across the sea. She was not a colonizing state, though the inhabitants of Tarentum, in southern Italy, and of Lyttus, in Crete, claimed her as their mother-city. Moreover, she had no share in the expansion of Greek commerce and Greek culture; and, though she bore the reputation of hating tyrants and putting them down where possible, there can be little doubt that this was done in the interests of oligarchy rather than of liberty. Her military greatness and that of the states under her hegemony formed her sole claim to lead the Greek race: that she should truly represent it was impossible. Constitution.—Of the internal development of Sparta down to this time but little is recorded. This want of information was attributed by most of the Greeks to the stability of the Spartan constitution, which had lasted unchanged from the days of Lycurgus. But it is, in fact, due also to the absence of an historical literature at Sparta, to the small part played by written laws, which were, according to tradition, expressly prohibited by an ordinance of Lycurgus, and to the secrecy which always characterizes an oligarchical rule. At the head of the state stood two hereditary kings, of the Agiad and Eurypontid families, equal in authority, so that one could not act against the veto of his colleague, though the Agiad king received greater honour in virtue of the seniority of his family (Herod, vi. 51). Kingship. This dual kingship, a phenomenon unique in Greek history, was explained in Sparta by the tradition that on Aristodemus's death he had been succeeded by his twin sons, and that this joint rule had been perpetuated. Modern scholars have advanced various theories to account for the anomaly. Some suppose that it must be explained as an attempt to avoid absolutism, and is paralleled by the analogous instance of the consuls at Rome. Others think that it points to a compromise arrived at to end the struggle between two families or communities, or that the two royal houses represent respectively the Spartan conquerors and their Achaean predecessors: those who hold this last view appeal to the words attributed by Herodotus (v. 72) to Cleomenes I.: “I am no Dorian, but an Achaean.” The duties of the kings were mainly religious, judicial and military. They were the chief priests of the state, and had to perform certain sacrifices and to maintain communication with the Delphian sanctuary, which always exercised great authority in Spartan politics. Their judicial functions had at the time when Herodotus wrote (about 430 B.C.) been restricted to cases dealing with heiresses, adoptions and the public roads: civil cases were decided by the ephors, ​criminal jurisdiction had passed to the council of elders and the ephors. It was in the military sphere that the powers of the kings were most unrestricted. Aristotle describes the kingship at Sparta as “a kind of unlimited and perpetual generalship” (Pol. iii. 1285a), while Isocrates refers to the Spartans as “subject to an oligarchy at home, to a kingship on campaign” (iii. 24). Here also, however, the royal prerogatives were curtailed in course of time: from the period of the Persian wars the king lost the right of declaring war on whom he pleased, he was accompanied to the field by two ephors, and he was supplanted also by the ephors in the control of foreign policy. More and more, as time went on, the kings became mere figure-heads, except in their capacity as generals, and the real power was transferred to the ephors and to the gerousia (q.v.). The reason for this change lay partly in the fact that the ephors, chosen by popular election from the whole body of citizens, represented a democratic element in the constitution without violating those oligarchical methods which seemed necessary for its satisfactory administration; partly in the weakness of the kingship, the dual character of which inevitably gave rise to jealousy and discord between the two holders of the office, often resulting in a practical deadlock; partly in the loss of prestige suffered by the kingship, especially during the 5th century, owing to these quarrels, to the frequency with which kings ascended the throne as minors and a regency was necessary, and to the many cases in which a king was, rightly or wrongly, suspected of having accepted bribes from the enemies of the state and was condemned and banished. In the powers exercised by the assembly of the citizens or apella (q.v.) we cannot trace any development, owing to the scantiness of our sources. The Spartan was essentially a soldier, trained to obedience and endurance: he became a politician only if chosen as ephor for a single year or elected a life member of the council after his sixtieth year had brought freedom from military service. Shortly after birth the child was brought before the elders of the tribe, who decided whether it was to be reared: if Training of Citizens. defective or weakly, it was exposed in the so-called Apothetae (αἱ Ἀποθέται, from ἀπόθετος, hidden). Thus was secured, as far as could be, the maintenance of a high standard of physical efficiency, and thus from the earliest days of the Spartan the absolute claim of the state to his life and service was indicated and enforced. Till their seventh year boys were educated at home: from that time their training was undertaken by the state and supervised by the παιδονόμος, an official appointed for that purpose. This training consisted for the most part in physical exercises, such as dancing, gymnastics, ball-games, &c., with music and literature occupying a subordinate position. From the twentieth year began the Spartan's liability to military service and his membership of one of the ἀνδρεῖα or φιδίτια (dining messes or clubs), composed of about fifteen members each, to one of which every citizen must belong. At thirty began the full citizen rights and duties. For the exercise of these three conditions were requisite: Spartiate birth, the training prescribed by law, and participation in and contribution to one of the dining-clubs. Those who fulfilled these conditions were the ὁμοῖοι (peers), citizens in the fullest sense of the word, while those who failed were called ὑπομείονες (lesser men), and retained only the civil rights of citizenship. Spartiates were absolutely debarred by law from trade or manufacture, which consequently rested in the hands of the Social System.perioeci (q.v.), and were forbidden to possess either gold or silver, the currency consisting of bars of iron: but there can be no doubt that this prohibition was evaded in various ways. Wealth was, in theory at least, derived entirely from landed property, and consisted in the annual return made by the helots (q.v.) who cultivated the plots of ground allotted to the Spartiates. But this attempt to equalize property proved a failure: from early times there were marked differences of wealth within the state, and these became even more serious after the law of Epitadeus, passed at some time after the Peloponnesian War, removed the legal prohibition of the gift or bequest of land. Later we find the soil coming more and more into the possession of large landholders, and by the middle of the 3rd century B.C. nearly two-fifths of Laconia belonged to women. Hand in hand with this process went a serious diminution in the number of full citizens, who had numbered 8000 at the beginning of the 5th century, but had sunk by Aristotle's day to less than 1000, and had further decreased to 700 at the accession of Agis IV. in 244 B.C. The Spartans did what they could to remedy this by law: certain penalties were imposed upon those who remained unmarried or who married too late in life. But the decay was too deep-rooted to be eradicated by such means, and we shall see that at a late period in Sparta's history an attempt was made without success to deal with the evil by much more drastic measures. The 5th CenturyB.C.—The beginning of the 5th century saw Sparta at the height of her power, though her prestige must have suffered in the fruitless attempts made to impose upon Athens an oligarchical regime after the fall of the Peisistratid tyranny in 510. But after the Persian Wars the Spartan supremacy could no longer remain unchallenged. Sparta had despatched an army in 490 to aid Athens in repelling the armament sent against it by Darius under the command of Datis and Artaphernes: but it arrived after the battle of Marathon had been fought and the issue of the conflict decided. In the second campaign, conducted ten years later by Xerxes in person, Sparta took a more active share and assumed the command of the combined Greek forces by sea and land. Yet, in spite of the heroic defence of Thermopylae by the Spartan king Leonidas (q.v.), the glory of the decisive victory at Salamis fell in Persian Wars. great measure to the Athenians, and their patriotism, self-sacrifice and energy contrasted strongly with the hesitation of the Spartans and the selfish policy which they advocated of defending the Peloponnese only. By the battle of Plataea (479 B.C.), won by a Spartan general, and decided chiefly by the steadfastness of Spartan troops, the state partially recovered its prestige, but only so far as land operations were concerned: the victory of Mycale, won in the same year, was achieved by the united Greek fleet, and the capture of Sestos, which followed, was due to the Athenians, the Peloponnesians having returned home before the siege was begun. Sparta felt that an effort was necessary to recover her position, and Pausanias, the victor of Plataea, was sent out as admiral of the Greek fleet. But though he won considerable successes, his overbearing and despotic behaviour and the suspicion that he was intriguing with the Persian king alienated the sympathies of those under his command: he was recalled by the ephors, and his successor, Dorcis, was a weak man who allowed the transference of the hegemony from Sparta to Athens to take place without striking a blow (see Delian League). By the withdrawal of Sparta and her Peloponnesian allies from the fleet the perils and the glories of the Persian War were left to Athens, who, though at the outset merely the leading state in a confederacy of free allies, soon began to make herself the mistress of an empire. Sparta took no steps at first to prevent this. Her interests and those of Athens did not directly clash, for Athens included in her empire only the islands of the Aegean and the towns on its north and east coasts, which lay outside the Spartan political horizon: with the Peloponnese Athens did not meddle. Moreover, Sparta's attention was at this time fully occupied by troubles nearer home—the plots of Pausanias not only with the Persian king but with the Laconian helots; the revolt of Tegea (c. 473-71), rendered all the more formidable by the participation of Argos; the earthquake which in 464 devastated Sparta; and the rising of the Messenian helots, which immediately followed. But there was a growing estrangement from Athens, which ended at length in an open breach. The insulting dismissal of a large body of Athenian troops which War with Athens. had come, under Cimon, to aid the Spartans in the siege of the Messenian stronghold of Ithome, the consummation of the Attic democracy under Ephialtes and Pericles, the conclusion of an alliance between Athens ​and Argos, which also about this time became democratic, united with other causes to bring about a rupture between the Athenians and the Peloponnesian League. In this so-called first Peloponnesian War Sparta herself took but a small share beyond helping to inflict a defeat on the Athenians at Tanagra in 457 B.C. After this battle they concluded a truce, which gave the Athenians an opportunity of taking their revenge on the Boeotians at the battle of Oenophyta, of annexing to their empire Boeotia, Phocis and Locris, and of subjugating Aegina. In 449 the war was ended by a five years' truce, but after Athens had lost her mainland empire by the battle of Coronea and the revolt of Megara a thirty years' peace was concluded, probably in the winter 446-445 B.C. By this Athens was obliged to surrender Troezen, Achaea and the two Megarian ports, Nisaea and Pegae, but otherwise the status quo was maintained. A fresh struggle, the great Peloponnesian War (q.v.), broke out in 431 B.C. This may be to a certain extent regarded as a contest between Ionian and Dorian; it may with greater truth be called a struggle between the democratic and oligarchic Peloponnesian War. principles of government; but at bottom its cause was neither racial nor constitutional, but economic. The maritime supremacy of Athens was used for commercial purposes, and important members of the Peloponnesian confederacy, whose wealth depended largely on their commerce, notably Corinth, Megara, Sicyon and Epidaurus, were being slowly but relentlessly crushed. Materially Sparta must have remained almost unaffected, but she was forced to take action by the pressure of her allies and by the necessities imposed by her position as head of the league. She did not, however, prosecute the war with any marked vigour: her operations were almost confined to an annual inroad into Attica, and when in 425 a body of Spartiates was captured by the Athenians at Pylos she was ready, and even anxious, to terminate the war on any reasonable conditions. That the terms of the Peace of Nicias, which in 421 concluded the first phase of the war, were rather in favour of Sparta than of Athens was due almost entirely to the energy and insight of an individual Spartan, Brasidas (q.v.), and the disastrous attempt of Athens to regain its lost land-empire. The final success of Sparta and the capture of Athens in 405 were brought about partly by the treachery of Alcibiades, who induced the state to send Gylippus to conduct the defence of Syracuse, to fortify Decelea in northern Attica, and to adopt a vigorous policy of aiding Athenian allies to revolt. The lack of funds which would have proved fatal to Spartan naval warfare was remedied by the intervention of Persia, which supplied large subsidies, and Spartan good fortune culminated in the possession at this time of an admiral of boundless vigour and considerable military ability, Lysander, to whom much of Sparta's success is attributable. The 4th Century.—The fall of Athens left Sparta once again supreme in the Greek world and demonstrated clearly her total unfitness for rule. Everywhere democracy was replaced by a philo-Laconian oligarchy, usually consisting of ten men Spartan Empire. under a harmost or governor pledged to Spartan interests, and even in Laconia itself the narrow and selfish character of the Spartan rule led to a serious conspiracy. For a short time, indeed, under the energetic rule of Agesilaus, it seemed as if Sparta would pursue a Hellenic policy and carry on the war against Persia. But troubles soon broke out in Greece, Agesilaus was recalled from Asia Minor, and his schemes and successes were rendered fruitless. Further, the naval activity displayed by Sparta during the closing years of the Peloponnesian War abated when Persian subsidies were withdrawn, and the ambitious projects of Lysander led to his disgrace, which was followed by his death at Haliartus in 395. In the following year the Spartan navy under Peisander, Agesilaus' brother-in-law, was defeated off Cnidus by the Persian fleet under Conon and Pharnabazus, and for the future Sparta ceased to be a maritime power. In Greece itself meanwhile the opposition to Sparta was growing increasingly powerful, and, though at Coronea Agesilaus had slightly the better of the Boeotians and at Corinth the Spartans maintained their position, yet they felt it necessary to rid themselves of Persian hostility and if possible use the Persian power to strengthen their own position at home: they therefore concluded with Artaxerxes II. the humiliating Peace of Antalcidas (387 B.C.), by which they surrendered to the Great King the Greek cities of the Asia Minor coast and of Cyprus, and stipulated for the independence of all other Greek cities. This last clause led to a long and desultory war with Thebes, which refused to acknowledge the independence of the Boeotian towns under its hegemony: the Cadmeia, the citadel of Thebes, was treacherously seized by Phoebidas in 382 and held by the Spartans until 379. Still more momentous was the Spartan action in crushing the Olynthiac Confederation (see Olynthus), which might have been able to stay the growth of Macedonian power. In 371 a fresh peace congress was summoned at Sparta to ratify the Peace of Callias. Again the Thebans refused to renounce their Boeotian hegemony, and the Spartan attempt at coercion ended in the defeat of the Spartan army at the battle of Leuctra and the death of its leader, King Cleombrotus. The result of the battle was to transfer the Greek supremacy from Sparta to Thebes. In the course of three expeditions to the Peloponnese conducted by Epaminondas, the greatest soldier and statesman Decline of Sparta. Thebes ever produced, Sparta was weakened by the loss of Messenia, which was restored to an independent position with the newly built Messene as its capital, and by the foundation of Megalopolis as the capital of Arcadia. The invading army even made its way into Laconia and devastated the whole of its southern portion; but the courage and coolness of Agesilaus saved Sparta itself from attack. On Epaminondas' fourth expedition Sparta was again within an ace of capture, but once more the danger was averted just in time; and though at Mantinea (362 B.C.) the Thebans, together with the Arcadians, Messenians and Argives, gained a victory over the combined Mantinean, Athenian and Spartan forces, yet the death of Epaminondas in the battle more than counterbalanced the Theban victory and led to the speedy break-up of their supremacy. But Sparta had neither the men nor the money to recover her lost position, and the continued existence on her borders of an independent Messenia and Arcadia kept her in constant fear for her own safety. She did, indeed, join with Athens and Achaea in 353 to prevent Philip of Macedon passing Thermopylae and entering Phocis, Rise of Macedon. but beyond this she took no part in the struggle of Greece with the new power which had sprung up on her northern borders. No Spartiate fought on the field of Chaeronea. After the battle, however, she refused to submit voluntarily to Philip, and was forced to do so by the devastation of Laconia and the transference of certain border districts to the neighbouring states of Argos, Arcadia and Messenia. During the absence of Alexander the Great in the East Agis III. revolted, but the rising was crushed by Antipater, and a similar attempt to throw off the Macedonian yoke made by Archidamus IV. in the troublous period which succeeded Alexander's death was frustrated by Demetrius Poliorcetes in 294 B.C. Twenty-two years later the city was attacked by an immense force under Pyrrhus, but Spartan bravery had not died out and the formidable enemy was repulsed, even the women taking part in the defence of the city. About 244 an Aetolian army overran Laconia, working irreparable harm and carrying off, it is said, 50,000 captives. But the social evils within the state were even harder to combat than foes without. Avarice, luxury and the glaring inequality in the distribution of wealth, threatened to bring about the speedy fall of the state if no cure could be found. Agis IV. and Cleomenes III. (qq.v.) made an heroic and entirely disinterested attempt in the latter part of the 3rd century to improve the conditions by a redistribution of land, a widening of the citizen body, and a restoration of the old severe training and simple life. But the evil was too deep-seated to be remedied by these artificial means; Agis was assassinated, and the ​reforms of Cleomenes seem to have had no permanent effect. The reign of Cleomenes is marked also by a determined effort to cope with the rising power of the Achaean League (q.v.) and to recover for Sparta her long-lost supremacy in the Peloponnese, and even throughout Greece. The battle of Sellasia (222 B.C.), in which Cleomenes was defeated by the Achaeans and Antigonus Doson of Macedonia, and the death of the king, which occurred shortly afterwards in Egypt, put an end to these hopes. The same reign saw also an important constitutional change, the substitution of a board of patronomi for the ephors, whose power had become almost despotic, and the curtailment of the functions exercised by the gerousia; these measures were, however, cancelled by Antigonus. It was not long afterwards that the dual kingship ceased and Sparta fell under the sway of a series of cruel and rapacious tyrants—Lycurgus, Machanidas, who was killed by Philopoemen, and Nabis, who, if we may trust the accounts given by Polybius and Livy, was little better than a bandit chieftain, holding Sparta by means of extreme cruelty and oppression, and using mercenary troops to a large extent in his wars. The Intervention of Rome.—We must admit, however, that a vigorous struggle was maintained with the Achaean League and with Macedon until the Romans, after the conclusion of their war with Philip V., sent an army into Laconia under T. Quinctius Flamininus. Nabis was forced to capitulate, evacuating all his possessions outside Laconia, surrendering the Laconian seaports and his navy, and paying an indemnity of 500 talents (Livy xxxiv. 33-43). On the departure of the Romans he succeeded in recovering Gythium, in spite of an attempt to relieve it made by the Achaeans under Philopoemen, but in an encounter he suffered a crushing defeat at the hands of that general, who for thirty days ravaged Laconia unopposed. Nabis was assassinated in 192, and Sparta was forced by Philopoemen to enrol itself as a member of the Achaean LeagueAchaean League. (q.v.) under a phil-Achaean aristocracy. But this gave rise to chronic disorders and disputes, which led to armed intervention on the part of the Achaeans, who compelled the Spartans to submit to the overthrow of their city walls, the dismissal of their mercenary troops, the recall of all exiles, the abandonment of the old Lycurgan constitution and the adoption of the Achaean laws and institutions (188 B.C.). Again and again the relations between the Spartans and the Achaean League formed the occasion of discussions in the Roman senate or of the despatch of Roman embassies to Greece, but no decisive intervention took place until a fresh dispute about the position of Sparta in the league led to a decision of the Romans that Sparta, Corinth, Argos, Arcadian Orchomenus and Heraclea on Oeta should be severed from it. This resulted in an open breach between the league and Rome, and eventually, in 146 B.C., after the sack of Corinth, in the dissolution of the league and the annexation of Greece to the Roman province of Macedonia. For Sparta the long era of war and intestine struggle had ceased and one of peace and a revived prosperity took its place, as is witnessed by the numerous extant inscriptions belonging to this period. As an allied city it was exempt from direct taxation, though compelled on occasions to make “voluntary” presents to Roman generals. Political ambition was restricted to the tenure of the municipal magistracies, culminating in the offices of nomophylax, ephor and patronomus. Augustus showed marked favour to the city, Hadrian twice visited it during his journeys in the East and accepted the title of eponymous patronomus. The old warlike spirit found an outlet chiefly in the vigorous but peaceful contests held in the gymnasium, the ball-place, and the arena before the temple of Artemis Orthia: sometimes too it found a vent in actual campaigning, as when Spartans were enrolled for service against the Parthians by the emperors Lucius Verus, Septimius Severus and Caracalla. Laconia was subsequently overrun, like so much of the Roman Empire, by barbarian hordes. Medieval Sparta.—In A.D. 396 Alaric destroyed the city and at a later period Laconia was invaded and settled by Slavonic tribes, especially the Melings and Ezerits, who in turn had to give way before the advance of the Byzantine power, though preserving a partial independence in the mountainous regions. The Franks on their arrival in the Morea found a fortified city named Lacedaemonia occupying part of the site of ancient Sparta, and this continued to exist, though greatly depopulated, even after Guillaume de Villehardouin had in 1248-1249 founded the fortress and city of Misithra, or Mistra, on a spur of Taygetus some 3 m. north-west of Sparta. This passed shortly afterwards into the hands of the Byzantines, who retained it until the Turks under Mahommed II. captured it in 1460. In 1687 it came into the possession of the Venetians, from whom it was wrested in 1715 by the Turks. Thus for nearly six centuries it was Mistra and not Sparta which formed the centre and focus of Laconian history. The Modern City.—In 1834, after the War of Independence had resulted in the liberation of Greece, the modern town of Sparta was built on part of the ancient site from the designs of Baron Jochmus, and Mistra decayed until now it is in ruins and almost deserted. Sparta is the capital of the prefecture (νομός) of Lacedaemon and has a population, according to the census taken in 1907, of 4456: but with the exception of several silk factories there is but little industry, and the development of the city is hampered by the unhealthiness of its situation, its distance from the sea and the absence of railway communication with the rest of Greece. As a result of popular clamour, however, a survey for a railway was begun in 1907, an event of great importance for the prosperity of Sparta and of the whole Eurotas Plain. II.—Archaeology There is a well-known passage in Thucydides which runs thus: “Suppose the city of Sparta to be deserted, and nothing left but the temples and the ground-plan, distant ages would be very unwilling to believe that the power of the Lacedaemonians was at all equal to their fame. . . . Their city is not built continuously, and has no splendid temples or other edifices; it rather resembles a group of villages, like the ancient towns of Hellas, and would therefore make a poor show” (i. 10, trans. Jowett). And the first feeling of most travellers who visit modern Sparta is one of disappointment with the ancient remains: it is rather the loveliness and grandeur of the situation and the fascination of Mistra, with its grass-grown streets, its decaying houses, its ruined fortress and its beautiful Byzantine churches, that remain as a lasting and cherished memory. Until 1905 the chief ancient buildings at Sparta were the theatre, of which, however, little shows above ground except portions of the retaining walls; the so-called Tomb of Leonidas, a quadrangular building, perhaps a temple, constructed of immense blocks of stone and containing two chambers; the foundation of an ancient bridge over the Eurotas; the ruins of a circular structure; some remains of late Roman fortifications; several brick buildings and mosaic pavements. To these must be added the inscriptions, sculptures and other objects collected in the local museum, founded by Stamatakis in 1872 and enlarged in 1907, or built into the walls of houses or churches. Though excavations were carried on near Sparta, on the site of the Amyclaeum in 1890 by Tsountas, and in 1904 by Furtwängler, and at the shrine of Menelaus in Therapne by Ross in 1833 and 1841, and by Kastriotis in 1889 and 1900, yet no organized work was tried in Sparta itself save the partial excavation of the “round building” undertaken in 1892 and 1893 by the American School at Athens; the structure has been since found to be a semicircular retaining-wall of good Hellenic work, though partly restored in Roman times. In 1904 the British School at Athens began a thorough exploration of Laconia, and in the following year excavations were made at Thalamae, Geronthrae, and Angelona near Monemvasia, while several medieval fortresses were surveyed. In 1906 excavations began in Sparta itself with results of great value, which have been published in the British School Annual, vol. xii. sqq. ​A “small circus” described by Leake, but subsequently almost lost to view, proved to be a theatre-like building constructed soon after A.D. 200 round the altar and in front of the temple of Artemis Orthia. Here musical and gymnastic contests took place as well as the famous flogging-ordeal (diamastigosis). The temple, which can be dated to the 2nd century B.C. rests on the foundation of an older temple of the 6th century, and close beside it were found the scanty remains of a yet earlier temple, dating from the 9th or even the 10th century. The votive offerings in clay, amber, bronze, ivory and lead found in great profusion within the precinct range from the 9th to the 4th century B.C. and supply invaluable evidence for early Spartan art; they prove that Sparta reached her artistic zenith in the 7th century and that her decline had already begun in the 6th. In 1907 the sanctuary of Athena “of the Brazen House” (Χαλκίοικος) was located on the Acropolis immediately above the theatre, and though the actual temple is almost completely destroyed, fragments of the capitals show that it was Doric in style, and the site has produced the longest extant archaic inscription of Laconia, numerous bronze nails and plates and a considerable number of votive offerings, some of them of great interest. The Greek city-wall, built in successive stages from the 4th to the 2nd century, was traced for a great part of its circuit, which measured 48 stades or nearly 6 m. (Polyb. ix. 21). The late Roman wall enclosing the Acropolis, part of which probably dates from the years following the Gothic raid of A.D. 262, was also investigated. Besides the actual buildings discovered, a number of points were fixed which greatly facilitate the study of Spartan topography, based upon the description left us by Pausanias. Excavations carried on in 1910 showed that the town of the “Mycenean” period which lay on the left bank of the Eurotas a little to the south-east of Sparta was roughly triangular in shape, with its apex towards the north: its area is approximately equal to that of Sparta, but denudation and destruction have wrought havoc with its buildings and nothing is left save ruined foundations and broken potsherds."}
